running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp72/PPO_1
----------------------------------
| reward             | -0.0763   |
| reward_contact     | 0         |
| reward_ctrl        | -0.0242   |
| reward_motion      | -0.1      |
| reward_orientation | -0.00196  |
| reward_position    | -0.0763   |
| reward_rotation    | 1.45      |
| reward_torque      | -3.04e-05 |
| reward_velocity    | 1.25      |
| rollout/           |           |
|    ep_len_mean     | 138       |
|    ep_rew_mean     | -5.1      |
| time/              |           |
|    fps             | 369       |
|    iterations      | 1         |
|    time_elapsed    | 2         |
|    total_timesteps | 1024      |
----------------------------------
---------------------------------------
| reward                  | -0.224    |
| reward_contact          | -0.0167   |
| reward_ctrl             | -0.0458   |
| reward_motion           | -0.0708   |
| reward_orientation      | -0.00328  |
| reward_position         | -0.224    |
| reward_rotation         | 1.52      |
| reward_torque           | -4.88e-05 |
| reward_velocity         | 1.43      |
| rollout/                |           |
|    ep_len_mean          | 241       |
|    ep_rew_mean          | -68.2     |
| time/                   |           |
|    fps                  | 239       |
|    iterations           | 2         |
|    time_elapsed         | 8         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 1.6142322 |
|    clip_fraction        | 0.649     |
|    clip_range           | 0.4       |
|    entropy_loss         | -28.5     |
|    explained_variance   | -3.96     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0341    |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0423   |
|    std                  | 0.36      |
|    value_loss           | 0.0389    |
---------------------------------------
---------------------------------------
| reward                  | -0.351    |
| reward_contact          | -0.0149   |
| reward_ctrl             | -0.0472   |
| reward_motion           | -0.00793  |
| reward_orientation      | -0.00472  |
| reward_position         | -0.351    |
| reward_rotation         | 1.52      |
| reward_torque           | -6.12e-05 |
| reward_velocity         | 1.56      |
| rollout/                |           |
|    ep_len_mean          | 328       |
|    ep_rew_mean          | -115      |
| time/                   |           |
|    fps                  | 212       |
|    iterations           | 3         |
|    time_elapsed         | 14        |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.149842  |
|    clip_fraction        | 0.229     |
|    clip_range           | 0.4       |
|    entropy_loss         | -30.6     |
|    explained_variance   | 0.141     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00509   |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0907   |
|    std                  | 0.358     |
|    value_loss           | 3.26      |
---------------------------------------
-----------------------------------------
| reward                  | -0.377      |
| reward_contact          | -0.0411     |
| reward_ctrl             | -0.0484     |
| reward_motion           | -0.0171     |
| reward_orientation      | -0.00608    |
| reward_position         | -0.377      |
| reward_rotation         | 1.44        |
| reward_torque           | -6.99e-05   |
| reward_velocity         | 1.53        |
| rollout/                |             |
|    ep_len_mean          | 339         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 4           |
|    time_elapsed         | 20          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.106220715 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.426       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.08       |
|    std                  | 0.357       |
|    value_loss           | 2.75        |
-----------------------------------------
----------------------------------------
| reward                  | -0.258     |
| reward_contact          | -0.0274    |
| reward_ctrl             | -0.0995    |
| reward_motion           | -0.0448    |
| reward_orientation      | -0.00711   |
| reward_position         | -0.258     |
| reward_rotation         | 1.28       |
| reward_torque           | -9.41e-05  |
| reward_velocity         | 1.48       |
| rollout/                |            |
|    ep_len_mean          | 297        |
|    ep_rew_mean          | -83.2      |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 5          |
|    time_elapsed         | 25         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.60471404 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -31.9      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0696     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0826    |
|    std                  | 0.351      |
|    value_loss           | 0.447      |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -80.87
Saving new best model to rl/out_dir/models/exp72/best_model.zip
----------------------------------------
| reward                  | -0.238     |
| reward_contact          | -0.0365    |
| reward_ctrl             | -0.122     |
| reward_motion           | -0.0293    |
| reward_orientation      | -0.00751   |
| reward_position         | -0.238     |
| reward_rotation         | 1.22       |
| reward_torque           | -0.000109  |
| reward_velocity         | 1.51       |
| rollout/                |            |
|    ep_len_mean          | 312        |
|    ep_rew_mean          | -80.9      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 6          |
|    time_elapsed         | 29         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.40697506 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | -0.542     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.13      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.34       |
|    value_loss           | 0.116      |
----------------------------------------
----------------------------------------
| reward                  | -0.204     |
| reward_contact          | -0.0337    |
| reward_ctrl             | -0.142     |
| reward_motion           | 0.00476    |
| reward_orientation      | -0.00708   |
| reward_position         | -0.204     |
| reward_rotation         | 1.2        |
| reward_torque           | -0.000112  |
| reward_velocity         | 1.5        |
| rollout/                |            |
|    ep_len_mean          | 284        |
|    ep_rew_mean          | -66        |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 7          |
|    time_elapsed         | 34         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.27019837 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.3      |
|    explained_variance   | -7.8       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.229     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.33       |
|    value_loss           | 0.28       |
----------------------------------------
---------------------------------------
| reward                  | -0.188    |
| reward_contact          | -0.0289   |
| reward_ctrl             | -0.153    |
| reward_motion           | -0.0102   |
| reward_orientation      | -0.00756  |
| reward_position         | -0.188    |
| reward_rotation         | 1.18      |
| reward_torque           | -0.000115 |
| reward_velocity         | 1.44      |
| rollout/                |           |
|    ep_len_mean          | 270       |
|    ep_rew_mean          | -59.9     |
| time/                   |           |
|    fps                  | 211       |
|    iterations           | 8         |
|    time_elapsed         | 38        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.1454254 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.4       |
|    entropy_loss         | -36.1     |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0832   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.137    |
|    std                  | 0.326     |
|    value_loss           | 0.416     |
---------------------------------------
----------------------------------------
| reward                  | -0.184     |
| reward_contact          | -0.0367    |
| reward_ctrl             | -0.154     |
| reward_motion           | -0.0238    |
| reward_orientation      | -0.00776   |
| reward_position         | -0.184     |
| reward_rotation         | 1.2        |
| reward_torque           | -0.000122  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 257        |
|    ep_rew_mean          | -55        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 9          |
|    time_elapsed         | 43         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.15896106 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.4      |
|    explained_variance   | -1.03      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.193     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.321      |
|    value_loss           | 0.307      |
----------------------------------------
----------------------------------------
| reward                  | -0.213     |
| reward_contact          | -0.038     |
| reward_ctrl             | -0.155     |
| reward_motion           | -0.0219    |
| reward_orientation      | -0.00705   |
| reward_position         | -0.213     |
| reward_rotation         | 1.19       |
| reward_torque           | -0.000132  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 249        |
|    ep_rew_mean          | -64.4      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 10         |
|    time_elapsed         | 48         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.09662958 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.8      |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0775    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.319      |
|    value_loss           | 1.65       |
----------------------------------------
-----------------------------------------
| reward                  | -0.241      |
| reward_contact          | -0.0418     |
| reward_ctrl             | -0.154      |
| reward_motion           | -0.0175     |
| reward_orientation      | -0.00728    |
| reward_position         | -0.241      |
| reward_rotation         | 1.21        |
| reward_torque           | -0.000132   |
| reward_velocity         | 1.47        |
| rollout/                |             |
|    ep_len_mean          | 256         |
|    ep_rew_mean          | -69.3       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 11          |
|    time_elapsed         | 53          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.062842555 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -37         |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0741     |
|    std                  | 0.319       |
|    value_loss           | 6.69        |
-----------------------------------------
Num timesteps: 12000
Best mean reward: -80.87 - Last mean reward per episode: -65.67
Saving new best model to rl/out_dir/models/exp72/best_model.zip
-----------------------------------------
| reward                  | -0.219      |
| reward_contact          | -0.0358     |
| reward_ctrl             | -0.161      |
| reward_motion           | -0.0293     |
| reward_orientation      | -0.00742    |
| reward_position         | -0.219      |
| reward_rotation         | 1.22        |
| reward_torque           | -0.000139   |
| reward_velocity         | 1.41        |
| rollout/                |             |
|    ep_len_mean          | 240         |
|    ep_rew_mean          | -61.4       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 12          |
|    time_elapsed         | 59          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.053224392 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.9       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.86        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0672     |
|    std                  | 0.319       |
|    value_loss           | 5.4         |
-----------------------------------------
-----------------------------------------
| reward                  | -0.191      |
| reward_contact          | -0.0304     |
| reward_ctrl             | -0.162      |
| reward_motion           | -0.0403     |
| reward_orientation      | -0.00741    |
| reward_position         | -0.191      |
| reward_rotation         | 1.22        |
| reward_torque           | -0.000138   |
| reward_velocity         | 1.41        |
| rollout/                |             |
|    ep_len_mean          | 221         |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 13          |
|    time_elapsed         | 65          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.107824765 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.2       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.13       |
|    std                  | 0.317       |
|    value_loss           | 0.372       |
-----------------------------------------
----------------------------------------
| reward                  | -0.218     |
| reward_contact          | -0.0376    |
| reward_ctrl             | -0.163     |
| reward_motion           | -0.0351    |
| reward_orientation      | -0.00762   |
| reward_position         | -0.218     |
| reward_rotation         | 1.21       |
| reward_torque           | -0.000137  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | -64.4      |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 14         |
|    time_elapsed         | 70         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.15367284 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.7      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.313      |
|    value_loss           | 0.153      |
----------------------------------------
----------------------------------------
| reward                  | -0.224     |
| reward_contact          | -0.0398    |
| reward_ctrl             | -0.163     |
| reward_motion           | -0.0334    |
| reward_orientation      | -0.0078    |
| reward_position         | -0.224     |
| reward_rotation         | 1.17       |
| reward_torque           | -0.000139  |
| reward_velocity         | 1.45       |
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | -65        |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 15         |
|    time_elapsed         | 76         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.07273345 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.9      |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.313      |
|    value_loss           | 5.06       |
----------------------------------------
-----------------------------------------
| reward                  | -0.215      |
| reward_contact          | -0.037      |
| reward_ctrl             | -0.161      |
| reward_motion           | -0.0381     |
| reward_orientation      | -0.00779    |
| reward_position         | -0.215      |
| reward_rotation         | 1.17        |
| reward_torque           | -0.000143   |
| reward_velocity         | 1.44        |
| rollout/                |             |
|    ep_len_mean          | 225         |
|    ep_rew_mean          | -62.1       |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 16          |
|    time_elapsed         | 82          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.049145654 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.5       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.61        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0712     |
|    std                  | 0.312       |
|    value_loss           | 6.02        |
-----------------------------------------
----------------------------------------
| reward                  | -0.228     |
| reward_contact          | -0.0396    |
| reward_ctrl             | -0.161     |
| reward_motion           | -0.0333    |
| reward_orientation      | -0.00773   |
| reward_position         | -0.228     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.44       |
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | -64        |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 17         |
|    time_elapsed         | 87         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.15272915 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.1      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.196     |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.309      |
|    value_loss           | 0.258      |
----------------------------------------
Num timesteps: 18000
Best mean reward: -65.67 - Last mean reward per episode: -64.05
Saving new best model to rl/out_dir/models/exp72/best_model.zip
----------------------------------------
| reward                  | -0.254     |
| reward_contact          | -0.0439    |
| reward_ctrl             | -0.161     |
| reward_motion           | -0.0299    |
| reward_orientation      | -0.00789   |
| reward_position         | -0.254     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000145  |
| reward_velocity         | 1.46       |
| rollout/                |            |
|    ep_len_mean          | 242        |
|    ep_rew_mean          | -73.1      |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 18         |
|    time_elapsed         | 91         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.04906241 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.4        |
|    entropy_loss         | -38        |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.29       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0717    |
|    std                  | 0.308      |
|    value_loss           | 6.94       |
----------------------------------------
----------------------------------------
| reward                  | -0.245     |
| reward_contact          | -0.0499    |
| reward_ctrl             | -0.161     |
| reward_motion           | -0.0325    |
| reward_orientation      | -0.00775   |
| reward_position         | -0.245     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.46       |
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | -70.5      |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 19         |
|    time_elapsed         | 97         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.08275504 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.4      |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0736     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0896    |
|    std                  | 0.308      |
|    value_loss           | 7.27       |
----------------------------------------
----------------------------------------
| reward                  | -0.247     |
| reward_contact          | -0.0622    |
| reward_ctrl             | -0.16      |
| reward_motion           | -0.0295    |
| reward_orientation      | -0.0079    |
| reward_position         | -0.247     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.48       |
| rollout/                |            |
|    ep_len_mean          | 247        |
|    ep_rew_mean          | -71        |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 20         |
|    time_elapsed         | 103        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.18666425 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.4      |
|    explained_variance   | 0.393      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.207     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.303      |
|    value_loss           | 0.134      |
----------------------------------------
----------------------------------------
| reward                  | -0.247     |
| reward_contact          | -0.0626    |
| reward_ctrl             | -0.159     |
| reward_motion           | -0.0288    |
| reward_orientation      | -0.00789   |
| reward_position         | -0.247     |
| reward_rotation         | 1.19       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.48       |
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | -71.6      |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 21         |
|    time_elapsed         | 109        |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.12674856 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.202     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.301      |
|    value_loss           | 0.381      |
----------------------------------------
---------------------------------------
| reward                  | -0.24     |
| reward_contact          | -0.0618   |
| reward_ctrl             | -0.162    |
| reward_motion           | -0.0249   |
| reward_orientation      | -0.00781  |
| reward_position         | -0.24     |
| reward_rotation         | 1.19      |
| reward_torque           | -0.000148 |
| reward_velocity         | 1.47      |
| rollout/                |           |
|    ep_len_mean          | 253       |
|    ep_rew_mean          | -68.3     |
| time/                   |           |
|    fps                  | 195       |
|    iterations           | 22        |
|    time_elapsed         | 115       |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0901089 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39       |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.107    |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.128    |
|    std                  | 0.299     |
|    value_loss           | 1.15      |
---------------------------------------
----------------------------------------
| reward                  | -0.242     |
| reward_contact          | -0.0647    |
| reward_ctrl             | -0.164     |
| reward_motion           | -0.0205    |
| reward_orientation      | -0.00808   |
| reward_position         | -0.242     |
| reward_rotation         | 1.2        |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.49       |
| rollout/                |            |
|    ep_len_mean          | 261        |
|    ep_rew_mean          | -70.2      |
| time/                   |            |
|    fps                  | 195        |
|    iterations           | 23         |
|    time_elapsed         | 120        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.09155572 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.1       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.298      |
|    value_loss           | 0.893      |
----------------------------------------
Num timesteps: 24000
Best mean reward: -64.05 - Last mean reward per episode: -65.99
----------------------------------------
| reward                  | -0.231     |
| reward_contact          | -0.0607    |
| reward_ctrl             | -0.164     |
| reward_motion           | -0.0255    |
| reward_orientation      | -0.00817   |
| reward_position         | -0.231     |
| reward_rotation         | 1.2        |
| reward_torque           | -0.000149  |
| reward_velocity         | 1.47       |
| rollout/                |            |
|    ep_len_mean          | 250        |
|    ep_rew_mean          | -66        |
| time/                   |            |
|    fps                  | 195        |
|    iterations           | 24         |
|    time_elapsed         | 125        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.06733551 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0954    |
|    std                  | 0.298      |
|    value_loss           | 2.21       |
----------------------------------------
----------------------------------------
| reward                  | -0.245     |
| reward_contact          | -0.06      |
| reward_ctrl             | -0.169     |
| reward_motion           | -0.0158    |
| reward_orientation      | -0.0084    |
| reward_position         | -0.245     |
| reward_rotation         | 1.19       |
| reward_torque           | -0.000149  |
| reward_velocity         | 1.46       |
| rollout/                |            |
|    ep_len_mean          | 253        |
|    ep_rew_mean          | -70.1      |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 25         |
|    time_elapsed         | 130        |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.10991797 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40        |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.163     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.297      |
|    value_loss           | 0.694      |
----------------------------------------
-----------------------------------------
| reward                  | -0.246      |
| reward_contact          | -0.06       |
| reward_ctrl             | -0.179      |
| reward_motion           | -0.0158     |
| reward_orientation      | -0.00859    |
| reward_position         | -0.246      |
| reward_rotation         | 1.18        |
| reward_torque           | -0.000151   |
| reward_velocity         | 1.47        |
| rollout/                |             |
|    ep_len_mean          | 252         |
|    ep_rew_mean          | -70.1       |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 26          |
|    time_elapsed         | 135         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.069555156 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.1       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.75        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0862     |
|    std                  | 0.297       |
|    value_loss           | 7.94        |
-----------------------------------------
----------------------------------------
| reward                  | -0.232     |
| reward_contact          | -0.0611    |
| reward_ctrl             | -0.185     |
| reward_motion           | -0.0172    |
| reward_orientation      | -0.00865   |
| reward_position         | -0.232     |
| reward_rotation         | 1.17       |
| reward_torque           | -0.000155  |
| reward_velocity         | 1.44       |
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | -66.9      |
| time/                   |            |
|    fps                  | 195        |
|    iterations           | 27         |
|    time_elapsed         | 141        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.11412039 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.1      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.103     |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.295      |
|    value_loss           | 2.14       |
----------------------------------------
-----------------------------------------
| reward                  | -0.229      |
| reward_contact          | -0.0632     |
| reward_ctrl             | -0.184      |
| reward_motion           | -0.034      |
| reward_orientation      | -0.00828    |
| reward_position         | -0.229      |
| reward_rotation         | 1.2         |
| reward_torque           | -0.000154   |
| reward_velocity         | 1.42        |
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | -63.4       |
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 28          |
|    time_elapsed         | 147         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.057972036 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.892       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0845     |
|    std                  | 0.295       |
|    value_loss           | 6.47        |
-----------------------------------------
----------------------------------------
| reward                  | -0.242     |
| reward_contact          | -0.0781    |
| reward_ctrl             | -0.181     |
| reward_motion           | -0.0315    |
| reward_orientation      | -0.00847   |
| reward_position         | -0.242     |
| reward_rotation         | 1.21       |
| reward_torque           | -0.000154  |
| reward_velocity         | 1.44       |
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | -67.6      |
| time/                   |            |
|    fps                  | 194        |
|    iterations           | 29         |
|    time_elapsed         | 152        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.10638805 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0507    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.293      |
|    value_loss           | 0.895      |
----------------------------------------
Num timesteps: 30000
Best mean reward: -64.05 - Last mean reward per episode: -67.54
-----------------------------------------
| reward                  | -0.24       |
| reward_contact          | -0.0781     |
| reward_ctrl             | -0.182      |
| reward_motion           | -0.0315     |
| reward_orientation      | -0.00836    |
| reward_position         | -0.24       |
| reward_rotation         | 1.22        |
| reward_torque           | -0.000155   |
| reward_velocity         | 1.44        |
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | -67.5       |
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 30          |
|    time_elapsed         | 158         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.049807265 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.4       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.554       |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0714     |
|    std                  | 0.293       |
|    value_loss           | 8.54        |
-----------------------------------------
---------------------------------------
| reward                  | -0.202    |
| reward_contact          | -0.0686   |
| reward_ctrl             | -0.182    |
| reward_motion           | -0.041    |
| reward_orientation      | -0.00857  |
| reward_position         | -0.202    |
| reward_rotation         | 1.18      |
| reward_torque           | -0.00015  |
| reward_velocity         | 1.43      |
| rollout/                |           |
|    ep_len_mean          | 207       |
|    ep_rew_mean          | -55.8     |
| time/                   |           |
|    fps                  | 192       |
|    iterations           | 31        |
|    time_elapsed         | 164       |
|    total_timesteps      | 31744     |
| train/                  |           |
|    approx_kl            | 0.1453565 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -40.4     |
|    explained_variance   | 0.905     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.222    |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.29      |
|    value_loss           | 0.294     |
---------------------------------------
---------------------------------------
| reward                  | -0.199    |
| reward_contact          | -0.0686   |
| reward_ctrl             | -0.185    |
| reward_motion           | -0.041    |
| reward_orientation      | -0.00886  |
| reward_position         | -0.199    |
| reward_rotation         | 1.15      |
| reward_torque           | -0.000145 |
| reward_velocity         | 1.43      |
| rollout/                |           |
|    ep_len_mean          | 205       |
|    ep_rew_mean          | -55.3     |
| time/                   |           |
|    fps                  | 192       |
|    iterations           | 32        |
|    time_elapsed         | 170       |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.0988114 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.2     |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00609  |
|    n_updates            | 620       |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.289     |
|    value_loss           | 0.848     |
---------------------------------------
---------------------------------------
| reward                  | -0.199    |
| reward_contact          | -0.0685   |
| reward_ctrl             | -0.184    |
| reward_motion           | -0.041    |
| reward_orientation      | -0.00879  |
| reward_position         | -0.199    |
| reward_rotation         | 1.14      |
| reward_torque           | -0.000144 |
| reward_velocity         | 1.44      |
| rollout/                |           |
|    ep_len_mean          | 214       |
|    ep_rew_mean          | -60.5     |
| time/                   |           |
|    fps                  | 191       |
|    iterations           | 33        |
|    time_elapsed         | 176       |
|    total_timesteps      | 33792     |
| train/                  |           |
|    approx_kl            | 0.1539258 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.8     |
|    explained_variance   | 0.263     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.18     |
|    n_updates            | 640       |
|    policy_gradient_loss | -0.146    |
|    std                  | 0.286     |
|    value_loss           | 0.338     |
---------------------------------------
----------------------------------------
| reward                  | -0.177     |
| reward_contact          | -0.0672    |
| reward_ctrl             | -0.19      |
| reward_motion           | -0.0503    |
| reward_orientation      | -0.00843   |
| reward_position         | -0.177     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000143  |
| reward_velocity         | 1.41       |
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | -48.4      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 34         |
|    time_elapsed         | 182        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.09601023 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.1      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.271      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.285      |
|    value_loss           | 3.54       |
----------------------------------------
----------------------------------------
| reward                  | -0.221     |
| reward_contact          | -0.0702    |
| reward_ctrl             | -0.191     |
| reward_motion           | -0.0463    |
| reward_orientation      | -0.00861   |
| reward_position         | -0.221     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000144  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 203        |
|    ep_rew_mean          | -63        |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 35         |
|    time_elapsed         | 188        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.15555988 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.0421     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.207     |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.282      |
|    value_loss           | 0.393      |
----------------------------------------
Num timesteps: 36000
Best mean reward: -64.05 - Last mean reward per episode: -63.00
Saving new best model to rl/out_dir/models/exp72/best_model.zip
-----------------------------------------
| reward                  | -0.24       |
| reward_contact          | -0.0731     |
| reward_ctrl             | -0.193      |
| reward_motion           | -0.0414     |
| reward_orientation      | -0.00856    |
| reward_position         | -0.24       |
| reward_rotation         | 1.18        |
| reward_torque           | -0.000142   |
| reward_velocity         | 1.43        |
| rollout/                |             |
|    ep_len_mean          | 212         |
|    ep_rew_mean          | -71.2       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 36          |
|    time_elapsed         | 194         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.038778625 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0571     |
|    std                  | 0.281       |
|    value_loss           | 24.9        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.253      |
| reward_contact          | -0.0763     |
| reward_ctrl             | -0.193      |
| reward_motion           | -0.035      |
| reward_orientation      | -0.00863    |
| reward_position         | -0.253      |
| reward_rotation         | 1.17        |
| reward_torque           | -0.00014    |
| reward_velocity         | 1.44        |
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | -74.5       |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 37          |
|    time_elapsed         | 200         |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.054506384 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42         |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.98        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0796     |
|    std                  | 0.281       |
|    value_loss           | 10          |
-----------------------------------------
----------------------------------------
| reward                  | -0.246     |
| reward_contact          | -0.0746    |
| reward_ctrl             | -0.195     |
| reward_motion           | -0.0396    |
| reward_orientation      | -0.00852   |
| reward_position         | -0.246     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000136  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | -74.2      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 38         |
|    time_elapsed         | 205        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.03930262 |
|    clip_fraction        | 0.0736     |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.04       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0614    |
|    std                  | 0.281      |
|    value_loss           | 14.2       |
----------------------------------------
----------------------------------------
| reward                  | -0.248     |
| reward_contact          | -0.0768    |
| reward_ctrl             | -0.194     |
| reward_motion           | -0.0364    |
| reward_orientation      | -0.0087    |
| reward_position         | -0.248     |
| reward_rotation         | 1.16       |
| reward_torque           | -0.000136  |
| reward_velocity         | 1.45       |
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | -75.2      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 39         |
|    time_elapsed         | 210        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.06202689 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.31       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0982    |
|    std                  | 0.281      |
|    value_loss           | 6.48       |
----------------------------------------
-----------------------------------------
| reward                  | -0.227      |
| reward_contact          | -0.0731     |
| reward_ctrl             | -0.196      |
| reward_motion           | -0.0396     |
| reward_orientation      | -0.0086     |
| reward_position         | -0.227      |
| reward_rotation         | 1.15        |
| reward_torque           | -0.000134   |
| reward_velocity         | 1.44        |
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 40          |
|    time_elapsed         | 214         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.089564666 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.9       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.122      |
|    std                  | 0.28        |
|    value_loss           | 1.74        |
-----------------------------------------
----------------------------------------
| reward                  | -0.253     |
| reward_contact          | -0.0763    |
| reward_ctrl             | -0.196     |
| reward_motion           | -0.0359    |
| reward_orientation      | -0.00885   |
| reward_position         | -0.253     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000132  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | -76        |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 41         |
|    time_elapsed         | 220        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.07365474 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0556     |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0971    |
|    std                  | 0.279      |
|    value_loss           | 2.55       |
----------------------------------------
Num timesteps: 42000
Best mean reward: -63.00 - Last mean reward per episode: -75.98
-----------------------------------------
| reward                  | -0.251      |
| reward_contact          | -0.0638     |
| reward_ctrl             | -0.196      |
| reward_motion           | -0.0455     |
| reward_orientation      | -0.00851    |
| reward_position         | -0.251      |
| reward_rotation         | 1.16        |
| reward_torque           | -0.000132   |
| reward_velocity         | 1.41        |
| rollout/                |             |
|    ep_len_mean          | 210         |
|    ep_rew_mean          | -77.4       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 42          |
|    time_elapsed         | 226         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.045256577 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.1        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0658     |
|    std                  | 0.279       |
|    value_loss           | 16.5        |
-----------------------------------------
----------------------------------------
| reward                  | -0.24      |
| reward_contact          | -0.06      |
| reward_ctrl             | -0.192     |
| reward_motion           | -0.051     |
| reward_orientation      | -0.00789   |
| reward_position         | -0.24      |
| reward_rotation         | 1.16       |
| reward_torque           | -0.000131  |
| reward_velocity         | 1.41       |
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | -73.9      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 43         |
|    time_elapsed         | 232        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.10015687 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.525      |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.277      |
|    value_loss           | 2.69       |
----------------------------------------
----------------------------------------
| reward                  | -0.225     |
| reward_contact          | -0.0582    |
| reward_ctrl             | -0.195     |
| reward_motion           | -0.0572    |
| reward_orientation      | -0.00845   |
| reward_position         | -0.225     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000129  |
| reward_velocity         | 1.41       |
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | -69.3      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 44         |
|    time_elapsed         | 238        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.15808178 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.2       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.274      |
|    value_loss           | 0.315      |
----------------------------------------
----------------------------------------
| reward                  | -0.203     |
| reward_contact          | -0.0542    |
| reward_ctrl             | -0.195     |
| reward_motion           | -0.0706    |
| reward_orientation      | -0.00839   |
| reward_position         | -0.203     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000133  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | -60.2      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 45         |
|    time_elapsed         | 243        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.17367797 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.226     |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.271      |
|    value_loss           | 0.29       |
----------------------------------------
----------------------------------------
| reward                  | -0.189     |
| reward_contact          | -0.0333    |
| reward_ctrl             | -0.192     |
| reward_motion           | -0.0732    |
| reward_orientation      | -0.00786   |
| reward_position         | -0.189     |
| reward_rotation         | 1.12       |
| reward_torque           | -0.000137  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | -55.6      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 46         |
|    time_elapsed         | 248        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.12303675 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.17      |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.269      |
|    value_loss           | 0.443      |
----------------------------------------
Num timesteps: 48000
Best mean reward: -63.00 - Last mean reward per episode: -54.42
Saving new best model to rl/out_dir/models/exp72/best_model.zip
----------------------------------------
| reward                  | -0.189     |
| reward_contact          | -0.0417    |
| reward_ctrl             | -0.194     |
| reward_motion           | -0.0732    |
| reward_orientation      | -0.00757   |
| reward_position         | -0.189     |
| reward_rotation         | 1.13       |
| reward_torque           | -0.00013   |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | -54.4      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 47         |
|    time_elapsed         | 252        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.12937152 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.13      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.268      |
|    value_loss           | 0.495      |
----------------------------------------
----------------------------------------
| reward                  | -0.188     |
| reward_contact          | -0.0417    |
| reward_ctrl             | -0.199     |
| reward_motion           | -0.0732    |
| reward_orientation      | -0.00734   |
| reward_position         | -0.188     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000135  |
| reward_velocity         | 1.4        |
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | -54.3      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 48         |
|    time_elapsed         | 257        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.13992336 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.219     |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.266      |
|    value_loss           | 0.363      |
----------------------------------------
----------------------------------------
| reward                  | -0.192     |
| reward_contact          | -0.0445    |
| reward_ctrl             | -0.194     |
| reward_motion           | -0.068     |
| reward_orientation      | -0.00682   |
| reward_position         | -0.192     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000138  |
| reward_velocity         | 1.41       |
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | -55.7      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 49         |
|    time_elapsed         | 261        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.12182296 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.222     |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.264      |
|    value_loss           | 0.337      |
----------------------------------------
----------------------------------------
| reward                  | -0.18      |
| reward_contact          | -0.0491    |
| reward_ctrl             | -0.196     |
| reward_motion           | -0.068     |
| reward_orientation      | -0.00752   |
| reward_position         | -0.18      |
| reward_rotation         | 1.17       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | -51.1      |
| time/                   |            |
|    fps                  | 192        |
|    iterations           | 50         |
|    time_elapsed         | 266        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.07509392 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.7      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0689    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.263      |
|    value_loss           | 1.16       |
----------------------------------------
----------------------------------------
| reward                  | -0.178     |
| reward_contact          | -0.0491    |
| reward_ctrl             | -0.191     |
| reward_motion           | -0.068     |
| reward_orientation      | -0.00743   |
| reward_position         | -0.178     |
| reward_rotation         | 1.17       |
| reward_torque           | -0.000143  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | -51        |
| time/                   |            |
|    fps                  | 192        |
|    iterations           | 51         |
|    time_elapsed         | 270        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.16975318 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.4        |
|    entropy_loss         | -45        |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.183     |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.262      |
|    value_loss           | 0.274      |
----------------------------------------
----------------------------------------
| reward                  | -0.134     |
| reward_contact          | -0.0465    |
| reward_ctrl             | -0.19      |
| reward_motion           | -0.0833    |
| reward_orientation      | -0.007     |
| reward_position         | -0.134     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 152        |
|    ep_rew_mean          | -33.3      |
| time/                   |            |
|    fps                  | 192        |
|    iterations           | 52         |
|    time_elapsed         | 276        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07845594 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0964    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.261      |
|    value_loss           | 2.15       |
----------------------------------------
Num timesteps: 54000
Best mean reward: -54.42 - Last mean reward per episode: -32.44
Saving new best model to rl/out_dir/models/exp72/best_model.zip
-----------------------------------------
| reward                  | -0.128      |
| reward_contact          | -0.0481     |
| reward_ctrl             | -0.188      |
| reward_motion           | -0.078      |
| reward_orientation      | -0.00699    |
| reward_position         | -0.128      |
| reward_rotation         | 1.18        |
| reward_torque           | -0.000149   |
| reward_velocity         | 1.38        |
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | -32.4       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 53          |
|    time_elapsed         | 282         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.042325236 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.6       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0732     |
|    std                  | 0.261       |
|    value_loss           | 15.7        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.128      |
| reward_contact          | -0.0549     |
| reward_ctrl             | -0.19       |
| reward_motion           | -0.0778     |
| reward_orientation      | -0.00706    |
| reward_position         | -0.128      |
| reward_rotation         | 1.17        |
| reward_torque           | -0.000147   |
| reward_velocity         | 1.38        |
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | -32.2       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 54          |
|    time_elapsed         | 288         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.059364945 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.5       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00844     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.101      |
|    std                  | 0.261       |
|    value_loss           | 2.71        |
-----------------------------------------
----------------------------------------
| reward                  | -0.095     |
| reward_contact          | -0.0551    |
| reward_ctrl             | -0.192     |
| reward_motion           | -0.0782    |
| reward_orientation      | -0.00671   |
| reward_position         | -0.095     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000155  |
| reward_velocity         | 1.35       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -18.7      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 55         |
|    time_elapsed         | 294        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.15848884 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.7      |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.195     |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.26       |
|    value_loss           | 0.369      |
----------------------------------------
----------------------------------------
| reward                  | -0.111     |
| reward_contact          | -0.0595    |
| reward_ctrl             | -0.192     |
| reward_motion           | -0.0746    |
| reward_orientation      | -0.0069    |
| reward_position         | -0.111     |
| reward_rotation         | 1.18       |
| reward_torque           | -0.000155  |
| reward_velocity         | 1.37       |
| rollout/                |            |
|    ep_len_mean          | 137        |
|    ep_rew_mean          | -24.6      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 56         |
|    time_elapsed         | 300        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.16585267 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -45.4      |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.18      |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.258      |
|    value_loss           | 0.477      |
----------------------------------------
-----------------------------------------
| reward                  | -0.132      |
| reward_contact          | -0.0595     |
| reward_ctrl             | -0.19       |
| reward_motion           | -0.0746     |
| reward_orientation      | -0.00658    |
| reward_position         | -0.132      |
| reward_rotation         | 1.15        |
| reward_torque           | -0.000156   |
| reward_velocity         | 1.38        |
| rollout/                |             |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | -30.9       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 57          |
|    time_elapsed         | 305         |
|    total_timesteps      | 58368       |
| train/                  |             |
|    approx_kl            | 0.078370556 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.4         |
|    entropy_loss         | -45.1       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.56        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.108      |
|    std                  | 0.257       |
|    value_loss           | 7.49        |
-----------------------------------------
----------------------------------------
| reward                  | -0.144     |
| reward_contact          | -0.061     |
| reward_ctrl             | -0.19      |
| reward_motion           | -0.0713    |
| reward_orientation      | -0.00661   |
| reward_position         | -0.144     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000154  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 145        |
|    ep_rew_mean          | -35.9      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 58         |
|    time_elapsed         | 310        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.03927708 |
|    clip_fraction        | 0.0766     |
|    clip_range           | 0.4        |
|    entropy_loss         | -45.5      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.41       |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.257      |
|    value_loss           | 19.4       |
----------------------------------------
Num timesteps: 60000
Best mean reward: -32.44 - Last mean reward per episode: -35.90
-----------------------------------------
| reward                  | -0.159      |
| reward_contact          | -0.066      |
| reward_ctrl             | -0.189      |
| reward_motion           | -0.0689     |
| reward_orientation      | -0.00652    |
| reward_position         | -0.159      |
| reward_rotation         | 1.17        |
| reward_torque           | -0.000156   |
| reward_velocity         | 1.41        |
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | -41.2       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 59          |
|    time_elapsed         | 315         |
|    total_timesteps      | 60416       |
| train/                  |             |
|    approx_kl            | 0.058480494 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.4         |
|    entropy_loss         | -45.3       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0949     |
|    std                  | 0.257       |
|    value_loss           | 6.02        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.158      |
| reward_contact          | -0.0707     |
| reward_ctrl             | -0.191      |
| reward_motion           | -0.0629     |
| reward_orientation      | -0.00627    |
| reward_position         | -0.158      |
| reward_rotation         | 1.17        |
| reward_torque           | -0.000152   |
| reward_velocity         | 1.42        |
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | -41.1       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 60          |
|    time_elapsed         | 321         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.061254904 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -45.2       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.541       |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0983     |
|    std                  | 0.256       |
|    value_loss           | 8           |
-----------------------------------------
----------------------------------------
| reward                  | -0.157     |
| reward_contact          | -0.0732    |
| reward_ctrl             | -0.186     |
| reward_motion           | -0.0569    |
| reward_orientation      | -0.00639   |
| reward_position         | -0.157     |
| reward_rotation         | 1.13       |
| reward_torque           | -0.000148  |
| reward_velocity         | 1.4        |
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | -41.6      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 61         |
|    time_elapsed         | 327        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.15561205 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.4        |
|    entropy_loss         | -45.6      |
|    explained_variance   | -0.93      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.229     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.254      |
|    value_loss           | 0.253      |
----------------------------------------
----------------------------------------
| reward                  | -0.168     |
| reward_contact          | -0.0827    |
| reward_ctrl             | -0.189     |
| reward_motion           | -0.0504    |
| reward_orientation      | -0.00657   |
| reward_position         | -0.168     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.4        |
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | -46.4      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 62         |
|    time_elapsed         | 331        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.14705548 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46        |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.222     |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.25       |
|    value_loss           | 0.358      |
----------------------------------------
-----------------------------------------
| reward                  | -0.158      |
| reward_contact          | -0.0743     |
| reward_ctrl             | -0.187      |
| reward_motion           | -0.0504     |
| reward_orientation      | -0.00668    |
| reward_position         | -0.158      |
| reward_rotation         | 1.12        |
| reward_torque           | -0.000151   |
| reward_velocity         | 1.37        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -45.7       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 63          |
|    time_elapsed         | 336         |
|    total_timesteps      | 64512       |
| train/                  |             |
|    approx_kl            | 0.032825727 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.4         |
|    entropy_loss         | -45.9       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.8        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.249       |
|    value_loss           | 15          |
-----------------------------------------
---------------------------------------
| reward                  | -0.165    |
| reward_contact          | -0.0715   |
| reward_ctrl             | -0.183    |
| reward_motion           | -0.0555   |
| reward_orientation      | -0.00667  |
| reward_position         | -0.165    |
| reward_rotation         | 1.12      |
| reward_torque           | -0.00015  |
| reward_velocity         | 1.34      |
| rollout/                |           |
|    ep_len_mean          | 160       |
|    ep_rew_mean          | -45       |
| time/                   |           |
|    fps                  | 192       |
|    iterations           | 64        |
|    time_elapsed         | 341       |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.1841326 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.4       |
|    entropy_loss         | -46.8     |
|    explained_variance   | -0.949    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.259    |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.247     |
|    value_loss           | 0.187     |
---------------------------------------
Num timesteps: 66000
Best mean reward: -32.44 - Last mean reward per episode: -44.77
----------------------------------------
| reward                  | -0.164     |
| reward_contact          | -0.0715    |
| reward_ctrl             | -0.183     |
| reward_motion           | -0.0555    |
| reward_orientation      | -0.00642   |
| reward_position         | -0.164     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000149  |
| reward_velocity         | 1.32       |
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | -44.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 65         |
|    time_elapsed         | 346        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.12209074 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.7      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.129     |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.246      |
|    value_loss           | 0.701      |
----------------------------------------
----------------------------------------
| reward                  | -0.181     |
| reward_contact          | -0.0748    |
| reward_ctrl             | -0.182     |
| reward_motion           | -0.0492    |
| reward_orientation      | -0.00643   |
| reward_position         | -0.181     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000149  |
| reward_velocity         | 1.33       |
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | -50        |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 66         |
|    time_elapsed         | 352        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.15597919 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.6      |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.249     |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.244      |
|    value_loss           | 0.176      |
----------------------------------------
----------------------------------------
| reward                  | -0.183     |
| reward_contact          | -0.0688    |
| reward_ctrl             | -0.184     |
| reward_motion           | -0.0492    |
| reward_orientation      | -0.00645   |
| reward_position         | -0.183     |
| reward_rotation         | 1.12       |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.32       |
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | -50.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 67         |
|    time_elapsed         | 358        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.04522354 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.3      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.73       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0763    |
|    std                  | 0.244      |
|    value_loss           | 10         |
----------------------------------------
----------------------------------------
| reward                  | -0.191     |
| reward_contact          | -0.0643    |
| reward_ctrl             | -0.185     |
| reward_motion           | -0.0449    |
| reward_orientation      | -0.00635   |
| reward_position         | -0.191     |
| reward_rotation         | 1.13       |
| reward_torque           | -0.000152  |
| reward_velocity         | 1.31       |
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | -53.9      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 68         |
|    time_elapsed         | 364        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.12174669 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.1      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.209     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.242      |
|    value_loss           | 0.481      |
----------------------------------------
----------------------------------------
| reward                  | -0.175     |
| reward_contact          | -0.0751    |
| reward_ctrl             | -0.188     |
| reward_motion           | -0.0449    |
| reward_orientation      | -0.00622   |
| reward_position         | -0.175     |
| reward_rotation         | 1.11       |
| reward_torque           | -0.000153  |
| reward_velocity         | 1.31       |
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | -47.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 69         |
|    time_elapsed         | 368        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.06366697 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.5      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0901    |
|    std                  | 0.242      |
|    value_loss           | 4.15       |
----------------------------------------
-----------------------------------------
| reward                  | -0.168      |
| reward_contact          | -0.0778     |
| reward_ctrl             | -0.188      |
| reward_motion           | -0.0517     |
| reward_orientation      | -0.00604    |
| reward_position         | -0.168      |
| reward_rotation         | 1.1         |
| reward_torque           | -0.000151   |
| reward_velocity         | 1.29        |
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | -55.3       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 70          |
|    time_elapsed         | 374         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.049250115 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.4         |
|    entropy_loss         | -46.8       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.48        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0793     |
|    std                  | 0.242       |
|    value_loss           | 10.2        |
-----------------------------------------
Num timesteps: 72000
Best mean reward: -32.44 - Last mean reward per episode: -54.64
-----------------------------------------
| reward                  | -0.191      |
| reward_contact          | -0.0762     |
| reward_ctrl             | -0.185      |
| reward_motion           | -0.0497     |
| reward_orientation      | -0.00605    |
| reward_position         | -0.191      |
| reward_rotation         | 1.09        |
| reward_torque           | -0.000149   |
| reward_velocity         | 1.29        |
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 71          |
|    time_elapsed         | 380         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.052054152 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.4         |
|    entropy_loss         | -46.4       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.785       |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0798     |
|    std                  | 0.242       |
|    value_loss           | 17.2        |
-----------------------------------------
----------------------------------------
| reward                  | -0.201     |
| reward_contact          | -0.0678    |
| reward_ctrl             | -0.184     |
| reward_motion           | -0.046     |
| reward_orientation      | -0.00628   |
| reward_position         | -0.201     |
| reward_rotation         | 1.12       |
| reward_torque           | -0.000154  |
| reward_velocity         | 1.31       |
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | -58.2      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 72         |
|    time_elapsed         | 385        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.10136129 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.5      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0605    |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.241      |
|    value_loss           | 1.33       |
----------------------------------------
----------------------------------------
| reward                  | -0.221     |
| reward_contact          | -0.0678    |
| reward_ctrl             | -0.186     |
| reward_motion           | -0.0525    |
| reward_orientation      | -0.00636   |
| reward_position         | -0.221     |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000155  |
| reward_velocity         | 1.32       |
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | -66.7      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 73         |
|    time_elapsed         | 391        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.08262017 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -46.9      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0197    |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.24       |
|    value_loss           | 4.05       |
----------------------------------------
----------------------------------------
| reward                  | -0.183     |
| reward_contact          | -0.068     |
| reward_ctrl             | -0.192     |
| reward_motion           | -0.0594    |
| reward_orientation      | -0.00597   |
| reward_position         | -0.183     |
| reward_rotation         | 1.1        |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.27       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -48.6      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 74         |
|    time_elapsed         | 397        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.04474646 |
|    clip_fraction        | 0.0879     |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.3      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.29       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0806    |
|    std                  | 0.24       |
|    value_loss           | 16.5       |
----------------------------------------
-----------------------------------------
| reward                  | -0.177      |
| reward_contact          | -0.0591     |
| reward_ctrl             | -0.192      |
| reward_motion           | -0.0623     |
| reward_orientation      | -0.00601    |
| reward_position         | -0.177      |
| reward_rotation         | 1.09        |
| reward_torque           | -0.00015    |
| reward_velocity         | 1.25        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -46.7       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 75          |
|    time_elapsed         | 403         |
|    total_timesteps      | 76800       |
| train/                  |             |
|    approx_kl            | 0.084770195 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -47.5       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0638     |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.121      |
|    std                  | 0.24        |
|    value_loss           | 2.25        |
-----------------------------------------
----------------------------------------
| reward                  | -0.175     |
| reward_contact          | -0.0622    |
| reward_ctrl             | -0.195     |
| reward_motion           | -0.0683    |
| reward_orientation      | -0.00599   |
| reward_position         | -0.175     |
| reward_rotation         | 1.05       |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.23       |
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | -46.1      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 76         |
|    time_elapsed         | 409        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07131979 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.1      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.369      |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.239      |
|    value_loss           | 3.73       |
----------------------------------------
Num timesteps: 78000
Best mean reward: -32.44 - Last mean reward per episode: -46.12
----------------------------------------
| reward                  | -0.197     |
| reward_contact          | -0.0527    |
| reward_ctrl             | -0.193     |
| reward_motion           | -0.0679    |
| reward_orientation      | -0.00581   |
| reward_position         | -0.197     |
| reward_rotation         | 1.05       |
| reward_torque           | -0.000152  |
| reward_velocity         | 1.23       |
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | -53.4      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 77         |
|    time_elapsed         | 414        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.11742522 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.7      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.184     |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.237      |
|    value_loss           | 0.552      |
----------------------------------------
-----------------------------------------
| reward                  | -0.206      |
| reward_contact          | -0.0548     |
| reward_ctrl             | -0.196      |
| reward_motion           | -0.0624     |
| reward_orientation      | -0.00589    |
| reward_position         | -0.206      |
| reward_rotation         | 1.05        |
| reward_torque           | -0.000152   |
| reward_velocity         | 1.24        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 78          |
|    time_elapsed         | 419         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.053294666 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.4         |
|    entropy_loss         | -47.6       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.2        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0755     |
|    std                  | 0.237       |
|    value_loss           | 40.2        |
-----------------------------------------
----------------------------------------
| reward                  | -0.206     |
| reward_contact          | -0.0608    |
| reward_ctrl             | -0.195     |
| reward_motion           | -0.0624    |
| reward_orientation      | -0.0057    |
| reward_position         | -0.206     |
| reward_rotation         | 1.06       |
| reward_torque           | -0.000152  |
| reward_velocity         | 1.28       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -56.5      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 79         |
|    time_elapsed         | 423        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.09249072 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.2      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00381    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.237      |
|    value_loss           | 2.24       |
----------------------------------------
---------------------------------------
| reward                  | -0.208    |
| reward_contact          | -0.0844   |
| reward_ctrl             | -0.191    |
| reward_motion           | -0.0624   |
| reward_orientation      | -0.00576  |
| reward_position         | -0.208    |
| reward_rotation         | 1.08      |
| reward_torque           | -0.00015  |
| reward_velocity         | 1.3       |
| rollout/                |           |
|    ep_len_mean          | 172       |
|    ep_rew_mean          | -57.4     |
| time/                   |           |
|    fps                  | 191       |
|    iterations           | 80        |
|    time_elapsed         | 428       |
|    total_timesteps      | 81920     |
| train/                  |           |
|    approx_kl            | 0.2467036 |
|    clip_fraction        | 0.504     |
|    clip_range           | 0.4       |
|    entropy_loss         | -47.8     |
|    explained_variance   | 0.0847    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.254    |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.233     |
|    value_loss           | 0.198     |
---------------------------------------
----------------------------------------
| reward                  | -0.216     |
| reward_contact          | -0.0933    |
| reward_ctrl             | -0.191     |
| reward_motion           | -0.06      |
| reward_orientation      | -0.00595   |
| reward_position         | -0.216     |
| reward_rotation         | 1.08       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.3        |
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | -60.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 81         |
|    time_elapsed         | 433        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.16337098 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.5      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.219     |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.231      |
|    value_loss           | 0.392      |
----------------------------------------
----------------------------------------
| reward                  | -0.226     |
| reward_contact          | -0.0968    |
| reward_ctrl             | -0.191     |
| reward_motion           | -0.0556    |
| reward_orientation      | -0.00604   |
| reward_position         | -0.226     |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000152  |
| reward_velocity         | 1.32       |
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | -64.6      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 82         |
|    time_elapsed         | 439        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.07324909 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.2      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.57       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.231      |
|    value_loss           | 4.7        |
----------------------------------------
Num timesteps: 84000
Best mean reward: -32.44 - Last mean reward per episode: -64.62
----------------------------------------
| reward                  | -0.22      |
| reward_contact          | -0.0968    |
| reward_ctrl             | -0.19      |
| reward_motion           | -0.0556    |
| reward_orientation      | -0.00616   |
| reward_position         | -0.22      |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.33       |
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | -64.7      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 83         |
|    time_elapsed         | 445        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.04503596 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.4        |
|    entropy_loss         | -47.8      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.555      |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0917    |
|    std                  | 0.231      |
|    value_loss           | 10.3       |
----------------------------------------
----------------------------------------
| reward                  | -0.234     |
| reward_contact          | -0.0968    |
| reward_ctrl             | -0.189     |
| reward_motion           | -0.0556    |
| reward_orientation      | -0.00615   |
| reward_position         | -0.234     |
| reward_rotation         | 1.1        |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.34       |
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | -71.2      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 84         |
|    time_elapsed         | 450        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.13016936 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.3      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.223     |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.23       |
|    value_loss           | 0.893      |
----------------------------------------
----------------------------------------
| reward                  | -0.211     |
| reward_contact          | -0.0961    |
| reward_ctrl             | -0.185     |
| reward_motion           | -0.0662    |
| reward_orientation      | -0.0062    |
| reward_position         | -0.211     |
| reward_rotation         | 1.08       |
| reward_torque           | -0.000145  |
| reward_velocity         | 1.36       |
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | -62        |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 85         |
|    time_elapsed         | 455        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.05900567 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.6      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.47       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.229      |
|    value_loss           | 8.41       |
----------------------------------------
----------------------------------------
| reward                  | -0.199     |
| reward_contact          | -0.0788    |
| reward_ctrl             | -0.184     |
| reward_motion           | -0.0662    |
| reward_orientation      | -0.00648   |
| reward_position         | -0.199     |
| reward_rotation         | 1.07       |
| reward_torque           | -0.000143  |
| reward_velocity         | 1.36       |
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | -57.3      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 86         |
|    time_elapsed         | 459        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.17079967 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.8      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.199     |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.227      |
|    value_loss           | 0.283      |
----------------------------------------
----------------------------------------
| reward                  | -0.187     |
| reward_contact          | -0.0706    |
| reward_ctrl             | -0.186     |
| reward_motion           | -0.0686    |
| reward_orientation      | -0.00647   |
| reward_position         | -0.187     |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.36       |
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 87         |
|    time_elapsed         | 465        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.15182869 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.7      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.204     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.226      |
|    value_loss           | 0.465      |
----------------------------------------
Num timesteps: 90000
Best mean reward: -32.44 - Last mean reward per episode: -55.86
-----------------------------------------
| reward                  | -0.191      |
| reward_contact          | -0.0689     |
| reward_ctrl             | -0.185      |
| reward_motion           | -0.0654     |
| reward_orientation      | -0.0066     |
| reward_position         | -0.191      |
| reward_rotation         | 1.09        |
| reward_torque           | -0.000142   |
| reward_velocity         | 1.39        |
| rollout/                |             |
|    ep_len_mean          | 171         |
|    ep_rew_mean          | -55.9       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 88          |
|    time_elapsed         | 471         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.034685254 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.4         |
|    entropy_loss         | -48.5       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.11        |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0739     |
|    std                  | 0.226       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | -0.205     |
| reward_contact          | -0.0809    |
| reward_ctrl             | -0.185     |
| reward_motion           | -0.0586    |
| reward_orientation      | -0.00673   |
| reward_position         | -0.205     |
| reward_rotation         | 1.08       |
| reward_torque           | -0.000142  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | -60.4      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 89         |
|    time_elapsed         | 476        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.05879599 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.7      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.259      |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.225      |
|    value_loss           | 7.7        |
----------------------------------------
-----------------------------------------
| reward                  | -0.185      |
| reward_contact          | -0.0809     |
| reward_ctrl             | -0.183      |
| reward_motion           | -0.0586     |
| reward_orientation      | -0.00652    |
| reward_position         | -0.185      |
| reward_rotation         | 1.1         |
| reward_torque           | -0.000142   |
| reward_velocity         | 1.39        |
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | -51.9       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 90          |
|    time_elapsed         | 481         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.061640657 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.4         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47        |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.106      |
|    std                  | 0.225       |
|    value_loss           | 5.97        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.19       |
| reward_contact          | -0.0773     |
| reward_ctrl             | -0.184      |
| reward_motion           | -0.0586     |
| reward_orientation      | -0.00668    |
| reward_position         | -0.19       |
| reward_rotation         | 1.11        |
| reward_torque           | -0.000142   |
| reward_velocity         | 1.39        |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | -53.8       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 91          |
|    time_elapsed         | 486         |
|    total_timesteps      | 93184       |
| train/                  |             |
|    approx_kl            | 0.115567625 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.4         |
|    entropy_loss         | -48.6       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.198      |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.161      |
|    std                  | 0.223       |
|    value_loss           | 0.677       |
-----------------------------------------
----------------------------------------
| reward                  | -0.217     |
| reward_contact          | -0.081     |
| reward_ctrl             | -0.181     |
| reward_motion           | -0.0518    |
| reward_orientation      | -0.00705   |
| reward_position         | -0.217     |
| reward_rotation         | 1.1        |
| reward_torque           | -0.000143  |
| reward_velocity         | 1.41       |
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | -61.7      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 92         |
|    time_elapsed         | 491        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.06513859 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.4        |
|    entropy_loss         | -48.7      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.223      |
|    value_loss           | 6.77       |
----------------------------------------
-----------------------------------------
| reward                  | -0.205      |
| reward_contact          | -0.087      |
| reward_ctrl             | -0.186      |
| reward_motion           | -0.0416     |
| reward_orientation      | -0.00752    |
| reward_position         | -0.205      |
| reward_rotation         | 1.1         |
| reward_torque           | -0.000145   |
| reward_velocity         | 1.44        |
| rollout/                |             |
|    ep_len_mean          | 199         |
|    ep_rew_mean          | -61.3       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 93          |
|    time_elapsed         | 496         |
|    total_timesteps      | 95232       |
| train/                  |             |
|    approx_kl            | 0.044611827 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.4         |
|    entropy_loss         | -49         |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0732     |
|    std                  | 0.223       |
|    value_loss           | 19.2        |
-----------------------------------------
Num timesteps: 96000
Best mean reward: -32.44 - Last mean reward per episode: -61.65
----------------------------------------
| reward                  | -0.209     |
| reward_contact          | -0.081     |
| reward_ctrl             | -0.182     |
| reward_motion           | -0.0416    |
| reward_orientation      | -0.0075    |
| reward_position         | -0.209     |
| reward_rotation         | 1.11       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 201        |
|    ep_rew_mean          | -61.6      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 94         |
|    time_elapsed         | 502        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.24991168 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49        |
|    explained_variance   | 0.00491    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.241     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.221      |
|    value_loss           | 0.266      |
----------------------------------------
-----------------------------------------
| reward                  | -0.213      |
| reward_contact          | -0.082      |
| reward_ctrl             | -0.177      |
| reward_motion           | -0.0449     |
| reward_orientation      | -0.00767    |
| reward_position         | -0.213      |
| reward_rotation         | 1.13        |
| reward_torque           | -0.000149   |
| reward_velocity         | 1.45        |
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | -63.3       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 95          |
|    time_elapsed         | 508         |
|    total_timesteps      | 97280       |
| train/                  |             |
|    approx_kl            | 0.117032185 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.4         |
|    entropy_loss         | -49.1       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.141      |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.141      |
|    std                  | 0.22        |
|    value_loss           | 1.25        |
-----------------------------------------
----------------------------------------
| reward                  | -0.217     |
| reward_contact          | -0.0834    |
| reward_ctrl             | -0.175     |
| reward_motion           | -0.0419    |
| reward_orientation      | -0.00789   |
| reward_position         | -0.217     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000149  |
| reward_velocity         | 1.48       |
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | -64.4      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 96         |
|    time_elapsed         | 514        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.06004342 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.2      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.99       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0989    |
|    std                  | 0.22       |
|    value_loss           | 7.89       |
----------------------------------------
----------------------------------------
| reward                  | -0.257     |
| reward_contact          | -0.0845    |
| reward_ctrl             | -0.174     |
| reward_motion           | -0.0356    |
| reward_orientation      | -0.00798   |
| reward_position         | -0.257     |
| reward_rotation         | 1.15       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.49       |
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | -81.4      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 97         |
|    time_elapsed         | 520        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.16928567 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.1      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.214     |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.218      |
|    value_loss           | 0.414      |
----------------------------------------
-----------------------------------------
| reward                  | -0.257      |
| reward_contact          | -0.0845     |
| reward_ctrl             | -0.174      |
| reward_motion           | -0.0356     |
| reward_orientation      | -0.00798    |
| reward_position         | -0.257      |
| reward_rotation         | 1.15        |
| reward_torque           | -0.000151   |
| reward_velocity         | 1.49        |
| rollout/                |             |
|    ep_len_mean          | 231         |
|    ep_rew_mean          | -82.7       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 98          |
|    time_elapsed         | 525         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.039288044 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.4         |
|    entropy_loss         | -49         |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0589     |
|    std                  | 0.218       |
|    value_loss           | 47          |
-----------------------------------------
----------------------------------------
| reward                  | -0.223     |
| reward_contact          | -0.0888    |
| reward_ctrl             | -0.171     |
| reward_motion           | -0.0444    |
| reward_orientation      | -0.00845   |
| reward_position         | -0.223     |
| reward_rotation         | 1.16       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.5        |
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | -67.8      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 99         |
|    time_elapsed         | 530        |
|    total_timesteps      | 101376     |
| train/                  |            |
|    approx_kl            | 0.10955411 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.1      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0998    |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.217      |
|    value_loss           | 1.59       |
----------------------------------------
Num timesteps: 102000
Best mean reward: -32.44 - Last mean reward per episode: -68.19
----------------------------------------
| reward                  | -0.224     |
| reward_contact          | -0.091     |
| reward_ctrl             | -0.171     |
| reward_motion           | -0.0444    |
| reward_orientation      | -0.00852   |
| reward_position         | -0.224     |
| reward_rotation         | 1.14       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.48       |
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | -68.2      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 100        |
|    time_elapsed         | 534        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.15349692 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.7      |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.183     |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.216      |
|    value_loss           | 0.599      |
----------------------------------------
----------------------------------------
| reward                  | -0.239     |
| reward_contact          | -0.0662    |
| reward_ctrl             | -0.17      |
| reward_motion           | -0.0444    |
| reward_orientation      | -0.00909   |
| reward_position         | -0.239     |
| reward_rotation         | 1.12       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.47       |
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | -72.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 101        |
|    time_elapsed         | 540        |
|    total_timesteps      | 103424     |
| train/                  |            |
|    approx_kl            | 0.12445484 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.7      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.202     |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.215      |
|    value_loss           | 0.761      |
----------------------------------------
-----------------------------------------
| reward                  | -0.232      |
| reward_contact          | -0.0671     |
| reward_ctrl             | -0.173      |
| reward_motion           | -0.0469     |
| reward_orientation      | -0.00873    |
| reward_position         | -0.232      |
| reward_rotation         | 1.14        |
| reward_torque           | -0.000147   |
| reward_velocity         | 1.48        |
| rollout/                |             |
|    ep_len_mean          | 209         |
|    ep_rew_mean          | -69.4       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 102         |
|    time_elapsed         | 546         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.074406326 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.4         |
|    entropy_loss         | -50         |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.99        |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.215       |
|    value_loss           | 5.19        |
-----------------------------------------
----------------------------------------
| reward                  | -0.231     |
| reward_contact          | -0.0677    |
| reward_ctrl             | -0.175     |
| reward_motion           | -0.0512    |
| reward_orientation      | -0.00873   |
| reward_position         | -0.231     |
| reward_rotation         | 1.13       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.47       |
| rollout/                |            |
|    ep_len_mean          | 209        |
|    ep_rew_mean          | -68.8      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 103        |
|    time_elapsed         | 552        |
|    total_timesteps      | 105472     |
| train/                  |            |
|    approx_kl            | 0.10654734 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -50.1      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.172     |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.214      |
|    value_loss           | 1.02       |
----------------------------------------
-----------------------------------------
| reward                  | -0.233      |
| reward_contact          | -0.0707     |
| reward_ctrl             | -0.174      |
| reward_motion           | -0.0485     |
| reward_orientation      | -0.00893    |
| reward_position         | -0.233      |
| reward_rotation         | 1.12        |
| reward_torque           | -0.000147   |
| reward_velocity         | 1.48        |
| rollout/                |             |
|    ep_len_mean          | 208         |
|    ep_rew_mean          | -69.1       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 104         |
|    time_elapsed         | 557         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.059428103 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.4         |
|    entropy_loss         | -49.9       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.52        |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.105      |
|    std                  | 0.213       |
|    value_loss           | 5.78        |
-----------------------------------------
---------------------------------------
| reward                  | -0.251    |
| reward_contact          | -0.0741   |
| reward_ctrl             | -0.175    |
| reward_motion           | -0.0435   |
| reward_orientation      | -0.00923  |
| reward_position         | -0.251    |
| reward_rotation         | 1.12      |
| reward_torque           | -0.000147 |
| reward_velocity         | 1.47      |
| rollout/                |           |
|    ep_len_mean          | 217       |
|    ep_rew_mean          | -76.1     |
| time/                   |           |
|    fps                  | 191       |
|    iterations           | 105       |
|    time_elapsed         | 562       |
|    total_timesteps      | 107520    |
| train/                  |           |
|    approx_kl            | 0.047427  |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.4       |
|    entropy_loss         | -50       |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.0861   |
|    std                  | 0.213     |
|    value_loss           | 15.1      |
---------------------------------------
Num timesteps: 108000
Best mean reward: -32.44 - Last mean reward per episode: -76.12
-----------------------------------------
| reward                  | -0.251      |
| reward_contact          | -0.0741     |
| reward_ctrl             | -0.175      |
| reward_motion           | -0.0435     |
| reward_orientation      | -0.00923    |
| reward_position         | -0.251      |
| reward_rotation         | 1.12        |
| reward_torque           | -0.000147   |
| reward_velocity         | 1.47        |
| rollout/                |             |
|    ep_len_mean          | 226         |
|    ep_rew_mean          | -77.3       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 106         |
|    time_elapsed         | 567         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.049153134 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.4         |
|    entropy_loss         | -50.1       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0817     |
|    std                  | 0.213       |
|    value_loss           | 17.2        |
-----------------------------------------
----------------------------------------
| reward                  | -0.251     |
| reward_contact          | -0.0783    |
| reward_ctrl             | -0.178     |
| reward_motion           | -0.0374    |
| reward_orientation      | -0.0091    |
| reward_position         | -0.251     |
| reward_rotation         | 1.11       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.46       |
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | -77.1      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 107        |
|    time_elapsed         | 572        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.21192777 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.9      |
|    explained_variance   | -1.06      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.16      |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.212      |
|    value_loss           | 0.384      |
----------------------------------------
----------------------------------------
| reward                  | -0.268     |
| reward_contact          | -0.0862    |
| reward_ctrl             | -0.178     |
| reward_motion           | -0.033     |
| reward_orientation      | -0.0092    |
| reward_position         | -0.268     |
| reward_rotation         | 1.11       |
| reward_torque           | -0.000151  |
| reward_velocity         | 1.49       |
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | -81.5      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 108        |
|    time_elapsed         | 578        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.10584489 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -49.9      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.123     |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.211      |
|    value_loss           | 1.15       |
----------------------------------------
-----------------------------------------
| reward                  | -0.266      |
| reward_contact          | -0.0822     |
| reward_ctrl             | -0.181      |
| reward_motion           | -0.033      |
| reward_orientation      | -0.00946    |
| reward_position         | -0.266      |
| reward_rotation         | 1.11        |
| reward_torque           | -0.000157   |
| reward_velocity         | 1.49        |
| rollout/                |             |
|    ep_len_mean          | 239         |
|    ep_rew_mean          | -82.9       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 109         |
|    time_elapsed         | 582         |
|    total_timesteps      | 111616      |
| train/                  |             |
|    approx_kl            | 0.044208795 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -50.3       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.61        |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0779     |
|    std                  | 0.21        |
|    value_loss           | 9.68        |
-----------------------------------------
----------------------------------------
| reward                  | -0.269     |
| reward_contact          | -0.0822    |
| reward_ctrl             | -0.182     |
| reward_motion           | -0.0269    |
| reward_orientation      | -0.00964   |
| reward_position         | -0.269     |
| reward_rotation         | 1.11       |
| reward_torque           | -0.000156  |
| reward_velocity         | 1.5        |
| rollout/                |            |
|    ep_len_mean          | 248        |
|    ep_rew_mean          | -84.2      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 110        |
|    time_elapsed         | 588        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.12954023 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -50.5      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0563    |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.21       |
|    value_loss           | 0.843      |
----------------------------------------
----------------------------------------
| reward                  | -0.188     |
| reward_contact          | -0.0754    |
| reward_ctrl             | -0.184     |
| reward_motion           | -0.0504    |
| reward_orientation      | -0.00854   |
| reward_position         | -0.188     |
| reward_rotation         | 1.05       |
| reward_torque           | -0.000152  |
| reward_velocity         | 1.46       |
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | -59.4      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 111        |
|    time_elapsed         | 594        |
|    total_timesteps      | 113664     |
| train/                  |            |
|    approx_kl            | 0.16994235 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.4        |
|    entropy_loss         | -50.2      |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.18      |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.208      |
|    value_loss           | 0.605      |
----------------------------------------
Num timesteps: 114000
Best mean reward: -32.44 - Last mean reward per episode: -59.45
----------------------------------------
| reward                  | -0.169     |
| reward_contact          | -0.0723    |
| reward_ctrl             | -0.189     |
| reward_motion           | -0.0627    |
| reward_orientation      | -0.00782   |
| reward_position         | -0.169     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.43       |
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | -53.8      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 112        |
|    time_elapsed         | 600        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.21895862 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.1      |
|    explained_variance   | 0.288      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.232     |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.206      |
|    value_loss           | 0.256      |
----------------------------------------
----------------------------------------
| reward                  | -0.186     |
| reward_contact          | -0.0745    |
| reward_ctrl             | -0.188     |
| reward_motion           | -0.06      |
| reward_orientation      | -0.00801   |
| reward_position         | -0.186     |
| reward_rotation         | 1.01       |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.45       |
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | -59.9      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 113        |
|    time_elapsed         | 606        |
|    total_timesteps      | 115712     |
| train/                  |            |
|    approx_kl            | 0.22800735 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -50.9      |
|    explained_variance   | -0.321     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.255     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.202      |
|    value_loss           | 0.162      |
----------------------------------------
-----------------------------------------
| reward                  | -0.141      |
| reward_contact          | -0.0696     |
| reward_ctrl             | -0.197      |
| reward_motion           | -0.073      |
| reward_orientation      | -0.00783    |
| reward_position         | -0.141      |
| reward_rotation         | 0.999       |
| reward_torque           | -0.000151   |
| reward_velocity         | 1.4         |
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | -40.7       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 114         |
|    time_elapsed         | 611         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.041118763 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.4         |
|    entropy_loss         | -50.5       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.38        |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.202       |
|    value_loss           | 14.1        |
-----------------------------------------
---------------------------------------
| reward                  | -0.137    |
| reward_contact          | -0.072    |
| reward_ctrl             | -0.196    |
| reward_motion           | -0.073    |
| reward_orientation      | -0.00784  |
| reward_position         | -0.137    |
| reward_rotation         | 1.02      |
| reward_torque           | -0.000151 |
| reward_velocity         | 1.37      |
| rollout/                |           |
|    ep_len_mean          | 161       |
|    ep_rew_mean          | -40.1     |
| time/                   |           |
|    fps                  | 190       |
|    iterations           | 115       |
|    time_elapsed         | 616       |
|    total_timesteps      | 117760    |
| train/                  |           |
|    approx_kl            | 0.2529075 |
|    clip_fraction        | 0.493     |
|    clip_range           | 0.4       |
|    entropy_loss         | -51.1     |
|    explained_variance   | -0.772    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.237    |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.198    |
|    std                  | 0.201     |
|    value_loss           | 0.138     |
---------------------------------------
----------------------------------------
| reward                  | -0.138     |
| reward_contact          | -0.0758    |
| reward_ctrl             | -0.199     |
| reward_motion           | -0.073     |
| reward_orientation      | -0.00789   |
| reward_position         | -0.138     |
| reward_rotation         | 1.01       |
| reward_torque           | -0.000156  |
| reward_velocity         | 1.38       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -40.2      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 116        |
|    time_elapsed         | 622        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.18876001 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.4      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.211     |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.199      |
|    value_loss           | 0.191      |
----------------------------------------
-----------------------------------------
| reward                  | -0.136      |
| reward_contact          | -0.0741     |
| reward_ctrl             | -0.196      |
| reward_motion           | -0.0705     |
| reward_orientation      | -0.0079     |
| reward_position         | -0.136      |
| reward_rotation         | 1.02        |
| reward_torque           | -0.000155   |
| reward_velocity         | 1.38        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -39.1       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 117         |
|    time_elapsed         | 628         |
|    total_timesteps      | 119808      |
| train/                  |             |
|    approx_kl            | 0.121643476 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.1       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.153      |
|    std                  | 0.197       |
|    value_loss           | 0.403       |
-----------------------------------------
Num timesteps: 120000
Best mean reward: -32.44 - Last mean reward per episode: -39.09
----------------------------------------
| reward                  | -0.125     |
| reward_contact          | -0.0638    |
| reward_ctrl             | -0.2       |
| reward_motion           | -0.0657    |
| reward_orientation      | -0.00757   |
| reward_position         | -0.125     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.000156  |
| reward_velocity         | 1.37       |
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | -31.3      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 118        |
|    time_elapsed         | 634        |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.06101491 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.4        |
|    entropy_loss         | -50.6      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.28       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0901    |
|    std                  | 0.197      |
|    value_loss           | 3.66       |
----------------------------------------
-----------------------------------------
| reward                  | -0.0744     |
| reward_contact          | -0.0505     |
| reward_ctrl             | -0.204      |
| reward_motion           | -0.0839     |
| reward_orientation      | -0.00691    |
| reward_position         | -0.0744     |
| reward_rotation         | 1.05        |
| reward_torque           | -0.000152   |
| reward_velocity         | 1.36        |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -18.8       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 119         |
|    time_elapsed         | 639         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.054865286 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.2       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.498       |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0909     |
|    std                  | 0.197       |
|    value_loss           | 7.36        |
-----------------------------------------
----------------------------------------
| reward                  | -0.0848    |
| reward_contact          | -0.0531    |
| reward_ctrl             | -0.204     |
| reward_motion           | -0.0799    |
| reward_orientation      | -0.00678   |
| reward_position         | -0.0848    |
| reward_rotation         | 1.05       |
| reward_torque           | -0.00015   |
| reward_velocity         | 1.37       |
| rollout/                |            |
|    ep_len_mean          | 118        |
|    ep_rew_mean          | -21.3      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 120        |
|    time_elapsed         | 645        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.13222547 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.6      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.143     |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.197      |
|    value_loss           | 0.605      |
----------------------------------------
-----------------------------------------
| reward                  | -0.091      |
| reward_contact          | -0.0531     |
| reward_ctrl             | -0.208      |
| reward_motion           | -0.0799     |
| reward_orientation      | -0.00687    |
| reward_position         | -0.091      |
| reward_rotation         | 1.08        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -21.6       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 121         |
|    time_elapsed         | 651         |
|    total_timesteps      | 123904      |
| train/                  |             |
|    approx_kl            | 0.053930458 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.1       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.373       |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0838     |
|    std                  | 0.197       |
|    value_loss           | 7.15        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.101      |
| reward_contact          | -0.0507     |
| reward_ctrl             | -0.207      |
| reward_motion           | -0.0834     |
| reward_orientation      | -0.00725    |
| reward_position         | -0.101      |
| reward_rotation         | 1.06        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | -24.3       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 122         |
|    time_elapsed         | 657         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.116713464 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.4       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.14       |
|    std                  | 0.196       |
|    value_loss           | 0.918       |
-----------------------------------------
---------------------------------------
| reward                  | -0.131    |
| reward_contact          | -0.0507   |
| reward_ctrl             | -0.208    |
| reward_motion           | -0.077    |
| reward_orientation      | -0.0074   |
| reward_position         | -0.131    |
| reward_rotation         | 1.06      |
| reward_torque           | -0.000145 |
| reward_velocity         | 1.35      |
| rollout/                |           |
|    ep_len_mean          | 126       |
|    ep_rew_mean          | -34.4     |
| time/                   |           |
|    fps                  | 190       |
|    iterations           | 123       |
|    time_elapsed         | 662       |
|    total_timesteps      | 125952    |
| train/                  |           |
|    approx_kl            | 0.0440026 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.4       |
|    entropy_loss         | -51.4     |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.849     |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.0823   |
|    std                  | 0.196     |
|    value_loss           | 9.52      |
---------------------------------------
Num timesteps: 126000
Best mean reward: -32.44 - Last mean reward per episode: -34.45
-----------------------------------------
| reward                  | -0.131      |
| reward_contact          | -0.0505     |
| reward_ctrl             | -0.208      |
| reward_motion           | -0.077      |
| reward_orientation      | -0.00745    |
| reward_position         | -0.131      |
| reward_rotation         | 1.08        |
| reward_torque           | -0.000146   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | -35         |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 124         |
|    time_elapsed         | 668         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.041768067 |
|    clip_fraction        | 0.0743      |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.1       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.61        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.0666     |
|    std                  | 0.196       |
|    value_loss           | 22.2        |
-----------------------------------------
----------------------------------------
| reward                  | -0.133     |
| reward_contact          | -0.0532    |
| reward_ctrl             | -0.212     |
| reward_motion           | -0.0751    |
| reward_orientation      | -0.00754   |
| reward_position         | -0.133     |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000144  |
| reward_velocity         | 1.38       |
| rollout/                |            |
|    ep_len_mean          | 136        |
|    ep_rew_mean          | -35        |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 125        |
|    time_elapsed         | 672        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.19782966 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.2      |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.208     |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.195      |
|    value_loss           | 0.329      |
----------------------------------------
----------------------------------------
| reward                  | -0.147     |
| reward_contact          | -0.0607    |
| reward_ctrl             | -0.219     |
| reward_motion           | -0.065     |
| reward_orientation      | -0.00794   |
| reward_position         | -0.147     |
| reward_rotation         | 1.06       |
| reward_torque           | -0.00014   |
| reward_velocity         | 1.38       |
| rollout/                |            |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | -39.2      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 126        |
|    time_elapsed         | 678        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.08852021 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.4      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0694    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.195      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | -0.162     |
| reward_contact          | -0.0584    |
| reward_ctrl             | -0.213     |
| reward_motion           | -0.0676    |
| reward_orientation      | -0.0078    |
| reward_position         | -0.162     |
| reward_rotation         | 1.07       |
| reward_torque           | -0.00014   |
| reward_velocity         | 1.38       |
| rollout/                |            |
|    ep_len_mean          | 142        |
|    ep_rew_mean          | -44.9      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 127        |
|    time_elapsed         | 684        |
|    total_timesteps      | 130048     |
| train/                  |            |
|    approx_kl            | 0.09119276 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.7      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0884    |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.195      |
|    value_loss           | 1.89       |
----------------------------------------
-----------------------------------------
| reward                  | -0.164      |
| reward_contact          | -0.0464     |
| reward_ctrl             | -0.217      |
| reward_motion           | -0.0676     |
| reward_orientation      | -0.00706    |
| reward_position         | -0.164      |
| reward_rotation         | 1.08        |
| reward_torque           | -0.000141   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | -44.9       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 128         |
|    time_elapsed         | 690         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.037107266 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.5       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.94        |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0633     |
|    std                  | 0.195       |
|    value_loss           | 28.5        |
-----------------------------------------
Num timesteps: 132000
Best mean reward: -32.44 - Last mean reward per episode: -49.87
----------------------------------------
| reward                  | -0.179     |
| reward_contact          | -0.0496    |
| reward_ctrl             | -0.217     |
| reward_motion           | -0.0676    |
| reward_orientation      | -0.00723   |
| reward_position         | -0.179     |
| reward_rotation         | 1.08       |
| reward_torque           | -0.00014   |
| reward_velocity         | 1.35       |
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | -50        |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 129        |
|    time_elapsed         | 696        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.24120396 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.1      |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.229     |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.193      |
|    value_loss           | 0.21       |
----------------------------------------
-----------------------------------------
| reward                  | -0.179      |
| reward_contact          | -0.0496     |
| reward_ctrl             | -0.216      |
| reward_motion           | -0.0676     |
| reward_orientation      | -0.00698    |
| reward_position         | -0.179      |
| reward_rotation         | 1.08        |
| reward_torque           | -0.000139   |
| reward_velocity         | 1.37        |
| rollout/                |             |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | -50.1       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 130         |
|    time_elapsed         | 702         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.044535957 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.0605     |
|    std                  | 0.193       |
|    value_loss           | 10.7        |
-----------------------------------------
----------------------------------------
| reward                  | -0.193     |
| reward_contact          | -0.0436    |
| reward_ctrl             | -0.217     |
| reward_motion           | -0.0676    |
| reward_orientation      | -0.00721   |
| reward_position         | -0.193     |
| reward_rotation         | 1.1        |
| reward_torque           | -0.000136  |
| reward_velocity         | 1.37       |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -54.7      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 131        |
|    time_elapsed         | 708        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.15664104 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.2      |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.181     |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.192      |
|    value_loss           | 0.347      |
----------------------------------------
----------------------------------------
| reward                  | -0.185     |
| reward_contact          | -0.0446    |
| reward_ctrl             | -0.221     |
| reward_motion           | -0.075     |
| reward_orientation      | -0.00705   |
| reward_position         | -0.185     |
| reward_rotation         | 1.09       |
| reward_torque           | -0.000136  |
| reward_velocity         | 1.34       |
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | -49.7      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 132        |
|    time_elapsed         | 713        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.04870929 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.1      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.85       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.191      |
|    value_loss           | 7.29       |
----------------------------------------
----------------------------------------
| reward                  | -0.191     |
| reward_contact          | -0.0464    |
| reward_ctrl             | -0.222     |
| reward_motion           | -0.075     |
| reward_orientation      | -0.00705   |
| reward_position         | -0.191     |
| reward_rotation         | 1.08       |
| reward_torque           | -0.000134  |
| reward_velocity         | 1.34       |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -52        |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 133        |
|    time_elapsed         | 718        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.04011096 |
|    clip_fraction        | 0.0721     |
|    clip_range           | 0.4        |
|    entropy_loss         | -51.9      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.627      |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.191      |
|    value_loss           | 7.73       |
----------------------------------------
---------------------------------------
| reward                  | -0.193    |
| reward_contact          | -0.055    |
| reward_ctrl             | -0.223    |
| reward_motion           | -0.075    |
| reward_orientation      | -0.00719  |
| reward_position         | -0.193    |
| reward_rotation         | 1.06      |
| reward_torque           | -0.000129 |
| reward_velocity         | 1.38      |
| rollout/                |           |
|    ep_len_mean          | 164       |
|    ep_rew_mean          | -52.2     |
| time/                   |           |
|    fps                  | 189       |
|    iterations           | 134       |
|    time_elapsed         | 722       |
|    total_timesteps      | 137216    |
| train/                  |           |
|    approx_kl            | 0.1061435 |
|    clip_fraction        | 0.261     |
|    clip_range           | 0.4       |
|    entropy_loss         | -51.9     |
|    explained_variance   | 0.809     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.175    |
|    n_updates            | 2660      |
|    policy_gradient_loss | -0.149    |
|    std                  | 0.19      |
|    value_loss           | 1.33      |
---------------------------------------
Num timesteps: 138000
Best mean reward: -32.44 - Last mean reward per episode: -52.51
----------------------------------------
| reward                  | -0.197     |
| reward_contact          | -0.055     |
| reward_ctrl             | -0.217     |
| reward_motion           | -0.075     |
| reward_orientation      | -0.00732   |
| reward_position         | -0.197     |
| reward_rotation         | 1.05       |
| reward_torque           | -0.000133  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | -52.7      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 135        |
|    time_elapsed         | 728        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.25949445 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.4      |
|    explained_variance   | -0.00532   |
|    learning_rate        | 0.0003     |
|    loss                 | -0.199     |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.188      |
|    value_loss           | 0.281      |
----------------------------------------
----------------------------------------
| reward                  | -0.199     |
| reward_contact          | -0.061     |
| reward_ctrl             | -0.221     |
| reward_motion           | -0.075     |
| reward_orientation      | -0.00705   |
| reward_position         | -0.199     |
| reward_rotation         | 1.01       |
| reward_torque           | -0.000135  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | -52.8      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 136        |
|    time_elapsed         | 733        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.19729161 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.6      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.214     |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.186      |
|    value_loss           | 0.368      |
----------------------------------------
----------------------------------------
| reward                  | -0.198     |
| reward_contact          | -0.0693    |
| reward_ctrl             | -0.218     |
| reward_motion           | -0.073     |
| reward_orientation      | -0.00691   |
| reward_position         | -0.198     |
| reward_rotation         | 0.993      |
| reward_torque           | -0.000139  |
| reward_velocity         | 1.39       |
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | -54.4      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 137        |
|    time_elapsed         | 739        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.09030221 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.3      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.157     |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.185      |
|    value_loss           | 0.884      |
----------------------------------------
----------------------------------------
| reward                  | -0.186     |
| reward_contact          | -0.0678    |
| reward_ctrl             | -0.219     |
| reward_motion           | -0.0756    |
| reward_orientation      | -0.00664   |
| reward_position         | -0.186     |
| reward_rotation         | 1          |
| reward_torque           | -0.000138  |
| reward_velocity         | 1.36       |
| rollout/                |            |
|    ep_len_mean          | 161        |
|    ep_rew_mean          | -50.4      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 138        |
|    time_elapsed         | 745        |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.04533606 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.8      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.901      |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0805    |
|    std                  | 0.185      |
|    value_loss           | 15.7       |
----------------------------------------
----------------------------------------
| reward                  | -0.172     |
| reward_contact          | -0.0679    |
| reward_ctrl             | -0.22      |
| reward_motion           | -0.0773    |
| reward_orientation      | -0.00624   |
| reward_position         | -0.172     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.000143  |
| reward_velocity         | 1.34       |
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | -44.4      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 139        |
|    time_elapsed         | 751        |
|    total_timesteps      | 142336     |
| train/                  |            |
|    approx_kl            | 0.09909581 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.3      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.16      |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.185      |
|    value_loss           | 0.92       |
----------------------------------------
-----------------------------------------
| reward                  | -0.169      |
| reward_contact          | -0.0652     |
| reward_ctrl             | -0.216      |
| reward_motion           | -0.0792     |
| reward_orientation      | -0.00605    |
| reward_position         | -0.169      |
| reward_rotation         | 1.01        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.31        |
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | -44.4       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 140         |
|    time_elapsed         | 757         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.053568773 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.6       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.564       |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.0865     |
|    std                  | 0.185       |
|    value_loss           | 7.39        |
-----------------------------------------
Num timesteps: 144000
Best mean reward: -32.44 - Last mean reward per episode: -47.78
-----------------------------------------
| reward                  | -0.179      |
| reward_contact          | -0.0648     |
| reward_ctrl             | -0.217      |
| reward_motion           | -0.0806     |
| reward_orientation      | -0.00614    |
| reward_position         | -0.179      |
| reward_rotation         | 1.02        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.31        |
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | -47.8       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 141         |
|    time_elapsed         | 761         |
|    total_timesteps      | 144384      |
| train/                  |             |
|    approx_kl            | 0.084923744 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.737       |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.185       |
|    value_loss           | 4.62        |
-----------------------------------------
----------------------------------------
| reward                  | -0.196     |
| reward_contact          | -0.0649    |
| reward_ctrl             | -0.22      |
| reward_motion           | -0.0816    |
| reward_orientation      | -0.00606   |
| reward_position         | -0.196     |
| reward_rotation         | 1.03       |
| reward_torque           | -0.000145  |
| reward_velocity         | 1.32       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -54.1      |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 142        |
|    time_elapsed         | 766        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.04751419 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.4      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.877      |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.083     |
|    std                  | 0.185      |
|    value_loss           | 13.7       |
----------------------------------------
-----------------------------------------
| reward                  | -0.188      |
| reward_contact          | -0.0627     |
| reward_ctrl             | -0.215      |
| reward_motion           | -0.0753     |
| reward_orientation      | -0.00602    |
| reward_position         | -0.188      |
| reward_rotation         | 1.03        |
| reward_torque           | -0.000145   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | -49.6       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 143         |
|    time_elapsed         | 770         |
|    total_timesteps      | 146432      |
| train/                  |             |
|    approx_kl            | 0.048239853 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.25        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0795     |
|    std                  | 0.185       |
|    value_loss           | 13.6        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.188      |
| reward_contact          | -0.0627     |
| reward_ctrl             | -0.215      |
| reward_motion           | -0.0753     |
| reward_orientation      | -0.00608    |
| reward_position         | -0.188      |
| reward_rotation         | 1.03        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.35        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -49.7       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 144         |
|    time_elapsed         | 775         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.030981041 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0524     |
|    std                  | 0.185       |
|    value_loss           | 40.7        |
-----------------------------------------
-----------------------------------------
| reward                  | -0.178      |
| reward_contact          | -0.0635     |
| reward_ctrl             | -0.215      |
| reward_motion           | -0.0753     |
| reward_orientation      | -0.0061     |
| reward_position         | -0.178      |
| reward_rotation         | 1.05        |
| reward_torque           | -0.000144   |
| reward_velocity         | 1.39        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -48.5       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 145         |
|    time_elapsed         | 780         |
|    total_timesteps      | 148480      |
| train/                  |             |
|    approx_kl            | 0.106877685 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.5       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.14       |
|    std                  | 0.184       |
|    value_loss           | 1.64        |
-----------------------------------------
---------------------------------------
| reward                  | -0.156    |
| reward_contact          | -0.0653   |
| reward_ctrl             | -0.208    |
| reward_motion           | -0.0753   |
| reward_orientation      | -0.00582  |
| reward_position         | -0.156    |
| reward_rotation         | 1.01      |
| reward_torque           | -0.000143 |
| reward_velocity         | 1.4       |
| rollout/                |           |
|    ep_len_mean          | 141       |
|    ep_rew_mean          | -41.4     |
| time/                   |           |
|    fps                  | 190       |
|    iterations           | 146       |
|    time_elapsed         | 785       |
|    total_timesteps      | 149504    |
| train/                  |           |
|    approx_kl            | 0.0871038 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.4       |
|    entropy_loss         | -52.8     |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.4       |
|    n_updates            | 2900      |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.184     |
|    value_loss           | 4.14      |
---------------------------------------
Num timesteps: 150000
Best mean reward: -32.44 - Last mean reward per episode: -41.35
---------------------------------------
| reward                  | -0.152    |
| reward_contact          | -0.065    |
| reward_ctrl             | -0.204    |
| reward_motion           | -0.0729   |
| reward_orientation      | -0.00576  |
| reward_position         | -0.152    |
| reward_rotation         | 1.01      |
| reward_torque           | -0.000145 |
| reward_velocity         | 1.41      |
| rollout/                |           |
|    ep_len_mean          | 142       |
|    ep_rew_mean          | -40.5     |
| time/                   |           |
|    fps                  | 190       |
|    iterations           | 147       |
|    time_elapsed         | 790       |
|    total_timesteps      | 150528    |
| train/                  |           |
|    approx_kl            | 0.2876087 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -52.7     |
|    explained_variance   | 0.402     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.264    |
|    n_updates            | 2920      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.182     |
|    value_loss           | 0.241     |
---------------------------------------
----------------------------------------
| reward                  | -0.153     |
| reward_contact          | -0.065     |
| reward_ctrl             | -0.201     |
| reward_motion           | -0.0729    |
| reward_orientation      | -0.00603   |
| reward_position         | -0.153     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.4        |
| rollout/                |            |
|    ep_len_mean          | 142        |
|    ep_rew_mean          | -40.5      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 148        |
|    time_elapsed         | 796        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.11576788 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.6      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.157      |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.181      |
|    value_loss           | 1.49       |
----------------------------------------
----------------------------------------
| reward                  | -0.165     |
| reward_contact          | -0.059     |
| reward_ctrl             | -0.201     |
| reward_motion           | -0.0702    |
| reward_orientation      | -0.00609   |
| reward_position         | -0.165     |
| reward_rotation         | 1.05       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.4        |
| rollout/                |            |
|    ep_len_mean          | 148        |
|    ep_rew_mean          | -46.6      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 149        |
|    time_elapsed         | 801        |
|    total_timesteps      | 152576     |
| train/                  |            |
|    approx_kl            | 0.09890415 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.4      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.138     |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.181      |
|    value_loss           | 2.22       |
----------------------------------------
----------------------------------------
| reward                  | -0.182     |
| reward_contact          | -0.0617    |
| reward_ctrl             | -0.2       |
| reward_motion           | -0.0684    |
| reward_orientation      | -0.00627   |
| reward_position         | -0.182     |
| reward_rotation         | 1.06       |
| reward_torque           | -0.000147  |
| reward_velocity         | 1.42       |
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | -51.8      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 150        |
|    time_elapsed         | 808        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.07681603 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.7      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0791    |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.181      |
|    value_loss           | 3.13       |
----------------------------------------
-----------------------------------------
| reward                  | -0.188      |
| reward_contact          | -0.0718     |
| reward_ctrl             | -0.198      |
| reward_motion           | -0.0684     |
| reward_orientation      | -0.00652    |
| reward_position         | -0.188      |
| reward_rotation         | 1.05        |
| reward_torque           | -0.000147   |
| reward_velocity         | 1.39        |
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 151         |
|    time_elapsed         | 814         |
|    total_timesteps      | 154624      |
| train/                  |             |
|    approx_kl            | 0.047234744 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.5       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.88        |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.0696     |
|    std                  | 0.181       |
|    value_loss           | 24.7        |
-----------------------------------------
----------------------------------------
| reward                  | -0.167     |
| reward_contact          | -0.0655    |
| reward_ctrl             | -0.198     |
| reward_motion           | -0.0743    |
| reward_orientation      | -0.00685   |
| reward_position         | -0.167     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.37       |
| rollout/                |            |
|    ep_len_mean          | 152        |
|    ep_rew_mean          | -48.1      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 152        |
|    time_elapsed         | 819        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.09067643 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.8      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0797     |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.18       |
|    value_loss           | 3.73       |
----------------------------------------
Num timesteps: 156000
Best mean reward: -32.44 - Last mean reward per episode: -59.83
----------------------------------------
| reward                  | -0.201     |
| reward_contact          | -0.0687    |
| reward_ctrl             | -0.199     |
| reward_motion           | -0.0696    |
| reward_orientation      | -0.00705   |
| reward_position         | -0.201     |
| reward_rotation         | 1.02       |
| reward_torque           | -0.000146  |
| reward_velocity         | 1.38       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -59.8      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 153        |
|    time_elapsed         | 823        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.07152702 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -52.6      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.18       |
|    value_loss           | 4.32       |
----------------------------------------
-----------------------------------------
| reward                  | -0.201      |
| reward_contact          | -0.0688     |
| reward_ctrl             | -0.197      |
| reward_motion           | -0.0705     |
| reward_orientation      | -0.0071     |
| reward_position         | -0.201      |
| reward_rotation         | 1           |
| reward_torque           | -0.000141   |
| reward_velocity         | 1.42        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -58.9       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 154         |
|    time_elapsed         | 829         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.041830722 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.6       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.45        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.18        |
|    value_loss           | 41          |
-----------------------------------------
----------------------------------------
| reward                  | -0.184     |
| reward_contact          | -0.0578    |
| reward_ctrl             | -0.2       |
| reward_motion           | -0.0728    |
| reward_orientation      | -0.0077    |
| reward_position         | -0.184     |
| reward_rotation         | 0.994      |
| reward_torque           | -0.000135  |
| reward_velocity         | 1.44       |
| rollout/                |            |
|    ep_len_mean          | 152        |
|    ep_rew_mean          | -51.2      |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 155        |
|    time_elapsed         | 835        |
|    total_timesteps      | 158720     |
| train/                  |            |
|    approx_kl            | 0.07558974 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -53        |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.923      |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.18       |
|    value_loss           | 6.16       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp72/PPO_2
---------------------------------
| reward             | 0.0241   |
| reward_contact     | 0.00485  |
| reward_ctrl        | 0.00222  |
| reward_motion      | -0.1     |
| reward_orientation | 0.00184  |
| reward_position    | 0.00456  |
| reward_rotation    | 6.89e-11 |
| reward_torque      | 0.00369  |
| reward_velocity    | 0.107    |
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | 0.884    |
| time/              |          |
|    fps             | 260      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | 0.0159    |
| reward_contact          | 0.0231    |
| reward_ctrl             | 0.0102    |
| reward_motion           | -0.1      |
| reward_orientation      | 0.018     |
| reward_position         | 0.0033    |
| reward_rotation         | 6.09e-05  |
| reward_torque           | 0.0182    |
| reward_velocity         | 0.043     |
| rollout/                |           |
|    ep_len_mean          | 187       |
|    ep_rew_mean          | 52.1      |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 2         |
|    time_elapsed         | 8         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 1.2271278 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.3     |
|    explained_variance   | 0.00148   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.615     |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0567   |
|    std                  | 0.361     |
|    value_loss           | 2.36      |
---------------------------------------
----------------------------------------
| reward                  | 0.0801     |
| reward_contact          | 0.0314     |
| reward_ctrl             | 0.0151     |
| reward_motion           | -0.0713    |
| reward_orientation      | 0.0264     |
| reward_position         | 0.00703    |
| reward_rotation         | 0.000879   |
| reward_torque           | 0.0255     |
| reward_velocity         | 0.045      |
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 57.4       |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 3          |
|    time_elapsed         | 14         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.56645393 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.376      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.093     |
|    std                  | 0.356      |
|    value_loss           | 1.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.0815     |
| reward_contact          | 0.0354     |
| reward_ctrl             | 0.016      |
| reward_motion           | -0.0764    |
| reward_orientation      | 0.0293     |
| reward_position         | 0.00668    |
| reward_rotation         | 0.00152    |
| reward_torque           | 0.0274     |
| reward_velocity         | 0.0415     |
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 52.4       |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 4          |
|    time_elapsed         | 20         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.14624476 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.4      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.755      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.355      |
|    value_loss           | 3.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.0839     |
| reward_contact          | 0.0389     |
| reward_ctrl             | 0.0173     |
| reward_motion           | -0.0825    |
| reward_orientation      | 0.0329     |
| reward_position         | 0.00527    |
| reward_rotation         | 0.00156    |
| reward_torque           | 0.0308     |
| reward_velocity         | 0.0397     |
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 64.4       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 5          |
|    time_elapsed         | 25         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.36732396 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.173      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.452      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.35       |
|    value_loss           | 2.32       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 47.87
Saving new best model to rl/out_dir/models/exp72/best_model.zip
----------------------------------------
| reward                  | 0.0891     |
| reward_contact          | 0.0455     |
| reward_ctrl             | 0.0136     |
| reward_motion           | -0.0882    |
| reward_orientation      | 0.0373     |
| reward_position         | 0.00632    |
| reward_rotation         | 0.00255    |
| reward_torque           | 0.0315     |
| reward_velocity         | 0.0405     |
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 47.9       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 6          |
|    time_elapsed         | 29         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.18601578 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.311      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.348      |
|    value_loss           | 1.86       |
----------------------------------------
----------------------------------------
| reward                  | 0.111      |
| reward_contact          | 0.0442     |
| reward_ctrl             | 0.0132     |
| reward_motion           | -0.0692    |
| reward_orientation      | 0.0369     |
| reward_position         | 0.00614    |
| reward_rotation         | 0.00249    |
| reward_torque           | 0.0314     |
| reward_velocity         | 0.0459     |
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 60.7       |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 7          |
|    time_elapsed         | 34         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.19630928 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.799      |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.346      |
|    value_loss           | 2.6        |
----------------------------------------
----------------------------------------
| reward                  | 0.135      |
| reward_contact          | 0.0462     |
| reward_ctrl             | 0.0162     |
| reward_motion           | -0.0558    |
| reward_orientation      | 0.0381     |
| reward_position         | 0.00582    |
| reward_rotation         | 0.00418    |
| reward_torque           | 0.0326     |
| reward_velocity         | 0.0476     |
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 65.9       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 8          |
|    time_elapsed         | 39         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.20898744 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.3      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.444      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.343      |
|    value_loss           | 3.42       |
----------------------------------------
---------------------------------------
| reward                  | 0.134     |
| reward_contact          | 0.0474    |
| reward_ctrl             | 0.0183    |
| reward_motion           | -0.0598   |
| reward_orientation      | 0.0387    |
| reward_position         | 0.00593   |
| reward_rotation         | 0.00478   |
| reward_torque           | 0.034     |
| reward_velocity         | 0.0443    |
| rollout/                |           |
|    ep_len_mean          | 208       |
|    ep_rew_mean          | 72        |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 9         |
|    time_elapsed         | 45        |
|    total_timesteps      | 9216      |
| train/                  |           |
|    approx_kl            | 0.1798244 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.4       |
|    entropy_loss         | -25.2     |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.436     |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.341     |
|    value_loss           | 2.87      |
---------------------------------------
----------------------------------------
| reward                  | 0.135      |
| reward_contact          | 0.0477     |
| reward_ctrl             | 0.0201     |
| reward_motion           | -0.0607    |
| reward_orientation      | 0.0386     |
| reward_position         | 0.0058     |
| reward_rotation         | 0.00468    |
| reward_torque           | 0.0345     |
| reward_velocity         | 0.044      |
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 71.1       |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 10         |
|    time_elapsed         | 51         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.09818892 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26.1      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.34       |
|    value_loss           | 5.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.145      |
| reward_contact          | 0.0476     |
| reward_ctrl             | 0.021      |
| reward_motion           | -0.0545    |
| reward_orientation      | 0.0388     |
| reward_position         | 0.0057     |
| reward_rotation         | 0.0051     |
| reward_torque           | 0.035      |
| reward_velocity         | 0.0461     |
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 76.7       |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 11         |
|    time_elapsed         | 56         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.10969414 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.9      |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.585      |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.339      |
|    value_loss           | 3.51       |
----------------------------------------
Num timesteps: 12000
Best mean reward: 47.87 - Last mean reward per episode: 84.44
Saving new best model to rl/out_dir/models/exp72/best_model.zip
----------------------------------------
| reward                  | 0.154      |
| reward_contact          | 0.0469     |
| reward_ctrl             | 0.0212     |
| reward_motion           | -0.0489    |
| reward_orientation      | 0.0385     |
| reward_position         | 0.00558    |
| reward_rotation         | 0.00522    |
| reward_torque           | 0.0352     |
| reward_velocity         | 0.0499     |
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 84.4       |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 12         |
|    time_elapsed         | 61         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16099128 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0949    |
|    std                  | 0.338      |
|    value_loss           | 5.13       |
----------------------------------------
-----------------------------------------
| reward                  | 0.147       |
| reward_contact          | 0.0465      |
| reward_ctrl             | 0.0202      |
| reward_motion           | -0.0518     |
| reward_orientation      | 0.038       |
| reward_position         | 0.00527     |
| reward_rotation         | 0.00582     |
| reward_torque           | 0.0349      |
| reward_velocity         | 0.0485      |
| rollout/                |             |
|    ep_len_mean          | 245         |
|    ep_rew_mean          | 87.7        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 13          |
|    time_elapsed         | 67          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.098767914 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.4         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.61        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0819     |
|    std                  | 0.337       |
|    value_loss           | 5.51        |
-----------------------------------------
----------------------------------------
| reward                  | 0.145      |
| reward_contact          | 0.0467     |
| reward_ctrl             | 0.0198     |
| reward_motion           | -0.0527    |
| reward_orientation      | 0.0378     |
| reward_position         | 0.00517    |
| reward_rotation         | 0.00572    |
| reward_torque           | 0.0346     |
| reward_velocity         | 0.048      |
| rollout/                |            |
|    ep_len_mean          | 259        |
|    ep_rew_mean          | 94.9       |
| time/                   |            |
|    fps                  | 194        |
|    iterations           | 14         |
|    time_elapsed         | 73         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.14231601 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26.4      |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.11       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.336      |
|    value_loss           | 4.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.144      |
| reward_contact          | 0.0472     |
| reward_ctrl             | 0.02       |
| reward_motion           | -0.0544    |
| reward_orientation      | 0.0372     |
| reward_position         | 0.00499    |
| reward_rotation         | 0.00552    |
| reward_torque           | 0.0349     |
| reward_velocity         | 0.0489     |
| rollout/                |            |
|    ep_len_mean          | 271        |
|    ep_rew_mean          | 101        |
| time/                   |            |
|    fps                  | 193        |
|    iterations           | 15         |
|    time_elapsed         | 79         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.13869265 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.295      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.72       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0932    |
|    std                  | 0.335      |
|    value_loss           | 3.1        |
----------------------------------------
----------------------------------------
| reward                  | 0.144      |
| reward_contact          | 0.0474     |
| reward_ctrl             | 0.0197     |
| reward_motion           | -0.0552    |
| reward_orientation      | 0.037      |
| reward_position         | 0.0049     |
| reward_rotation         | 0.00543    |
| reward_torque           | 0.0347     |
| reward_velocity         | 0.0503     |
| rollout/                |            |
|    ep_len_mean          | 277        |
|    ep_rew_mean          | 105        |
| time/                   |            |
|    fps                  | 192        |
|    iterations           | 16         |
|    time_elapsed         | 85         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.12491295 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26.2      |
|    explained_variance   | -0.0856    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.745      |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.334      |
|    value_loss           | 3.57       |
----------------------------------------
----------------------------------------
| reward                  | 0.142      |
| reward_contact          | 0.0477     |
| reward_ctrl             | 0.0211     |
| reward_motion           | -0.0594    |
| reward_orientation      | 0.0372     |
| reward_position         | 0.00497    |
| reward_rotation         | 0.0065     |
| reward_torque           | 0.0353     |
| reward_velocity         | 0.0482     |
| rollout/                |            |
|    ep_len_mean          | 267        |
|    ep_rew_mean          | 100        |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 17         |
|    time_elapsed         | 91         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.13374323 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -25.8      |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0935    |
|    std                  | 0.333      |
|    value_loss           | 5.62       |
----------------------------------------
