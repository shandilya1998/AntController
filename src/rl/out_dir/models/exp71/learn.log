running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp71/PPO_2
---------------------------------
| reward             | 0.115    |
| reward_contact     | 0.04     |
| reward_ctrl        | 0.0226   |
| reward_motion      | -0.1     |
| reward_orientation | 0.05     |
| reward_position    | 2.87e-06 |
| reward_rotation    | 0.0028   |
| reward_torque      | 0.0443   |
| reward_velocity    | 0.0554   |
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 48.8     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | 0.116     |
| reward_contact          | 0.0525    |
| reward_ctrl             | 0.0299    |
| reward_motion           | -0.1      |
| reward_orientation      | 0.0478    |
| reward_position         | 0.00259   |
| reward_rotation         | 0.00412   |
| reward_torque           | 0.0422    |
| reward_velocity         | 0.0369    |
| rollout/                |           |
|    ep_len_mean          | 249       |
|    ep_rew_mean          | 63.5      |
| time/                   |           |
|    fps                  | 324       |
|    iterations           | 2         |
|    time_elapsed         | 6         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 1.7187345 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.2     |
|    explained_variance   | -0.241    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.241     |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.00708   |
|    std                  | 0.362     |
|    value_loss           | 0.666     |
---------------------------------------
----------------------------------------
| reward                  | 0.111      |
| reward_contact          | 0.0533     |
| reward_ctrl             | 0.0288     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0449     |
| reward_position         | 0.0023     |
| reward_rotation         | 0.00628    |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0328     |
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 59.3       |
| time/                   |            |
|    fps                  | 300        |
|    iterations           | 3          |
|    time_elapsed         | 10         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.27637875 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.389      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.427      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.36       |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.0996     |
| reward_contact          | 0.053      |
| reward_ctrl             | 0.0212     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0444     |
| reward_position         | 0.00493    |
| reward_rotation         | 0.00958    |
| reward_torque           | 0.0395     |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 69.4       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 4          |
|    time_elapsed         | 14         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.14225076 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.372      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.357      |
|    value_loss           | 1.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.093      |
| reward_contact          | 0.0508     |
| reward_ctrl             | 0.0197     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0445     |
| reward_position         | 0.00455    |
| reward_rotation         | 0.0144     |
| reward_torque           | 0.0391     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 53.2       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 5          |
|    time_elapsed         | 17         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.17105049 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.315      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.355      |
|    value_loss           | 1.71       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 54.66
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.0922     |
| reward_contact          | 0.0512     |
| reward_ctrl             | 0.02       |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0442     |
| reward_position         | 0.00438    |
| reward_rotation         | 0.014      |
| reward_torque           | 0.0393     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 54.7       |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.28671187 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.234      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.353      |
|    value_loss           | 1.74       |
----------------------------------------
--------------------------------------
| reward                  | 0.125    |
| reward_contact          | 0.0525   |
| reward_ctrl             | 0.023    |
| reward_motion           | -0.0822  |
| reward_orientation      | 0.0432   |
| reward_position         | 0.00793  |
| reward_rotation         | 0.0136   |
| reward_torque           | 0.04     |
| reward_velocity         | 0.0271   |
| rollout/                |          |
|    ep_len_mean          | 214      |
|    ep_rew_mean          | 63.4     |
| time/                   |          |
|    fps                  | 280      |
|    iterations           | 7        |
|    time_elapsed         | 25       |
|    total_timesteps      | 7168     |
| train/                  |          |
|    approx_kl            | 0.515725 |
|    clip_fraction        | 0.393    |
|    clip_range           | 0.4      |
|    entropy_loss         | -24.4    |
|    explained_variance   | -0.258   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.651    |
|    n_updates            | 120      |
|    policy_gradient_loss | -0.125   |
|    std                  | 0.351    |
|    value_loss           | 2.33     |
--------------------------------------
----------------------------------------
| reward                  | 0.124      |
| reward_contact          | 0.0517     |
| reward_ctrl             | 0.0217     |
| reward_motion           | -0.0833    |
| reward_orientation      | 0.0424     |
| reward_position         | 0.00746    |
| reward_rotation         | 0.0129     |
| reward_torque           | 0.0391     |
| reward_velocity         | 0.0316     |
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 73.7       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 8          |
|    time_elapsed         | 29         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.15744084 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.8      |
|    explained_variance   | 0.29       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.922      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.349      |
|    value_loss           | 2.79       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp71/PPO_3
---------------------------------
| reward             | -0.0412  |
| reward_contact     | 0.0121   |
| reward_ctrl        | 0.00573  |
| reward_motion      | -0.1     |
| reward_orientation | 0.00728  |
| reward_position    | 1.34e-10 |
| reward_rotation    | 0.00887  |
| reward_torque      | 0.00848  |
| reward_velocity    | 0.0163   |
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | -0.0412   |
| reward_contact          | 0.0121    |
| reward_ctrl             | 0.00573   |
| reward_motion           | -0.1      |
| reward_orientation      | 0.00728   |
| reward_position         | 1.34e-10  |
| reward_rotation         | 0.00887   |
| reward_torque           | 0.00848   |
| reward_velocity         | 0.0163    |
| rollout/                |           |
|    ep_len_mean          | 255       |
|    ep_rew_mean          | 55.1      |
| time/                   |           |
|    fps                  | 323       |
|    iterations           | 2         |
|    time_elapsed         | 6         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 2.8188975 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.7     |
|    explained_variance   | -0.139    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.298     |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.00798  |
|    std                  | 0.361     |
|    value_loss           | 0.483     |
---------------------------------------
---------------------------------------
| reward                  | 0.0683    |
| reward_contact          | 0.0183    |
| reward_ctrl             | 0.00873   |
| reward_motion           | -0.0361   |
| reward_orientation      | 0.0137    |
| reward_position         | 0.00379   |
| reward_rotation         | 0.0062    |
| reward_torque           | 0.0158    |
| reward_velocity         | 0.0379    |
| rollout/                |           |
|    ep_len_mean          | 231       |
|    ep_rew_mean          | 49.7      |
| time/                   |           |
|    fps                  | 302       |
|    iterations           | 3         |
|    time_elapsed         | 10        |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 1.0778701 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | -0.0824   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.402     |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.356     |
|    value_loss           | 1.97      |
---------------------------------------
---------------------------------------
| reward                  | 0.119     |
| reward_contact          | 0.0343    |
| reward_ctrl             | 0.0178    |
| reward_motion           | -0.0315   |
| reward_orientation      | 0.0271    |
| reward_position         | 0.00324   |
| reward_rotation         | 0.00608   |
| reward_torque           | 0.0261    |
| reward_velocity         | 0.0358    |
| rollout/                |           |
|    ep_len_mean          | 205       |
|    ep_rew_mean          | 57.2      |
| time/                   |           |
|    fps                  | 294       |
|    iterations           | 4         |
|    time_elapsed         | 13        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.2924347 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.6     |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.562     |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0836   |
|    std                  | 0.351     |
|    value_loss           | 2.76      |
---------------------------------------
---------------------------------------
| reward                  | 0.127     |
| reward_contact          | 0.0367    |
| reward_ctrl             | 0.0204    |
| reward_motion           | -0.0383   |
| reward_orientation      | 0.0283    |
| reward_position         | 0.00292   |
| reward_rotation         | 0.00559   |
| reward_torque           | 0.028     |
| reward_velocity         | 0.043     |
| rollout/                |           |
|    ep_len_mean          | 238       |
|    ep_rew_mean          | 72.7      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 5         |
|    time_elapsed         | 17        |
|    total_timesteps      | 5120      |
| train/                  |           |
|    approx_kl            | 0.2526486 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.8     |
|    explained_variance   | 0.305     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.423     |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.115    |
|    std                  | 0.349     |
|    value_loss           | 2.02      |
---------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 85.61
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.118      |
| reward_contact          | 0.0391     |
| reward_ctrl             | 0.023      |
| reward_motion           | -0.0507    |
| reward_orientation      | 0.0317     |
| reward_position         | 0.00244    |
| reward_rotation         | 0.005      |
| reward_torque           | 0.0315     |
| reward_velocity         | 0.0359     |
| rollout/                |            |
|    ep_len_mean          | 243        |
|    ep_rew_mean          | 80.3       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.15626642 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.5      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.566      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.346      |
|    value_loss           | 2.9        |
----------------------------------------
----------------------------------------
| reward                  | 0.116      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0221     |
| reward_motion           | -0.0526    |
| reward_orientation      | 0.0321     |
| reward_position         | 0.00234    |
| reward_rotation         | 0.0048     |
| reward_torque           | 0.0313     |
| reward_velocity         | 0.0364     |
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 78.1       |
| time/                   |            |
|    fps                  | 280        |
|    iterations           | 7          |
|    time_elapsed         | 25         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.14347634 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26        |
|    explained_variance   | 0.309      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.681      |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.345      |
|    value_loss           | 2.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.114      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0208     |
| reward_motion           | -0.0575    |
| reward_orientation      | 0.0324     |
| reward_position         | 0.00366    |
| reward_rotation         | 0.00431    |
| reward_torque           | 0.0318     |
| reward_velocity         | 0.0386     |
| rollout/                |            |
|    ep_len_mean          | 259        |
|    ep_rew_mean          | 90.5       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 8          |
|    time_elapsed         | 29         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.17489353 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -28        |
|    explained_variance   | 0.0555     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.345      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.344      |
|    value_loss           | 2.44       |
----------------------------------------
---------------------------------------
| reward                  | 0.141     |
| reward_contact          | 0.0387    |
| reward_ctrl             | 0.0203    |
| reward_motion           | -0.0357   |
| reward_orientation      | 0.032     |
| reward_position         | 0.00354   |
| reward_rotation         | 0.00418   |
| reward_torque           | 0.032     |
| reward_velocity         | 0.0465    |
| rollout/                |           |
|    ep_len_mean          | 285       |
|    ep_rew_mean          | 103       |
| time/                   |           |
|    fps                  | 274       |
|    iterations           | 9         |
|    time_elapsed         | 33        |
|    total_timesteps      | 9216      |
| train/                  |           |
|    approx_kl            | 0.1429543 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.4       |
|    entropy_loss         | -25.4     |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.405     |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0908   |
|    std                  | 0.342     |
|    value_loss           | 2.18      |
---------------------------------------
----------------------------------------
| reward                  | 0.143      |
| reward_contact          | 0.0394     |
| reward_ctrl             | 0.0198     |
| reward_motion           | -0.0378    |
| reward_orientation      | 0.0317     |
| reward_position         | 0.00343    |
| reward_rotation         | 0.00404    |
| reward_torque           | 0.0321     |
| reward_velocity         | 0.0507     |
| rollout/                |            |
|    ep_len_mean          | 309        |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 272        |
|    iterations           | 10         |
|    time_elapsed         | 37         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.12721698 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26.4      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.437      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0918    |
|    std                  | 0.341      |
|    value_loss           | 2.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.161      |
| reward_contact          | 0.0382     |
| reward_ctrl             | 0.0195     |
| reward_motion           | -0.0226    |
| reward_orientation      | 0.0314     |
| reward_position         | 0.00332    |
| reward_rotation         | 0.00416    |
| reward_torque           | 0.0323     |
| reward_velocity         | 0.055      |
| rollout/                |            |
|    ep_len_mean          | 331        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 271        |
|    iterations           | 11         |
|    time_elapsed         | 41         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08720869 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -27.4      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.34       |
|    value_loss           | 3.86       |
----------------------------------------
Num timesteps: 12000
Best mean reward: 85.61 - Last mean reward per episode: 131.42
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.175      |
| reward_contact          | 0.0387     |
| reward_ctrl             | 0.0221     |
| reward_motion           | -0.0246    |
| reward_orientation      | 0.0307     |
| reward_position         | 0.00304    |
| reward_rotation         | 0.0058     |
| reward_torque           | 0.033      |
| reward_velocity         | 0.0658     |
| rollout/                |            |
|    ep_len_mean          | 339        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 12         |
|    time_elapsed         | 45         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.12414294 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.503      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.339      |
|    value_loss           | 3.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.166       |
| reward_contact          | 0.0409      |
| reward_ctrl             | 0.0212      |
| reward_motion           | -0.0372     |
| reward_orientation      | 0.034       |
| reward_position         | 0.00304     |
| reward_rotation         | 0.00735     |
| reward_torque           | 0.0339      |
| reward_velocity         | 0.0628      |
| rollout/                |             |
|    ep_len_mean          | 317         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 13          |
|    time_elapsed         | 49          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.092851505 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.597       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.095      |
|    std                  | 0.339       |
|    value_loss           | 3.45        |
-----------------------------------------
----------------------------------------
| reward                  | 0.166      |
| reward_contact          | 0.0409     |
| reward_ctrl             | 0.0212     |
| reward_motion           | -0.0372    |
| reward_orientation      | 0.034      |
| reward_position         | 0.00304    |
| reward_rotation         | 0.00735    |
| reward_torque           | 0.0339     |
| reward_velocity         | 0.0628     |
| rollout/                |            |
|    ep_len_mean          | 333        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 14         |
|    time_elapsed         | 53         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.06355721 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.98       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0817    |
|    std                  | 0.339      |
|    value_loss           | 8.61       |
----------------------------------------
----------------------------------------
| reward                  | 0.177      |
| reward_contact          | 0.0432     |
| reward_ctrl             | 0.0224     |
| reward_motion           | -0.0313    |
| reward_orientation      | 0.035      |
| reward_position         | 0.00376    |
| reward_rotation         | 0.00661    |
| reward_torque           | 0.0352     |
| reward_velocity         | 0.0616     |
| rollout/                |            |
|    ep_len_mean          | 306        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 266        |
|    iterations           | 15         |
|    time_elapsed         | 57         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.12882093 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.5      |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.337      |
|    value_loss           | 3.81       |
----------------------------------------
----------------------------------------
| reward                  | 0.181      |
| reward_contact          | 0.0434     |
| reward_ctrl             | 0.0235     |
| reward_motion           | -0.0318    |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00356    |
| reward_rotation         | 0.00791    |
| reward_torque           | 0.0358     |
| reward_velocity         | 0.0635     |
| rollout/                |            |
|    ep_len_mean          | 310        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 265        |
|    iterations           | 16         |
|    time_elapsed         | 61         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.12712355 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.337      |
|    value_loss           | 4.85       |
----------------------------------------
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0437     |
| reward_ctrl             | 0.0235     |
| reward_motion           | -0.0212    |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00349    |
| reward_rotation         | 0.00782    |
| reward_torque           | 0.0359     |
| reward_velocity         | 0.0669     |
| rollout/                |            |
|    ep_len_mean          | 324        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 17         |
|    time_elapsed         | 65         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.10581872 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.2      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.11       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0945    |
|    std                  | 0.336      |
|    value_loss           | 6.5        |
----------------------------------------
Num timesteps: 18000
Best mean reward: 131.42 - Last mean reward per episode: 136.08
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.205       |
| reward_contact          | 0.044       |
| reward_ctrl             | 0.0272      |
| reward_motion           | -0.0243     |
| reward_orientation      | 0.0344      |
| reward_position         | 0.00325     |
| reward_rotation         | 0.0146      |
| reward_torque           | 0.037       |
| reward_velocity         | 0.0692      |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 264         |
|    iterations           | 18          |
|    time_elapsed         | 69          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.121353015 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.569       |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0961     |
|    std                  | 0.336       |
|    value_loss           | 3.57        |
-----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0443     |
| reward_ctrl             | 0.0269     |
| reward_motion           | -0.0256    |
| reward_orientation      | 0.0343     |
| reward_position         | 0.00319    |
| reward_rotation         | 0.0144     |
| reward_torque           | 0.0371     |
| reward_velocity         | 0.0703     |
| rollout/                |            |
|    ep_len_mean          | 335        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 263        |
|    iterations           | 19         |
|    time_elapsed         | 73         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.12110752 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.4      |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.335      |
|    value_loss           | 4.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0449     |
| reward_ctrl             | 0.0263     |
| reward_motion           | -0.0282    |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00356    |
| reward_rotation         | 0.0142     |
| reward_torque           | 0.0369     |
| reward_velocity         | 0.068      |
| rollout/                |            |
|    ep_len_mean          | 331        |
|    ep_rew_mean          | 135        |
| time/                   |            |
|    fps                  | 263        |
|    iterations           | 20         |
|    time_elapsed         | 77         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.09075759 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.88       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0728    |
|    std                  | 0.334      |
|    value_loss           | 3.04       |
----------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0462     |
| reward_ctrl             | 0.0245     |
| reward_motion           | -0.0313    |
| reward_orientation      | 0.036      |
| reward_position         | 0.00845    |
| reward_rotation         | 0.0143     |
| reward_torque           | 0.0369     |
| reward_velocity         | 0.0645     |
| rollout/                |            |
|    ep_len_mean          | 308        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 21         |
|    time_elapsed         | 81         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.10482955 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.7      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.818      |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0998    |
|    std                  | 0.333      |
|    value_loss           | 3.54       |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0456     |
| reward_ctrl             | 0.0247     |
| reward_motion           | -0.0252    |
| reward_orientation      | 0.0358     |
| reward_position         | 0.00833    |
| reward_rotation         | 0.0143     |
| reward_torque           | 0.0371     |
| reward_velocity         | 0.0671     |
| rollout/                |            |
|    ep_len_mean          | 318        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 22         |
|    time_elapsed         | 85         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07880996 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.7      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.83       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.333      |
|    value_loss           | 11         |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0458     |
| reward_ctrl             | 0.0251     |
| reward_motion           | -0.0262    |
| reward_orientation      | 0.036      |
| reward_position         | 0.00821    |
| reward_rotation         | 0.0141     |
| reward_torque           | 0.0373     |
| reward_velocity         | 0.0662     |
| rollout/                |            |
|    ep_len_mean          | 318        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 23         |
|    time_elapsed         | 89         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.14121073 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30        |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.745      |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.332      |
|    value_loss           | 2.78       |
----------------------------------------
Num timesteps: 24000
Best mean reward: 136.08 - Last mean reward per episode: 136.50
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0454     |
| reward_ctrl             | 0.0258     |
| reward_motion           | -0.0243    |
| reward_orientation      | 0.036      |
| reward_position         | 0.0085     |
| reward_rotation         | 0.014      |
| reward_torque           | 0.0376     |
| reward_velocity         | 0.0684     |
| rollout/                |            |
|    ep_len_mean          | 324        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 24         |
|    time_elapsed         | 93         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.13927178 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.8      |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.475      |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.33       |
|    value_loss           | 2.7        |
----------------------------------------
-----------------------------------------
| reward                  | 0.21        |
| reward_contact          | 0.0448      |
| reward_ctrl             | 0.0255      |
| reward_motion           | -0.0253     |
| reward_orientation      | 0.0359      |
| reward_position         | 0.00839     |
| reward_rotation         | 0.0138      |
| reward_torque           | 0.0376      |
| reward_velocity         | 0.0693      |
| rollout/                |             |
|    ep_len_mean          | 334         |
|    ep_rew_mean          | 143         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 25          |
|    time_elapsed         | 97          |
|    total_timesteps      | 25600       |
| train/                  |             |
|    approx_kl            | 0.089158475 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.793       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.329       |
|    value_loss           | 3.57        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.22        |
| reward_contact          | 0.0442      |
| reward_ctrl             | 0.0257      |
| reward_motion           | -0.0187     |
| reward_orientation      | 0.0357      |
| reward_position         | 0.00827     |
| reward_rotation         | 0.0158      |
| reward_torque           | 0.0378      |
| reward_velocity         | 0.0709      |
| rollout/                |             |
|    ep_len_mean          | 343         |
|    ep_rew_mean          | 148         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 26          |
|    time_elapsed         | 101         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.104831964 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.663       |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0915     |
|    std                  | 0.328       |
|    value_loss           | 4.46        |
-----------------------------------------
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.0437     |
| reward_ctrl             | 0.0267     |
| reward_motion           | -0.0169    |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00817    |
| reward_rotation         | 0.0179     |
| reward_torque           | 0.038      |
| reward_velocity         | 0.0738     |
| rollout/                |            |
|    ep_len_mean          | 352        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 27         |
|    time_elapsed         | 105        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.13401636 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.9      |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.798      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.327      |
|    value_loss           | 3.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0435     |
| reward_ctrl             | 0.0268     |
| reward_motion           | -0.019     |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00866    |
| reward_rotation         | 0.0174     |
| reward_torque           | 0.0382     |
| reward_velocity         | 0.0765     |
| rollout/                |            |
|    ep_len_mean          | 357        |
|    ep_rew_mean          | 158        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 28         |
|    time_elapsed         | 109        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.09000303 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.631      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0919    |
|    std                  | 0.326      |
|    value_loss           | 4.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.231      |
| reward_contact          | 0.0434     |
| reward_ctrl             | 0.0263     |
| reward_motion           | -0.0155    |
| reward_orientation      | 0.0358     |
| reward_position         | 0.00834    |
| reward_rotation         | 0.0178     |
| reward_torque           | 0.0383     |
| reward_velocity         | 0.0768     |
| rollout/                |            |
|    ep_len_mean          | 362        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 29         |
|    time_elapsed         | 113        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.10544984 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32        |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0948    |
|    std                  | 0.325      |
|    value_loss           | 5.86       |
----------------------------------------
Num timesteps: 30000
Best mean reward: 136.50 - Last mean reward per episode: 161.07
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.231      |
| reward_contact          | 0.043      |
| reward_ctrl             | 0.0267     |
| reward_motion           | -0.0165    |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00823    |
| reward_rotation         | 0.0176     |
| reward_torque           | 0.0385     |
| reward_velocity         | 0.0777     |
| rollout/                |            |
|    ep_len_mean          | 370        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 30         |
|    time_elapsed         | 117        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.09660232 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.72       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.324      |
|    value_loss           | 6.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0268     |
| reward_motion           | -0.0128    |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00813    |
| reward_rotation         | 0.0177     |
| reward_torque           | 0.0386     |
| reward_velocity         | 0.08       |
| rollout/                |            |
|    ep_len_mean          | 378        |
|    ep_rew_mean          | 170        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 31         |
|    time_elapsed         | 121        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.09786753 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.324      |
|    value_loss           | 7.55       |
----------------------------------------
-----------------------------------------
| reward                  | 0.232       |
| reward_contact          | 0.0431      |
| reward_ctrl             | 0.0269      |
| reward_motion           | -0.0158     |
| reward_orientation      | 0.0357      |
| reward_position         | 0.00785     |
| reward_rotation         | 0.0174      |
| reward_torque           | 0.0387      |
| reward_velocity         | 0.0779      |
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 168         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 32          |
|    time_elapsed         | 125         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.072510794 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -31.6       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0984     |
|    std                  | 0.324       |
|    value_loss           | 7.45        |
-----------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.0433    |
| reward_ctrl             | 0.0267    |
| reward_motion           | -0.0168   |
| reward_orientation      | 0.0357    |
| reward_position         | 0.00776   |
| reward_rotation         | 0.0172    |
| reward_torque           | 0.0387    |
| reward_velocity         | 0.0771    |
| rollout/                |           |
|    ep_len_mean          | 383       |
|    ep_rew_mean          | 173       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 33        |
|    time_elapsed         | 128       |
|    total_timesteps      | 33792     |
| train/                  |           |
|    approx_kl            | 0.1286506 |
|    clip_fraction        | 0.274     |
|    clip_range           | 0.4       |
|    entropy_loss         | -31.3     |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.13      |
|    n_updates            | 640       |
|    policy_gradient_loss | -0.0787   |
|    std                  | 0.323     |
|    value_loss           | 15.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.239      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0273     |
| reward_motion           | -0.012     |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00759    |
| reward_rotation         | 0.0181     |
| reward_torque           | 0.0388     |
| reward_velocity         | 0.0811     |
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 178        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 34         |
|    time_elapsed         | 132        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.17250113 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.322      |
|    value_loss           | 2.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.244      |
| reward_contact          | 0.0421     |
| reward_ctrl             | 0.0281     |
| reward_motion           | -0.0106    |
| reward_orientation      | 0.0353     |
| reward_position         | 0.0075     |
| reward_rotation         | 0.0198     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0831     |
| rollout/                |            |
|    ep_len_mean          | 394        |
|    ep_rew_mean          | 180        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 35         |
|    time_elapsed         | 136        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.14800394 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.2      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0852    |
|    std                  | 0.321      |
|    value_loss           | 4.1        |
----------------------------------------
Num timesteps: 36000
Best mean reward: 161.07 - Last mean reward per episode: 178.21
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.242      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0275     |
| reward_motion           | -0.0126    |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00856    |
| reward_rotation         | 0.0194     |
| reward_torque           | 0.0387     |
| reward_velocity         | 0.0822     |
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 178        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 36         |
|    time_elapsed         | 141        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.10492569 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.937      |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.321      |
|    value_loss           | 4.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.243      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0287     |
| reward_motion           | -0.0135    |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00847    |
| reward_rotation         | 0.0193     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0832     |
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | 183        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 37         |
|    time_elapsed         | 145        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.07338588 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.9      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.563      |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.319      |
|    value_loss           | 3.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.248      |
| reward_contact          | 0.0423     |
| reward_ctrl             | 0.0287     |
| reward_motion           | -0.00964   |
| reward_orientation      | 0.0351     |
| reward_position         | 0.00838    |
| reward_rotation         | 0.0191     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0851     |
| rollout/                |            |
|    ep_len_mean          | 403        |
|    ep_rew_mean          | 187        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 38         |
|    time_elapsed         | 149        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.08345885 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | -0.157     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0952    |
|    std                  | 0.319      |
|    value_loss           | 5.51       |
----------------------------------------
----------------------------------------
| reward                  | 0.252      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0285     |
| reward_motion           | -0.00593   |
| reward_orientation      | 0.035      |
| reward_position         | 0.00829    |
| reward_rotation         | 0.0189     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0868     |
| rollout/                |            |
|    ep_len_mean          | 410        |
|    ep_rew_mean          | 191        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 39         |
|    time_elapsed         | 153        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.13997321 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.4      |
|    explained_variance   | 0.381      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.318      |
|    value_loss           | 4.07       |
----------------------------------------
-----------------------------------------
| reward                  | 0.251       |
| reward_contact          | 0.0415      |
| reward_ctrl             | 0.0283      |
| reward_motion           | -0.00691    |
| reward_orientation      | 0.0349      |
| reward_position         | 0.0082      |
| reward_rotation         | 0.0187      |
| reward_torque           | 0.0391      |
| reward_velocity         | 0.0874      |
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 40          |
|    time_elapsed         | 157         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.122116044 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.4         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.338       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.103      |
|    std                  | 0.317       |
|    value_loss           | 2.23        |
-----------------------------------------
----------------------------------------
| reward                  | 0.257      |
| reward_contact          | 0.0412     |
| reward_ctrl             | 0.0295     |
| reward_motion           | -0.00475   |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00812    |
| reward_rotation         | 0.0198     |
| reward_torque           | 0.0393     |
| reward_velocity         | 0.0892     |
| rollout/                |            |
|    ep_len_mean          | 422        |
|    ep_rew_mean          | 201        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 41         |
|    time_elapsed         | 161        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.14308947 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.8      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0968    |
|    std                  | 0.315      |
|    value_loss           | 1.76       |
----------------------------------------
Num timesteps: 42000
Best mean reward: 178.21 - Last mean reward per episode: 201.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.256       |
| reward_contact          | 0.0414      |
| reward_ctrl             | 0.0293      |
| reward_motion           | -0.00572    |
| reward_orientation      | 0.0346      |
| reward_position         | 0.00804     |
| reward_rotation         | 0.0196      |
| reward_torque           | 0.0393      |
| reward_velocity         | 0.0895      |
| rollout/                |             |
|    ep_len_mean          | 429         |
|    ep_rew_mean          | 205         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 42          |
|    time_elapsed         | 165         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.096792825 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.951       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0869     |
|    std                  | 0.314       |
|    value_loss           | 5.39        |
-----------------------------------------
--------------------------------------
| reward                  | 0.263    |
| reward_contact          | 0.0415   |
| reward_ctrl             | 0.0296   |
| reward_motion           | 8.66e-05 |
| reward_orientation      | 0.0345   |
| reward_position         | 0.00795  |
| reward_rotation         | 0.0195   |
| reward_torque           | 0.0394   |
| reward_velocity         | 0.0907   |
| rollout/                |          |
|    ep_len_mean          | 435      |
|    ep_rew_mean          | 210      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 43       |
|    time_elapsed         | 169      |
|    total_timesteps      | 44032    |
| train/                  |          |
|    approx_kl            | 0.086929 |
|    clip_fraction        | 0.176    |
|    clip_range           | 0.4      |
|    entropy_loss         | -33.4    |
|    explained_variance   | 0.621    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.409    |
|    n_updates            | 840      |
|    policy_gradient_loss | -0.0838  |
|    std                  | 0.313    |
|    value_loss           | 2.45     |
--------------------------------------
----------------------------------------
| reward                  | 0.269      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0301     |
| reward_motion           | 0.00178    |
| reward_orientation      | 0.0349     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0202     |
| reward_torque           | 0.0398     |
| reward_velocity         | 0.093      |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 217        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 44         |
|    time_elapsed         | 173        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.08752781 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.852      |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0807    |
|    std                  | 0.312      |
|    value_loss           | 4.11       |
----------------------------------------
-----------------------------------------
| reward                  | 0.277       |
| reward_contact          | 0.0423      |
| reward_ctrl             | 0.0302      |
| reward_motion           | 0.00565     |
| reward_orientation      | 0.0351      |
| reward_position         | 0.00788     |
| reward_rotation         | 0.02        |
| reward_torque           | 0.0402      |
| reward_velocity         | 0.0958      |
| rollout/                |             |
|    ep_len_mean          | 455         |
|    ep_rew_mean          | 223         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 45          |
|    time_elapsed         | 177         |
|    total_timesteps      | 46080       |
| train/                  |             |
|    approx_kl            | 0.071741566 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32         |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.25        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0762     |
|    std                  | 0.312       |
|    value_loss           | 9.17        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.286       |
| reward_contact          | 0.0428      |
| reward_ctrl             | 0.0302      |
| reward_motion           | 0.0123      |
| reward_orientation      | 0.0352      |
| reward_position         | 0.00788     |
| reward_rotation         | 0.02        |
| reward_torque           | 0.0405      |
| reward_velocity         | 0.097       |
| rollout/                |             |
|    ep_len_mean          | 461         |
|    ep_rew_mean          | 229         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 46          |
|    time_elapsed         | 181         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.084220745 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32.6       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.635       |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0969     |
|    std                  | 0.312       |
|    value_loss           | 6.41        |
-----------------------------------------
Num timesteps: 48000
Best mean reward: 201.31 - Last mean reward per episode: 235.97
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.295      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0181     |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0203     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0999     |
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | 236        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 47         |
|    time_elapsed         | 184        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.18948185 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.371      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.31       |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.302      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0226     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0203     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.102      |
| rollout/                |            |
|    ep_len_mean          | 479        |
|    ep_rew_mean          | 242        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 48         |
|    time_elapsed         | 188        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.08534669 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.913      |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.309      |
|    value_loss           | 4.86       |
----------------------------------------
----------------------------------------
| reward                  | 0.309      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0.026      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0202     |
| reward_torque           | 0.0413     |
| reward_velocity         | 0.105      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 49         |
|    time_elapsed         | 192        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.10656306 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.1      |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.48       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.309      |
|    value_loss           | 6.39       |
----------------------------------------
----------------------------------------
| reward                  | 0.31       |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0309     |
| reward_motion           | 0.0259     |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0205     |
| reward_torque           | 0.0415     |
| reward_velocity         | 0.105      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 252        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 50         |
|    time_elapsed         | 196        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.08580808 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.3      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.666      |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.309      |
|    value_loss           | 4.69       |
----------------------------------------
----------------------------------------
| reward                  | 0.316      |
| reward_contact          | 0.0422     |
| reward_ctrl             | 0.031      |
| reward_motion           | 0.0276     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0225     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.107      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 258        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 51         |
|    time_elapsed         | 200        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.08360584 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.75       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.309      |
|    value_loss           | 11.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.32       |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0308     |
| reward_motion           | 0.0296     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0238     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.11       |
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | 264        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 52         |
|    time_elapsed         | 204        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.11466862 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.998      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.308      |
|    value_loss           | 5.64       |
----------------------------------------
Num timesteps: 54000
Best mean reward: 235.97 - Last mean reward per episode: 266.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.318      |
| reward_contact          | 0.0414     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0255     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0255     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.11       |
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | 266        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 53         |
|    time_elapsed         | 208        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.14964464 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.307      |
|    value_loss           | 3.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.326      |
| reward_contact          | 0.041      |
| reward_ctrl             | 0.0311     |
| reward_motion           | 0.0301     |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0256     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | 273        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 54         |
|    time_elapsed         | 212        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.09345175 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.89       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.307      |
|    value_loss           | 6.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.333      |
| reward_contact          | 0.0406     |
| reward_ctrl             | 0.0318     |
| reward_motion           | 0.0339     |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0262     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 526        |
|    ep_rew_mean          | 279        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 55         |
|    time_elapsed         | 216        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.10636637 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.7      |
|    explained_variance   | 0.744      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.885      |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.306      |
|    value_loss           | 5.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.34       |
| reward_contact          | 0.0402     |
| reward_ctrl             | 0.0328     |
| reward_motion           | 0.0372     |
| reward_orientation      | 0.0351     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.027      |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.118      |
| rollout/                |            |
|    ep_len_mean          | 536        |
|    ep_rew_mean          | 286        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 56         |
|    time_elapsed         | 220        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.23972933 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.219      |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.306      |
|    value_loss           | 1.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.349      |
| reward_contact          | 0.0403     |
| reward_ctrl             | 0.0333     |
| reward_motion           | 0.0419     |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.0272     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 545        |
|    ep_rew_mean          | 292        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 57         |
|    time_elapsed         | 223        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.08778707 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.6        |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.306      |
|    value_loss           | 5.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.353      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0338     |
| reward_motion           | 0.0438     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.0274     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 553        |
|    ep_rew_mean          | 299        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 58         |
|    time_elapsed         | 227        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.10045493 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.669      |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.305      |
|    value_loss           | 5.44       |
----------------------------------------
Num timesteps: 60000
Best mean reward: 266.31 - Last mean reward per episode: 304.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.352      |
| reward_contact          | 0.0394     |
| reward_ctrl             | 0.0338     |
| reward_motion           | 0.0438     |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00749    |
| reward_rotation         | 0.0273     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 563        |
|    ep_rew_mean          | 305        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 59         |
|    time_elapsed         | 232        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.08165057 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.46       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0773    |
|    std                  | 0.305      |
|    value_loss           | 9.75       |
----------------------------------------
-----------------------------------------
| reward                  | 0.36        |
| reward_contact          | 0.0389      |
| reward_ctrl             | 0.0343      |
| reward_motion           | 0.0494      |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00729     |
| reward_rotation         | 0.0274      |
| reward_torque           | 0.0429      |
| reward_velocity         | 0.126       |
| rollout/                |             |
|    ep_len_mean          | 573         |
|    ep_rew_mean          | 311         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 60          |
|    time_elapsed         | 236         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.098490864 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.836       |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.106      |
|    std                  | 0.305       |
|    value_loss           | 4.06        |
-----------------------------------------
----------------------------------------
| reward                  | 0.367      |
| reward_contact          | 0.0385     |
| reward_ctrl             | 0.0343     |
| reward_motion           | 0.0521     |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00729    |
| reward_rotation         | 0.0288     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.129      |
| rollout/                |            |
|    ep_len_mean          | 583        |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 61         |
|    time_elapsed         | 240        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.08731906 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.304      |
|    value_loss           | 3.86       |
----------------------------------------
---------------------------------------
| reward                  | 0.371     |
| reward_contact          | 0.0379    |
| reward_ctrl             | 0.0336    |
| reward_motion           | 0.0567    |
| reward_orientation      | 0.0339    |
| reward_position         | 0.00729   |
| reward_rotation         | 0.0291    |
| reward_torque           | 0.0427    |
| reward_velocity         | 0.13      |
| rollout/                |           |
|    ep_len_mean          | 583       |
|    ep_rew_mean          | 319       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 62        |
|    time_elapsed         | 244       |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.2156363 |
|    clip_fraction        | 0.248     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.9     |
|    explained_variance   | 0.969     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.554     |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.0813   |
|    std                  | 0.304     |
|    value_loss           | 2.45      |
---------------------------------------
----------------------------------------
| reward                  | 0.38       |
| reward_contact          | 0.0373     |
| reward_ctrl             | 0.0348     |
| reward_motion           | 0.0618     |
| reward_orientation      | 0.034      |
| reward_position         | 0.00729    |
| reward_rotation         | 0.0303     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.131      |
| rollout/                |            |
|    ep_len_mean          | 593        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 63         |
|    time_elapsed         | 247        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.07750149 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.3      |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.26       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0685    |
|    std                  | 0.303      |
|    value_loss           | 7.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.388       |
| reward_contact          | 0.0367      |
| reward_ctrl             | 0.035       |
| reward_motion           | 0.068       |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00729     |
| reward_rotation         | 0.0307      |
| reward_torque           | 0.043       |
| reward_velocity         | 0.134       |
| rollout/                |             |
|    ep_len_mean          | 593         |
|    ep_rew_mean          | 325         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 64          |
|    time_elapsed         | 251         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.059819594 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51        |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0714     |
|    std                  | 0.303       |
|    value_loss           | 7.62        |
-----------------------------------------
Num timesteps: 66000
Best mean reward: 304.99 - Last mean reward per episode: 329.54
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.389      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0356     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0335     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0306     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.134      |
| rollout/                |            |
|    ep_len_mean          | 606        |
|    ep_rew_mean          | 333        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 65         |
|    time_elapsed         | 255        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.09172962 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0932    |
|    std                  | 0.303      |
|    value_loss           | 10.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.393      |
| reward_contact          | 0.0366     |
| reward_ctrl             | 0.036      |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 614        |
|    ep_rew_mean          | 339        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 66         |
|    time_elapsed         | 259        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.07323794 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.15       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.303      |
|    value_loss           | 15.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.393      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0365     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0337     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0432     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 618        |
|    ep_rew_mean          | 341        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 67         |
|    time_elapsed         | 263        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.08804298 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.302      |
|    value_loss           | 9.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.395      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0323     |
| reward_torque           | 0.0435     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 627        |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 68         |
|    time_elapsed         | 267        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.05851388 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.5      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0707    |
|    std                  | 0.302      |
|    value_loss           | 3.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.403      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0387     |
| reward_motion           | 0.0722     |
| reward_orientation      | 0.034      |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0334     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.137      |
| rollout/                |            |
|    ep_len_mean          | 627        |
|    ep_rew_mean          | 348        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 69         |
|    time_elapsed         | 271        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.07668175 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.302      |
|    value_loss           | 7.53       |
----------------------------------------
-----------------------------------------
| reward                  | 0.413       |
| reward_contact          | 0.0361      |
| reward_ctrl             | 0.0388      |
| reward_motion           | 0.0788      |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00723     |
| reward_rotation         | 0.0344      |
| reward_torque           | 0.0439      |
| reward_velocity         | 0.14        |
| rollout/                |             |
|    ep_len_mean          | 635         |
|    ep_rew_mean          | 355         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 70          |
|    time_elapsed         | 275         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.069769934 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0775     |
|    std                  | 0.301       |
|    value_loss           | 3.42        |
-----------------------------------------
Num timesteps: 72000
Best mean reward: 329.54 - Last mean reward per episode: 362.51
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.419      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0402     |
| reward_motion           | 0.0799     |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0441     |
| reward_velocity         | 0.142      |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 363        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 71         |
|    time_elapsed         | 279        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.07062052 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.419      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.22       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0676    |
|    std                  | 0.301      |
|    value_loss           | 19.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.412      |
| reward_contact          | 0.0362     |
| reward_ctrl             | 0.0411     |
| reward_motion           | 0.073      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.139      |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 365        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 72         |
|    time_elapsed         | 283        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.10795828 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.5      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.301      |
|    value_loss           | 18.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.419      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0.0785     |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 73         |
|    time_elapsed         | 286        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.07978987 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.4        |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.301      |
|    value_loss           | 20.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.418     |
| reward_contact          | 0.0357    |
| reward_ctrl             | 0.0422    |
| reward_motion           | 0.0773    |
| reward_orientation      | 0.0345    |
| reward_position         | 0.00681   |
| reward_rotation         | 0.0374    |
| reward_torque           | 0.0446    |
| reward_velocity         | 0.14      |
| rollout/                |           |
|    ep_len_mean          | 645       |
|    ep_rew_mean          | 368       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 74        |
|    time_elapsed         | 290       |
|    total_timesteps      | 75776     |
| train/                  |           |
|    approx_kl            | 0.1111098 |
|    clip_fraction        | 0.257     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.7     |
|    explained_variance   | 0.432     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.32      |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0731   |
|    std                  | 0.301     |
|    value_loss           | 14.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.423      |
| reward_contact          | 0.0356     |
| reward_ctrl             | 0.0422     |
| reward_motion           | 0.0826     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0445     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 370        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 75         |
|    time_elapsed         | 294        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.07241102 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0868    |
|    std                  | 0.301      |
|    value_loss           | 10.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.423       |
| reward_contact          | 0.0356      |
| reward_ctrl             | 0.0423      |
| reward_motion           | 0.0826      |
| reward_orientation      | 0.0348      |
| reward_position         | 0.00681     |
| reward_rotation         | 0.0369      |
| reward_torque           | 0.0447      |
| reward_velocity         | 0.139       |
| rollout/                |             |
|    ep_len_mean          | 653         |
|    ep_rew_mean          | 376         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 76          |
|    time_elapsed         | 298         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.080832265 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.301       |
|    value_loss           | 8.54        |
-----------------------------------------
Num timesteps: 78000
Best mean reward: 362.51 - Last mean reward per episode: 376.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.427      |
| reward_contact          | 0.0351     |
| reward_ctrl             | 0.0419     |
| reward_motion           | 0.0851     |
| reward_orientation      | 0.0348     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0383     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 662        |
|    ep_rew_mean          | 383        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 77         |
|    time_elapsed         | 302        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.05964671 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.02       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.301      |
|    value_loss           | 13.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.436       |
| reward_contact          | 0.0345      |
| reward_ctrl             | 0.0425      |
| reward_motion           | 0.0902      |
| reward_orientation      | 0.0348      |
| reward_position         | 0.00681     |
| reward_rotation         | 0.0393      |
| reward_torque           | 0.0448      |
| reward_velocity         | 0.143       |
| rollout/                |             |
|    ep_len_mean          | 662         |
|    ep_rew_mean          | 386         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 78          |
|    time_elapsed         | 306         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.103833094 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 60.4        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0619     |
|    std                  | 0.301       |
|    value_loss           | 39.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.443      |
| reward_contact          | 0.034      |
| reward_ctrl             | 0.0429     |
| reward_motion           | 0.0968     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.144      |
| rollout/                |            |
|    ep_len_mean          | 672        |
|    ep_rew_mean          | 393        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 79         |
|    time_elapsed         | 310        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.13581583 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0959    |
|    std                  | 0.3        |
|    value_loss           | 11.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.452      |
| reward_contact          | 0.0336     |
| reward_ctrl             | 0.0431     |
| reward_motion           | 0.103      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.147      |
| rollout/                |            |
|    ep_len_mean          | 682        |
|    ep_rew_mean          | 400        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 80         |
|    time_elapsed         | 314        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.06820634 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.236      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.22       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0778    |
|    std                  | 0.3        |
|    value_loss           | 14.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.452      |
| reward_contact          | 0.0342     |
| reward_ctrl             | 0.0431     |
| reward_motion           | 0.103      |
| reward_orientation      | 0.034      |
| reward_position         | 0.0068     |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.146      |
| rollout/                |            |
|    ep_len_mean          | 689        |
|    ep_rew_mean          | 405        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 81         |
|    time_elapsed         | 318        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.13237971 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.3        |
|    value_loss           | 16.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.456      |
| reward_contact          | 0.0336     |
| reward_ctrl             | 0.0428     |
| reward_motion           | 0.107      |
| reward_orientation      | 0.034      |
| reward_position         | 0.0066     |
| reward_rotation         | 0.0394     |
| reward_torque           | 0.0449     |
| reward_velocity         | 0.148      |
| rollout/                |            |
|    ep_len_mean          | 699        |
|    ep_rew_mean          | 412        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 82         |
|    time_elapsed         | 321        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.09671812 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.5      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.993      |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0862    |
|    std                  | 0.3        |
|    value_loss           | 4.85       |
----------------------------------------
Num timesteps: 84000
Best mean reward: 376.31 - Last mean reward per episode: 418.04
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.465      |
| reward_contact          | 0.033      |
| reward_ctrl             | 0.044      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.0395     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.149      |
| rollout/                |            |
|    ep_len_mean          | 707        |
|    ep_rew_mean          | 418        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 83         |
|    time_elapsed         | 325        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.10577983 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.86       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.3        |
|    value_loss           | 8.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.471      |
| reward_contact          | 0.0324     |
| reward_ctrl             | 0.0439     |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0332     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.04       |
| reward_torque           | 0.045      |
| reward_velocity         | 0.15       |
| rollout/                |            |
|    ep_len_mean          | 717        |
|    ep_rew_mean          | 426        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 84         |
|    time_elapsed         | 329        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.07027109 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.9      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.03       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.299      |
|    value_loss           | 3.17       |
----------------------------------------
-----------------------------------------
| reward                  | 0.478       |
| reward_contact          | 0.0318      |
| reward_ctrl             | 0.0446      |
| reward_motion           | 0.121       |
| reward_orientation      | 0.0336      |
| reward_position         | 0.0066      |
| reward_rotation         | 0.0421      |
| reward_torque           | 0.0452      |
| reward_velocity         | 0.152       |
| rollout/                |             |
|    ep_len_mean          | 717         |
|    ep_rew_mean          | 428         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 85          |
|    time_elapsed         | 333         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.067421064 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.299       |
|    value_loss           | 8.93        |
-----------------------------------------
----------------------------------------
| reward                  | 0.486      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0443     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0333     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.043      |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.153      |
| rollout/                |            |
|    ep_len_mean          | 727        |
|    ep_rew_mean          | 436        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 86         |
|    time_elapsed         | 337        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.09245771 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.3      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.89       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.299      |
|    value_loss           | 4.61       |
----------------------------------------
-----------------------------------------
| reward                  | 0.478       |
| reward_contact          | 0.0312      |
| reward_ctrl             | 0.0443      |
| reward_motion           | 0.122       |
| reward_orientation      | 0.0329      |
| reward_position         | 0.00607     |
| reward_rotation         | 0.0438      |
| reward_torque           | 0.0451      |
| reward_velocity         | 0.153       |
| rollout/                |             |
|    ep_len_mean          | 744         |
|    ep_rew_mean          | 448         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 87          |
|    time_elapsed         | 341         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.073994204 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.5       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.736       |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0755     |
|    std                  | 0.298       |
|    value_loss           | 2.74        |
-----------------------------------------
Num timesteps: 90000
Best mean reward: 418.04 - Last mean reward per episode: 455.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.485      |
| reward_contact          | 0.0307     |
| reward_ctrl             | 0.0449     |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0326     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0452     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.156      |
| rollout/                |            |
|    ep_len_mean          | 753        |
|    ep_rew_mean          | 456        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 88         |
|    time_elapsed         | 345        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.07807736 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0761    |
|    std                  | 0.298      |
|    value_loss           | 8.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.493      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.0461     |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0329     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.158      |
| rollout/                |            |
|    ep_len_mean          | 762        |
|    ep_rew_mean          | 464        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 89         |
|    time_elapsed         | 349        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.07425137 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.86       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.298      |
|    value_loss           | 16.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.495      |
| reward_contact          | 0.0299     |
| reward_ctrl             | 0.0466     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0329     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0477     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.158      |
| rollout/                |            |
|    ep_len_mean          | 762        |
|    ep_rew_mean          | 465        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 90         |
|    time_elapsed         | 353        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.07237715 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.83       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0731    |
|    std                  | 0.297      |
|    value_loss           | 15.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.505     |
| reward_contact          | 0.0293    |
| reward_ctrl             | 0.0479    |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0332    |
| reward_position         | 0.00609   |
| reward_rotation         | 0.0495    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.161     |
| rollout/                |           |
|    ep_len_mean          | 771       |
|    ep_rew_mean          | 473       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 91        |
|    time_elapsed         | 357       |
|    total_timesteps      | 93184     |
| train/                  |           |
|    approx_kl            | 0.0687594 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -36.7     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.85      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.0664   |
|    std                  | 0.297     |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.506     |
| reward_contact          | 0.0293    |
| reward_ctrl             | 0.0484    |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0333    |
| reward_position         | 0.00608   |
| reward_rotation         | 0.0496    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.161     |
| rollout/                |           |
|    ep_len_mean          | 780       |
|    ep_rew_mean          | 479       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 92        |
|    time_elapsed         | 361       |
|    total_timesteps      | 94208     |
| train/                  |           |
|    approx_kl            | 0.1301598 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.6     |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.851     |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.0686   |
|    std                  | 0.297     |
|    value_loss           | 3.91      |
---------------------------------------
----------------------------------------
| reward                  | 0.505      |
| reward_contact          | 0.0287     |
| reward_ctrl             | 0.0496     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0519     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.162      |
| rollout/                |            |
|    ep_len_mean          | 780        |
|    ep_rew_mean          | 482        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 93         |
|    time_elapsed         | 365        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.10852234 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.72       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.297      |
|    value_loss           | 5.57       |
----------------------------------------
Num timesteps: 96000
Best mean reward: 455.99 - Last mean reward per episode: 486.21
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.507       |
| reward_contact          | 0.0292      |
| reward_ctrl             | 0.0486      |
| reward_motion           | 0.131       |
| reward_orientation      | 0.0337      |
| reward_position         | 0.00608     |
| reward_rotation         | 0.0513      |
| reward_torque           | 0.0455      |
| reward_velocity         | 0.162       |
| rollout/                |             |
|    ep_len_mean          | 780         |
|    ep_rew_mean          | 486         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 94          |
|    time_elapsed         | 369         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.057091806 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34         |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0496     |
|    std                  | 0.297       |
|    value_loss           | 20.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.515      |
| reward_contact          | 0.0292     |
| reward_ctrl             | 0.0486     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0524     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.162      |
| rollout/                |            |
|    ep_len_mean          | 789        |
|    ep_rew_mean          | 494        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 95         |
|    time_elapsed         | 373        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.07392697 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.7      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.95       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.297      |
|    value_loss           | 10.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.517       |
| reward_contact          | 0.0286      |
| reward_ctrl             | 0.0484      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00608     |
| reward_rotation         | 0.0531      |
| reward_torque           | 0.0455      |
| reward_velocity         | 0.164       |
| rollout/                |             |
|    ep_len_mean          | 798         |
|    ep_rew_mean          | 502         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 96          |
|    time_elapsed         | 378         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.071486704 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.075      |
|    std                  | 0.296       |
|    value_loss           | 4.92        |
-----------------------------------------
----------------------------------------
| reward                  | 0.518      |
| reward_contact          | 0.028      |
| reward_ctrl             | 0.0494     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0515     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.166      |
| rollout/                |            |
|    ep_len_mean          | 805        |
|    ep_rew_mean          | 509        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 97         |
|    time_elapsed         | 382        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.10340509 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.754      |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.296      |
|    value_loss           | 4.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.528      |
| reward_contact          | 0.0274     |
| reward_ctrl             | 0.0512     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0344     |
| reward_position         | 0.00585    |
| reward_rotation         | 0.0526     |
| reward_torque           | 0.0461     |
| reward_velocity         | 0.167      |
| rollout/                |            |
|    ep_len_mean          | 806        |
|    ep_rew_mean          | 514        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 98         |
|    time_elapsed         | 386        |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.16705886 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.307      |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 0.296      |
|    value_loss           | 2.19       |
----------------------------------------
-----------------------------------------
| reward                  | 0.535       |
| reward_contact          | 0.0268      |
| reward_ctrl             | 0.0514      |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0347      |
| reward_position         | 0.00579     |
| reward_rotation         | 0.0529      |
| reward_torque           | 0.0462      |
| reward_velocity         | 0.169       |
| rollout/                |             |
|    ep_len_mean          | 816         |
|    ep_rew_mean          | 522         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 99          |
|    time_elapsed         | 389         |
|    total_timesteps      | 101376      |
| train/                  |             |
|    approx_kl            | 0.046520814 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.3       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 3           |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.296       |
|    value_loss           | 19.5        |
-----------------------------------------
Num timesteps: 102000
Best mean reward: 486.21 - Last mean reward per episode: 522.11
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.528       |
| reward_contact          | 0.0274      |
| reward_ctrl             | 0.0512      |
| reward_motion           | 0.143       |
| reward_orientation      | 0.0349      |
| reward_position         | 0.00579     |
| reward_rotation         | 0.0529      |
| reward_torque           | 0.0461      |
| reward_velocity         | 0.167       |
| rollout/                |             |
|    ep_len_mean          | 815         |
|    ep_rew_mean          | 525         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 100         |
|    time_elapsed         | 393         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.045892596 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0436     |
|    std                  | 0.296       |
|    value_loss           | 34.6        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.534       |
| reward_contact          | 0.0268      |
| reward_ctrl             | 0.0522      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0347      |
| reward_position         | 0.00562     |
| reward_rotation         | 0.0553      |
| reward_torque           | 0.0462      |
| reward_velocity         | 0.17        |
| rollout/                |             |
|    ep_len_mean          | 825         |
|    ep_rew_mean          | 533         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 101         |
|    time_elapsed         | 397         |
|    total_timesteps      | 103424      |
| train/                  |             |
|    approx_kl            | 0.038358767 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.8       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0575     |
|    std                  | 0.296       |
|    value_loss           | 5.2         |
-----------------------------------------
----------------------------------------
| reward                  | 0.543      |
| reward_contact          | 0.0262     |
| reward_ctrl             | 0.0529     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00539    |
| reward_rotation         | 0.0572     |
| reward_torque           | 0.0464     |
| reward_velocity         | 0.172      |
| rollout/                |            |
|    ep_len_mean          | 835        |
|    ep_rew_mean          | 541        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 102        |
|    time_elapsed         | 401        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07268999 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.06       |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.296      |
|    value_loss           | 20.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.542      |
| reward_contact          | 0.0256     |
| reward_ctrl             | 0.0531     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00429    |
| reward_rotation         | 0.0566     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.173      |
| rollout/                |            |
|    ep_len_mean          | 844        |
|    ep_rew_mean          | 549        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 103        |
|    time_elapsed         | 405        |
|    total_timesteps      | 105472     |
| train/                  |            |
|    approx_kl            | 0.10307126 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.221      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.79       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0449    |
|    std                  | 0.296      |
|    value_loss           | 25.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.541      |
| reward_contact          | 0.0256     |
| reward_ctrl             | 0.0535     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.034      |
| reward_position         | 0.00323    |
| reward_rotation         | 0.0566     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.173      |
| rollout/                |            |
|    ep_len_mean          | 852        |
|    ep_rew_mean          | 556        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 104        |
|    time_elapsed         | 410        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.10403752 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.37       |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.296      |
|    value_loss           | 4.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.545      |
| reward_contact          | 0.0251     |
| reward_ctrl             | 0.0549     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0568     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.175      |
| rollout/                |            |
|    ep_len_mean          | 862        |
|    ep_rew_mean          | 562        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 105        |
|    time_elapsed         | 414        |
|    total_timesteps      | 107520     |
| train/                  |            |
|    approx_kl            | 0.06929568 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0874    |
|    std                  | 0.296      |
|    value_loss           | 6.98       |
----------------------------------------
Num timesteps: 108000
Best mean reward: 522.11 - Last mean reward per episode: 569.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.545       |
| reward_contact          | 0.0246      |
| reward_ctrl             | 0.0556      |
| reward_motion           | 0.151       |
| reward_orientation      | 0.0338      |
| reward_position         | 0.00207     |
| reward_rotation         | 0.0563      |
| reward_torque           | 0.0472      |
| reward_velocity         | 0.175       |
| rollout/                |             |
|    ep_len_mean          | 871         |
|    ep_rew_mean          | 569         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 106         |
|    time_elapsed         | 418         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.084463835 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.54        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.116      |
|    std                  | 0.296       |
|    value_loss           | 11.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.552      |
| reward_contact          | 0.0242     |
| reward_ctrl             | 0.057      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0578     |
| reward_torque           | 0.0475     |
| reward_velocity         | 0.176      |
| rollout/                |            |
|    ep_len_mean          | 881        |
|    ep_rew_mean          | 577        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 107        |
|    time_elapsed         | 422        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.15689455 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.69       |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.296      |
|    value_loss           | 14.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.56       |
| reward_contact          | 0.0236     |
| reward_ctrl             | 0.0585     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.034      |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0602     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.179      |
| rollout/                |            |
|    ep_len_mean          | 889        |
|    ep_rew_mean          | 585        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 108        |
|    time_elapsed         | 426        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.12359036 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.907      |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0778    |
|    std                  | 0.296      |
|    value_loss           | 4.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.558      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0589     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0346     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0615     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.18       |
| rollout/                |            |
|    ep_len_mean          | 892        |
|    ep_rew_mean          | 591        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 109        |
|    time_elapsed         | 429        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.06287424 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.55       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0593    |
|    std                  | 0.296      |
|    value_loss           | 11.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.566      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0596     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0616     |
| reward_torque           | 0.0479     |
| reward_velocity         | 0.182      |
| rollout/                |            |
|    ep_len_mean          | 898        |
|    ep_rew_mean          | 598        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 110        |
|    time_elapsed         | 433        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.05106736 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.8      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.27       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.295      |
|    value_loss           | 28.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.567       |
| reward_contact          | 0.0235      |
| reward_ctrl             | 0.0604      |
| reward_motion           | 0.153       |
| reward_orientation      | 0.035       |
| reward_position         | 0.00207     |
| reward_rotation         | 0.0637      |
| reward_torque           | 0.048       |
| reward_velocity         | 0.181       |
| rollout/                |             |
|    ep_len_mean          | 898         |
|    ep_rew_mean          | 600         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 111         |
|    time_elapsed         | 437         |
|    total_timesteps      | 113664      |
| train/                  |             |
|    approx_kl            | 0.085767195 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.41        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.295       |
|    value_loss           | 13.3        |
-----------------------------------------
Num timesteps: 114000
Best mean reward: 569.26 - Last mean reward per episode: 600.00
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.575      |
| reward_contact          | 0.0229     |
| reward_ctrl             | 0.0613     |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0646     |
| reward_torque           | 0.048      |
| reward_velocity         | 0.183      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 112        |
|    time_elapsed         | 441        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.12279582 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.883      |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.295      |
|    value_loss           | 4.73       |
----------------------------------------
----------------------------------------
| reward                  | 0.583      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0624     |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0348     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0654     |
| reward_torque           | 0.0482     |
| reward_velocity         | 0.185      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 610        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 113        |
|    time_elapsed         | 445        |
|    total_timesteps      | 115712     |
| train/                  |            |
|    approx_kl            | 0.15132055 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.295      |
|    value_loss           | 9.3        |
----------------------------------------
----------------------------------------
| reward                  | 0.583      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.063      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.035      |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0649     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.185      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 612        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 114        |
|    time_elapsed         | 449        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.07537727 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0585    |
|    std                  | 0.295      |
|    value_loss           | 16.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.577      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0632     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0632     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.182      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 613        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 115        |
|    time_elapsed         | 453        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.06908874 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0493    |
|    std                  | 0.294      |
|    value_loss           | 8.22       |
----------------------------------------
----------------------------------------
| reward                  | 0.581      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.063      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0647     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.181      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 116        |
|    time_elapsed         | 457        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.14941782 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.294      |
|    value_loss           | 4.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.581      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0644     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.067      |
| reward_torque           | 0.0485     |
| reward_velocity         | 0.181      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 617        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 117        |
|    time_elapsed         | 460        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.15306525 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.4      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0946    |
|    std                  | 0.294      |
|    value_loss           | 13.5       |
----------------------------------------
Num timesteps: 120000
Best mean reward: 600.00 - Last mean reward per episode: 617.14
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.589      |
| reward_contact          | 0.0228     |
| reward_ctrl             | 0.0656     |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0686     |
| reward_torque           | 0.0487     |
| reward_velocity         | 0.184      |
| rollout/                |            |
|    ep_len_mean          | 914        |
|    ep_rew_mean          | 625        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 118        |
|    time_elapsed         | 464        |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.07449144 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0712    |
|    std                  | 0.294      |
|    value_loss           | 13.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.595       |
| reward_contact          | 0.0222      |
| reward_ctrl             | 0.0668      |
| reward_motion           | 0.163       |
| reward_orientation      | 0.0355      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0701      |
| reward_torque           | 0.0489      |
| reward_velocity         | 0.187       |
| rollout/                |             |
|    ep_len_mean          | 922         |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 119         |
|    time_elapsed         | 468         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.089235544 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.294       |
|    value_loss           | 28.8        |
-----------------------------------------
---------------------------------------
| reward                  | 0.601     |
| reward_contact          | 0.0221    |
| reward_ctrl             | 0.0677    |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0357    |
| reward_position         | 0.00119   |
| reward_rotation         | 0.0724    |
| reward_torque           | 0.049     |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 922       |
|    ep_rew_mean          | 635       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 120       |
|    time_elapsed         | 472       |
|    total_timesteps      | 122880    |
| train/                  |           |
|    approx_kl            | 0.1289843 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.4     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.47      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -0.0779   |
|    std                  | 0.294     |
|    value_loss           | 15.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.603      |
| reward_contact          | 0.0221     |
| reward_ctrl             | 0.0688     |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0744     |
| reward_torque           | 0.0491     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 922        |
|    ep_rew_mean          | 638        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 121        |
|    time_elapsed         | 476        |
|    total_timesteps      | 123904     |
| train/                  |            |
|    approx_kl            | 0.10008621 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.823      |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.293      |
|    value_loss           | 3.22       |
----------------------------------------
-----------------------------------------
| reward                  | 0.609       |
| reward_contact          | 0.0215      |
| reward_ctrl             | 0.0702      |
| reward_motion           | 0.165       |
| reward_orientation      | 0.0355      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0767      |
| reward_torque           | 0.0493      |
| reward_velocity         | 0.19        |
| rollout/                |             |
|    ep_len_mean          | 925         |
|    ep_rew_mean          | 644         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 122         |
|    time_elapsed         | 480         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.061297253 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.5       |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0504     |
|    std                  | 0.293       |
|    value_loss           | 25.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.61       |
| reward_contact          | 0.0215     |
| reward_ctrl             | 0.0702     |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0767     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.19       |
| rollout/                |            |
|    ep_len_mean          | 930        |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 123        |
|    time_elapsed         | 483        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.07653153 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.1       |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0706    |
|    std                  | 0.293      |
|    value_loss           | 12.7       |
----------------------------------------
Num timesteps: 126000
Best mean reward: 617.14 - Last mean reward per episode: 648.87
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0215     |
| reward_ctrl             | 0.0708     |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0782     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 939        |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 124        |
|    time_elapsed         | 487        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.18368107 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.847      |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0928    |
|    std                  | 0.293      |
|    value_loss           | 5.47       |
----------------------------------------
-----------------------------------------
| reward                  | 0.62        |
| reward_contact          | 0.0209      |
| reward_ctrl             | 0.0717      |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0352      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0782      |
| reward_torque           | 0.0495      |
| reward_velocity         | 0.192       |
| rollout/                |             |
|    ep_len_mean          | 948         |
|    ep_rew_mean          | 664         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 125         |
|    time_elapsed         | 491         |
|    total_timesteps      | 128000      |
| train/                  |             |
|    approx_kl            | 0.049489558 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.1       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.27        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.065      |
|    std                  | 0.293       |
|    value_loss           | 10.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.616      |
| reward_contact          | 0.0214     |
| reward_ctrl             | 0.0713     |
| reward_motion           | 0.169      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0777     |
| reward_torque           | 0.0494     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 666        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 126        |
|    time_elapsed         | 495        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.07996367 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.815      |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0959    |
|    std                  | 0.293      |
|    value_loss           | 3.91       |
----------------------------------------
----------------------------------------
| reward                  | 0.619      |
| reward_contact          | 0.0216     |
| reward_ctrl             | 0.0727     |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0786     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 667        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 127        |
|    time_elapsed         | 499        |
|    total_timesteps      | 130048     |
| train/                  |            |
|    approx_kl            | 0.07502734 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.087     |
|    std                  | 0.293      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.619      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0733     |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0789     |
| reward_torque           | 0.0498     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 670        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 128        |
|    time_elapsed         | 503        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.20551392 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.292      |
|    value_loss           | 2.04       |
----------------------------------------
Num timesteps: 132000
Best mean reward: 648.87 - Last mean reward per episode: 677.19
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0211     |
| reward_ctrl             | 0.0735     |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.08       |
| reward_torque           | 0.05       |
| reward_velocity         | 0.193      |
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | 677        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 129        |
|    time_elapsed         | 507        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.13319708 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.864      |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.292      |
|    value_loss           | 3.72       |
----------------------------------------
----------------------------------------
| reward                  | 0.634      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0747     |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0356     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.082      |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.194      |
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 686        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 130        |
|    time_elapsed         | 510        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.37947002 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.992      |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.292      |
|    value_loss           | 5.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.634      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0738     |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0356     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0825     |
| reward_torque           | 0.0502     |
| reward_velocity         | 0.195      |
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 687        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 131        |
|    time_elapsed         | 514        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.18544862 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.9      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.9        |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0916    |
|    std                  | 0.292      |
|    value_loss           | 3.4        |
----------------------------------------
---------------------------------------
| reward                  | 0.636     |
| reward_contact          | 0.0204    |
| reward_ctrl             | 0.0741    |
| reward_motion           | 0.175     |
| reward_orientation      | 0.036     |
| reward_position         | 6.24e-05  |
| reward_rotation         | 0.0845    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.195     |
| rollout/                |           |
|    ep_len_mean          | 968       |
|    ep_rew_mean          | 690       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 132       |
|    time_elapsed         | 518       |
|    total_timesteps      | 135168    |
| train/                  |           |
|    approx_kl            | 0.0858034 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.5     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.612     |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.0785   |
|    std                  | 0.292     |
|    value_loss           | 3.7       |
---------------------------------------
-----------------------------------------
| reward                  | 0.632       |
| reward_contact          | 0.0204      |
| reward_ctrl             | 0.075       |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0363      |
| reward_position         | 6.24e-05    |
| reward_rotation         | 0.0845      |
| reward_torque           | 0.0504      |
| reward_velocity         | 0.194       |
| rollout/                |             |
|    ep_len_mean          | 968         |
|    ep_rew_mean          | 691         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 133         |
|    time_elapsed         | 522         |
|    total_timesteps      | 136192      |
| train/                  |             |
|    approx_kl            | 0.061183065 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.291       |
|    value_loss           | 15.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.0204     |
| reward_ctrl             | 0.0761     |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0366     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0849     |
| reward_torque           | 0.0506     |
| reward_velocity         | 0.195      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 692        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 134        |
|    time_elapsed         | 526        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.10948655 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.801      |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.291      |
|    value_loss           | 5.23       |
----------------------------------------
Num timesteps: 138000
Best mean reward: 677.19 - Last mean reward per episode: 692.62
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.633      |
| reward_contact          | 0.0209     |
| reward_ctrl             | 0.0755     |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0369     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0837     |
| reward_torque           | 0.0505     |
| reward_velocity         | 0.193      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 693        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 135        |
|    time_elapsed         | 529        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.06692734 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.24       |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.291      |
|    value_loss           | 8          |
----------------------------------------
----------------------------------------
| reward                  | 0.635      |
| reward_contact          | 0.0209     |
| reward_ctrl             | 0.0768     |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0371     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0838     |
| reward_torque           | 0.0507     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 694        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 136        |
|    time_elapsed         | 533        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.06882158 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0725    |
|    std                  | 0.291      |
|    value_loss           | 8.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.623       |
| reward_contact          | 0.0214      |
| reward_ctrl             | 0.0763      |
| reward_motion           | 0.164       |
| reward_orientation      | 0.0373      |
| reward_position         | 6.24e-05    |
| reward_rotation         | 0.083       |
| reward_torque           | 0.0506      |
| reward_velocity         | 0.19        |
| rollout/                |             |
|    ep_len_mean          | 960         |
|    ep_rew_mean          | 693         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 137         |
|    time_elapsed         | 537         |
|    total_timesteps      | 140288      |
| train/                  |             |
|    approx_kl            | 0.082028516 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.34        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.0757     |
|    std                  | 0.291       |
|    value_loss           | 12.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.628      |
| reward_contact          | 0.0214     |
| reward_ctrl             | 0.0777     |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0373     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0844     |
| reward_torque           | 0.0509     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 699        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 138        |
|    time_elapsed         | 541        |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.06576674 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.92       |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.291      |
|    value_loss           | 7.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.623      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0784     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0374     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0843     |
| reward_torque           | 0.0511     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 139        |
|    time_elapsed         | 545        |
|    total_timesteps      | 142336     |
| train/                  |            |
|    approx_kl            | 0.05558774 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 9.81       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.291      |
|    value_loss           | 13.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0202     |
| reward_ctrl             | 0.0798     |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0377     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0866     |
| reward_torque           | 0.0513     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 703        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 140        |
|    time_elapsed         | 549        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.11320653 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.908      |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.29       |
|    value_loss           | 3.67       |
----------------------------------------
Num timesteps: 144000
Best mean reward: 692.62 - Last mean reward per episode: 703.24
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.622       |
| reward_contact          | 0.0202      |
| reward_ctrl             | 0.0805      |
| reward_motion           | 0.152       |
| reward_orientation      | 0.0377      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.0881      |
| reward_torque           | 0.0514      |
| reward_velocity         | 0.192       |
| rollout/                |             |
|    ep_len_mean          | 965         |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 141         |
|    time_elapsed         | 553         |
|    total_timesteps      | 144384      |
| train/                  |             |
|    approx_kl            | 0.057991542 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0409     |
|    std                  | 0.29        |
|    value_loss           | 37.1        |
-----------------------------------------
--------------------------------------
| reward                  | 0.617    |
| reward_contact          | 0.0207   |
| reward_ctrl             | 0.0812   |
| reward_motion           | 0.147    |
| reward_orientation      | 0.0381   |
| reward_position         | 6.25e-05 |
| reward_rotation         | 0.088    |
| reward_torque           | 0.0516   |
| reward_velocity         | 0.19     |
| rollout/                |          |
|    ep_len_mean          | 961      |
|    ep_rew_mean          | 704      |
| time/                   |          |
|    fps                  | 261      |
|    iterations           | 142      |
|    time_elapsed         | 556      |
|    total_timesteps      | 145408   |
| train/                  |          |
|    approx_kl            | 0.166669 |
|    clip_fraction        | 0.261    |
|    clip_range           | 0.4      |
|    entropy_loss         | -34.5    |
|    explained_variance   | 0.972    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.784    |
|    n_updates            | 2820     |
|    policy_gradient_loss | -0.0859  |
|    std                  | 0.29     |
|    value_loss           | 3.48     |
--------------------------------------
----------------------------------------
| reward                  | 0.621      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0825     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0384     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0897     |
| reward_torque           | 0.0517     |
| reward_velocity         | 0.19       |
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 706        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 143        |
|    time_elapsed         | 560        |
|    total_timesteps      | 146432     |
| train/                  |            |
|    approx_kl            | 0.14389788 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.343      |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.29       |
|    value_loss           | 2.6        |
----------------------------------------
----------------------------------------
| reward                  | 0.616      |
| reward_contact          | 0.0212     |
| reward_ctrl             | 0.0832     |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0385     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0894     |
| reward_torque           | 0.0518     |
| reward_velocity         | 0.189      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 705        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 144        |
|    time_elapsed         | 564        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.08992606 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.063     |
|    std                  | 0.29       |
|    value_loss           | 34.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.617     |
| reward_contact          | 0.021     |
| reward_ctrl             | 0.0845    |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0388    |
| reward_position         | 6.25e-05  |
| reward_rotation         | 0.0897    |
| reward_torque           | 0.052     |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 959       |
|    ep_rew_mean          | 706       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 145       |
|    time_elapsed         | 568       |
|    total_timesteps      | 148480    |
| train/                  |           |
|    approx_kl            | 0.0778404 |
|    clip_fraction        | 0.212     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.5     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.77      |
|    n_updates            | 2880      |
|    policy_gradient_loss | -0.0762   |
|    std                  | 0.29      |
|    value_loss           | 5.67      |
---------------------------------------
---------------------------------------
| reward                  | 0.621     |
| reward_contact          | 0.021     |
| reward_ctrl             | 0.086     |
| reward_motion           | 0.145     |
| reward_orientation      | 0.0391    |
| reward_position         | 6.25e-05  |
| reward_rotation         | 0.0889    |
| reward_torque           | 0.0522    |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 959       |
|    ep_rew_mean          | 709       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 146       |
|    time_elapsed         | 572       |
|    total_timesteps      | 149504    |
| train/                  |           |
|    approx_kl            | 0.1071461 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.8     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.902     |
|    n_updates            | 2900      |
|    policy_gradient_loss | -0.0905   |
|    std                  | 0.29      |
|    value_loss           | 4.88      |
---------------------------------------
Num timesteps: 150000
Best mean reward: 703.24 - Last mean reward per episode: 710.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.623       |
| reward_contact          | 0.0209      |
| reward_ctrl             | 0.0871      |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0394      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.0875      |
| reward_torque           | 0.0524      |
| reward_velocity         | 0.188       |
| rollout/                |             |
|    ep_len_mean          | 959         |
|    ep_rew_mean          | 711         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 147         |
|    time_elapsed         | 576         |
|    total_timesteps      | 150528      |
| train/                  |             |
|    approx_kl            | 0.052659698 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.56        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0503     |
|    std                  | 0.29        |
|    value_loss           | 34.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.623      |
| reward_contact          | 0.0207     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0895     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 148        |
|    time_elapsed         | 580        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.06989728 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.29       |
|    value_loss           | 5.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0877     |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0889     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 149        |
|    time_elapsed         | 584        |
|    total_timesteps      | 152576     |
| train/                  |            |
|    approx_kl            | 0.15838864 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.289      |
|    value_loss           | 4.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0882     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0905     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 715        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 150        |
|    time_elapsed         | 588        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.13472316 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.47       |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.289      |
|    value_loss           | 3.8        |
----------------------------------------
-----------------------------------------
| reward                  | 0.618       |
| reward_contact          | 0.0208      |
| reward_ctrl             | 0.0863      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.04        |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.09        |
| reward_torque           | 0.0522      |
| reward_velocity         | 0.185       |
| rollout/                |             |
|    ep_len_mean          | 957         |
|    ep_rew_mean          | 716         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 151         |
|    time_elapsed         | 592         |
|    total_timesteps      | 154624      |
| train/                  |             |
|    approx_kl            | 0.103915185 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.526       |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.1        |
|    std                  | 0.289       |
|    value_loss           | 2.62        |
-----------------------------------------
----------------------------------------
| reward                  | 0.625      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0403     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0919     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 718        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 152        |
|    time_elapsed         | 596        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.12535065 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.8        |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0994    |
|    std                  | 0.289      |
|    value_loss           | 4.2        |
----------------------------------------
Num timesteps: 156000
Best mean reward: 710.70 - Last mean reward per episode: 717.54
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.625      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0403     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0919     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 720        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 153        |
|    time_elapsed         | 600        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.06462793 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.19       |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.288      |
|    value_loss           | 8.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0884     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0407     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0925     |
| reward_torque           | 0.0526     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 721        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 154        |
|    time_elapsed         | 604        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.14862442 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.611      |
|    n_updates            | 3060       |
|    policy_gradient_loss | -0.0879    |
|    std                  | 0.288      |
|    value_loss           | 3.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.622      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0903     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0412     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.093      |
| reward_torque           | 0.0529     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 723        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 155        |
|    time_elapsed         | 608        |
|    total_timesteps      | 158720     |
| train/                  |            |
|    approx_kl            | 0.14422327 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.495      |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.288      |
|    value_loss           | 3.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.621      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0905     |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0412     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.094      |
| reward_torque           | 0.0529     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 727        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 156        |
|    time_elapsed         | 612        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.06450029 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.3       |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.0593    |
|    std                  | 0.288      |
|    value_loss           | 22.7       |
----------------------------------------
-----------------------------------------
| reward                  | 0.621       |
| reward_contact          | 0.0206      |
| reward_ctrl             | 0.0905      |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0412      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.094       |
| reward_torque           | 0.0529      |
| reward_velocity         | 0.186       |
| rollout/                |             |
|    ep_len_mean          | 957         |
|    ep_rew_mean          | 731         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 157         |
|    time_elapsed         | 616         |
|    total_timesteps      | 160768      |
| train/                  |             |
|    approx_kl            | 0.054567218 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.2       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.053      |
|    std                  | 0.288       |
|    value_loss           | 26.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0915     |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0415     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.096      |
| reward_torque           | 0.053      |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 735        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 158        |
|    time_elapsed         | 620        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.08203598 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.202      |
|    learning_rate        | 0.0003     |
|    loss                 | 19         |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.288      |
|    value_loss           | 21.5       |
----------------------------------------
Num timesteps: 162000
Best mean reward: 717.54 - Last mean reward per episode: 734.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.632     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0932    |
| reward_motion           | 0.132     |
| reward_orientation      | 0.0417    |
| reward_position         | 9.19e-05  |
| reward_rotation         | 0.101     |
| reward_torque           | 0.0532    |
| reward_velocity         | 0.192     |
| rollout/                |           |
|    ep_len_mean          | 963       |
|    ep_rew_mean          | 740       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 159       |
|    time_elapsed         | 624       |
|    total_timesteps      | 162816    |
| train/                  |           |
|    approx_kl            | 0.0555567 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.4     |
|    explained_variance   | 0.248     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.03      |
|    n_updates            | 3160      |
|    policy_gradient_loss | -0.0532   |
|    std                  | 0.288     |
|    value_loss           | 21.7      |
---------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.02       |
| reward_ctrl             | 0.0938     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0417     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.1        |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 742        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 160        |
|    time_elapsed         | 627        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.42139512 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.87       |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0815    |
|    std                  | 0.288      |
|    value_loss           | 5.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.02       |
| reward_ctrl             | 0.0938     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0417     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.1        |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 749        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 161        |
|    time_elapsed         | 631        |
|    total_timesteps      | 164864     |
| train/                  |            |
|    approx_kl            | 0.05712959 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.287      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.288      |
|    value_loss           | 25.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.648     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0941    |
| reward_motion           | 0.142     |
| reward_orientation      | 0.0418    |
| reward_position         | 9.19e-05  |
| reward_rotation         | 0.102     |
| reward_torque           | 0.0533    |
| reward_velocity         | 0.195     |
| rollout/                |           |
|    ep_len_mean          | 969       |
|    ep_rew_mean          | 749       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 162       |
|    time_elapsed         | 635       |
|    total_timesteps      | 165888    |
| train/                  |           |
|    approx_kl            | 0.0988584 |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.8     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.591     |
|    n_updates            | 3220      |
|    policy_gradient_loss | -0.0841   |
|    std                  | 0.287     |
|    value_loss           | 9.28      |
---------------------------------------
----------------------------------------
| reward                  | 0.652      |
| reward_contact          | 0.0188     |
| reward_ctrl             | 0.0939     |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0414     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.104      |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.198      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 752        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 163        |
|    time_elapsed         | 639        |
|    total_timesteps      | 166912     |
| train/                  |            |
|    approx_kl            | 0.20166099 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.699      |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.287      |
|    value_loss           | 4.39       |
----------------------------------------
-----------------------------------------
| reward                  | 0.651       |
| reward_contact          | 0.0188      |
| reward_ctrl             | 0.0951      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0415      |
| reward_position         | 9.19e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 753         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 164         |
|    time_elapsed         | 643         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.089540504 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.529       |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.07       |
|    std                  | 0.287       |
|    value_loss           | 3.14        |
-----------------------------------------
Num timesteps: 168000
Best mean reward: 734.72 - Last mean reward per episode: 753.25
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.651       |
| reward_contact          | 0.0188      |
| reward_ctrl             | 0.0951      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0415      |
| reward_position         | 9.19e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 754         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 165         |
|    time_elapsed         | 647         |
|    total_timesteps      | 168960      |
| train/                  |             |
|    approx_kl            | 0.081179164 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.287       |
|    value_loss           | 3.06        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.653       |
| reward_contact          | 0.0187      |
| reward_ctrl             | 0.095       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0419      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 756         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 166         |
|    time_elapsed         | 651         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.096646935 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.0938     |
|    std                  | 0.287       |
|    value_loss           | 2.52        |
-----------------------------------------
----------------------------------------
| reward                  | 0.647      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.0961     |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0418     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.105      |
| reward_torque           | 0.0535     |
| reward_velocity         | 0.197      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 755        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 167        |
|    time_elapsed         | 655        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.10382956 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.5      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.287      |
|    value_loss           | 2          |
----------------------------------------
-----------------------------------------
| reward                  | 0.648       |
| reward_contact          | 0.0186      |
| reward_ctrl             | 0.097       |
| reward_motion           | 0.132       |
| reward_orientation      | 0.0419      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.107       |
| reward_torque           | 0.0536      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 756         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 168         |
|    time_elapsed         | 659         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.050349392 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.058      |
|    std                  | 0.287       |
|    value_loss           | 9.67        |
-----------------------------------------
----------------------------------------
| reward                  | 0.65       |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.0984     |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0419     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.109      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.197      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 758        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 169        |
|    time_elapsed         | 663        |
|    total_timesteps      | 173056     |
| train/                  |            |
|    approx_kl            | 0.14275405 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.335      |
|    n_updates            | 3360       |
|    policy_gradient_loss | -0.0883    |
|    std                  | 0.286      |
|    value_loss           | 2.09       |
----------------------------------------
Num timesteps: 174000
Best mean reward: 753.25 - Last mean reward per episode: 760.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.658      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.0992     |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0416     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.11       |
| reward_torque           | 0.054      |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 761        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 170        |
|    time_elapsed         | 667        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.06053085 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.637      |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0768    |
|    std                  | 0.286      |
|    value_loss           | 5.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.658      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.0997     |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0416     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.111      |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 761        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 171        |
|    time_elapsed         | 671        |
|    total_timesteps      | 175104     |
| train/                  |            |
|    approx_kl            | 0.40313876 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.358      |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.286      |
|    value_loss           | 2.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.659      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.136      |
| reward_orientation      | 0.042      |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.11       |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.198      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 760        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 172        |
|    time_elapsed         | 675        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.09005588 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0336     |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.286      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.66       |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0423     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.112      |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 762        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 173        |
|    time_elapsed         | 679        |
|    total_timesteps      | 177152     |
| train/                  |            |
|    approx_kl            | 0.09291803 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.92       |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.286      |
|    value_loss           | 13.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.655      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0425     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.113      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 174        |
|    time_elapsed         | 682        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.12863752 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.36       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.285      |
|    value_loss           | 2.08       |
----------------------------------------
----------------------------------------
| reward                  | 0.663      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.129      |
| reward_orientation      | 0.0425     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.115      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 767        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 175        |
|    time_elapsed         | 686        |
|    total_timesteps      | 179200     |
| train/                  |            |
|    approx_kl            | 0.21886471 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.606      |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.285      |
|    value_loss           | 2.26       |
----------------------------------------
Num timesteps: 180000
Best mean reward: 760.72 - Last mean reward per episode: 768.74
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.665      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0426     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 176        |
|    time_elapsed         | 690        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.08834024 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.7        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.285      |
|    value_loss           | 16.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.656      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0427     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 177        |
|    time_elapsed         | 694        |
|    total_timesteps      | 181248     |
| train/                  |            |
|    approx_kl            | 0.19440275 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.524      |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.0883    |
|    std                  | 0.285      |
|    value_loss           | 4.03       |
----------------------------------------
---------------------------------------
| reward                  | 0.658     |
| reward_contact          | 0.0183    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.12      |
| reward_orientation      | 0.0427    |
| reward_position         | 9.18e-05  |
| reward_rotation         | 0.117     |
| reward_torque           | 0.0547    |
| reward_velocity         | 0.202     |
| rollout/                |           |
|    ep_len_mean          | 966       |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 178       |
|    time_elapsed         | 698       |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.1197983 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -36.7     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 3540      |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.285     |
|    value_loss           | 3.81      |
---------------------------------------
-----------------------------------------
| reward                  | 0.657       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.121       |
| reward_orientation      | 0.0427      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.115       |
| reward_torque           | 0.0546      |
| reward_velocity         | 0.202       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 179         |
|    time_elapsed         | 702         |
|    total_timesteps      | 183296      |
| train/                  |             |
|    approx_kl            | 0.048548646 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.056      |
|    std                  | 0.285       |
|    value_loss           | 12.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.649      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.043      |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.114      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 771        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 180        |
|    time_elapsed         | 706        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.22106248 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.883      |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.285      |
|    value_loss           | 2.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0429     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.114      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 181        |
|    time_elapsed         | 710        |
|    total_timesteps      | 185344     |
| train/                  |            |
|    approx_kl            | 0.10555091 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.284      |
|    value_loss           | 6.93       |
----------------------------------------
Num timesteps: 186000
Best mean reward: 768.74 - Last mean reward per episode: 773.48
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.662      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0432     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.116      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 182        |
|    time_elapsed         | 714        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.10725984 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.48       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.284      |
|    value_loss           | 3.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.663      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0435     |
| reward_position         | 7.12e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 183        |
|    time_elapsed         | 718        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.09395177 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.284      |
|    value_loss           | 5.48       |
----------------------------------------
-----------------------------------------
| reward                  | 0.664       |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.124       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000124    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.203       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 184         |
|    time_elapsed         | 722         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.067573786 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.2       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58        |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0786     |
|    std                  | 0.284       |
|    value_loss           | 5.75        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.656       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.118       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000124    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0548      |
| reward_velocity         | 0.201       |
| rollout/                |             |
|    ep_len_mean          | 959         |
|    ep_rew_mean          | 771         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 185         |
|    time_elapsed         | 726         |
|    total_timesteps      | 189440      |
| train/                  |             |
|    approx_kl            | 0.084478304 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.5       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.67        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.066      |
|    std                  | 0.284       |
|    value_loss           | 23.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.661      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000124   |
| reward_rotation         | 0.118      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 186        |
|    time_elapsed         | 730        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.09867191 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.657      |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.284      |
|    value_loss           | 12.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.656       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.117       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000357    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.203       |
| rollout/                |             |
|    ep_len_mean          | 951         |
|    ep_rew_mean          | 765         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 187         |
|    time_elapsed         | 734         |
|    total_timesteps      | 191488      |
| train/                  |             |
|    approx_kl            | 0.081378326 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.284       |
|    value_loss           | 16.3        |
-----------------------------------------
Num timesteps: 192000
Best mean reward: 773.48 - Last mean reward per episode: 764.06
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0184     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.204      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 188        |
|    time_elapsed         | 738        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.08671241 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.38       |
|    n_updates            | 3740       |
|    policy_gradient_loss | -0.088     |
|    std                  | 0.283      |
|    value_loss           | 17.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.654      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.204      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 189        |
|    time_elapsed         | 741        |
|    total_timesteps      | 193536     |
| train/                  |            |
|    approx_kl            | 0.06937146 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.2       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.0762    |
|    std                  | 0.283      |
|    value_loss           | 22.7       |
----------------------------------------
--------------------------------------
| reward                  | 0.654    |
| reward_contact          | 0.0191   |
| reward_ctrl             | 0.106    |
| reward_motion           | 0.111    |
| reward_orientation      | 0.0428   |
| reward_position         | 0.000357 |
| reward_rotation         | 0.117    |
| reward_torque           | 0.0551   |
| reward_velocity         | 0.203    |
| rollout/                |          |
|    ep_len_mean          | 951      |
|    ep_rew_mean          | 765      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 190      |
|    time_elapsed         | 745      |
|    total_timesteps      | 194560   |
| train/                  |          |
|    approx_kl            | 0.26849  |
|    clip_fraction        | 0.31     |
|    clip_range           | 0.4      |
|    entropy_loss         | -37.3    |
|    explained_variance   | 0.989    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.375    |
|    n_updates            | 3780     |
|    policy_gradient_loss | -0.116   |
|    std                  | 0.283    |
|    value_loss           | 2.76     |
--------------------------------------
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0191     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.108      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.205      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 765        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 191        |
|    time_elapsed         | 749        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.24904266 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0953    |
|    std                  | 0.283      |
|    value_loss           | 3.14       |
----------------------------------------
-----------------------------------------
| reward                  | 0.66        |
| reward_contact          | 0.0185      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.113       |
| reward_orientation      | 0.0429      |
| reward_position         | 0.000357    |
| reward_rotation         | 0.119       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.206       |
| rollout/                |             |
|    ep_len_mean          | 960         |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 192         |
|    time_elapsed         | 753         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.076792315 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.1       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.24        |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.0569     |
|    std                  | 0.283       |
|    value_loss           | 12.3        |
-----------------------------------------
----------------------------------------
| reward                  | 0.661      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0426     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.205      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 771        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 193        |
|    time_elapsed         | 757        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.20254421 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.316      |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.283      |
|    value_loss           | 2.88       |
----------------------------------------
Num timesteps: 198000
Best mean reward: 773.48 - Last mean reward per episode: 770.95
----------------------------------------
| reward                  | 0.668      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0423     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 194        |
|    time_elapsed         | 761        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.09727071 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.5      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.711      |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.282      |
|    value_loss           | 4.74       |
----------------------------------------
----------------------------------------
| reward                  | 0.667      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0422     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.118      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 195        |
|    time_elapsed         | 765        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.22269462 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.686      |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.282      |
|    value_loss           | 3.86       |
----------------------------------------
----------------------------------------
| reward                  | 0.666      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0424     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 196        |
|    time_elapsed         | 769        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.07757417 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0974    |
|    std                  | 0.282      |
|    value_loss           | 3.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.676      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.124      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.12       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.208      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 197        |
|    time_elapsed         | 773        |
|    total_timesteps      | 201728     |
| train/                  |            |
|    approx_kl            | 0.12799951 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.718      |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.281      |
|    value_loss           | 2.79       |
----------------------------------------
---------------------------------------
| reward                  | 0.683     |
| reward_contact          | 0.0174    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.125     |
| reward_orientation      | 0.043     |
| reward_position         | 0.000779  |
| reward_rotation         | 0.122     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.211     |
| rollout/                |           |
|    ep_len_mean          | 963       |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 198       |
|    time_elapsed         | 777       |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.0695606 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.1     |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 3940      |
|    policy_gradient_loss | -0.0584   |
|    std                  | 0.281     |
|    value_loss           | 32.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.685      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0433     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.211      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 199        |
|    time_elapsed         | 781        |
|    total_timesteps      | 203776     |
| train/                  |            |
|    approx_kl            | 0.12865774 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.72       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.281      |
|    value_loss           | 2.53       |
----------------------------------------
Num timesteps: 204000
Best mean reward: 773.48 - Last mean reward per episode: 773.24
----------------------------------------
| reward                  | 0.691      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 200        |
|    time_elapsed         | 784        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.09465802 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.281      |
|    value_loss           | 8.06       |
----------------------------------------
----------------------------------------
| reward                  | 0.69       |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 201        |
|    time_elapsed         | 788        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.16142128 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.3      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 19         |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.281      |
|    value_loss           | 26.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.687      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 202        |
|    time_elapsed         | 792        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.13441858 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.0996    |
|    std                  | 0.281      |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.689      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 203        |
|    time_elapsed         | 796        |
|    total_timesteps      | 207872     |
| train/                  |            |
|    approx_kl            | 0.31247383 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.281      |
|    value_loss           | 1.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.696       |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.134       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.123       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.212       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 779         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 204         |
|    time_elapsed         | 800         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.074120626 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.6       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.15        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.0708     |
|    std                  | 0.28        |
|    value_loss           | 24.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 205        |
|    time_elapsed         | 804        |
|    total_timesteps      | 209920     |
| train/                  |            |
|    approx_kl            | 0.13207534 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0724    |
|    std                  | 0.28       |
|    value_loss           | 7.46       |
----------------------------------------
Num timesteps: 210000
Best mean reward: 773.48 - Last mean reward per episode: 779.77
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 206        |
|    time_elapsed         | 808        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.06353199 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.51       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.28       |
|    value_loss           | 14.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.126      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 207        |
|    time_elapsed         | 812        |
|    total_timesteps      | 211968     |
| train/                  |            |
|    approx_kl            | 0.43054584 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.0909    |
|    std                  | 0.28       |
|    value_loss           | 2.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.69       |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.126      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.212      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 208        |
|    time_elapsed         | 816        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.16970724 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.281      |
|    value_loss           | 3.57       |
----------------------------------------
-----------------------------------------
| reward                  | 0.688       |
| reward_contact          | 0.0177      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.122       |
| reward_orientation      | 0.0435      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.127       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.213       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 780         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 209         |
|    time_elapsed         | 820         |
|    total_timesteps      | 214016      |
| train/                  |             |
|    approx_kl            | 0.117287226 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.583       |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.103      |
|    std                  | 0.28        |
|    value_loss           | 2.24        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.695       |
| reward_contact          | 0.0177      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.129       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.127       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.214       |
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 781         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 210         |
|    time_elapsed         | 824         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.115168765 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.2       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.0705     |
|    std                  | 0.28        |
|    value_loss           | 12.4        |
-----------------------------------------
Num timesteps: 216000
Best mean reward: 779.77 - Last mean reward per episode: 782.12
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.7        |
| reward_contact          | 0.0182     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 971        |
|    ep_rew_mean          | 782        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 211        |
|    time_elapsed         | 828        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.44846207 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.28       |
|    value_loss           | 2.55       |
----------------------------------------
---------------------------------------
| reward                  | 0.705     |
| reward_contact          | 0.0176    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0433    |
| reward_position         | 0.000737  |
| reward_rotation         | 0.129     |
| reward_torque           | 0.0555    |
| reward_velocity         | 0.216     |
| rollout/                |           |
|    ep_len_mean          | 981       |
|    ep_rew_mean          | 790       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 212       |
|    time_elapsed         | 832       |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0660864 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.2     |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 4220      |
|    policy_gradient_loss | -0.0559   |
|    std                  | 0.28      |
|    value_loss           | 20.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.704      |
| reward_contact          | 0.0176     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000737   |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 789        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 213        |
|    time_elapsed         | 836        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.15172195 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.73       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.28       |
|    value_loss           | 2.09       |
----------------------------------------
---------------------------------------
| reward                  | 0.705     |
| reward_contact          | 0.0176    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0429    |
| reward_position         | 0.000737  |
| reward_rotation         | 0.129     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.216     |
| rollout/                |           |
|    ep_len_mean          | 981       |
|    ep_rew_mean          | 789       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 214       |
|    time_elapsed         | 840       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.3710906 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.8     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.329     |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.116    |
|    std                  | 0.28      |
|    value_loss           | 1.88      |
---------------------------------------
-----------------------------------------
| reward                  | 0.7         |
| reward_contact          | 0.0182      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.134       |
| reward_orientation      | 0.0428      |
| reward_position         | 0.000737    |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.215       |
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 789         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 215         |
|    time_elapsed         | 844         |
|    total_timesteps      | 220160      |
| train/                  |             |
|    approx_kl            | 0.059279088 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.8         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0657     |
|    std                  | 0.28        |
|    value_loss           | 13.9        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.7         |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0426      |
| reward_position         | 0.000737    |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.215       |
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 216         |
|    time_elapsed         | 848         |
|    total_timesteps      | 221184      |
| train/                  |             |
|    approx_kl            | 0.065936334 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.1       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0892     |
|    std                  | 0.279       |
|    value_loss           | 5.63        |
-----------------------------------------
Num timesteps: 222000
Best mean reward: 782.12 - Last mean reward per episode: 788.09
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.702      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.215      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 217        |
|    time_elapsed         | 852        |
|    total_timesteps      | 222208     |
| train/                  |            |
|    approx_kl            | 0.11577028 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.6      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.663      |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0998    |
|    std                  | 0.279      |
|    value_loss           | 2.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 218        |
|    time_elapsed         | 856        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.09963942 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.6      |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.279      |
|    value_loss           | 23.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.712       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.217       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 790         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 219         |
|    time_elapsed         | 860         |
|    total_timesteps      | 224256      |
| train/                  |             |
|    approx_kl            | 0.066470146 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38         |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.2         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0877     |
|    std                  | 0.279       |
|    value_loss           | 12.2        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.707       |
| reward_contact          | 0.0184      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.125       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 220         |
|    time_elapsed         | 864         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.091230586 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.6       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.279       |
|    value_loss           | 3.45        |
-----------------------------------------
---------------------------------------
| reward                  | 0.714     |
| reward_contact          | 0.0186    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.00074   |
| reward_rotation         | 0.127     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.22      |
| rollout/                |           |
|    ep_len_mean          | 982       |
|    ep_rew_mean          | 788       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 221       |
|    time_elapsed         | 868       |
|    total_timesteps      | 226304    |
| train/                  |           |
|    approx_kl            | 0.1391182 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.4       |
|    entropy_loss         | -40       |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.12      |
|    n_updates            | 4400      |
|    policy_gradient_loss | -0.0994   |
|    std                  | 0.279     |
|    value_loss           | 15.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0182     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 222        |
|    time_elapsed         | 872        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.42598796 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.298      |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.279      |
|    value_loss           | 1.63       |
----------------------------------------
Num timesteps: 228000
Best mean reward: 788.09 - Last mean reward per episode: 787.66
-----------------------------------------
| reward                  | 0.718       |
| reward_contact          | 0.018       |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.145       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.129       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 223         |
|    time_elapsed         | 876         |
|    total_timesteps      | 228352      |
| train/                  |             |
|    approx_kl            | 0.075850934 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.7       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.782       |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0991     |
|    std                  | 0.279       |
|    value_loss           | 2.87        |
-----------------------------------------
----------------------------------------
| reward                  | 0.718      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0433     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 224        |
|    time_elapsed         | 880        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.11311416 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.278      |
|    value_loss           | 3.91       |
----------------------------------------
-----------------------------------------
| reward                  | 0.72        |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.13        |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 789         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 225         |
|    time_elapsed         | 884         |
|    total_timesteps      | 230400      |
| train/                  |             |
|    approx_kl            | 0.075297266 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.56        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.0765     |
|    std                  | 0.278       |
|    value_loss           | 7.57        |
-----------------------------------------
----------------------------------------
| reward                  | 0.718      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.13       |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 226        |
|    time_elapsed         | 888        |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.26172417 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.577      |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.278      |
|    value_loss           | 2.65       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.143       |
| reward_orientation      | 0.0436      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.131       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 790         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 227         |
|    time_elapsed         | 892         |
|    total_timesteps      | 232448      |
| train/                  |             |
|    approx_kl            | 0.047955044 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.2       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0547     |
|    std                  | 0.278       |
|    value_loss           | 23.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.724      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.132      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 228        |
|    time_elapsed         | 896        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09163224 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.46       |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0777    |
|    std                  | 0.278      |
|    value_loss           | 7.28       |
----------------------------------------
Num timesteps: 234000
Best mean reward: 788.09 - Last mean reward per episode: 789.30
Saving new best model to rl/out_dir/models/exp71/best_model.zip
--------------------------------------
| reward                  | 0.722    |
| reward_contact          | 0.0179   |
| reward_ctrl             | 0.107    |
| reward_motion           | 0.146    |
| reward_orientation      | 0.0432   |
| reward_position         | 0.00074  |
| reward_rotation         | 0.132    |
| reward_torque           | 0.0552   |
| reward_velocity         | 0.221    |
| rollout/                |          |
|    ep_len_mean          | 982      |
|    ep_rew_mean          | 789      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 229      |
|    time_elapsed         | 900      |
|    total_timesteps      | 234496   |
| train/                  |          |
|    approx_kl            | 0.119911 |
|    clip_fraction        | 0.32     |
|    clip_range           | 0.4      |
|    entropy_loss         | -39.3    |
|    explained_variance   | 0.975    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.773    |
|    n_updates            | 4560     |
|    policy_gradient_loss | -0.107   |
|    std                  | 0.278    |
|    value_loss           | 3.61     |
--------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0181     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.134      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 984        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 230        |
|    time_elapsed         | 904        |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.14359546 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.277      |
|    value_loss           | 3.25       |
----------------------------------------
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0175     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.136      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 985        |
|    ep_rew_mean          | 791        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 231        |
|    time_elapsed         | 908        |
|    total_timesteps      | 236544     |
| train/                  |            |
|    approx_kl            | 0.25051856 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40        |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.679      |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.278      |
|    value_loss           | 3.13       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0169     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 986        |
|    ep_rew_mean          | 792        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 232        |
|    time_elapsed         | 912        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.09740679 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.39       |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0931    |
|    std                  | 0.278      |
|    value_loss           | 2.89       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 987        |
|    ep_rew_mean          | 795        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 233        |
|    time_elapsed         | 915        |
|    total_timesteps      | 238592     |
| train/                  |            |
|    approx_kl            | 0.19745895 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0879     |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.277      |
|    value_loss           | 1.85       |
----------------------------------------
-----------------------------------------
| reward                  | 0.744       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.151       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 234         |
|    time_elapsed         | 919         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.054082498 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0552     |
|    std                  | 0.277       |
|    value_loss           | 23.2        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: 789.30 - Last mean reward per episode: 797.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 797        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 235        |
|    time_elapsed         | 923        |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.18764104 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.277      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 798        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 236        |
|    time_elapsed         | 927        |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.09910023 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.7      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.91       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.277      |
|    value_loss           | 12.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 796        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 237        |
|    time_elapsed         | 931        |
|    total_timesteps      | 242688     |
| train/                  |            |
|    approx_kl            | 0.13132326 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.277      |
|    value_loss           | 3.23       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 796        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 238        |
|    time_elapsed         | 935        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.11802027 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.484      |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.277      |
|    value_loss           | 3.05       |
----------------------------------------
---------------------------------------
| reward                  | 0.746     |
| reward_contact          | 0.0154    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.158     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.00074   |
| reward_rotation         | 0.14      |
| reward_torque           | 0.0551    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 994       |
|    ep_rew_mean          | 800       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 239       |
|    time_elapsed         | 939       |
|    total_timesteps      | 244736    |
| train/                  |           |
|    approx_kl            | 0.7266917 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39       |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.353     |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.123    |
|    std                  | 0.277     |
|    value_loss           | 2.64      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 994        |
|    ep_rew_mean          | 799        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 240        |
|    time_elapsed         | 943        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.15365112 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.3      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.276      |
|    value_loss           | 1.71       |
----------------------------------------
Num timesteps: 246000
Best mean reward: 797.26 - Last mean reward per episode: 799.37
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 241        |
|    time_elapsed         | 947        |
|    total_timesteps      | 246784     |
| train/                  |            |
|    approx_kl            | 0.11831712 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.1      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.305      |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.276      |
|    value_loss           | 2.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 242        |
|    time_elapsed         | 951        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.12328817 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.451      |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.276      |
|    value_loss           | 2.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 243        |
|    time_elapsed         | 955        |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.11957366 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.276      |
|    value_loss           | 2.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 244        |
|    time_elapsed         | 959        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.09971682 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.523      |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.276      |
|    value_loss           | 2.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 245        |
|    time_elapsed         | 963        |
|    total_timesteps      | 250880     |
| train/                  |            |
|    approx_kl            | 0.11762114 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39        |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.579      |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.276      |
|    value_loss           | 2.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 246        |
|    time_elapsed         | 967        |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.08995411 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 4900       |
|    policy_gradient_loss | -0.0964    |
|    std                  | 0.275      |
|    value_loss           | 3.59       |
----------------------------------------
Num timesteps: 252000
Best mean reward: 799.37 - Last mean reward per episode: 800.68
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 247        |
|    time_elapsed         | 971        |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.12363757 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0846    |
|    std                  | 0.275      |
|    value_loss           | 2.84       |
----------------------------------------
-----------------------------------------
| reward                  | 0.733       |
| reward_contact          | 0.0154      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0548      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 996         |
|    ep_rew_mean          | 800         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 248         |
|    time_elapsed         | 975         |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.098926604 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0889     |
|    std                  | 0.275       |
|    value_loss           | 8.34        |
-----------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 249        |
|    time_elapsed         | 979        |
|    total_timesteps      | 254976     |
| train/                  |            |
|    approx_kl            | 0.10817122 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.447      |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.275      |
|    value_loss           | 2.7        |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 250        |
|    time_elapsed         | 982        |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.07255809 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.47       |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.0477    |
|    std                  | 0.275      |
|    value_loss           | 16.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 804        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 251        |
|    time_elapsed         | 986        |
|    total_timesteps      | 257024     |
| train/                  |            |
|    approx_kl            | 0.06664982 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.8      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.275      |
|    value_loss           | 14.6       |
----------------------------------------
Num timesteps: 258000
Best mean reward: 800.68 - Last mean reward per episode: 804.55
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 805        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 252        |
|    time_elapsed         | 990        |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.07991566 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.934      |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0674    |
|    std                  | 0.274      |
|    value_loss           | 7.49       |
----------------------------------------
-----------------------------------------
| reward                  | 0.736       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0431      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 805         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 253         |
|    time_elapsed         | 995         |
|    total_timesteps      | 259072      |
| train/                  |             |
|    approx_kl            | 0.107865684 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.906       |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.0787     |
|    std                  | 0.274       |
|    value_loss           | 4.68        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.734       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.147       |
| reward_orientation      | 0.0431      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 804         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 254         |
|    time_elapsed         | 999         |
|    total_timesteps      | 260096      |
| train/                  |             |
|    approx_kl            | 0.084903374 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.22        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0694     |
|    std                  | 0.274       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 255        |
|    time_elapsed         | 1002       |
|    total_timesteps      | 261120     |
| train/                  |            |
|    approx_kl            | 0.16947773 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.384      |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.0892    |
|    std                  | 0.274      |
|    value_loss           | 1.51       |
----------------------------------------
---------------------------------------
| reward                  | 0.73      |
| reward_contact          | 0.016     |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.149     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.000711  |
| reward_rotation         | 0.142     |
| reward_torque           | 0.0546    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 998       |
|    ep_rew_mean          | 803       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 256       |
|    time_elapsed         | 1006      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.2143392 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39.1     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.452     |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.134    |
|    std                  | 0.274     |
|    value_loss           | 1.86      |
---------------------------------------
----------------------------------------
| reward                  | 0.724      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 257        |
|    time_elapsed         | 1010       |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.08330659 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.43       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.273      |
|    value_loss           | 7.09       |
----------------------------------------
Num timesteps: 264000
Best mean reward: 804.55 - Last mean reward per episode: 801.95
----------------------------------------
| reward                  | 0.713      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 258        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.06518588 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 5140       |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.273      |
|    value_loss           | 7.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.711      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.0998     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 259        |
|    time_elapsed         | 1018       |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.38526678 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.6      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.389      |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.273      |
|    value_loss           | 1.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.709      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 260        |
|    time_elapsed         | 1022       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.08361219 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.808      |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0984    |
|    std                  | 0.273      |
|    value_loss           | 3.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.71       |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.0994     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0433     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 261        |
|    time_elapsed         | 1026       |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.31391382 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.273      |
|    value_loss           | 3.05       |
----------------------------------------
---------------------------------------
| reward                  | 0.71      |
| reward_contact          | 0.016     |
| reward_ctrl             | 0.0993    |
| reward_motion           | 0.142     |
| reward_orientation      | 0.0434    |
| reward_position         | 0.000711  |
| reward_rotation         | 0.137     |
| reward_torque           | 0.0543    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 998       |
|    ep_rew_mean          | 802       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 262       |
|    time_elapsed         | 1030      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0733189 |
|    clip_fraction        | 0.282     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39.4     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.29      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.0602   |
|    std                  | 0.273     |
|    value_loss           | 7.89      |
---------------------------------------
-----------------------------------------
| reward                  | 0.709       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.0987      |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.137       |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 803         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 263         |
|    time_elapsed         | 1034        |
|    total_timesteps      | 269312      |
| train/                  |             |
|    approx_kl            | 0.054735593 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.629       |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0665     |
|    std                  | 0.272       |
|    value_loss           | 12.7        |
-----------------------------------------
Num timesteps: 270000
Best mean reward: 804.55 - Last mean reward per episode: 802.79
----------------------------------------
| reward                  | 0.715      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.0985     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 264        |
|    time_elapsed         | 1038       |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.07660645 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.13       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.93       |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.053     |
|    std                  | 0.272      |
|    value_loss           | 24.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0986     |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 805        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 265        |
|    time_elapsed         | 1041       |
|    total_timesteps      | 271360     |
| train/                  |            |
|    approx_kl            | 0.05354607 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.5       |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0605    |
|    std                  | 0.272      |
|    value_loss           | 21.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0972     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.054      |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 266        |
|    time_elapsed         | 1045       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.20370428 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.1      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.689      |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.091     |
|    std                  | 0.272      |
|    value_loss           | 2.17       |
----------------------------------------
----------------------------------------
| reward                  | 0.712      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.0959     |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.137      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 267        |
|    time_elapsed         | 1049       |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.15861273 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.235      |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.272      |
|    value_loss           | 1.76       |
----------------------------------------
-----------------------------------------
| reward                  | 0.714       |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.0964      |
| reward_motion           | 0.145       |
| reward_orientation      | 0.044       |
| reward_position         | 0.000711    |
| reward_rotation         | 0.138       |
| reward_torque           | 0.0539      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 806         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 268         |
|    time_elapsed         | 1054        |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.048325207 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 5           |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0588     |
|    std                  | 0.272       |
|    value_loss           | 20.6        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.711       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0958      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0442      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.136       |
| reward_torque           | 0.0538      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 806         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 269         |
|    time_elapsed         | 1058        |
|    total_timesteps      | 275456      |
| train/                  |             |
|    approx_kl            | 0.108048886 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.774       |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.093      |
|    std                  | 0.272       |
|    value_loss           | 2.69        |
-----------------------------------------
Num timesteps: 276000
Best mean reward: 804.55 - Last mean reward per episode: 807.69
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.71       |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0958     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0443     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 270        |
|    time_elapsed         | 1063       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.09814938 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.272      |
|    value_loss           | 3          |
----------------------------------------
-----------------------------------------
| reward                  | 0.71        |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0959      |
| reward_motion           | 0.14        |
| reward_orientation      | 0.0443      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.138       |
| reward_torque           | 0.0539      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 271         |
|    time_elapsed         | 1068        |
|    total_timesteps      | 277504      |
| train/                  |             |
|    approx_kl            | 0.112396814 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.879       |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0869     |
|    std                  | 0.272       |
|    value_loss           | 3.37        |
-----------------------------------------
----------------------------------------
| reward                  | 0.716      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0445     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 272        |
|    time_elapsed         | 1073       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.10278291 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.243      |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.272      |
|    value_loss           | 1.45       |
----------------------------------------
-----------------------------------------
| reward                  | 0.714       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0955      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.139       |
| reward_torque           | 0.0538      |
| reward_velocity         | 0.22        |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 807         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 273         |
|    time_elapsed         | 1078        |
|    total_timesteps      | 279552      |
| train/                  |             |
|    approx_kl            | 0.083422765 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0618     |
|    std                  | 0.271       |
|    value_loss           | 11          |
-----------------------------------------
----------------------------------------
| reward                  | 0.717      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0947     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0538     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 274        |
|    time_elapsed         | 1082       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.07705648 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.271      |
|    value_loss           | 4.27       |
----------------------------------------
-----------------------------------------
| reward                  | 0.726       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.096       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.139       |
| reward_torque           | 0.0541      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 275         |
|    time_elapsed         | 1088        |
|    total_timesteps      | 281600      |
| train/                  |             |
|    approx_kl            | 0.060665432 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.8       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.271       |
|    value_loss           | 8.5         |
-----------------------------------------
Num timesteps: 282000
Best mean reward: 807.69 - Last mean reward per episode: 807.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.045      |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 276        |
|    time_elapsed         | 1095       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.08215463 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.7      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.271      |
|    value_loss           | 9.18       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0146      |
| reward_ctrl             | 0.0974      |
| reward_motion           | 0.15        |
| reward_orientation      | 0.0451      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.14        |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 277         |
|    time_elapsed         | 1101        |
|    total_timesteps      | 283648      |
| train/                  |             |
|    approx_kl            | 0.081559576 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.8       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.271       |
|    value_loss           | 17.4        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.0987     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 810        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 278        |
|    time_elapsed         | 1107       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.10431288 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0942    |
|    std                  | 0.271      |
|    value_loss           | 3.95       |
----------------------------------------
-----------------------------------------
| reward                  | 0.731       |
| reward_contact          | 0.014       |
| reward_ctrl             | 0.0985      |
| reward_motion           | 0.15        |
| reward_orientation      | 0.0454      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 812         |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 279         |
|    time_elapsed         | 1113        |
|    total_timesteps      | 285696      |
| train/                  |             |
|    approx_kl            | 0.046462562 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40         |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.43        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.0568     |
|    std                  | 0.271       |
|    value_loss           | 20.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0455     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 812        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 280        |
|    time_elapsed         | 1119       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.20781538 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0941    |
|    std                  | 0.271      |
|    value_loss           | 1.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0455     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 812        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 281        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 287744     |
| train/                  |            |
|    approx_kl            | 0.14470401 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.729      |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.0865    |
|    std                  | 0.271      |
|    value_loss           | 2          |
----------------------------------------
Num timesteps: 288000
Best mean reward: 807.99 - Last mean reward per episode: 811.88
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.0978     |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 811        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 282        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.24099842 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.437      |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.27       |
|    value_loss           | 2.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.0976     |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0461     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 811        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 283        |
|    time_elapsed         | 1134       |
|    total_timesteps      | 289792     |
| train/                  |            |
|    approx_kl            | 0.08805196 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.7      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.27       |
|    value_loss           | 12.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.097      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000658   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 816        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 284        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.11301737 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.868      |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.27       |
|    value_loss           | 4.37       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0973     |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.137      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 815        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 285        |
|    time_elapsed         | 1145       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.17451546 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.624      |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.0797    |
|    std                  | 0.27       |
|    value_loss           | 2.44       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.098       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0461      |
| reward_position         | 0.000658    |
| reward_rotation         | 0.137       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 817         |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 286         |
|    time_elapsed         | 1151        |
|    total_timesteps      | 292864      |
| train/                  |             |
|    approx_kl            | 0.117796354 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41         |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.27        |
|    value_loss           | 2.19        |
-----------------------------------------
----------------------------------------
| reward                  | 0.725      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.0979     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0462     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 825        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 287        |
|    time_elapsed         | 1156       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.41603482 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0913     |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.27       |
|    value_loss           | 1.43       |
----------------------------------------
Num timesteps: 294000
Best mean reward: 811.88 - Last mean reward per episode: 825.15
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.726     |
| reward_contact          | 0.0145    |
| reward_ctrl             | 0.0969    |
| reward_motion           | 0.157     |
| reward_orientation      | 0.0464    |
| reward_position         | 0.000425  |
| reward_rotation         | 0.139     |
| reward_torque           | 0.0543    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 826       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 288       |
|    time_elapsed         | 1162      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.1540857 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.1     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.315     |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.0925   |
|    std                  | 0.269     |
|    value_loss           | 1.87      |
---------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.0966      |
| reward_motion           | 0.159       |
| reward_orientation      | 0.0465      |
| reward_position         | 0.000425    |
| reward_rotation         | 0.14        |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 826         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 289         |
|    time_elapsed         | 1168        |
|    total_timesteps      | 295936      |
| train/                  |             |
|    approx_kl            | 0.069010586 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.0704     |
|    std                  | 0.269       |
|    value_loss           | 4.79        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.0959      |
| reward_motion           | 0.159       |
| reward_orientation      | 0.0465      |
| reward_position         | 0.000425    |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 827         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 290         |
|    time_elapsed         | 1173        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.108881816 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.2       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0595     |
|    std                  | 0.269       |
|    value_loss           | 8.26        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.0959     |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0465     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 827        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 291        |
|    time_elapsed         | 1178       |
|    total_timesteps      | 297984     |
| train/                  |            |
|    approx_kl            | 0.18210863 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.466      |
|    n_updates            | 5800       |
|    policy_gradient_loss | -0.0934    |
|    std                  | 0.269      |
|    value_loss           | 3.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.0951     |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0464     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 828        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 292        |
|    time_elapsed         | 1183       |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.11979908 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 5820       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.269      |
|    value_loss           | 2.66       |
----------------------------------------
Num timesteps: 300000
Best mean reward: 825.15 - Last mean reward per episode: 829.32
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.0952     |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0467     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 829        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 293        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 300032     |
| train/                  |            |
|    approx_kl            | 0.05434999 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.46       |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.269      |
|    value_loss           | 8.89       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.0964     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0471     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 832        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 294        |
|    time_elapsed         | 1195       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.14454904 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.15       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0964    |
|    std                  | 0.269      |
|    value_loss           | 2.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0966     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0475     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 833        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 295        |
|    time_elapsed         | 1200       |
|    total_timesteps      | 302080     |
| train/                  |            |
|    approx_kl            | 0.12774941 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.04       |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0472    |
|    std                  | 0.269      |
|    value_loss           | 5.16       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0962     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0472     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 296        |
|    time_elapsed         | 1206       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.12712756 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.21       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.269      |
|    value_loss           | 1.46       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.0965     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0472     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 297        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 304128     |
| train/                  |            |
|    approx_kl            | 0.08620262 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.263      |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.0777    |
|    std                  | 0.268      |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.725      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0471     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 298        |
|    time_elapsed         | 1218       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.07004221 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.84       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0427    |
|    std                  | 0.268      |
|    value_loss           | 19         |
----------------------------------------
Num timesteps: 306000
Best mean reward: 829.32 - Last mean reward per episode: 834.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.726       |
| reward_contact          | 0.0148      |
| reward_ctrl             | 0.0963      |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0471      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 835         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 299         |
|    time_elapsed         | 1223        |
|    total_timesteps      | 306176      |
| train/                  |             |
|    approx_kl            | 0.056079336 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.268       |
|    value_loss           | 6.26        |
-----------------------------------------
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.0962     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 836        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 300        |
|    time_elapsed         | 1228       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.06719802 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.12       |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0653    |
|    std                  | 0.268      |
|    value_loss           | 8.65       |
----------------------------------------
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0148      |
| reward_ctrl             | 0.0972      |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0474      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0545      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 837         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 301         |
|    time_elapsed         | 1232        |
|    total_timesteps      | 308224      |
| train/                  |             |
|    approx_kl            | 0.061720807 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.37        |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.0639     |
|    std                  | 0.268       |
|    value_loss           | 10.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0967     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 837        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 302        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.09308397 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.268      |
|    value_loss           | 17.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0971     |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 837        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 303        |
|    time_elapsed         | 1244       |
|    total_timesteps      | 310272     |
| train/                  |            |
|    approx_kl            | 0.15197122 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.424      |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.268      |
|    value_loss           | 2.61       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.097      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 838        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 304        |
|    time_elapsed         | 1250       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.13442828 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0722    |
|    std                  | 0.268      |
|    value_loss           | 2.2        |
----------------------------------------
Num timesteps: 312000
Best mean reward: 834.70 - Last mean reward per episode: 837.87
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0137      |
| reward_ctrl             | 0.0971      |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0476      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0545      |
| reward_velocity         | 0.22        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 838         |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 305         |
|    time_elapsed         | 1255        |
|    total_timesteps      | 312320      |
| train/                  |             |
|    approx_kl            | 0.080127835 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.538       |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.0693     |
|    std                  | 0.268       |
|    value_loss           | 8.51        |
-----------------------------------------
---------------------------------------
| reward                  | 0.731     |
| reward_contact          | 0.0137    |
| reward_ctrl             | 0.097     |
| reward_motion           | 0.149     |
| reward_orientation      | 0.0477    |
| reward_position         | 3.3e-06   |
| reward_rotation         | 0.149     |
| reward_torque           | 0.0545    |
| reward_velocity         | 0.22      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 839       |
| time/                   |           |
|    fps                  | 248       |
|    iterations           | 306       |
|    time_elapsed         | 1261      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0966893 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -40.7     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.774     |
|    n_updates            | 6100      |
|    policy_gradient_loss | -0.0759   |
|    std                  | 0.268     |
|    value_loss           | 3.27      |
---------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0964     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 840        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 307        |
|    time_elapsed         | 1267       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.07179193 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.5      |
|    explained_variance   | 0.246      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.7       |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.0535    |
|    std                  | 0.268      |
|    value_loss           | 27.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0977     |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 841        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 308        |
|    time_elapsed         | 1273       |
|    total_timesteps      | 315392     |
| train/                  |            |
|    approx_kl            | 0.05680317 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 6140       |
|    policy_gradient_loss | -0.0484    |
|    std                  | 0.268      |
|    value_loss           | 13.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.736      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0971     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.048      |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 842        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 309        |
|    time_elapsed         | 1278       |
|    total_timesteps      | 316416     |
| train/                  |            |
|    approx_kl            | 0.05833668 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.33       |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.0465    |
|    std                  | 0.268      |
|    value_loss           | 17.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.098      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0483     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 843        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 310        |
|    time_elapsed         | 1283       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.07074337 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.34       |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.268      |
|    value_loss           | 18.4       |
----------------------------------------
Num timesteps: 318000
Best mean reward: 837.87 - Last mean reward per episode: 843.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0483     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 844        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 311        |
|    time_elapsed         | 1288       |
|    total_timesteps      | 318464     |
| train/                  |            |
|    approx_kl            | 0.08706002 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.619      |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.0779    |
|    std                  | 0.267      |
|    value_loss           | 2.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.098      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0486     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 844        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 312        |
|    time_elapsed         | 1294       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.06661334 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.267      |
|    value_loss           | 13.8       |
----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0155      |
| reward_ctrl             | 0.0973      |
| reward_motion           | 0.145       |
| reward_orientation      | 0.0487      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 844         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 313         |
|    time_elapsed         | 1300        |
|    total_timesteps      | 320512      |
| train/                  |             |
|    approx_kl            | 0.110263005 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.535       |
|    n_updates            | 6240        |
|    policy_gradient_loss | -0.0932     |
|    std                  | 0.267       |
|    value_loss           | 2.31        |
-----------------------------------------
--------------------------------------
| reward                  | 0.729    |
| reward_contact          | 0.0154   |
| reward_ctrl             | 0.0984   |
| reward_motion           | 0.142    |
| reward_orientation      | 0.0488   |
| reward_position         | 3.3e-06  |
| reward_rotation         | 0.147    |
| reward_torque           | 0.0548   |
| reward_velocity         | 0.222    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 844      |
| time/                   |          |
|    fps                  | 246      |
|    iterations           | 314      |
|    time_elapsed         | 1306     |
|    total_timesteps      | 321536   |
| train/                  |          |
|    approx_kl            | 0.265162 |
|    clip_fraction        | 0.369    |
|    clip_range           | 0.4      |
|    entropy_loss         | -41.2    |
|    explained_variance   | 0.992    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.26     |
|    n_updates            | 6260     |
|    policy_gradient_loss | -0.0946  |
|    std                  | 0.267    |
|    value_loss           | 4.89     |
--------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0997     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0488     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 845        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 315        |
|    time_elapsed         | 1312       |
|    total_timesteps      | 322560     |
| train/                  |            |
|    approx_kl            | 0.19701514 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.89       |
|    n_updates            | 6280       |
|    policy_gradient_loss | -0.0624    |
|    std                  | 0.267      |
|    value_loss           | 8.08       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0492     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.147      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 846        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 316        |
|    time_elapsed         | 1318       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.14349028 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.48       |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.0806    |
|    std                  | 0.267      |
|    value_loss           | 7.37       |
----------------------------------------
Num timesteps: 324000
Best mean reward: 843.72 - Last mean reward per episode: 846.95
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0142     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0492     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 317        |
|    time_elapsed         | 1324       |
|    total_timesteps      | 324608     |
| train/                  |            |
|    approx_kl            | 0.14883742 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.41       |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.0706    |
|    std                  | 0.267      |
|    value_loss           | 4.31       |
----------------------------------------
-----------------------------------------
| reward                  | 0.725       |
| reward_contact          | 0.0142      |
| reward_ctrl             | 0.0998      |
| reward_motion           | 0.136       |
| reward_orientation      | 0.0493      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 847         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 318         |
|    time_elapsed         | 1330        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.069601014 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.7       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.0498     |
|    std                  | 0.267       |
|    value_loss           | 17.1        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.725       |
| reward_contact          | 0.0136      |
| reward_ctrl             | 0.101       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0495      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 319         |
|    time_elapsed         | 1335        |
|    total_timesteps      | 326656      |
| train/                  |             |
|    approx_kl            | 0.082963794 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.0468     |
|    std                  | 0.267       |
|    value_loss           | 21.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0498     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 320        |
|    time_elapsed         | 1340       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.07678221 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.83       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.0498    |
|    std                  | 0.267      |
|    value_loss           | 14.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0502     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 321        |
|    time_elapsed         | 1345       |
|    total_timesteps      | 328704     |
| train/                  |            |
|    approx_kl            | 0.14463776 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.267      |
|    value_loss           | 3.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0506     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 322        |
|    time_elapsed         | 1349       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.58815205 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 6420       |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.267      |
|    value_loss           | 2.39       |
----------------------------------------
Num timesteps: 330000
Best mean reward: 846.95 - Last mean reward per episode: 852.32
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.729      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0506     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 323        |
|    time_elapsed         | 1355       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.15225565 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.391      |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.267      |
|    value_loss           | 2.49       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0505     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 324        |
|    time_elapsed         | 1361       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.27948576 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.301      |
|    n_updates            | 6460       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.267      |
|    value_loss           | 1.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0507     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 325        |
|    time_elapsed         | 1367       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.21981703 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.353      |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.0967    |
|    std                  | 0.267      |
|    value_loss           | 2          |
----------------------------------------
---------------------------------------
| reward                  | 0.728     |
| reward_contact          | 0.013     |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0508    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.15      |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.222     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 854       |
| time/                   |           |
|    fps                  | 242       |
|    iterations           | 326       |
|    time_elapsed         | 1373      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.1953329 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.6     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.115     |
|    n_updates            | 6500      |
|    policy_gradient_loss | -0.0941   |
|    std                  | 0.267     |
|    value_loss           | 1.4       |
---------------------------------------
-----------------------------------------
| reward                  | 0.731       |
| reward_contact          | 0.013       |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0507      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0554      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 855         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 327         |
|    time_elapsed         | 1379        |
|    total_timesteps      | 334848      |
| train/                  |             |
|    approx_kl            | 0.089071915 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.267       |
|    value_loss           | 3.48        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.013      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0508     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 328        |
|    time_elapsed         | 1383       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.08858572 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.3      |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.852      |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.0456    |
|    std                  | 0.267      |
|    value_loss           | 30.7       |
----------------------------------------
Num timesteps: 336000
Best mean reward: 852.32 - Last mean reward per episode: 855.59
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.051      |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 329        |
|    time_elapsed         | 1388       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.13890404 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.445      |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.267      |
|    value_loss           | 1.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0514     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 330        |
|    time_elapsed         | 1394       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.12827478 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.9        |
|    n_updates            | 6580       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.267      |
|    value_loss           | 3.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0513     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 331        |
|    time_elapsed         | 1400       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.17206544 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.266      |
|    value_loss           | 2.52       |
----------------------------------------
---------------------------------------
| reward                  | 0.735     |
| reward_contact          | 0.0124    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0512    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 858       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 332       |
|    time_elapsed         | 1405      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.2605111 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.2     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.936     |
|    n_updates            | 6620      |
|    policy_gradient_loss | -0.0836   |
|    std                  | 0.266     |
|    value_loss           | 2.34      |
---------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0512     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 333        |
|    time_elapsed         | 1412       |
|    total_timesteps      | 340992     |
| train/                  |            |
|    approx_kl            | 0.10314956 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.12       |
|    n_updates            | 6640       |
|    policy_gradient_loss | -0.0526    |
|    std                  | 0.266      |
|    value_loss           | 6.26       |
----------------------------------------
Num timesteps: 342000
Best mean reward: 855.59 - Last mean reward per episode: 858.19
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0512     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 334        |
|    time_elapsed         | 1417       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.15646465 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.249      |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.0907    |
|    std                  | 0.266      |
|    value_loss           | 1.82       |
----------------------------------------
-----------------------------------------
| reward                  | 0.738       |
| reward_contact          | 0.0124      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.137       |
| reward_orientation      | 0.0514      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.152       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 859         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 335         |
|    time_elapsed         | 1422        |
|    total_timesteps      | 343040      |
| train/                  |             |
|    approx_kl            | 0.047534503 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.41        |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.266       |
|    value_loss           | 7.15        |
-----------------------------------------
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0514     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 336        |
|    time_elapsed         | 1428       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.06308009 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.59       |
|    n_updates            | 6700       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.266      |
|    value_loss           | 6.47       |
----------------------------------------
-----------------------------------------
| reward                  | 0.736       |
| reward_contact          | 0.0122      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.13        |
| reward_orientation      | 0.0518      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.155       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 860         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 337         |
|    time_elapsed         | 1434        |
|    total_timesteps      | 345088      |
| train/                  |             |
|    approx_kl            | 0.073026195 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.5       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 6720        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.265       |
|    value_loss           | 22.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0521     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 338        |
|    time_elapsed         | 1439       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.18715525 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.0698    |
|    std                  | 0.265      |
|    value_loss           | 2.88       |
----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0521     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 339        |
|    time_elapsed         | 1445       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.06211298 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.93       |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.265      |
|    value_loss           | 13.8       |
----------------------------------------
Num timesteps: 348000
Best mean reward: 858.19 - Last mean reward per episode: 862.23
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.755       |
| reward_contact          | 0.0126      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0524      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.161       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.231       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 340         |
|    time_elapsed         | 1450        |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.085877754 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 6780        |
|    policy_gradient_loss | -0.0777     |
|    std                  | 0.265       |
|    value_loss           | 2.83        |
-----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0524     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 341        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.25781357 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.73       |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.265      |
|    value_loss           | 22.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0525     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 342        |
|    time_elapsed         | 1460       |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.37170777 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.296      |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.0954    |
|    std                  | 0.265      |
|    value_loss           | 1.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0529     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 343        |
|    time_elapsed         | 1466       |
|    total_timesteps      | 351232     |
| train/                  |            |
|    approx_kl            | 0.14535479 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.0921    |
|    std                  | 0.265      |
|    value_loss           | 3.33       |
----------------------------------------
---------------------------------------
| reward                  | 0.768     |
| reward_contact          | 0.0125    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.141     |
| reward_orientation      | 0.0529    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 239       |
|    iterations           | 344       |
|    time_elapsed         | 1470      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.1058441 |
|    clip_fraction        | 0.269     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.775     |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.0707   |
|    std                  | 0.264     |
|    value_loss           | 2.08      |
---------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0125     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0529     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 345        |
|    time_elapsed         | 1476       |
|    total_timesteps      | 353280     |
| train/                  |            |
|    approx_kl            | 0.06330973 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.195      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.9        |
|    n_updates            | 6880       |
|    policy_gradient_loss | -0.0468    |
|    std                  | 0.264      |
|    value_loss           | 26.2       |
----------------------------------------
Num timesteps: 354000
Best mean reward: 862.23 - Last mean reward per episode: 866.09
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.776      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.053      |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 346        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.06405512 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.264      |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.775     |
| reward_contact          | 0.0131    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.146     |
| reward_orientation      | 0.0529    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 347       |
|    time_elapsed         | 1488      |
|    total_timesteps      | 355328    |
| train/                  |           |
|    approx_kl            | 0.2617231 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.4     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.4       |
|    n_updates            | 6920      |
|    policy_gradient_loss | -0.0972   |
|    std                  | 0.264     |
|    value_loss           | 5.08      |
---------------------------------------
---------------------------------------
| reward                  | 0.776     |
| reward_contact          | 0.0131    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.145     |
| reward_orientation      | 0.0532    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.163     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 348       |
|    time_elapsed         | 1494      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.5615964 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0838   |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.0782   |
|    std                  | 0.264     |
|    value_loss           | 1.03      |
---------------------------------------
-----------------------------------------
| reward                  | 0.777       |
| reward_contact          | 0.0131      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0536      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 349         |
|    time_elapsed         | 1500        |
|    total_timesteps      | 357376      |
| train/                  |             |
|    approx_kl            | 0.059288148 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.25        |
|    n_updates            | 6960        |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.264       |
|    value_loss           | 24.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0125     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.147      |
| reward_orientation      | 0.0538     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 350        |
|    time_elapsed         | 1506       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.06062203 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.34       |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.0544    |
|    std                  | 0.264      |
|    value_loss           | 8.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 351        |
|    time_elapsed         | 1512       |
|    total_timesteps      | 359424     |
| train/                  |            |
|    approx_kl            | 0.07820898 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.04       |
|    n_updates            | 7000       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.264      |
|    value_loss           | 9.03       |
----------------------------------------
Num timesteps: 360000
Best mean reward: 866.09 - Last mean reward per episode: 867.76
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.147      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 352        |
|    time_elapsed         | 1518       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.11952571 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.576      |
|    n_updates            | 7020       |
|    policy_gradient_loss | -0.0757    |
|    std                  | 0.264      |
|    value_loss           | 2.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 353        |
|    time_elapsed         | 1523       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.36200026 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0922     |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.0846    |
|    std                  | 0.264      |
|    value_loss           | 1.51       |
----------------------------------------
-----------------------------------------
| reward                  | 0.779       |
| reward_contact          | 0.0125      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0534      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 354         |
|    time_elapsed         | 1528        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.046221856 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.4       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 7060        |
|    policy_gradient_loss | -0.0558     |
|    std                  | 0.264       |
|    value_loss           | 17.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 355        |
|    time_elapsed         | 1533       |
|    total_timesteps      | 363520     |
| train/                  |            |
|    approx_kl            | 0.07663175 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 0.264      |
|    value_loss           | 4.2        |
----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0536     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 356        |
|    time_elapsed         | 1538       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.23710397 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0598     |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.263      |
|    value_loss           | 1.32       |
----------------------------------------
---------------------------------------
| reward                  | 0.79      |
| reward_contact          | 0.0132    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.154     |
| reward_orientation      | 0.0534    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0562    |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 870       |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 357       |
|    time_elapsed         | 1544      |
|    total_timesteps      | 365568    |
| train/                  |           |
|    approx_kl            | 0.0728744 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.5     |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.59      |
|    n_updates            | 7120      |
|    policy_gradient_loss | -0.0474   |
|    std                  | 0.263     |
|    value_loss           | 16.8      |
---------------------------------------
Num timesteps: 366000
Best mean reward: 867.76 - Last mean reward per episode: 870.75
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 358        |
|    time_elapsed         | 1550       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.12093382 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.311      |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 0.263      |
|    value_loss           | 2.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.8        |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 359        |
|    time_elapsed         | 1556       |
|    total_timesteps      | 367616     |
| train/                  |            |
|    approx_kl            | 0.03586774 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.0327    |
|    std                  | 0.263      |
|    value_loss           | 28.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0537     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 872        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 360        |
|    time_elapsed         | 1560       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.11653953 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.263      |
|    value_loss           | 3.15       |
----------------------------------------
----------------------------------------
| reward                  | 0.802      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0538     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 361        |
|    time_elapsed         | 1565       |
|    total_timesteps      | 369664     |
| train/                  |            |
|    approx_kl            | 0.10203838 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 7200       |
|    policy_gradient_loss | -0.0758    |
|    std                  | 0.263      |
|    value_loss           | 6.29       |
----------------------------------------
---------------------------------------
| reward                  | 0.803     |
| reward_contact          | 0.0126    |
| reward_ctrl             | 0.112     |
| reward_motion           | 0.159     |
| reward_orientation      | 0.0538    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.169     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.241     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 873       |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 362       |
|    time_elapsed         | 1570      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.4398377 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.215     |
|    n_updates            | 7220      |
|    policy_gradient_loss | -0.0976   |
|    std                  | 0.263     |
|    value_loss           | 1.47      |
---------------------------------------
-----------------------------------------
| reward                  | 0.806       |
| reward_contact          | 0.0126      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.16        |
| reward_orientation      | 0.0539      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.241       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 873         |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 363         |
|    time_elapsed         | 1576        |
|    total_timesteps      | 371712      |
| train/                  |             |
|    approx_kl            | 0.048497554 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 7240        |
|    policy_gradient_loss | -0.0562     |
|    std                  | 0.263       |
|    value_loss           | 19.6        |
-----------------------------------------
Num timesteps: 372000
Best mean reward: 870.75 - Last mean reward per episode: 873.10
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 364        |
|    time_elapsed         | 1582       |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.08879957 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 7260       |
|    policy_gradient_loss | -0.076     |
|    std                  | 0.263      |
|    value_loss           | 3.63       |
----------------------------------------
---------------------------------------
| reward                  | 0.807     |
| reward_contact          | 0.0132    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.0535    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.168     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.241     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 874       |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 365       |
|    time_elapsed         | 1588      |
|    total_timesteps      | 373760    |
| train/                  |           |
|    approx_kl            | 0.1352642 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.6     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.75      |
|    n_updates            | 7280      |
|    policy_gradient_loss | -0.0676   |
|    std                  | 0.263     |
|    value_loss           | 3.77      |
---------------------------------------
----------------------------------------
| reward                  | 0.81       |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 366        |
|    time_elapsed         | 1594       |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.08276927 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.11       |
|    n_updates            | 7300       |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.263      |
|    value_loss           | 2.73       |
----------------------------------------
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0537     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 367        |
|    time_elapsed         | 1600       |
|    total_timesteps      | 375808     |
| train/                  |            |
|    approx_kl            | 0.12825412 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.49       |
|    n_updates            | 7320       |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.263      |
|    value_loss           | 1.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.812      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0537     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 368        |
|    time_elapsed         | 1606       |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.12419422 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.891      |
|    n_updates            | 7340       |
|    policy_gradient_loss | -0.0728    |
|    std                  | 0.262      |
|    value_loss           | 4.03       |
----------------------------------------
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0534     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 369        |
|    time_elapsed         | 1612       |
|    total_timesteps      | 377856     |
| train/                  |            |
|    approx_kl            | 0.11390327 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.3      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7360       |
|    policy_gradient_loss | -0.0504    |
|    std                  | 0.262      |
|    value_loss           | 12.1       |
----------------------------------------
Num timesteps: 378000
Best mean reward: 873.10 - Last mean reward per episode: 874.37
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0533     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 370        |
|    time_elapsed         | 1618       |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.12412284 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.4      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.757      |
|    n_updates            | 7380       |
|    policy_gradient_loss | -0.0939    |
|    std                  | 0.262      |
|    value_loss           | 1.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.816      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0529     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 371        |
|    time_elapsed         | 1623       |
|    total_timesteps      | 379904     |
| train/                  |            |
|    approx_kl            | 0.11696975 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 7400       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.262      |
|    value_loss           | 3.01       |
----------------------------------------
-----------------------------------------
| reward                  | 0.805       |
| reward_contact          | 0.0127      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.158       |
| reward_orientation      | 0.0525      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0565      |
| reward_velocity         | 0.242       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 372         |
|    time_elapsed         | 1629        |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.090974815 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.5       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.46        |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.083      |
|    std                  | 0.262       |
|    value_loss           | 9.29        |
-----------------------------------------
---------------------------------------
| reward                  | 0.804     |
| reward_contact          | 0.0127    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.16      |
| reward_orientation      | 0.0525    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.169     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.242     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 875       |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 373       |
|    time_elapsed         | 1635      |
|    total_timesteps      | 381952    |
| train/                  |           |
|    approx_kl            | 0.2094283 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.2     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0832    |
|    n_updates            | 7440      |
|    policy_gradient_loss | -0.101    |
|    std                  | 0.262     |
|    value_loss           | 1.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0523     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 374        |
|    time_elapsed         | 1641       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.12783518 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 7460       |
|    policy_gradient_loss | -0.0444    |
|    std                  | 0.262      |
|    value_loss           | 5.66       |
----------------------------------------
Num timesteps: 384000
Best mean reward: 874.37 - Last mean reward per episode: 874.61
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.155       |
| reward_orientation      | 0.052       |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.242       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 375         |
|    time_elapsed         | 1646        |
|    total_timesteps      | 384000      |
| train/                  |             |
|    approx_kl            | 0.066565424 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.6       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.18        |
|    n_updates            | 7480        |
|    policy_gradient_loss | -0.0684     |
|    std                  | 0.262       |
|    value_loss           | 7.34        |
-----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0521     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 376        |
|    time_elapsed         | 1651       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.19658199 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.552      |
|    n_updates            | 7500       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.262      |
|    value_loss           | 1.99       |
----------------------------------------
-----------------------------------------
| reward                  | 0.803       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.157       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 377         |
|    time_elapsed         | 1655        |
|    total_timesteps      | 386048      |
| train/                  |             |
|    approx_kl            | 0.096728265 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 7520        |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.261       |
|    value_loss           | 6.09        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.803       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.157       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 378         |
|    time_elapsed         | 1661        |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.074183166 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42         |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.36        |
|    n_updates            | 7540        |
|    policy_gradient_loss | -0.0701     |
|    std                  | 0.261       |
|    value_loss           | 9.93        |
-----------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 379        |
|    time_elapsed         | 1667       |
|    total_timesteps      | 388096     |
| train/                  |            |
|    approx_kl            | 0.05800061 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 7560       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.261      |
|    value_loss           | 26.3       |
----------------------------------------
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.153       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 380         |
|    time_elapsed         | 1674        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.097186156 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.844       |
|    n_updates            | 7580        |
|    policy_gradient_loss | -0.0472     |
|    std                  | 0.261       |
|    value_loss           | 8.06        |
-----------------------------------------
Num timesteps: 390000
Best mean reward: 874.61 - Last mean reward per episode: 874.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0129      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 381         |
|    time_elapsed         | 1679        |
|    total_timesteps      | 390144      |
| train/                  |             |
|    approx_kl            | 0.082256764 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.2       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.633       |
|    n_updates            | 7600        |
|    policy_gradient_loss | -0.0791     |
|    std                  | 0.261       |
|    value_loss           | 5.78        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.801       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.153       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.172       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 382         |
|    time_elapsed         | 1683        |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.067797236 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0611     |
|    std                  | 0.261       |
|    value_loss           | 13.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 383        |
|    time_elapsed         | 1689       |
|    total_timesteps      | 392192     |
| train/                  |            |
|    approx_kl            | 0.15364167 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 7640       |
|    policy_gradient_loss | -0.0763    |
|    std                  | 0.261      |
|    value_loss           | 2.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0517     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.245      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 384        |
|    time_elapsed         | 1694       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.35743248 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 7660       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.261      |
|    value_loss           | 2.09       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 385        |
|    time_elapsed         | 1700       |
|    total_timesteps      | 394240     |
| train/                  |            |
|    approx_kl            | 0.12311756 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.5        |
|    n_updates            | 7680       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.261      |
|    value_loss           | 6.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0518     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 386        |
|    time_elapsed         | 1705       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.09561122 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.12       |
|    n_updates            | 7700       |
|    policy_gradient_loss | -0.083     |
|    std                  | 0.261      |
|    value_loss           | 4.77       |
----------------------------------------
Num timesteps: 396000
Best mean reward: 874.70 - Last mean reward per episode: 875.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.167     |
| reward_orientation      | 0.0518    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.248     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 875       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 387       |
|    time_elapsed         | 1711      |
|    total_timesteps      | 396288    |
| train/                  |           |
|    approx_kl            | 0.5216126 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.6     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 7720      |
|    policy_gradient_loss | -0.0863   |
|    std                  | 0.261     |
|    value_loss           | 6.2       |
---------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0516     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 388        |
|    time_elapsed         | 1716       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.06653166 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.22       |
|    n_updates            | 7740       |
|    policy_gradient_loss | -0.0592    |
|    std                  | 0.261      |
|    value_loss           | 20.7       |
----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.167       |
| reward_orientation      | 0.0518      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.175       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.248       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 389         |
|    time_elapsed         | 1721        |
|    total_timesteps      | 398336      |
| train/                  |             |
|    approx_kl            | 0.097563505 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77        |
|    n_updates            | 7760        |
|    policy_gradient_loss | -0.0878     |
|    std                  | 0.261       |
|    value_loss           | 4.98        |
-----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0515     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.177      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.25       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 390        |
|    time_elapsed         | 1726       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.15344225 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7780       |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.261      |
|    value_loss           | 2.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0515     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 391        |
|    time_elapsed         | 1732       |
|    total_timesteps      | 400384     |
| train/                  |            |
|    approx_kl            | 0.06414022 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.44       |
|    n_updates            | 7800       |
|    policy_gradient_loss | -0.0666    |
|    std                  | 0.26       |
|    value_loss           | 12.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.114     |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0515    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.248     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 873       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 392       |
|    time_elapsed         | 1738      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.0929115 |
|    clip_fraction        | 0.225     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.6     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.57      |
|    n_updates            | 7820      |
|    policy_gradient_loss | -0.0835   |
|    std                  | 0.26      |
|    value_loss           | 2.76      |
---------------------------------------
Num timesteps: 402000
Best mean reward: 875.26 - Last mean reward per episode: 872.81
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0512     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 393        |
|    time_elapsed         | 1744       |
|    total_timesteps      | 402432     |
| train/                  |            |
|    approx_kl            | 0.34199274 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.334      |
|    n_updates            | 7840       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.26       |
|    value_loss           | 1.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 394        |
|    time_elapsed         | 1749       |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.12631921 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.454      |
|    n_updates            | 7860       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.26       |
|    value_loss           | 2.23       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 395        |
|    time_elapsed         | 1754       |
|    total_timesteps      | 404480     |
| train/                  |            |
|    approx_kl            | 0.12471636 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7880       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.26       |
|    value_loss           | 7.71       |
----------------------------------------
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.115     |
| reward_motion           | 0.167     |
| reward_orientation      | 0.0512    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.247     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 871       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 396       |
|    time_elapsed         | 1759      |
|    total_timesteps      | 405504    |
| train/                  |           |
|    approx_kl            | 0.1383692 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.3     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.916     |
|    n_updates            | 7900      |
|    policy_gradient_loss | -0.061    |
|    std                  | 0.26      |
|    value_loss           | 2.41      |
---------------------------------------
-----------------------------------------
| reward                  | 0.814       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.114       |
| reward_motion           | 0.162       |
| reward_orientation      | 0.0511      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 397         |
|    time_elapsed         | 1764        |
|    total_timesteps      | 406528      |
| train/                  |             |
|    approx_kl            | 0.073663875 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.3       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.18        |
|    n_updates            | 7920        |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.26        |
|    value_loss           | 27.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 398        |
|    time_elapsed         | 1770       |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.15638089 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 7940       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.26       |
|    value_loss           | 2.01       |
----------------------------------------
Num timesteps: 408000
Best mean reward: 875.26 - Last mean reward per episode: 869.64
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.169      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 399        |
|    time_elapsed         | 1776       |
|    total_timesteps      | 408576     |
| train/                  |            |
|    approx_kl            | 0.07320256 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.476      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 7960       |
|    policy_gradient_loss | -0.047     |
|    std                  | 0.26       |
|    value_loss           | 20.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0511      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.246       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 400         |
|    time_elapsed         | 1782        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.066526555 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.6       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.12        |
|    n_updates            | 7980        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.26        |
|    value_loss           | 9.87        |
-----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 401        |
|    time_elapsed         | 1788       |
|    total_timesteps      | 410624     |
| train/                  |            |
|    approx_kl            | 0.59169793 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0158     |
|    n_updates            | 8000       |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.259      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.819      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.051      |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 402        |
|    time_elapsed         | 1793       |
|    total_timesteps      | 411648     |
| train/                  |            |
|    approx_kl            | 0.07575175 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 8020       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.259      |
|    value_loss           | 8.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 403        |
|    time_elapsed         | 1799       |
|    total_timesteps      | 412672     |
| train/                  |            |
|    approx_kl            | 0.40041345 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.226      |
|    n_updates            | 8040       |
|    policy_gradient_loss | -0.0833    |
|    std                  | 0.259      |
|    value_loss           | 1.69       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.172      |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 404        |
|    time_elapsed         | 1804       |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.10629907 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.223      |
|    n_updates            | 8060       |
|    policy_gradient_loss | -0.0749    |
|    std                  | 0.259      |
|    value_loss           | 1.99       |
----------------------------------------
Num timesteps: 414000
Best mean reward: 875.26 - Last mean reward per episode: 868.80
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.175       |
| reward_orientation      | 0.051       |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 405         |
|    time_elapsed         | 1809        |
|    total_timesteps      | 414720      |
| train/                  |             |
|    approx_kl            | 0.088129334 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 8080        |
|    policy_gradient_loss | -0.0652     |
|    std                  | 0.259       |
|    value_loss           | 4.27        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.176       |
| reward_orientation      | 0.0509      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 406         |
|    time_elapsed         | 1814        |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.058865942 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.076      |
|    std                  | 0.259       |
|    value_loss           | 7.68        |
-----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.175      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.245      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 407        |
|    time_elapsed         | 1820       |
|    total_timesteps      | 416768     |
| train/                  |            |
|    approx_kl            | 0.11264556 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.455      |
|    n_updates            | 8120       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.259      |
|    value_loss           | 1.83       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 408        |
|    time_elapsed         | 1826       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.20351717 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 8140       |
|    policy_gradient_loss | -0.0875    |
|    std                  | 0.258      |
|    value_loss           | 1.66       |
----------------------------------------
----------------------------------------
| reward                  | 0.824      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 409        |
|    time_elapsed         | 1832       |
|    total_timesteps      | 418816     |
| train/                  |            |
|    approx_kl            | 0.07331851 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.39       |
|    n_updates            | 8160       |
|    policy_gradient_loss | -0.0576    |
|    std                  | 0.258      |
|    value_loss           | 26.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.823     |
| reward_contact          | 0.0105    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.179     |
| reward_orientation      | 0.0509    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.17      |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.244     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 410       |
|    time_elapsed         | 1838      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 1.2654184 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0531    |
|    n_updates            | 8180      |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.259     |
|    value_loss           | 1.57      |
---------------------------------------
Num timesteps: 420000
Best mean reward: 875.26 - Last mean reward per episode: 865.77
----------------------------------------
| reward                  | 0.825      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0506     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 411        |
|    time_elapsed         | 1843       |
|    total_timesteps      | 420864     |
| train/                  |            |
|    approx_kl            | 0.09515531 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 8200       |
|    policy_gradient_loss | -0.0884    |
|    std                  | 0.258      |
|    value_loss           | 4.16       |
----------------------------------------
-----------------------------------------
| reward                  | 0.819       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.177       |
| reward_orientation      | 0.0505      |
| reward_position         | 6.1e-11     |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 412         |
|    time_elapsed         | 1849        |
|    total_timesteps      | 421888      |
| train/                  |             |
|    approx_kl            | 0.113677114 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 8220        |
|    policy_gradient_loss | -0.092      |
|    std                  | 0.258       |
|    value_loss           | 1.67        |
-----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.00993    |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0508     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 413        |
|    time_elapsed         | 1854       |
|    total_timesteps      | 422912     |
| train/                  |            |
|    approx_kl            | 0.06169289 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.2       |
|    n_updates            | 8240       |
|    policy_gradient_loss | -0.0454    |
|    std                  | 0.258      |
|    value_loss           | 17.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.828       |
| reward_contact          | 0.00993     |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0508      |
| reward_position         | 6.1e-11     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 414         |
|    time_elapsed         | 1859        |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.053849638 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 8260        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.258       |
|    value_loss           | 8.47        |
-----------------------------------------
----------------------------------------
| reward                  | 0.828      |
| reward_contact          | 0.00992    |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.185      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 415        |
|    time_elapsed         | 1864       |
|    total_timesteps      | 424960     |
| train/                  |            |
|    approx_kl            | 0.13096769 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.993      |
|    n_updates            | 8280       |
|    policy_gradient_loss | -0.0642    |
|    std                  | 0.258      |
|    value_loss           | 7.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.833      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 416        |
|    time_elapsed         | 1869       |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.09408496 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.17       |
|    n_updates            | 8300       |
|    policy_gradient_loss | -0.0506    |
|    std                  | 0.258      |
|    value_loss           | 17.7       |
----------------------------------------
Num timesteps: 426000
Best mean reward: 875.26 - Last mean reward per episode: 867.11
----------------------------------------
| reward                  | 0.837      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 417        |
|    time_elapsed         | 1875       |
|    total_timesteps      | 427008     |
| train/                  |            |
|    approx_kl            | 0.21429619 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.45       |
|    n_updates            | 8320       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.258      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.839      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 418        |
|    time_elapsed         | 1881       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.12873016 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.672      |
|    n_updates            | 8340       |
|    policy_gradient_loss | -0.0924    |
|    std                  | 0.258      |
|    value_loss           | 4.12       |
----------------------------------------
-----------------------------------------
| reward                  | 0.839       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.192       |
| reward_orientation      | 0.0507      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 419         |
|    time_elapsed         | 1887        |
|    total_timesteps      | 429056      |
| train/                  |             |
|    approx_kl            | 0.089033976 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.5       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 8360        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.257       |
|    value_loss           | 2.65        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.836       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.19        |
| reward_orientation      | 0.0504      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.175       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 420         |
|    time_elapsed         | 1892        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.060670216 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.5       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 8380        |
|    policy_gradient_loss | -0.061      |
|    std                  | 0.257       |
|    value_loss           | 6.78        |
-----------------------------------------
----------------------------------------
| reward                  | 0.838      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0504     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.174      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 421        |
|    time_elapsed         | 1898       |
|    total_timesteps      | 431104     |
| train/                  |            |
|    approx_kl            | 0.21317141 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 8400       |
|    policy_gradient_loss | -0.0818    |
|    std                  | 0.257      |
|    value_loss           | 1.94       |
----------------------------------------
Num timesteps: 432000
Best mean reward: 875.26 - Last mean reward per episode: 864.49
----------------------------------------
| reward                  | 0.832      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 422        |
|    time_elapsed         | 1902       |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.06642513 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 4          |
|    n_updates            | 8420       |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.257      |
|    value_loss           | 7.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.833      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 423        |
|    time_elapsed         | 1906       |
|    total_timesteps      | 433152     |
| train/                  |            |
|    approx_kl            | 0.19337192 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.671      |
|    n_updates            | 8440       |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.257      |
|    value_loss           | 2.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.835      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.193      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 424        |
|    time_elapsed         | 1910       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.05366406 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 8460       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.257      |
|    value_loss           | 9.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.829      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 425        |
|    time_elapsed         | 1914       |
|    total_timesteps      | 435200     |
| train/                  |            |
|    approx_kl            | 0.09171594 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 8480       |
|    policy_gradient_loss | -0.0695    |
|    std                  | 0.257      |
|    value_loss           | 4.81       |
----------------------------------------
-----------------------------------------
| reward                  | 0.824       |
| reward_contact          | 0.0111      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.19        |
| reward_orientation      | 0.0506      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 426         |
|    time_elapsed         | 1918        |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.097682305 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.39        |
|    n_updates            | 8500        |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.257       |
|    value_loss           | 3.98        |
-----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.188      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 427        |
|    time_elapsed         | 1922       |
|    total_timesteps      | 437248     |
| train/                  |            |
|    approx_kl            | 0.11022976 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.458      |
|    n_updates            | 8520       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.257      |
|    value_loss           | 2.39       |
----------------------------------------
Num timesteps: 438000
Best mean reward: 875.26 - Last mean reward per episode: 862.80
----------------------------------------
| reward                  | 0.823      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 428        |
|    time_elapsed         | 1926       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.49428883 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.436      |
|    n_updates            | 8540       |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.257      |
|    value_loss           | 4.28       |
----------------------------------------
--------------------------------------
| reward                  | 0.824    |
| reward_contact          | 0.0117   |
| reward_ctrl             | 0.108    |
| reward_motion           | 0.19     |
| reward_orientation      | 0.0507   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.168    |
| reward_torque           | 0.0557   |
| reward_velocity         | 0.24     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 863      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 429      |
|    time_elapsed         | 1930     |
|    total_timesteps      | 439296   |
| train/                  |          |
|    approx_kl            | 1.406707 |
|    clip_fraction        | 0.46     |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.5    |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0764   |
|    n_updates            | 8560     |
|    policy_gradient_loss | -0.0918  |
|    std                  | 0.257    |
|    value_loss           | 1.34     |
--------------------------------------
----------------------------------------
| reward                  | 0.824      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0507     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 430        |
|    time_elapsed         | 1934       |
|    total_timesteps      | 440320     |
| train/                  |            |
|    approx_kl            | 0.11693258 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.707      |
|    n_updates            | 8580       |
|    policy_gradient_loss | -0.0354    |
|    std                  | 0.257      |
|    value_loss           | 4.56       |
----------------------------------------
-----------------------------------------
| reward                  | 0.826       |
| reward_contact          | 0.0115      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.193       |
| reward_orientation      | 0.051       |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 431         |
|    time_elapsed         | 1938        |
|    total_timesteps      | 441344      |
| train/                  |             |
|    approx_kl            | 0.077700645 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.863       |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.0786     |
|    std                  | 0.257       |
|    value_loss           | 3.11        |
-----------------------------------------
----------------------------------------
| reward                  | 0.826      |
| reward_contact          | 0.0121     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.195      |
| reward_orientation      | 0.0511     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 432        |
|    time_elapsed         | 1942       |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.08626939 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.942      |
|    n_updates            | 8620       |
|    policy_gradient_loss | -0.0624    |
|    std                  | 0.257      |
|    value_loss           | 6.76       |
----------------------------------------
-----------------------------------------
| reward                  | 0.828       |
| reward_contact          | 0.0121      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0511      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.166       |
| reward_torque           | 0.0554      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 433         |
|    time_elapsed         | 1947        |
|    total_timesteps      | 443392      |
| train/                  |             |
|    approx_kl            | 0.060638763 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.98        |
|    n_updates            | 8640        |
|    policy_gradient_loss | -0.0559     |
|    std                  | 0.257       |
|    value_loss           | 20.7        |
-----------------------------------------
Num timesteps: 444000
Best mean reward: 875.26 - Last mean reward per episode: 863.12
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0507     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 434        |
|    time_elapsed         | 1952       |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.15423091 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.413      |
|    n_updates            | 8660       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.256      |
|    value_loss           | 2.33       |
----------------------------------------
--------------------------------------
| reward                  | 0.825    |
| reward_contact          | 0.0121   |
| reward_ctrl             | 0.106    |
| reward_motion           | 0.195    |
| reward_orientation      | 0.0507   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.165    |
| reward_torque           | 0.0555   |
| reward_velocity         | 0.24     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 862      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 435      |
|    time_elapsed         | 1957     |
|    total_timesteps      | 445440   |
| train/                  |          |
|    approx_kl            | 0.371701 |
|    clip_fraction        | 0.419    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.5    |
|    explained_variance   | 0.99     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.375    |
|    n_updates            | 8680     |
|    policy_gradient_loss | -0.128   |
|    std                  | 0.256    |
|    value_loss           | 1.72     |
--------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 436        |
|    time_elapsed         | 1962       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.20857248 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 8700       |
|    policy_gradient_loss | -0.0828    |
|    std                  | 0.256      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.829      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.202      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 437        |
|    time_elapsed         | 1968       |
|    total_timesteps      | 447488     |
| train/                  |            |
|    approx_kl            | 0.12473744 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.02       |
|    n_updates            | 8720       |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.256      |
|    value_loss           | 12.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 438        |
|    time_elapsed         | 1972       |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.09645867 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 8740       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.256      |
|    value_loss           | 3.79       |
----------------------------------------
--------------------------------------
| reward                  | 0.823    |
| reward_contact          | 0.0116   |
| reward_ctrl             | 0.104    |
| reward_motion           | 0.199    |
| reward_orientation      | 0.0504   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.164    |
| reward_torque           | 0.0553   |
| reward_velocity         | 0.239    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 862      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 439      |
|    time_elapsed         | 1979     |
|    total_timesteps      | 449536   |
| train/                  |          |
|    approx_kl            | 0.14214  |
|    clip_fraction        | 0.292    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43      |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.807    |
|    n_updates            | 8760     |
|    policy_gradient_loss | -0.0723  |
|    std                  | 0.256    |
|    value_loss           | 2.55     |
--------------------------------------
Num timesteps: 450000
Best mean reward: 875.26 - Last mean reward per episode: 861.16
----------------------------------------
| reward                  | 0.818      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0504     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 440        |
|    time_elapsed         | 1985       |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.08733638 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 8780       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.256      |
|    value_loss           | 2.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.202      |
| reward_orientation      | 0.0502     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 441        |
|    time_elapsed         | 1990       |
|    total_timesteps      | 451584     |
| train/                  |            |
|    approx_kl            | 0.12661164 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 8800       |
|    policy_gradient_loss | -0.055     |
|    std                  | 0.256      |
|    value_loss           | 5.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.818      |
| reward_contact          | 0.0116     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.197      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 442        |
|    time_elapsed         | 1996       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.10178308 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.26       |
|    n_updates            | 8820       |
|    policy_gradient_loss | -0.0747    |
|    std                  | 0.256      |
|    value_loss           | 3.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.2        |
| reward_orientation      | 0.05       |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 443        |
|    time_elapsed         | 2002       |
|    total_timesteps      | 453632     |
| train/                  |            |
|    approx_kl            | 0.11130506 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 8840       |
|    policy_gradient_loss | -0.0403    |
|    std                  | 0.256      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.2        |
| reward_orientation      | 0.0496     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 444        |
|    time_elapsed         | 2008       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.10566397 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.256      |
|    value_loss           | 7.46       |
----------------------------------------
-----------------------------------------
| reward                  | 0.812       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0493      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.158       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 445         |
|    time_elapsed         | 2013        |
|    total_timesteps      | 455680      |
| train/                  |             |
|    approx_kl            | 0.101233244 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 8880        |
|    policy_gradient_loss | -0.0801     |
|    std                  | 0.256       |
|    value_loss           | 2.06        |
-----------------------------------------
Num timesteps: 456000
Best mean reward: 875.26 - Last mean reward per episode: 861.05
----------------------------------------
| reward                  | 0.808      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.196      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 446        |
|    time_elapsed         | 2018       |
|    total_timesteps      | 456704     |
| train/                  |            |
|    approx_kl            | 0.32299775 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.134      |
|    n_updates            | 8900       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.255      |
|    value_loss           | 1.64       |
----------------------------------------
---------------------------------------
| reward                  | 0.811     |
| reward_contact          | 0.0128    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.2       |
| reward_orientation      | 0.0494    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.156     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 863       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 447       |
|    time_elapsed         | 2024      |
|    total_timesteps      | 457728    |
| train/                  |           |
|    approx_kl            | 0.1332501 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.5     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.404     |
|    n_updates            | 8920      |
|    policy_gradient_loss | -0.0634   |
|    std                  | 0.255     |
|    value_loss           | 2.28      |
---------------------------------------
-----------------------------------------
| reward                  | 0.806       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.197       |
| reward_orientation      | 0.0494      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.155       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 448         |
|    time_elapsed         | 2030        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.056530066 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.63        |
|    n_updates            | 8940        |
|    policy_gradient_loss | -0.0535     |
|    std                  | 0.255       |
|    value_loss           | 25.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.194      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 449        |
|    time_elapsed         | 2035       |
|    total_timesteps      | 459776     |
| train/                  |            |
|    approx_kl            | 0.33013603 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 8960       |
|    policy_gradient_loss | -0.0828    |
|    std                  | 0.255      |
|    value_loss           | 2.17       |
----------------------------------------
---------------------------------------
| reward                  | 0.805     |
| reward_contact          | 0.014     |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.198     |
| reward_orientation      | 0.0491    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.153     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 450       |
|    time_elapsed         | 2040      |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.3178975 |
|    clip_fraction        | 0.419     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.8     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0682    |
|    n_updates            | 8980      |
|    policy_gradient_loss | -0.109    |
|    std                  | 0.255     |
|    value_loss           | 1.46      |
---------------------------------------
----------------------------------------
| reward                  | 0.807      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.203      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 451        |
|    time_elapsed         | 2044       |
|    total_timesteps      | 461824     |
| train/                  |            |
|    approx_kl            | 0.06875076 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.1        |
|    n_updates            | 9000       |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.255      |
|    value_loss           | 18.5       |
----------------------------------------
Num timesteps: 462000
Best mean reward: 875.26 - Last mean reward per episode: 859.62
---------------------------------------
| reward                  | 0.803     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.201     |
| reward_orientation      | 0.0494    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.149     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 452       |
|    time_elapsed         | 2049      |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.1651975 |
|    clip_fraction        | 0.346     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.7       |
|    n_updates            | 9020      |
|    policy_gradient_loss | -0.119    |
|    std                  | 0.255     |
|    value_loss           | 1.91      |
---------------------------------------
----------------------------------------
| reward                  | 0.802      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 453        |
|    time_elapsed         | 2053       |
|    total_timesteps      | 463872     |
| train/                  |            |
|    approx_kl            | 0.06466171 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.84       |
|    n_updates            | 9040       |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.255      |
|    value_loss           | 22         |
----------------------------------------
-----------------------------------------
| reward                  | 0.808       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.2         |
| reward_orientation      | 0.0496      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.152       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 858         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 454         |
|    time_elapsed         | 2057        |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.082441345 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.823       |
|    n_updates            | 9060        |
|    policy_gradient_loss | -0.0912     |
|    std                  | 0.254       |
|    value_loss           | 4.63        |
-----------------------------------------
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 455        |
|    time_elapsed         | 2061       |
|    total_timesteps      | 465920     |
| train/                  |            |
|    approx_kl            | 0.13231426 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.66       |
|    n_updates            | 9080       |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.254      |
|    value_loss           | 5.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.81       |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 456        |
|    time_elapsed         | 2065       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.08183888 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 9100       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.254      |
|    value_loss           | 7.1        |
----------------------------------------
-----------------------------------------
| reward                  | 0.809       |
| reward_contact          | 0.014       |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0492      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.154       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 859         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 457         |
|    time_elapsed         | 2069        |
|    total_timesteps      | 467968      |
| train/                  |             |
|    approx_kl            | 0.085577965 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.75        |
|    n_updates            | 9120        |
|    policy_gradient_loss | -0.0661     |
|    std                  | 0.254       |
|    value_loss           | 29.6        |
-----------------------------------------
Num timesteps: 468000
Best mean reward: 875.26 - Last mean reward per episode: 858.56
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0146      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.193       |
| reward_orientation      | 0.0489      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.153       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 858         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 458         |
|    time_elapsed         | 2073        |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.047817096 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.5       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.0509     |
|    std                  | 0.254       |
|    value_loss           | 12.9        |
-----------------------------------------
---------------------------------------
| reward                  | 0.794     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.188     |
| reward_orientation      | 0.0488    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0557    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 459       |
|    time_elapsed         | 2077      |
|    total_timesteps      | 470016    |
| train/                  |           |
|    approx_kl            | 0.1912297 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.457     |
|    n_updates            | 9160      |
|    policy_gradient_loss | -0.105    |
|    std                  | 0.254     |
|    value_loss           | 1.83      |
---------------------------------------
----------------------------------------
| reward                  | 0.795      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0487     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 460        |
|    time_elapsed         | 2081       |
|    total_timesteps      | 471040     |
| train/                  |            |
|    approx_kl            | 0.10118708 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.839      |
|    n_updates            | 9180       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.254      |
|    value_loss           | 2.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0486     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 461        |
|    time_elapsed         | 2085       |
|    total_timesteps      | 472064     |
| train/                  |            |
|    approx_kl            | 0.25231016 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 9200       |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.254      |
|    value_loss           | 1.84       |
----------------------------------------
----------------------------------------
| reward                  | 0.788      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0486     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 462        |
|    time_elapsed         | 2089       |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.07194422 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.67       |
|    n_updates            | 9220       |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.254      |
|    value_loss           | 19.3       |
----------------------------------------
Num timesteps: 474000
Best mean reward: 875.26 - Last mean reward per episode: 856.37
-----------------------------------------
| reward                  | 0.785       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0486      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 463         |
|    time_elapsed         | 2093        |
|    total_timesteps      | 474112      |
| train/                  |             |
|    approx_kl            | 0.104774505 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 9240        |
|    policy_gradient_loss | -0.0747     |
|    std                  | 0.254       |
|    value_loss           | 3.43        |
-----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 464        |
|    time_elapsed         | 2097       |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.06723575 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.8       |
|    n_updates            | 9260       |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.254      |
|    value_loss           | 22.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.786       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.188       |
| reward_orientation      | 0.0489      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 465         |
|    time_elapsed         | 2101        |
|    total_timesteps      | 476160      |
| train/                  |             |
|    approx_kl            | 0.077834636 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.627       |
|    n_updates            | 9280        |
|    policy_gradient_loss | -0.0671     |
|    std                  | 0.254       |
|    value_loss           | 3.78        |
-----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 466        |
|    time_elapsed         | 2105       |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.10053451 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.66       |
|    n_updates            | 9300       |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.254      |
|    value_loss           | 5.46       |
----------------------------------------
-----------------------------------------
| reward                  | 0.782       |
| reward_contact          | 0.0163      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.182       |
| reward_orientation      | 0.049       |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 857         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 467         |
|    time_elapsed         | 2109        |
|    total_timesteps      | 478208      |
| train/                  |             |
|    approx_kl            | 0.043632876 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.89        |
|    n_updates            | 9320        |
|    policy_gradient_loss | -0.0451     |
|    std                  | 0.253       |
|    value_loss           | 19.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.183      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 468        |
|    time_elapsed         | 2113       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.13748741 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 9340       |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.253      |
|    value_loss           | 6.14       |
----------------------------------------
Num timesteps: 480000
Best mean reward: 875.26 - Last mean reward per episode: 857.12
---------------------------------------
| reward                  | 0.784     |
| reward_contact          | 0.0169    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0494    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.0557    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 469       |
|    time_elapsed         | 2117      |
|    total_timesteps      | 480256    |
| train/                  |           |
|    approx_kl            | 0.1623579 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.255     |
|    n_updates            | 9360      |
|    policy_gradient_loss | -0.0696   |
|    std                  | 0.253     |
|    value_loss           | 3.16      |
---------------------------------------
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.182      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 470        |
|    time_elapsed         | 2121       |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.44838685 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 9380       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.253      |
|    value_loss           | 2.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.0169    |
| reward_ctrl             | 0.105     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0499    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0556    |
| reward_velocity         | 0.221     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 471       |
|    time_elapsed         | 2124      |
|    total_timesteps      | 482304    |
| train/                  |           |
|    approx_kl            | 0.0549325 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.509     |
|    learning_rate        | 0.0003    |
|    loss                 | 15.4      |
|    n_updates            | 9400      |
|    policy_gradient_loss | -0.0562   |
|    std                  | 0.253     |
|    value_loss           | 29        |
---------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0168     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 472        |
|    time_elapsed         | 2129       |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.10102114 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.63       |
|    n_updates            | 9420       |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.253      |
|    value_loss           | 3.35       |
----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0168     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0502     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 473        |
|    time_elapsed         | 2133       |
|    total_timesteps      | 484352     |
| train/                  |            |
|    approx_kl            | 0.40397963 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 9440       |
|    policy_gradient_loss | -0.0489    |
|    std                  | 0.253      |
|    value_loss           | 2.1        |
----------------------------------------
---------------------------------------
| reward                  | 0.786     |
| reward_contact          | 0.0162    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0505    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.148     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 859       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 474       |
|    time_elapsed         | 2136      |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0686019 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.2      |
|    n_updates            | 9460      |
|    policy_gradient_loss | -0.0557   |
|    std                  | 0.253     |
|    value_loss           | 18        |
---------------------------------------
Num timesteps: 486000
Best mean reward: 875.26 - Last mean reward per episode: 859.86
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0508     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 475        |
|    time_elapsed         | 2140       |
|    total_timesteps      | 486400     |
| train/                  |            |
|    approx_kl            | 0.05930209 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 9480       |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.253      |
|    value_loss           | 19.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.788      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0508     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 476        |
|    time_elapsed         | 2144       |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.05456622 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.61       |
|    n_updates            | 9500       |
|    policy_gradient_loss | -0.0575    |
|    std                  | 0.253      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.185      |
| reward_orientation      | 0.051      |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 477        |
|    time_elapsed         | 2148       |
|    total_timesteps      | 488448     |
| train/                  |            |
|    approx_kl            | 0.10120793 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.42       |
|    n_updates            | 9520       |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.253      |
|    value_loss           | 39.9       |
----------------------------------------
---------------------------------------
| reward                  | 0.786     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0511    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.146     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 478       |
|    time_elapsed         | 2152      |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 0.3738192 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.234     |
|    n_updates            | 9540      |
|    policy_gradient_loss | -0.11     |
|    std                  | 0.253     |
|    value_loss           | 2.28      |
---------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0508     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 479        |
|    time_elapsed         | 2156       |
|    total_timesteps      | 490496     |
| train/                  |            |
|    approx_kl            | 0.09465661 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 9560       |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.253      |
|    value_loss           | 7.13       |
----------------------------------------
-----------------------------------------
| reward                  | 0.788       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.188       |
| reward_orientation      | 0.0508      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 480         |
|    time_elapsed         | 2160        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.091873385 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.655       |
|    n_updates            | 9580        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 0.253       |
|    value_loss           | 5.85        |
-----------------------------------------
Num timesteps: 492000
Best mean reward: 875.26 - Last mean reward per episode: 861.00
-----------------------------------------
| reward                  | 0.787       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 481         |
|    time_elapsed         | 2164        |
|    total_timesteps      | 492544      |
| train/                  |             |
|    approx_kl            | 0.116025686 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 9600        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.253       |
|    value_loss           | 6.24        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.785       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.183       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 482         |
|    time_elapsed         | 2167        |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.122703984 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.5       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.725       |
|    n_updates            | 9620        |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.253       |
|    value_loss           | 2.25        |
-----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 483        |
|    time_elapsed         | 2171       |
|    total_timesteps      | 494592     |
| train/                  |            |
|    approx_kl            | 0.32508808 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.64       |
|    n_updates            | 9640       |
|    policy_gradient_loss | -0.0302    |
|    std                  | 0.253      |
|    value_loss           | 39.5       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.178     |
| reward_orientation      | 0.0506    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 484       |
|    time_elapsed         | 2175      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 2.1496882 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.339     |
|    n_updates            | 9660      |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.252     |
|    value_loss           | 1.97      |
---------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 485        |
|    time_elapsed         | 2179       |
|    total_timesteps      | 496640     |
| train/                  |            |
|    approx_kl            | 0.04604215 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.191      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.85       |
|    n_updates            | 9680       |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.252      |
|    value_loss           | 26.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.78        |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.176       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 486         |
|    time_elapsed         | 2183        |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.046984963 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    n_updates            | 9700        |
|    policy_gradient_loss | -0.055      |
|    std                  | 0.252       |
|    value_loss           | 7.37        |
-----------------------------------------
Num timesteps: 498000
Best mean reward: 875.26 - Last mean reward per episode: 861.57
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 487        |
|    time_elapsed         | 2188       |
|    total_timesteps      | 498688     |
| train/                  |            |
|    approx_kl            | 0.21521056 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.435      |
|    n_updates            | 9720       |
|    policy_gradient_loss | -0.0627    |
|    std                  | 0.252      |
|    value_loss           | 1.36       |
----------------------------------------
-----------------------------------------
| reward                  | 0.783       |
| reward_contact          | 0.0179      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.179       |
| reward_orientation      | 0.0506      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 488         |
|    time_elapsed         | 2192        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.089148976 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 9740        |
|    policy_gradient_loss | -0.0686     |
|    std                  | 0.252       |
|    value_loss           | 2.29        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.781       |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.177       |
| reward_orientation      | 0.0506      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 489         |
|    time_elapsed         | 2196        |
|    total_timesteps      | 500736      |
| train/                  |             |
|    approx_kl            | 0.065172076 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.46        |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.252       |
|    value_loss           | 3.71        |
-----------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 490        |
|    time_elapsed         | 2200       |
|    total_timesteps      | 501760     |
| train/                  |            |
|    approx_kl            | 0.15233779 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.31       |
|    n_updates            | 9780       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.252      |
|    value_loss           | 2.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0504     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 491        |
|    time_elapsed         | 2203       |
|    total_timesteps      | 502784     |
| train/                  |            |
|    approx_kl            | 0.07327686 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.252      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 492        |
|    time_elapsed         | 2207       |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.14566626 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.276      |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.252      |
|    value_loss           | 2.41       |
----------------------------------------
Num timesteps: 504000
Best mean reward: 875.26 - Last mean reward per episode: 863.38
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0504     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 493        |
|    time_elapsed         | 2211       |
|    total_timesteps      | 504832     |
| train/                  |            |
|    approx_kl            | 0.06796846 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.52       |
|    n_updates            | 9840       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 0.252      |
|    value_loss           | 22.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 494        |
|    time_elapsed         | 2215       |
|    total_timesteps      | 505856     |
| train/                  |            |
|    approx_kl            | 0.14200589 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.323      |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.252      |
|    value_loss           | 1.91       |
----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 495        |
|    time_elapsed         | 2219       |
|    total_timesteps      | 506880     |
| train/                  |            |
|    approx_kl            | 0.06337141 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.252      |
|    value_loss           | 6.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.787       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.184       |
| reward_orientation      | 0.0505      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 496         |
|    time_elapsed         | 2223        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.088687226 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.6        |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.0504     |
|    std                  | 0.252       |
|    value_loss           | 31.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 497        |
|    time_elapsed         | 2227       |
|    total_timesteps      | 508928     |
| train/                  |            |
|    approx_kl            | 0.14227937 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.318      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.252      |
|    value_loss           | 2.72       |
----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 498        |
|    time_elapsed         | 2231       |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.15373136 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.414      |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.252      |
|    value_loss           | 2          |
----------------------------------------
Num timesteps: 510000
Best mean reward: 875.26 - Last mean reward per episode: 867.50
----------------------------------------
| reward                  | 0.792      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0498     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 499        |
|    time_elapsed         | 2234       |
|    total_timesteps      | 510976     |
| train/                  |            |
|    approx_kl            | 0.07176496 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.753      |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.0669    |
|    std                  | 0.252      |
|    value_loss           | 7.9        |
----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0498     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 500        |
|    time_elapsed         | 2238       |
|    total_timesteps      | 512000     |
| train/                  |            |
|    approx_kl            | 0.16078682 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 9980       |
|    policy_gradient_loss | -0.0904    |
|    std                  | 0.252      |
|    value_loss           | 2.32       |
----------------------------------------
---------------------------------------
| reward                  | 0.783     |
| reward_contact          | 0.0161    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.181     |
| reward_orientation      | 0.0498    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 501       |
|    time_elapsed         | 2242      |
|    total_timesteps      | 513024    |
| train/                  |           |
|    approx_kl            | 0.7237595 |
|    clip_fraction        | 0.304     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.332     |
|    n_updates            | 10000     |
|    policy_gradient_loss | -0.0576   |
|    std                  | 0.252     |
|    value_loss           | 2.29      |
---------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 502        |
|    time_elapsed         | 2246       |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.07646564 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.252      |
|    value_loss           | 4.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 503        |
|    time_elapsed         | 2250       |
|    total_timesteps      | 515072     |
| train/                  |            |
|    approx_kl            | 0.04668444 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.65       |
|    n_updates            | 10040      |
|    policy_gradient_loss | -0.052     |
|    std                  | 0.252      |
|    value_loss           | 20.6       |
----------------------------------------
Num timesteps: 516000
Best mean reward: 875.26 - Last mean reward per episode: 867.95
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.174      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 504        |
|    time_elapsed         | 2254       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.14593178 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.543      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.252      |
|    value_loss           | 2.74       |
----------------------------------------
----------------------------------------
| reward                  | 0.778      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 505        |
|    time_elapsed         | 2258       |
|    total_timesteps      | 517120     |
| train/                  |            |
|    approx_kl            | 0.14287704 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.458      |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.252      |
|    value_loss           | 2.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 506        |
|    time_elapsed         | 2262       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.20603453 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.252      |
|    value_loss           | 1.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 507        |
|    time_elapsed         | 2265       |
|    total_timesteps      | 519168     |
| train/                  |            |
|    approx_kl            | 0.34805974 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.283      |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.0618    |
|    std                  | 0.251      |
|    value_loss           | 2.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 508        |
|    time_elapsed         | 2269       |
|    total_timesteps      | 520192     |
| train/                  |            |
|    approx_kl            | 0.17597699 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 10140      |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.251      |
|    value_loss           | 2.01       |
----------------------------------------
---------------------------------------
| reward                  | 0.77      |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.05      |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.229     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 509       |
|    time_elapsed         | 2273      |
|    total_timesteps      | 521216    |
| train/                  |           |
|    approx_kl            | 0.4200024 |
|    clip_fraction        | 0.456     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0943    |
|    n_updates            | 10160     |
|    policy_gradient_loss | -0.0888   |
|    std                  | 0.251     |
|    value_loss           | 1.27      |
---------------------------------------
Num timesteps: 522000
Best mean reward: 875.26 - Last mean reward per episode: 869.60
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 510        |
|    time_elapsed         | 2277       |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.16386592 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.285      |
|    n_updates            | 10180      |
|    policy_gradient_loss | -0.0355    |
|    std                  | 0.251      |
|    value_loss           | 1.16       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 511        |
|    time_elapsed         | 2281       |
|    total_timesteps      | 523264     |
| train/                  |            |
|    approx_kl            | 0.07005659 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.89       |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.251      |
|    value_loss           | 24         |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 512        |
|    time_elapsed         | 2284       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.13509196 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.377      |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.251      |
|    value_loss           | 1.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.773      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 513        |
|    time_elapsed         | 2288       |
|    total_timesteps      | 525312     |
| train/                  |            |
|    approx_kl            | 0.07728009 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.34       |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.0472    |
|    std                  | 0.251      |
|    value_loss           | 18.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.774     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.156     |
| reward_orientation      | 0.0501    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.153     |
| reward_torque           | 0.0562    |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 514       |
|    time_elapsed         | 2292      |
|    total_timesteps      | 526336    |
| train/                  |           |
|    approx_kl            | 0.0725341 |
|    clip_fraction        | 0.241     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.3     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.706     |
|    n_updates            | 10260     |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.251     |
|    value_loss           | 4.89      |
---------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 515        |
|    time_elapsed         | 2296       |
|    total_timesteps      | 527360     |
| train/                  |            |
|    approx_kl            | 0.12517308 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.779      |
|    n_updates            | 10280      |
|    policy_gradient_loss | -0.0839    |
|    std                  | 0.251      |
|    value_loss           | 3.99       |
----------------------------------------
Num timesteps: 528000
Best mean reward: 875.26 - Last mean reward per episode: 868.24
----------------------------------------
| reward                  | 0.768      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 516        |
|    time_elapsed         | 2300       |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.22875251 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 10300      |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.251      |
|    value_loss           | 2.1        |
----------------------------------------
---------------------------------------
| reward                  | 0.768     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.153     |
| reward_orientation      | 0.0499    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.151     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 517       |
|    time_elapsed         | 2304      |
|    total_timesteps      | 529408    |
| train/                  |           |
|    approx_kl            | 1.1021508 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.254     |
|    n_updates            | 10320     |
|    policy_gradient_loss | -0.0772   |
|    std                  | 0.251     |
|    value_loss           | 2.35      |
---------------------------------------
----------------------------------------
| reward                  | 0.761      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 518        |
|    time_elapsed         | 2307       |
|    total_timesteps      | 530432     |
| train/                  |            |
|    approx_kl            | 0.21016224 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0934     |
|    n_updates            | 10340      |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.251      |
|    value_loss           | 1.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.76       |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 519        |
|    time_elapsed         | 2311       |
|    total_timesteps      | 531456     |
| train/                  |            |
|    approx_kl            | 0.43316764 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.51       |
|    n_updates            | 10360      |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.251      |
|    value_loss           | 1.38       |
----------------------------------------
---------------------------------------
| reward                  | 0.755     |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.151     |
| reward_orientation      | 0.0504    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.145     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 520       |
|    time_elapsed         | 2315      |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 1.1664941 |
|    clip_fraction        | 0.438     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.224     |
|    n_updates            | 10380     |
|    policy_gradient_loss | -0.0867   |
|    std                  | 0.251     |
|    value_loss           | 1.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0184     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 521        |
|    time_elapsed         | 2319       |
|    total_timesteps      | 533504     |
| train/                  |            |
|    approx_kl            | 0.16124254 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.626      |
|    n_updates            | 10400      |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.251      |
|    value_loss           | 2.07       |
----------------------------------------
Num timesteps: 534000
Best mean reward: 875.26 - Last mean reward per episode: 868.07
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 522        |
|    time_elapsed         | 2323       |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.05537957 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.462      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 10420      |
|    policy_gradient_loss | -0.0549    |
|    std                  | 0.251      |
|    value_loss           | 45.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.152       |
| reward_orientation      | 0.0503      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 523         |
|    time_elapsed         | 2327        |
|    total_timesteps      | 535552      |
| train/                  |             |
|    approx_kl            | 0.048085444 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.0785      |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.0413     |
|    std                  | 0.251       |
|    value_loss           | 33.4        |
-----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 524        |
|    time_elapsed         | 2330       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.07042433 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.57       |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.0765    |
|    std                  | 0.251      |
|    value_loss           | 5.8        |
----------------------------------------
-----------------------------------------
| reward                  | 0.757       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0499      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 525         |
|    time_elapsed         | 2334        |
|    total_timesteps      | 537600      |
| train/                  |             |
|    approx_kl            | 0.103684336 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.74        |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.0454     |
|    std                  | 0.251       |
|    value_loss           | 10.8        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.763       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.155       |
| reward_orientation      | 0.0499      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 526         |
|    time_elapsed         | 2338        |
|    total_timesteps      | 538624      |
| train/                  |             |
|    approx_kl            | 0.066971764 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.512       |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.251       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 527        |
|    time_elapsed         | 2342       |
|    total_timesteps      | 539648     |
| train/                  |            |
|    approx_kl            | 0.28087366 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.576      |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.251      |
|    value_loss           | 2.43       |
----------------------------------------
Num timesteps: 540000
Best mean reward: 875.26 - Last mean reward per episode: 867.22
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0497     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 528        |
|    time_elapsed         | 2346       |
|    total_timesteps      | 540672     |
| train/                  |            |
|    approx_kl            | 0.16075626 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.23       |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.0952    |
|    std                  | 0.251      |
|    value_loss           | 1.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 529        |
|    time_elapsed         | 2350       |
|    total_timesteps      | 541696     |
| train/                  |            |
|    approx_kl            | 0.08056688 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.719      |
|    n_updates            | 10560      |
|    policy_gradient_loss | -0.0685    |
|    std                  | 0.251      |
|    value_loss           | 3.19       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0494     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 530        |
|    time_elapsed         | 2353       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.10804114 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.905      |
|    n_updates            | 10580      |
|    policy_gradient_loss | -0.0943    |
|    std                  | 0.25       |
|    value_loss           | 2.63       |
----------------------------------------
---------------------------------------
| reward                  | 0.755     |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.153     |
| reward_orientation      | 0.0495    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.143     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 531       |
|    time_elapsed         | 2357      |
|    total_timesteps      | 543744    |
| train/                  |           |
|    approx_kl            | 0.1454529 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.444     |
|    n_updates            | 10600     |
|    policy_gradient_loss | -0.0883   |
|    std                  | 0.25      |
|    value_loss           | 3.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 532        |
|    time_elapsed         | 2361       |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.20185016 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.213      |
|    n_updates            | 10620      |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.25       |
|    value_loss           | 2.07       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0172      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.148       |
| reward_orientation      | 0.049       |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 533         |
|    time_elapsed         | 2365        |
|    total_timesteps      | 545792      |
| train/                  |             |
|    approx_kl            | 0.087668404 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.613       |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.25        |
|    value_loss           | 2.68        |
-----------------------------------------
Num timesteps: 546000
Best mean reward: 875.26 - Last mean reward per episode: 864.40
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.0172      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0493      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 534         |
|    time_elapsed         | 2368        |
|    total_timesteps      | 546816      |
| train/                  |             |
|    approx_kl            | 0.069624566 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.72        |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.059      |
|    std                  | 0.25        |
|    value_loss           | 8.91        |
-----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 535        |
|    time_elapsed         | 2372       |
|    total_timesteps      | 547840     |
| train/                  |            |
|    approx_kl            | 0.48225394 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.053      |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.25       |
|    value_loss           | 1.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0494     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 536        |
|    time_elapsed         | 2375       |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.06866059 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.11       |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.0523    |
|    std                  | 0.25       |
|    value_loss           | 27.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 537        |
|    time_elapsed         | 2379       |
|    total_timesteps      | 549888     |
| train/                  |            |
|    approx_kl            | 0.42434525 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.21       |
|    n_updates            | 10720      |
|    policy_gradient_loss | -0.094     |
|    std                  | 0.25       |
|    value_loss           | 1.83       |
----------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | 0.0166    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0489    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 538       |
|    time_elapsed         | 2382      |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 0.3604675 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.452     |
|    n_updates            | 10740     |
|    policy_gradient_loss | -0.0724   |
|    std                  | 0.25      |
|    value_loss           | 2.19      |
---------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 539        |
|    time_elapsed         | 2385       |
|    total_timesteps      | 551936     |
| train/                  |            |
|    approx_kl            | 0.24526112 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.574      |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.25       |
|    value_loss           | 2.11       |
----------------------------------------
Num timesteps: 552000
Best mean reward: 875.26 - Last mean reward per episode: 864.80
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 540        |
|    time_elapsed         | 2389       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.43754292 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 10780      |
|    policy_gradient_loss | -0.0331    |
|    std                  | 0.25       |
|    value_loss           | 1.53       |
----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0491     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 541        |
|    time_elapsed         | 2392       |
|    total_timesteps      | 553984     |
| train/                  |            |
|    approx_kl            | 0.13341512 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.602      |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.25       |
|    value_loss           | 3.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0492     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 542        |
|    time_elapsed         | 2395       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.11006014 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.03       |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.25       |
|    value_loss           | 8.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 543        |
|    time_elapsed         | 2399       |
|    total_timesteps      | 556032     |
| train/                  |            |
|    approx_kl            | 0.07404641 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.57       |
|    n_updates            | 10840      |
|    policy_gradient_loss | -0.0523    |
|    std                  | 0.25       |
|    value_loss           | 30.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.744     |
| reward_contact          | 0.0157    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.136     |
| reward_orientation      | 0.0496    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.148     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 544       |
|    time_elapsed         | 2402      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.2541511 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.405     |
|    n_updates            | 10860     |
|    policy_gradient_loss | -0.0827   |
|    std                  | 0.249     |
|    value_loss           | 3.12      |
---------------------------------------
Num timesteps: 558000
Best mean reward: 875.26 - Last mean reward per episode: 864.70
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 545        |
|    time_elapsed         | 2405       |
|    total_timesteps      | 558080     |
| train/                  |            |
|    approx_kl            | 0.21288641 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.293      |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.0597    |
|    std                  | 0.249      |
|    value_loss           | 1.65       |
----------------------------------------
---------------------------------------
| reward                  | 0.745     |
| reward_contact          | 0.0151    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.134     |
| reward_orientation      | 0.0499    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.15      |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 864       |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 546       |
|    time_elapsed         | 2409      |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 0.3378772 |
|    clip_fraction        | 0.389     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.171     |
|    n_updates            | 10900     |
|    policy_gradient_loss | -0.0705   |
|    std                  | 0.249     |
|    value_loss           | 1.25      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 547        |
|    time_elapsed         | 2412       |
|    total_timesteps      | 560128     |
| train/                  |            |
|    approx_kl            | 0.11263375 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.492      |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.249      |
|    value_loss           | 2.75       |
----------------------------------------
---------------------------------------
| reward                  | 0.741     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.131     |
| reward_orientation      | 0.0496    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.151     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 864       |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 548       |
|    time_elapsed         | 2416      |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.0918726 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43       |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 10940     |
|    policy_gradient_loss | -0.0801   |
|    std                  | 0.249     |
|    value_loss           | 3.41      |
---------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 549        |
|    time_elapsed         | 2419       |
|    total_timesteps      | 562176     |
| train/                  |            |
|    approx_kl            | 0.28983003 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0315     |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.0781    |
|    std                  | 0.249      |
|    value_loss           | 1.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.741      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 550        |
|    time_elapsed         | 2422       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.06629478 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 10980      |
|    policy_gradient_loss | -0.0328    |
|    std                  | 0.249      |
|    value_loss           | 10.8       |
----------------------------------------
Num timesteps: 564000
Best mean reward: 875.26 - Last mean reward per episode: 864.73
----------------------------------------
| reward                  | 0.741      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 551        |
|    time_elapsed         | 2426       |
|    total_timesteps      | 564224     |
| train/                  |            |
|    approx_kl            | 0.22828011 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.548      |
|    n_updates            | 11000      |
|    policy_gradient_loss | -0.0967    |
|    std                  | 0.249      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0495     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 552        |
|    time_elapsed         | 2429       |
|    total_timesteps      | 565248     |
| train/                  |            |
|    approx_kl            | 0.08491241 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.96       |
|    n_updates            | 11020      |
|    policy_gradient_loss | -0.0507    |
|    std                  | 0.249      |
|    value_loss           | 5.06       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0495     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 553        |
|    time_elapsed         | 2432       |
|    total_timesteps      | 566272     |
| train/                  |            |
|    approx_kl            | 0.08199858 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 11040      |
|    policy_gradient_loss | -0.0362    |
|    std                  | 0.249      |
|    value_loss           | 29         |
----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0492     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 554        |
|    time_elapsed         | 2436       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.06276042 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.0635    |
|    std                  | 0.249      |
|    value_loss           | 6.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0491     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 555        |
|    time_elapsed         | 2439       |
|    total_timesteps      | 568320     |
| train/                  |            |
|    approx_kl            | 0.13898602 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.34       |
|    n_updates            | 11080      |
|    policy_gradient_loss | -0.0489    |
|    std                  | 0.249      |
|    value_loss           | 22.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 556        |
|    time_elapsed         | 2442       |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.11958004 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.385      |
|    n_updates            | 11100      |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.249      |
|    value_loss           | 2.16       |
----------------------------------------
Num timesteps: 570000
Best mean reward: 875.26 - Last mean reward per episode: 865.23
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0493     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 557        |
|    time_elapsed         | 2446       |
|    total_timesteps      | 570368     |
| train/                  |            |
|    approx_kl            | 0.11998257 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.763      |
|    n_updates            | 11120      |
|    policy_gradient_loss | -0.0933    |
|    std                  | 0.249      |
|    value_loss           | 2.59       |
----------------------------------------
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0496     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 558        |
|    time_elapsed         | 2449       |
|    total_timesteps      | 571392     |
| train/                  |            |
|    approx_kl            | 0.21293719 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 11140      |
|    policy_gradient_loss | -0.0946    |
|    std                  | 0.249      |
|    value_loss           | 1.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.767       |
| reward_contact          | 0.0123      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0494      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.159       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 866         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 559         |
|    time_elapsed         | 2453        |
|    total_timesteps      | 572416      |
| train/                  |             |
|    approx_kl            | 0.097502016 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.387       |
|    n_updates            | 11160       |
|    policy_gradient_loss | -0.0399     |
|    std                  | 0.249       |
|    value_loss           | 2.44        |
-----------------------------------------
----------------------------------------
| reward                  | 0.765      |
| reward_contact          | 0.0123     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 560        |
|    time_elapsed         | 2456       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.14848784 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 11180      |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.249      |
|    value_loss           | 14.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0123     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 561        |
|    time_elapsed         | 2459       |
|    total_timesteps      | 574464     |
| train/                  |            |
|    approx_kl            | 0.27951145 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.695      |
|    n_updates            | 11200      |
|    policy_gradient_loss | -0.0496    |
|    std                  | 0.249      |
|    value_loss           | 1.3        |
----------------------------------------
-----------------------------------------
| reward                  | 0.77        |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0491      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.239       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 562         |
|    time_elapsed         | 2463        |
|    total_timesteps      | 575488      |
| train/                  |             |
|    approx_kl            | 0.066412866 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.76        |
|    n_updates            | 11220       |
|    policy_gradient_loss | -0.0414     |
|    std                  | 0.249       |
|    value_loss           | 7.9         |
-----------------------------------------
Num timesteps: 576000
Best mean reward: 875.26 - Last mean reward per episode: 864.15
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0489     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 563        |
|    time_elapsed         | 2466       |
|    total_timesteps      | 576512     |
| train/                  |            |
|    approx_kl            | 0.18022248 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.477      |
|    n_updates            | 11240      |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.249      |
|    value_loss           | 2.09       |
----------------------------------------
-----------------------------------------
| reward                  | 0.762       |
| reward_contact          | 0.0122      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.136       |
| reward_orientation      | 0.0485      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 564         |
|    time_elapsed         | 2469        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.098722026 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 11260       |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.249       |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | 0.0122    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.132     |
| reward_orientation      | 0.0485    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.161     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 862       |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 565       |
|    time_elapsed         | 2473      |
|    total_timesteps      | 578560    |
| train/                  |           |
|    approx_kl            | 1.0474597 |
|    clip_fraction        | 0.539     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.222     |
|    n_updates            | 11280     |
|    policy_gradient_loss | -0.117    |
|    std                  | 0.249     |
|    value_loss           | 1.21      |
---------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0484     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 566        |
|    time_elapsed         | 2476       |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.07186464 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 11300      |
|    policy_gradient_loss | -0.0567    |
|    std                  | 0.248      |
|    value_loss           | 12.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 567        |
|    time_elapsed         | 2479       |
|    total_timesteps      | 580608     |
| train/                  |            |
|    approx_kl            | 0.12718612 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.879      |
|    n_updates            | 11320      |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.248      |
|    value_loss           | 16.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0482      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.159       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 568         |
|    time_elapsed         | 2483        |
|    total_timesteps      | 581632      |
| train/                  |             |
|    approx_kl            | 0.088829204 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.0514     |
|    std                  | 0.248       |
|    value_loss           | 20.5        |
-----------------------------------------
Num timesteps: 582000
Best mean reward: 875.26 - Last mean reward per episode: 861.70
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0481     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 569        |
|    time_elapsed         | 2486       |
|    total_timesteps      | 582656     |
| train/                  |            |
|    approx_kl            | 0.07441252 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.943      |
|    n_updates            | 11360      |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.248      |
|    value_loss           | 7.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0481     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 570        |
|    time_elapsed         | 2489       |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.15689853 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    n_updates            | 11380      |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.248      |
|    value_loss           | 19.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0478     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 571        |
|    time_elapsed         | 2493       |
|    total_timesteps      | 584704     |
| train/                  |            |
|    approx_kl            | 0.17300808 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.386      |
|    n_updates            | 11400      |
|    policy_gradient_loss | -0.0914    |
|    std                  | 0.248      |
|    value_loss           | 1.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.749     |
| reward_contact          | 0.0122    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.126     |
| reward_orientation      | 0.0477    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.161     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 234       |
|    iterations           | 572       |
|    time_elapsed         | 2496      |
|    total_timesteps      | 585728    |
| train/                  |           |
|    approx_kl            | 0.0744725 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | 19.3      |
|    n_updates            | 11420     |
|    policy_gradient_loss | -0.0658   |
|    std                  | 0.248     |
|    value_loss           | 17.2      |
---------------------------------------
