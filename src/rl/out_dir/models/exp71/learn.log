running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp71/PPO_2
---------------------------------
| reward             | 0.115    |
| reward_contact     | 0.04     |
| reward_ctrl        | 0.0226   |
| reward_motion      | -0.1     |
| reward_orientation | 0.05     |
| reward_position    | 2.87e-06 |
| reward_rotation    | 0.0028   |
| reward_torque      | 0.0443   |
| reward_velocity    | 0.0554   |
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 48.8     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | 0.116     |
| reward_contact          | 0.0525    |
| reward_ctrl             | 0.0299    |
| reward_motion           | -0.1      |
| reward_orientation      | 0.0478    |
| reward_position         | 0.00259   |
| reward_rotation         | 0.00412   |
| reward_torque           | 0.0422    |
| reward_velocity         | 0.0369    |
| rollout/                |           |
|    ep_len_mean          | 249       |
|    ep_rew_mean          | 63.5      |
| time/                   |           |
|    fps                  | 324       |
|    iterations           | 2         |
|    time_elapsed         | 6         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 1.7187345 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.2     |
|    explained_variance   | -0.241    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.241     |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.00708   |
|    std                  | 0.362     |
|    value_loss           | 0.666     |
---------------------------------------
----------------------------------------
| reward                  | 0.111      |
| reward_contact          | 0.0533     |
| reward_ctrl             | 0.0288     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0449     |
| reward_position         | 0.0023     |
| reward_rotation         | 0.00628    |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0328     |
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 59.3       |
| time/                   |            |
|    fps                  | 300        |
|    iterations           | 3          |
|    time_elapsed         | 10         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.27637875 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.389      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.427      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.36       |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.0996     |
| reward_contact          | 0.053      |
| reward_ctrl             | 0.0212     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0444     |
| reward_position         | 0.00493    |
| reward_rotation         | 0.00958    |
| reward_torque           | 0.0395     |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 69.4       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 4          |
|    time_elapsed         | 14         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.14225076 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.372      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.357      |
|    value_loss           | 1.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.093      |
| reward_contact          | 0.0508     |
| reward_ctrl             | 0.0197     |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0445     |
| reward_position         | 0.00455    |
| reward_rotation         | 0.0144     |
| reward_torque           | 0.0391     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 53.2       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 5          |
|    time_elapsed         | 17         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.17105049 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.315      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.355      |
|    value_loss           | 1.71       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 54.66
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.0922     |
| reward_contact          | 0.0512     |
| reward_ctrl             | 0.02       |
| reward_motion           | -0.1       |
| reward_orientation      | 0.0442     |
| reward_position         | 0.00438    |
| reward_rotation         | 0.014      |
| reward_torque           | 0.0393     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 54.7       |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.28671187 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.234      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.353      |
|    value_loss           | 1.74       |
----------------------------------------
--------------------------------------
| reward                  | 0.125    |
| reward_contact          | 0.0525   |
| reward_ctrl             | 0.023    |
| reward_motion           | -0.0822  |
| reward_orientation      | 0.0432   |
| reward_position         | 0.00793  |
| reward_rotation         | 0.0136   |
| reward_torque           | 0.04     |
| reward_velocity         | 0.0271   |
| rollout/                |          |
|    ep_len_mean          | 214      |
|    ep_rew_mean          | 63.4     |
| time/                   |          |
|    fps                  | 280      |
|    iterations           | 7        |
|    time_elapsed         | 25       |
|    total_timesteps      | 7168     |
| train/                  |          |
|    approx_kl            | 0.515725 |
|    clip_fraction        | 0.393    |
|    clip_range           | 0.4      |
|    entropy_loss         | -24.4    |
|    explained_variance   | -0.258   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.651    |
|    n_updates            | 120      |
|    policy_gradient_loss | -0.125   |
|    std                  | 0.351    |
|    value_loss           | 2.33     |
--------------------------------------
----------------------------------------
| reward                  | 0.124      |
| reward_contact          | 0.0517     |
| reward_ctrl             | 0.0217     |
| reward_motion           | -0.0833    |
| reward_orientation      | 0.0424     |
| reward_position         | 0.00746    |
| reward_rotation         | 0.0129     |
| reward_torque           | 0.0391     |
| reward_velocity         | 0.0316     |
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 73.7       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 8          |
|    time_elapsed         | 29         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.15744084 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.8      |
|    explained_variance   | 0.29       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.922      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.349      |
|    value_loss           | 2.79       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp71/PPO_3
---------------------------------
| reward             | -0.0412  |
| reward_contact     | 0.0121   |
| reward_ctrl        | 0.00573  |
| reward_motion      | -0.1     |
| reward_orientation | 0.00728  |
| reward_position    | 1.34e-10 |
| reward_rotation    | 0.00887  |
| reward_torque      | 0.00848  |
| reward_velocity    | 0.0163   |
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | -0.0412   |
| reward_contact          | 0.0121    |
| reward_ctrl             | 0.00573   |
| reward_motion           | -0.1      |
| reward_orientation      | 0.00728   |
| reward_position         | 1.34e-10  |
| reward_rotation         | 0.00887   |
| reward_torque           | 0.00848   |
| reward_velocity         | 0.0163    |
| rollout/                |           |
|    ep_len_mean          | 255       |
|    ep_rew_mean          | 55.1      |
| time/                   |           |
|    fps                  | 323       |
|    iterations           | 2         |
|    time_elapsed         | 6         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 2.8188975 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.7     |
|    explained_variance   | -0.139    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.298     |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.00798  |
|    std                  | 0.361     |
|    value_loss           | 0.483     |
---------------------------------------
---------------------------------------
| reward                  | 0.0683    |
| reward_contact          | 0.0183    |
| reward_ctrl             | 0.00873   |
| reward_motion           | -0.0361   |
| reward_orientation      | 0.0137    |
| reward_position         | 0.00379   |
| reward_rotation         | 0.0062    |
| reward_torque           | 0.0158    |
| reward_velocity         | 0.0379    |
| rollout/                |           |
|    ep_len_mean          | 231       |
|    ep_rew_mean          | 49.7      |
| time/                   |           |
|    fps                  | 302       |
|    iterations           | 3         |
|    time_elapsed         | 10        |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 1.0778701 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | -0.0824   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.402     |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.356     |
|    value_loss           | 1.97      |
---------------------------------------
---------------------------------------
| reward                  | 0.119     |
| reward_contact          | 0.0343    |
| reward_ctrl             | 0.0178    |
| reward_motion           | -0.0315   |
| reward_orientation      | 0.0271    |
| reward_position         | 0.00324   |
| reward_rotation         | 0.00608   |
| reward_torque           | 0.0261    |
| reward_velocity         | 0.0358    |
| rollout/                |           |
|    ep_len_mean          | 205       |
|    ep_rew_mean          | 57.2      |
| time/                   |           |
|    fps                  | 294       |
|    iterations           | 4         |
|    time_elapsed         | 13        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.2924347 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.6     |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.562     |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0836   |
|    std                  | 0.351     |
|    value_loss           | 2.76      |
---------------------------------------
---------------------------------------
| reward                  | 0.127     |
| reward_contact          | 0.0367    |
| reward_ctrl             | 0.0204    |
| reward_motion           | -0.0383   |
| reward_orientation      | 0.0283    |
| reward_position         | 0.00292   |
| reward_rotation         | 0.00559   |
| reward_torque           | 0.028     |
| reward_velocity         | 0.043     |
| rollout/                |           |
|    ep_len_mean          | 238       |
|    ep_rew_mean          | 72.7      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 5         |
|    time_elapsed         | 17        |
|    total_timesteps      | 5120      |
| train/                  |           |
|    approx_kl            | 0.2526486 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.8     |
|    explained_variance   | 0.305     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.423     |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.115    |
|    std                  | 0.349     |
|    value_loss           | 2.02      |
---------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 85.61
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.118      |
| reward_contact          | 0.0391     |
| reward_ctrl             | 0.023      |
| reward_motion           | -0.0507    |
| reward_orientation      | 0.0317     |
| reward_position         | 0.00244    |
| reward_rotation         | 0.005      |
| reward_torque           | 0.0315     |
| reward_velocity         | 0.0359     |
| rollout/                |            |
|    ep_len_mean          | 243        |
|    ep_rew_mean          | 80.3       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.15626642 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -24.5      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.566      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.346      |
|    value_loss           | 2.9        |
----------------------------------------
----------------------------------------
| reward                  | 0.116      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0221     |
| reward_motion           | -0.0526    |
| reward_orientation      | 0.0321     |
| reward_position         | 0.00234    |
| reward_rotation         | 0.0048     |
| reward_torque           | 0.0313     |
| reward_velocity         | 0.0364     |
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 78.1       |
| time/                   |            |
|    fps                  | 280        |
|    iterations           | 7          |
|    time_elapsed         | 25         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.14347634 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26        |
|    explained_variance   | 0.309      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.681      |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.345      |
|    value_loss           | 2.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.114      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0208     |
| reward_motion           | -0.0575    |
| reward_orientation      | 0.0324     |
| reward_position         | 0.00366    |
| reward_rotation         | 0.00431    |
| reward_torque           | 0.0318     |
| reward_velocity         | 0.0386     |
| rollout/                |            |
|    ep_len_mean          | 259        |
|    ep_rew_mean          | 90.5       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 8          |
|    time_elapsed         | 29         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.17489353 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -28        |
|    explained_variance   | 0.0555     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.345      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.344      |
|    value_loss           | 2.44       |
----------------------------------------
---------------------------------------
| reward                  | 0.141     |
| reward_contact          | 0.0387    |
| reward_ctrl             | 0.0203    |
| reward_motion           | -0.0357   |
| reward_orientation      | 0.032     |
| reward_position         | 0.00354   |
| reward_rotation         | 0.00418   |
| reward_torque           | 0.032     |
| reward_velocity         | 0.0465    |
| rollout/                |           |
|    ep_len_mean          | 285       |
|    ep_rew_mean          | 103       |
| time/                   |           |
|    fps                  | 274       |
|    iterations           | 9         |
|    time_elapsed         | 33        |
|    total_timesteps      | 9216      |
| train/                  |           |
|    approx_kl            | 0.1429543 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.4       |
|    entropy_loss         | -25.4     |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.405     |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0908   |
|    std                  | 0.342     |
|    value_loss           | 2.18      |
---------------------------------------
----------------------------------------
| reward                  | 0.143      |
| reward_contact          | 0.0394     |
| reward_ctrl             | 0.0198     |
| reward_motion           | -0.0378    |
| reward_orientation      | 0.0317     |
| reward_position         | 0.00343    |
| reward_rotation         | 0.00404    |
| reward_torque           | 0.0321     |
| reward_velocity         | 0.0507     |
| rollout/                |            |
|    ep_len_mean          | 309        |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 272        |
|    iterations           | 10         |
|    time_elapsed         | 37         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.12721698 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.4        |
|    entropy_loss         | -26.4      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.437      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0918    |
|    std                  | 0.341      |
|    value_loss           | 2.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.161      |
| reward_contact          | 0.0382     |
| reward_ctrl             | 0.0195     |
| reward_motion           | -0.0226    |
| reward_orientation      | 0.0314     |
| reward_position         | 0.00332    |
| reward_rotation         | 0.00416    |
| reward_torque           | 0.0323     |
| reward_velocity         | 0.055      |
| rollout/                |            |
|    ep_len_mean          | 331        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 271        |
|    iterations           | 11         |
|    time_elapsed         | 41         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08720869 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -27.4      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.34       |
|    value_loss           | 3.86       |
----------------------------------------
Num timesteps: 12000
Best mean reward: 85.61 - Last mean reward per episode: 131.42
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.175      |
| reward_contact          | 0.0387     |
| reward_ctrl             | 0.0221     |
| reward_motion           | -0.0246    |
| reward_orientation      | 0.0307     |
| reward_position         | 0.00304    |
| reward_rotation         | 0.0058     |
| reward_torque           | 0.033      |
| reward_velocity         | 0.0658     |
| rollout/                |            |
|    ep_len_mean          | 339        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 12         |
|    time_elapsed         | 45         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.12414294 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.503      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.339      |
|    value_loss           | 3.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.166       |
| reward_contact          | 0.0409      |
| reward_ctrl             | 0.0212      |
| reward_motion           | -0.0372     |
| reward_orientation      | 0.034       |
| reward_position         | 0.00304     |
| reward_rotation         | 0.00735     |
| reward_torque           | 0.0339      |
| reward_velocity         | 0.0628      |
| rollout/                |             |
|    ep_len_mean          | 317         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 13          |
|    time_elapsed         | 49          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.092851505 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.597       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.095      |
|    std                  | 0.339       |
|    value_loss           | 3.45        |
-----------------------------------------
----------------------------------------
| reward                  | 0.166      |
| reward_contact          | 0.0409     |
| reward_ctrl             | 0.0212     |
| reward_motion           | -0.0372    |
| reward_orientation      | 0.034      |
| reward_position         | 0.00304    |
| reward_rotation         | 0.00735    |
| reward_torque           | 0.0339     |
| reward_velocity         | 0.0628     |
| rollout/                |            |
|    ep_len_mean          | 333        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 14         |
|    time_elapsed         | 53         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.06355721 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.98       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0817    |
|    std                  | 0.339      |
|    value_loss           | 8.61       |
----------------------------------------
----------------------------------------
| reward                  | 0.177      |
| reward_contact          | 0.0432     |
| reward_ctrl             | 0.0224     |
| reward_motion           | -0.0313    |
| reward_orientation      | 0.035      |
| reward_position         | 0.00376    |
| reward_rotation         | 0.00661    |
| reward_torque           | 0.0352     |
| reward_velocity         | 0.0616     |
| rollout/                |            |
|    ep_len_mean          | 306        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 266        |
|    iterations           | 15         |
|    time_elapsed         | 57         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.12882093 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.5      |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.337      |
|    value_loss           | 3.81       |
----------------------------------------
----------------------------------------
| reward                  | 0.181      |
| reward_contact          | 0.0434     |
| reward_ctrl             | 0.0235     |
| reward_motion           | -0.0318    |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00356    |
| reward_rotation         | 0.00791    |
| reward_torque           | 0.0358     |
| reward_velocity         | 0.0635     |
| rollout/                |            |
|    ep_len_mean          | 310        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 265        |
|    iterations           | 16         |
|    time_elapsed         | 61         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.12712355 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.337      |
|    value_loss           | 4.85       |
----------------------------------------
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0437     |
| reward_ctrl             | 0.0235     |
| reward_motion           | -0.0212    |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00349    |
| reward_rotation         | 0.00782    |
| reward_torque           | 0.0359     |
| reward_velocity         | 0.0669     |
| rollout/                |            |
|    ep_len_mean          | 324        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 17         |
|    time_elapsed         | 65         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.10581872 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.2      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.11       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0945    |
|    std                  | 0.336      |
|    value_loss           | 6.5        |
----------------------------------------
Num timesteps: 18000
Best mean reward: 131.42 - Last mean reward per episode: 136.08
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.205       |
| reward_contact          | 0.044       |
| reward_ctrl             | 0.0272      |
| reward_motion           | -0.0243     |
| reward_orientation      | 0.0344      |
| reward_position         | 0.00325     |
| reward_rotation         | 0.0146      |
| reward_torque           | 0.037       |
| reward_velocity         | 0.0692      |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 264         |
|    iterations           | 18          |
|    time_elapsed         | 69          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.121353015 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.569       |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0961     |
|    std                  | 0.336       |
|    value_loss           | 3.57        |
-----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0443     |
| reward_ctrl             | 0.0269     |
| reward_motion           | -0.0256    |
| reward_orientation      | 0.0343     |
| reward_position         | 0.00319    |
| reward_rotation         | 0.0144     |
| reward_torque           | 0.0371     |
| reward_velocity         | 0.0703     |
| rollout/                |            |
|    ep_len_mean          | 335        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 263        |
|    iterations           | 19         |
|    time_elapsed         | 73         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.12110752 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.4      |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.335      |
|    value_loss           | 4.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0449     |
| reward_ctrl             | 0.0263     |
| reward_motion           | -0.0282    |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00356    |
| reward_rotation         | 0.0142     |
| reward_torque           | 0.0369     |
| reward_velocity         | 0.068      |
| rollout/                |            |
|    ep_len_mean          | 331        |
|    ep_rew_mean          | 135        |
| time/                   |            |
|    fps                  | 263        |
|    iterations           | 20         |
|    time_elapsed         | 77         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.09075759 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.88       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0728    |
|    std                  | 0.334      |
|    value_loss           | 3.04       |
----------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0462     |
| reward_ctrl             | 0.0245     |
| reward_motion           | -0.0313    |
| reward_orientation      | 0.036      |
| reward_position         | 0.00845    |
| reward_rotation         | 0.0143     |
| reward_torque           | 0.0369     |
| reward_velocity         | 0.0645     |
| rollout/                |            |
|    ep_len_mean          | 308        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 21         |
|    time_elapsed         | 81         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.10482955 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.4        |
|    entropy_loss         | -29.7      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.818      |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0998    |
|    std                  | 0.333      |
|    value_loss           | 3.54       |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0456     |
| reward_ctrl             | 0.0247     |
| reward_motion           | -0.0252    |
| reward_orientation      | 0.0358     |
| reward_position         | 0.00833    |
| reward_rotation         | 0.0143     |
| reward_torque           | 0.0371     |
| reward_velocity         | 0.0671     |
| rollout/                |            |
|    ep_len_mean          | 318        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 22         |
|    time_elapsed         | 85         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07880996 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -28.7      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.83       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.333      |
|    value_loss           | 11         |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0458     |
| reward_ctrl             | 0.0251     |
| reward_motion           | -0.0262    |
| reward_orientation      | 0.036      |
| reward_position         | 0.00821    |
| reward_rotation         | 0.0141     |
| reward_torque           | 0.0373     |
| reward_velocity         | 0.0662     |
| rollout/                |            |
|    ep_len_mean          | 318        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 23         |
|    time_elapsed         | 89         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.14121073 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30        |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.745      |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.332      |
|    value_loss           | 2.78       |
----------------------------------------
Num timesteps: 24000
Best mean reward: 136.08 - Last mean reward per episode: 136.50
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0454     |
| reward_ctrl             | 0.0258     |
| reward_motion           | -0.0243    |
| reward_orientation      | 0.036      |
| reward_position         | 0.0085     |
| reward_rotation         | 0.014      |
| reward_torque           | 0.0376     |
| reward_velocity         | 0.0684     |
| rollout/                |            |
|    ep_len_mean          | 324        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 24         |
|    time_elapsed         | 93         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.13927178 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.8      |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.475      |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.33       |
|    value_loss           | 2.7        |
----------------------------------------
-----------------------------------------
| reward                  | 0.21        |
| reward_contact          | 0.0448      |
| reward_ctrl             | 0.0255      |
| reward_motion           | -0.0253     |
| reward_orientation      | 0.0359      |
| reward_position         | 0.00839     |
| reward_rotation         | 0.0138      |
| reward_torque           | 0.0376      |
| reward_velocity         | 0.0693      |
| rollout/                |             |
|    ep_len_mean          | 334         |
|    ep_rew_mean          | 143         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 25          |
|    time_elapsed         | 97          |
|    total_timesteps      | 25600       |
| train/                  |             |
|    approx_kl            | 0.089158475 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.793       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.329       |
|    value_loss           | 3.57        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.22        |
| reward_contact          | 0.0442      |
| reward_ctrl             | 0.0257      |
| reward_motion           | -0.0187     |
| reward_orientation      | 0.0357      |
| reward_position         | 0.00827     |
| reward_rotation         | 0.0158      |
| reward_torque           | 0.0378      |
| reward_velocity         | 0.0709      |
| rollout/                |             |
|    ep_len_mean          | 343         |
|    ep_rew_mean          | 148         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 26          |
|    time_elapsed         | 101         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.104831964 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.663       |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0915     |
|    std                  | 0.328       |
|    value_loss           | 4.46        |
-----------------------------------------
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.0437     |
| reward_ctrl             | 0.0267     |
| reward_motion           | -0.0169    |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00817    |
| reward_rotation         | 0.0179     |
| reward_torque           | 0.038      |
| reward_velocity         | 0.0738     |
| rollout/                |            |
|    ep_len_mean          | 352        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 27         |
|    time_elapsed         | 105        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.13401636 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.9      |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.798      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.327      |
|    value_loss           | 3.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0435     |
| reward_ctrl             | 0.0268     |
| reward_motion           | -0.019     |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00866    |
| reward_rotation         | 0.0174     |
| reward_torque           | 0.0382     |
| reward_velocity         | 0.0765     |
| rollout/                |            |
|    ep_len_mean          | 357        |
|    ep_rew_mean          | 158        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 28         |
|    time_elapsed         | 109        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.09000303 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.631      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0919    |
|    std                  | 0.326      |
|    value_loss           | 4.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.231      |
| reward_contact          | 0.0434     |
| reward_ctrl             | 0.0263     |
| reward_motion           | -0.0155    |
| reward_orientation      | 0.0358     |
| reward_position         | 0.00834    |
| reward_rotation         | 0.0178     |
| reward_torque           | 0.0383     |
| reward_velocity         | 0.0768     |
| rollout/                |            |
|    ep_len_mean          | 362        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 29         |
|    time_elapsed         | 113        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.10544984 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32        |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0948    |
|    std                  | 0.325      |
|    value_loss           | 5.86       |
----------------------------------------
Num timesteps: 30000
Best mean reward: 136.50 - Last mean reward per episode: 161.07
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.231      |
| reward_contact          | 0.043      |
| reward_ctrl             | 0.0267     |
| reward_motion           | -0.0165    |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00823    |
| reward_rotation         | 0.0176     |
| reward_torque           | 0.0385     |
| reward_velocity         | 0.0777     |
| rollout/                |            |
|    ep_len_mean          | 370        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 30         |
|    time_elapsed         | 117        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.09660232 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.72       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.324      |
|    value_loss           | 6.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0268     |
| reward_motion           | -0.0128    |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00813    |
| reward_rotation         | 0.0177     |
| reward_torque           | 0.0386     |
| reward_velocity         | 0.08       |
| rollout/                |            |
|    ep_len_mean          | 378        |
|    ep_rew_mean          | 170        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 31         |
|    time_elapsed         | 121        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.09786753 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.324      |
|    value_loss           | 7.55       |
----------------------------------------
-----------------------------------------
| reward                  | 0.232       |
| reward_contact          | 0.0431      |
| reward_ctrl             | 0.0269      |
| reward_motion           | -0.0158     |
| reward_orientation      | 0.0357      |
| reward_position         | 0.00785     |
| reward_rotation         | 0.0174      |
| reward_torque           | 0.0387      |
| reward_velocity         | 0.0779      |
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 168         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 32          |
|    time_elapsed         | 125         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.072510794 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -31.6       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0984     |
|    std                  | 0.324       |
|    value_loss           | 7.45        |
-----------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.0433    |
| reward_ctrl             | 0.0267    |
| reward_motion           | -0.0168   |
| reward_orientation      | 0.0357    |
| reward_position         | 0.00776   |
| reward_rotation         | 0.0172    |
| reward_torque           | 0.0387    |
| reward_velocity         | 0.0771    |
| rollout/                |           |
|    ep_len_mean          | 383       |
|    ep_rew_mean          | 173       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 33        |
|    time_elapsed         | 128       |
|    total_timesteps      | 33792     |
| train/                  |           |
|    approx_kl            | 0.1286506 |
|    clip_fraction        | 0.274     |
|    clip_range           | 0.4       |
|    entropy_loss         | -31.3     |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.13      |
|    n_updates            | 640       |
|    policy_gradient_loss | -0.0787   |
|    std                  | 0.323     |
|    value_loss           | 15.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.239      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0273     |
| reward_motion           | -0.012     |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00759    |
| reward_rotation         | 0.0181     |
| reward_torque           | 0.0388     |
| reward_velocity         | 0.0811     |
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 178        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 34         |
|    time_elapsed         | 132        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.17250113 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.322      |
|    value_loss           | 2.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.244      |
| reward_contact          | 0.0421     |
| reward_ctrl             | 0.0281     |
| reward_motion           | -0.0106    |
| reward_orientation      | 0.0353     |
| reward_position         | 0.0075     |
| reward_rotation         | 0.0198     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0831     |
| rollout/                |            |
|    ep_len_mean          | 394        |
|    ep_rew_mean          | 180        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 35         |
|    time_elapsed         | 136        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.14800394 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.2      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0852    |
|    std                  | 0.321      |
|    value_loss           | 4.1        |
----------------------------------------
Num timesteps: 36000
Best mean reward: 161.07 - Last mean reward per episode: 178.21
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.242      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0275     |
| reward_motion           | -0.0126    |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00856    |
| reward_rotation         | 0.0194     |
| reward_torque           | 0.0387     |
| reward_velocity         | 0.0822     |
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 178        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 36         |
|    time_elapsed         | 141        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.10492569 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.937      |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.321      |
|    value_loss           | 4.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.243      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0287     |
| reward_motion           | -0.0135    |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00847    |
| reward_rotation         | 0.0193     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0832     |
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | 183        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 37         |
|    time_elapsed         | 145        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.07338588 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.9      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.563      |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.319      |
|    value_loss           | 3.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.248      |
| reward_contact          | 0.0423     |
| reward_ctrl             | 0.0287     |
| reward_motion           | -0.00964   |
| reward_orientation      | 0.0351     |
| reward_position         | 0.00838    |
| reward_rotation         | 0.0191     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0851     |
| rollout/                |            |
|    ep_len_mean          | 403        |
|    ep_rew_mean          | 187        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 38         |
|    time_elapsed         | 149        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.08345885 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | -0.157     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0952    |
|    std                  | 0.319      |
|    value_loss           | 5.51       |
----------------------------------------
----------------------------------------
| reward                  | 0.252      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0285     |
| reward_motion           | -0.00593   |
| reward_orientation      | 0.035      |
| reward_position         | 0.00829    |
| reward_rotation         | 0.0189     |
| reward_torque           | 0.039      |
| reward_velocity         | 0.0868     |
| rollout/                |            |
|    ep_len_mean          | 410        |
|    ep_rew_mean          | 191        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 39         |
|    time_elapsed         | 153        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.13997321 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.4      |
|    explained_variance   | 0.381      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.318      |
|    value_loss           | 4.07       |
----------------------------------------
-----------------------------------------
| reward                  | 0.251       |
| reward_contact          | 0.0415      |
| reward_ctrl             | 0.0283      |
| reward_motion           | -0.00691    |
| reward_orientation      | 0.0349      |
| reward_position         | 0.0082      |
| reward_rotation         | 0.0187      |
| reward_torque           | 0.0391      |
| reward_velocity         | 0.0874      |
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 40          |
|    time_elapsed         | 157         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.122116044 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.4         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.338       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.103      |
|    std                  | 0.317       |
|    value_loss           | 2.23        |
-----------------------------------------
----------------------------------------
| reward                  | 0.257      |
| reward_contact          | 0.0412     |
| reward_ctrl             | 0.0295     |
| reward_motion           | -0.00475   |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00812    |
| reward_rotation         | 0.0198     |
| reward_torque           | 0.0393     |
| reward_velocity         | 0.0892     |
| rollout/                |            |
|    ep_len_mean          | 422        |
|    ep_rew_mean          | 201        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 41         |
|    time_elapsed         | 161        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.14308947 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.8      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0968    |
|    std                  | 0.315      |
|    value_loss           | 1.76       |
----------------------------------------
Num timesteps: 42000
Best mean reward: 178.21 - Last mean reward per episode: 201.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.256       |
| reward_contact          | 0.0414      |
| reward_ctrl             | 0.0293      |
| reward_motion           | -0.00572    |
| reward_orientation      | 0.0346      |
| reward_position         | 0.00804     |
| reward_rotation         | 0.0196      |
| reward_torque           | 0.0393      |
| reward_velocity         | 0.0895      |
| rollout/                |             |
|    ep_len_mean          | 429         |
|    ep_rew_mean          | 205         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 42          |
|    time_elapsed         | 165         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.096792825 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.4         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.951       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0869     |
|    std                  | 0.314       |
|    value_loss           | 5.39        |
-----------------------------------------
--------------------------------------
| reward                  | 0.263    |
| reward_contact          | 0.0415   |
| reward_ctrl             | 0.0296   |
| reward_motion           | 8.66e-05 |
| reward_orientation      | 0.0345   |
| reward_position         | 0.00795  |
| reward_rotation         | 0.0195   |
| reward_torque           | 0.0394   |
| reward_velocity         | 0.0907   |
| rollout/                |          |
|    ep_len_mean          | 435      |
|    ep_rew_mean          | 210      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 43       |
|    time_elapsed         | 169      |
|    total_timesteps      | 44032    |
| train/                  |          |
|    approx_kl            | 0.086929 |
|    clip_fraction        | 0.176    |
|    clip_range           | 0.4      |
|    entropy_loss         | -33.4    |
|    explained_variance   | 0.621    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.409    |
|    n_updates            | 840      |
|    policy_gradient_loss | -0.0838  |
|    std                  | 0.313    |
|    value_loss           | 2.45     |
--------------------------------------
----------------------------------------
| reward                  | 0.269      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0301     |
| reward_motion           | 0.00178    |
| reward_orientation      | 0.0349     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0202     |
| reward_torque           | 0.0398     |
| reward_velocity         | 0.093      |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 217        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 44         |
|    time_elapsed         | 173        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.08752781 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.852      |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0807    |
|    std                  | 0.312      |
|    value_loss           | 4.11       |
----------------------------------------
-----------------------------------------
| reward                  | 0.277       |
| reward_contact          | 0.0423      |
| reward_ctrl             | 0.0302      |
| reward_motion           | 0.00565     |
| reward_orientation      | 0.0351      |
| reward_position         | 0.00788     |
| reward_rotation         | 0.02        |
| reward_torque           | 0.0402      |
| reward_velocity         | 0.0958      |
| rollout/                |             |
|    ep_len_mean          | 455         |
|    ep_rew_mean          | 223         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 45          |
|    time_elapsed         | 177         |
|    total_timesteps      | 46080       |
| train/                  |             |
|    approx_kl            | 0.071741566 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32         |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.25        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0762     |
|    std                  | 0.312       |
|    value_loss           | 9.17        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.286       |
| reward_contact          | 0.0428      |
| reward_ctrl             | 0.0302      |
| reward_motion           | 0.0123      |
| reward_orientation      | 0.0352      |
| reward_position         | 0.00788     |
| reward_rotation         | 0.02        |
| reward_torque           | 0.0405      |
| reward_velocity         | 0.097       |
| rollout/                |             |
|    ep_len_mean          | 461         |
|    ep_rew_mean          | 229         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 46          |
|    time_elapsed         | 181         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.084220745 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32.6       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.635       |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0969     |
|    std                  | 0.312       |
|    value_loss           | 6.41        |
-----------------------------------------
Num timesteps: 48000
Best mean reward: 201.31 - Last mean reward per episode: 235.97
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.295      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0181     |
| reward_orientation      | 0.0354     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0203     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0999     |
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | 236        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 47         |
|    time_elapsed         | 184        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.18948185 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.371      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.31       |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.302      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0226     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0203     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.102      |
| rollout/                |            |
|    ep_len_mean          | 479        |
|    ep_rew_mean          | 242        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 48         |
|    time_elapsed         | 188        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.08534669 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.913      |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.309      |
|    value_loss           | 4.86       |
----------------------------------------
----------------------------------------
| reward                  | 0.309      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0.026      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0202     |
| reward_torque           | 0.0413     |
| reward_velocity         | 0.105      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 49         |
|    time_elapsed         | 192        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.10656306 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.1      |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.48       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.309      |
|    value_loss           | 6.39       |
----------------------------------------
----------------------------------------
| reward                  | 0.31       |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0309     |
| reward_motion           | 0.0259     |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0205     |
| reward_torque           | 0.0415     |
| reward_velocity         | 0.105      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 252        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 50         |
|    time_elapsed         | 196        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.08580808 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.3      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.666      |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.309      |
|    value_loss           | 4.69       |
----------------------------------------
----------------------------------------
| reward                  | 0.316      |
| reward_contact          | 0.0422     |
| reward_ctrl             | 0.031      |
| reward_motion           | 0.0276     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00788    |
| reward_rotation         | 0.0225     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.107      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 258        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 51         |
|    time_elapsed         | 200        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.08360584 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.75       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.309      |
|    value_loss           | 11.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.32       |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0308     |
| reward_motion           | 0.0296     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0238     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.11       |
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | 264        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 52         |
|    time_elapsed         | 204        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.11466862 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.998      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.308      |
|    value_loss           | 5.64       |
----------------------------------------
Num timesteps: 54000
Best mean reward: 235.97 - Last mean reward per episode: 266.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.318      |
| reward_contact          | 0.0414     |
| reward_ctrl             | 0.0304     |
| reward_motion           | 0.0255     |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0255     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.11       |
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | 266        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 53         |
|    time_elapsed         | 208        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.14964464 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.307      |
|    value_loss           | 3.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.326      |
| reward_contact          | 0.041      |
| reward_ctrl             | 0.0311     |
| reward_motion           | 0.0301     |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0256     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | 273        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 54         |
|    time_elapsed         | 212        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.09345175 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.89       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.307      |
|    value_loss           | 6.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.333      |
| reward_contact          | 0.0406     |
| reward_ctrl             | 0.0318     |
| reward_motion           | 0.0339     |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00753    |
| reward_rotation         | 0.0262     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 526        |
|    ep_rew_mean          | 279        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 55         |
|    time_elapsed         | 216        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.10636637 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.7      |
|    explained_variance   | 0.744      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.885      |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.306      |
|    value_loss           | 5.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.34       |
| reward_contact          | 0.0402     |
| reward_ctrl             | 0.0328     |
| reward_motion           | 0.0372     |
| reward_orientation      | 0.0351     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.027      |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.118      |
| rollout/                |            |
|    ep_len_mean          | 536        |
|    ep_rew_mean          | 286        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 56         |
|    time_elapsed         | 220        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.23972933 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.219      |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.306      |
|    value_loss           | 1.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.349      |
| reward_contact          | 0.0403     |
| reward_ctrl             | 0.0333     |
| reward_motion           | 0.0419     |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.0272     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 545        |
|    ep_rew_mean          | 292        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 57         |
|    time_elapsed         | 223        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.08778707 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.6        |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.306      |
|    value_loss           | 5.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.353      |
| reward_contact          | 0.0399     |
| reward_ctrl             | 0.0338     |
| reward_motion           | 0.0438     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00752    |
| reward_rotation         | 0.0274     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 553        |
|    ep_rew_mean          | 299        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 58         |
|    time_elapsed         | 227        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.10045493 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.669      |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.305      |
|    value_loss           | 5.44       |
----------------------------------------
Num timesteps: 60000
Best mean reward: 266.31 - Last mean reward per episode: 304.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.352      |
| reward_contact          | 0.0394     |
| reward_ctrl             | 0.0338     |
| reward_motion           | 0.0438     |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00749    |
| reward_rotation         | 0.0273     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 563        |
|    ep_rew_mean          | 305        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 59         |
|    time_elapsed         | 232        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.08165057 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.46       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0773    |
|    std                  | 0.305      |
|    value_loss           | 9.75       |
----------------------------------------
-----------------------------------------
| reward                  | 0.36        |
| reward_contact          | 0.0389      |
| reward_ctrl             | 0.0343      |
| reward_motion           | 0.0494      |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00729     |
| reward_rotation         | 0.0274      |
| reward_torque           | 0.0429      |
| reward_velocity         | 0.126       |
| rollout/                |             |
|    ep_len_mean          | 573         |
|    ep_rew_mean          | 311         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 60          |
|    time_elapsed         | 236         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.098490864 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.836       |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.106      |
|    std                  | 0.305       |
|    value_loss           | 4.06        |
-----------------------------------------
----------------------------------------
| reward                  | 0.367      |
| reward_contact          | 0.0385     |
| reward_ctrl             | 0.0343     |
| reward_motion           | 0.0521     |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00729    |
| reward_rotation         | 0.0288     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.129      |
| rollout/                |            |
|    ep_len_mean          | 583        |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 61         |
|    time_elapsed         | 240        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.08731906 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.304      |
|    value_loss           | 3.86       |
----------------------------------------
---------------------------------------
| reward                  | 0.371     |
| reward_contact          | 0.0379    |
| reward_ctrl             | 0.0336    |
| reward_motion           | 0.0567    |
| reward_orientation      | 0.0339    |
| reward_position         | 0.00729   |
| reward_rotation         | 0.0291    |
| reward_torque           | 0.0427    |
| reward_velocity         | 0.13      |
| rollout/                |           |
|    ep_len_mean          | 583       |
|    ep_rew_mean          | 319       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 62        |
|    time_elapsed         | 244       |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.2156363 |
|    clip_fraction        | 0.248     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.9     |
|    explained_variance   | 0.969     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.554     |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.0813   |
|    std                  | 0.304     |
|    value_loss           | 2.45      |
---------------------------------------
----------------------------------------
| reward                  | 0.38       |
| reward_contact          | 0.0373     |
| reward_ctrl             | 0.0348     |
| reward_motion           | 0.0618     |
| reward_orientation      | 0.034      |
| reward_position         | 0.00729    |
| reward_rotation         | 0.0303     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.131      |
| rollout/                |            |
|    ep_len_mean          | 593        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 63         |
|    time_elapsed         | 247        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.07750149 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.3      |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.26       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0685    |
|    std                  | 0.303      |
|    value_loss           | 7.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.388       |
| reward_contact          | 0.0367      |
| reward_ctrl             | 0.035       |
| reward_motion           | 0.068       |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00729     |
| reward_rotation         | 0.0307      |
| reward_torque           | 0.043       |
| reward_velocity         | 0.134       |
| rollout/                |             |
|    ep_len_mean          | 593         |
|    ep_rew_mean          | 325         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 64          |
|    time_elapsed         | 251         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.059819594 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51        |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0714     |
|    std                  | 0.303       |
|    value_loss           | 7.62        |
-----------------------------------------
Num timesteps: 66000
Best mean reward: 304.99 - Last mean reward per episode: 329.54
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.389      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0356     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0335     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0306     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.134      |
| rollout/                |            |
|    ep_len_mean          | 606        |
|    ep_rew_mean          | 333        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 65         |
|    time_elapsed         | 255        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.09172962 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0932    |
|    std                  | 0.303      |
|    value_loss           | 10.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.393      |
| reward_contact          | 0.0366     |
| reward_ctrl             | 0.036      |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 614        |
|    ep_rew_mean          | 339        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 66         |
|    time_elapsed         | 259        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.07323794 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.15       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.303      |
|    value_loss           | 15.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.393      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0365     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0337     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0432     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 618        |
|    ep_rew_mean          | 341        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 67         |
|    time_elapsed         | 263        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.08804298 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.302      |
|    value_loss           | 9.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.395      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0.068      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0323     |
| reward_torque           | 0.0435     |
| reward_velocity         | 0.136      |
| rollout/                |            |
|    ep_len_mean          | 627        |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 68         |
|    time_elapsed         | 267        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.05851388 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.5      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0707    |
|    std                  | 0.302      |
|    value_loss           | 3.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.403      |
| reward_contact          | 0.0367     |
| reward_ctrl             | 0.0387     |
| reward_motion           | 0.0722     |
| reward_orientation      | 0.034      |
| reward_position         | 0.00727    |
| reward_rotation         | 0.0334     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.137      |
| rollout/                |            |
|    ep_len_mean          | 627        |
|    ep_rew_mean          | 348        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 69         |
|    time_elapsed         | 271        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.07668175 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.302      |
|    value_loss           | 7.53       |
----------------------------------------
-----------------------------------------
| reward                  | 0.413       |
| reward_contact          | 0.0361      |
| reward_ctrl             | 0.0388      |
| reward_motion           | 0.0788      |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00723     |
| reward_rotation         | 0.0344      |
| reward_torque           | 0.0439      |
| reward_velocity         | 0.14        |
| rollout/                |             |
|    ep_len_mean          | 635         |
|    ep_rew_mean          | 355         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 70          |
|    time_elapsed         | 275         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.069769934 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0775     |
|    std                  | 0.301       |
|    value_loss           | 3.42        |
-----------------------------------------
Num timesteps: 72000
Best mean reward: 329.54 - Last mean reward per episode: 362.51
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.419      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0402     |
| reward_motion           | 0.0799     |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0441     |
| reward_velocity         | 0.142      |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 363        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 71         |
|    time_elapsed         | 279        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.07062052 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.419      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.22       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0676    |
|    std                  | 0.301      |
|    value_loss           | 19.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.412      |
| reward_contact          | 0.0362     |
| reward_ctrl             | 0.0411     |
| reward_motion           | 0.073      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.139      |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 365        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 72         |
|    time_elapsed         | 283        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.10795828 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.5      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.301      |
|    value_loss           | 18.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.419      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0.0785     |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 73         |
|    time_elapsed         | 286        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.07978987 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.4        |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.301      |
|    value_loss           | 20.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.418     |
| reward_contact          | 0.0357    |
| reward_ctrl             | 0.0422    |
| reward_motion           | 0.0773    |
| reward_orientation      | 0.0345    |
| reward_position         | 0.00681   |
| reward_rotation         | 0.0374    |
| reward_torque           | 0.0446    |
| reward_velocity         | 0.14      |
| rollout/                |           |
|    ep_len_mean          | 645       |
|    ep_rew_mean          | 368       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 74        |
|    time_elapsed         | 290       |
|    total_timesteps      | 75776     |
| train/                  |           |
|    approx_kl            | 0.1111098 |
|    clip_fraction        | 0.257     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.7     |
|    explained_variance   | 0.432     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.32      |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0731   |
|    std                  | 0.301     |
|    value_loss           | 14.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.423      |
| reward_contact          | 0.0356     |
| reward_ctrl             | 0.0422     |
| reward_motion           | 0.0826     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0445     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 645        |
|    ep_rew_mean          | 370        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 75         |
|    time_elapsed         | 294        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.07241102 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0868    |
|    std                  | 0.301      |
|    value_loss           | 10.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.423       |
| reward_contact          | 0.0356      |
| reward_ctrl             | 0.0423      |
| reward_motion           | 0.0826      |
| reward_orientation      | 0.0348      |
| reward_position         | 0.00681     |
| reward_rotation         | 0.0369      |
| reward_torque           | 0.0447      |
| reward_velocity         | 0.139       |
| rollout/                |             |
|    ep_len_mean          | 653         |
|    ep_rew_mean          | 376         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 76          |
|    time_elapsed         | 298         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.080832265 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.59        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.301       |
|    value_loss           | 8.54        |
-----------------------------------------
Num timesteps: 78000
Best mean reward: 362.51 - Last mean reward per episode: 376.31
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.427      |
| reward_contact          | 0.0351     |
| reward_ctrl             | 0.0419     |
| reward_motion           | 0.0851     |
| reward_orientation      | 0.0348     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0383     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.14       |
| rollout/                |            |
|    ep_len_mean          | 662        |
|    ep_rew_mean          | 383        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 77         |
|    time_elapsed         | 302        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.05964671 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.02       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.301      |
|    value_loss           | 13.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.436       |
| reward_contact          | 0.0345      |
| reward_ctrl             | 0.0425      |
| reward_motion           | 0.0902      |
| reward_orientation      | 0.0348      |
| reward_position         | 0.00681     |
| reward_rotation         | 0.0393      |
| reward_torque           | 0.0448      |
| reward_velocity         | 0.143       |
| rollout/                |             |
|    ep_len_mean          | 662         |
|    ep_rew_mean          | 386         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 78          |
|    time_elapsed         | 306         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.103833094 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 60.4        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0619     |
|    std                  | 0.301       |
|    value_loss           | 39.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.443      |
| reward_contact          | 0.034      |
| reward_ctrl             | 0.0429     |
| reward_motion           | 0.0968     |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.144      |
| rollout/                |            |
|    ep_len_mean          | 672        |
|    ep_rew_mean          | 393        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 79         |
|    time_elapsed         | 310        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.13581583 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0959    |
|    std                  | 0.3        |
|    value_loss           | 11.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.452      |
| reward_contact          | 0.0336     |
| reward_ctrl             | 0.0431     |
| reward_motion           | 0.103      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00681    |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.147      |
| rollout/                |            |
|    ep_len_mean          | 682        |
|    ep_rew_mean          | 400        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 80         |
|    time_elapsed         | 314        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.06820634 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.236      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.22       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0778    |
|    std                  | 0.3        |
|    value_loss           | 14.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.452      |
| reward_contact          | 0.0342     |
| reward_ctrl             | 0.0431     |
| reward_motion           | 0.103      |
| reward_orientation      | 0.034      |
| reward_position         | 0.0068     |
| reward_rotation         | 0.0393     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.146      |
| rollout/                |            |
|    ep_len_mean          | 689        |
|    ep_rew_mean          | 405        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 81         |
|    time_elapsed         | 318        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.13237971 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.3        |
|    value_loss           | 16.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.456      |
| reward_contact          | 0.0336     |
| reward_ctrl             | 0.0428     |
| reward_motion           | 0.107      |
| reward_orientation      | 0.034      |
| reward_position         | 0.0066     |
| reward_rotation         | 0.0394     |
| reward_torque           | 0.0449     |
| reward_velocity         | 0.148      |
| rollout/                |            |
|    ep_len_mean          | 699        |
|    ep_rew_mean          | 412        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 82         |
|    time_elapsed         | 321        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.09671812 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.5      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.993      |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0862    |
|    std                  | 0.3        |
|    value_loss           | 4.85       |
----------------------------------------
Num timesteps: 84000
Best mean reward: 376.31 - Last mean reward per episode: 418.04
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.465      |
| reward_contact          | 0.033      |
| reward_ctrl             | 0.044      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.0395     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.149      |
| rollout/                |            |
|    ep_len_mean          | 707        |
|    ep_rew_mean          | 418        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 83         |
|    time_elapsed         | 325        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.10577983 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.86       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.3        |
|    value_loss           | 8.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.471      |
| reward_contact          | 0.0324     |
| reward_ctrl             | 0.0439     |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0332     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.04       |
| reward_torque           | 0.045      |
| reward_velocity         | 0.15       |
| rollout/                |            |
|    ep_len_mean          | 717        |
|    ep_rew_mean          | 426        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 84         |
|    time_elapsed         | 329        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.07027109 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.9      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.03       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.299      |
|    value_loss           | 3.17       |
----------------------------------------
-----------------------------------------
| reward                  | 0.478       |
| reward_contact          | 0.0318      |
| reward_ctrl             | 0.0446      |
| reward_motion           | 0.121       |
| reward_orientation      | 0.0336      |
| reward_position         | 0.0066      |
| reward_rotation         | 0.0421      |
| reward_torque           | 0.0452      |
| reward_velocity         | 0.152       |
| rollout/                |             |
|    ep_len_mean          | 717         |
|    ep_rew_mean          | 428         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 85          |
|    time_elapsed         | 333         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.067421064 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39        |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.299       |
|    value_loss           | 8.93        |
-----------------------------------------
----------------------------------------
| reward                  | 0.486      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0443     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0333     |
| reward_position         | 0.0066     |
| reward_rotation         | 0.043      |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.153      |
| rollout/                |            |
|    ep_len_mean          | 727        |
|    ep_rew_mean          | 436        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 86         |
|    time_elapsed         | 337        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.09245771 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.3      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.89       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.299      |
|    value_loss           | 4.61       |
----------------------------------------
-----------------------------------------
| reward                  | 0.478       |
| reward_contact          | 0.0312      |
| reward_ctrl             | 0.0443      |
| reward_motion           | 0.122       |
| reward_orientation      | 0.0329      |
| reward_position         | 0.00607     |
| reward_rotation         | 0.0438      |
| reward_torque           | 0.0451      |
| reward_velocity         | 0.153       |
| rollout/                |             |
|    ep_len_mean          | 744         |
|    ep_rew_mean          | 448         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 87          |
|    time_elapsed         | 341         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.073994204 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.5       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.736       |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0755     |
|    std                  | 0.298       |
|    value_loss           | 2.74        |
-----------------------------------------
Num timesteps: 90000
Best mean reward: 418.04 - Last mean reward per episode: 455.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.485      |
| reward_contact          | 0.0307     |
| reward_ctrl             | 0.0449     |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0326     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0452     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.156      |
| rollout/                |            |
|    ep_len_mean          | 753        |
|    ep_rew_mean          | 456        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 88         |
|    time_elapsed         | 345        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.07807736 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0761    |
|    std                  | 0.298      |
|    value_loss           | 8.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.493      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.0461     |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0329     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.158      |
| rollout/                |            |
|    ep_len_mean          | 762        |
|    ep_rew_mean          | 464        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 89         |
|    time_elapsed         | 349        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.07425137 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.86       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.298      |
|    value_loss           | 16.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.495      |
| reward_contact          | 0.0299     |
| reward_ctrl             | 0.0466     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0329     |
| reward_position         | 0.00609    |
| reward_rotation         | 0.0477     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.158      |
| rollout/                |            |
|    ep_len_mean          | 762        |
|    ep_rew_mean          | 465        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 90         |
|    time_elapsed         | 353        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.07237715 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.83       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0731    |
|    std                  | 0.297      |
|    value_loss           | 15.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.505     |
| reward_contact          | 0.0293    |
| reward_ctrl             | 0.0479    |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0332    |
| reward_position         | 0.00609   |
| reward_rotation         | 0.0495    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.161     |
| rollout/                |           |
|    ep_len_mean          | 771       |
|    ep_rew_mean          | 473       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 91        |
|    time_elapsed         | 357       |
|    total_timesteps      | 93184     |
| train/                  |           |
|    approx_kl            | 0.0687594 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -36.7     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.85      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.0664   |
|    std                  | 0.297     |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.506     |
| reward_contact          | 0.0293    |
| reward_ctrl             | 0.0484    |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0333    |
| reward_position         | 0.00608   |
| reward_rotation         | 0.0496    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.161     |
| rollout/                |           |
|    ep_len_mean          | 780       |
|    ep_rew_mean          | 479       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 92        |
|    time_elapsed         | 361       |
|    total_timesteps      | 94208     |
| train/                  |           |
|    approx_kl            | 0.1301598 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.6     |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.851     |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.0686   |
|    std                  | 0.297     |
|    value_loss           | 3.91      |
---------------------------------------
----------------------------------------
| reward                  | 0.505      |
| reward_contact          | 0.0287     |
| reward_ctrl             | 0.0496     |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0336     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0519     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.162      |
| rollout/                |            |
|    ep_len_mean          | 780        |
|    ep_rew_mean          | 482        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 93         |
|    time_elapsed         | 365        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.10852234 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.72       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.297      |
|    value_loss           | 5.57       |
----------------------------------------
Num timesteps: 96000
Best mean reward: 455.99 - Last mean reward per episode: 486.21
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.507       |
| reward_contact          | 0.0292      |
| reward_ctrl             | 0.0486      |
| reward_motion           | 0.131       |
| reward_orientation      | 0.0337      |
| reward_position         | 0.00608     |
| reward_rotation         | 0.0513      |
| reward_torque           | 0.0455      |
| reward_velocity         | 0.162       |
| rollout/                |             |
|    ep_len_mean          | 780         |
|    ep_rew_mean          | 486         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 94          |
|    time_elapsed         | 369         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.057091806 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34         |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0496     |
|    std                  | 0.297       |
|    value_loss           | 20.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.515      |
| reward_contact          | 0.0292     |
| reward_ctrl             | 0.0486     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0339     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0524     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.162      |
| rollout/                |            |
|    ep_len_mean          | 789        |
|    ep_rew_mean          | 494        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 95         |
|    time_elapsed         | 373        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.07392697 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.7      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.95       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.297      |
|    value_loss           | 10.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.517       |
| reward_contact          | 0.0286      |
| reward_ctrl             | 0.0484      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0339      |
| reward_position         | 0.00608     |
| reward_rotation         | 0.0531      |
| reward_torque           | 0.0455      |
| reward_velocity         | 0.164       |
| rollout/                |             |
|    ep_len_mean          | 798         |
|    ep_rew_mean          | 502         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 96          |
|    time_elapsed         | 378         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.071486704 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.075      |
|    std                  | 0.296       |
|    value_loss           | 4.92        |
-----------------------------------------
----------------------------------------
| reward                  | 0.518      |
| reward_contact          | 0.028      |
| reward_ctrl             | 0.0494     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00608    |
| reward_rotation         | 0.0515     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.166      |
| rollout/                |            |
|    ep_len_mean          | 805        |
|    ep_rew_mean          | 509        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 97         |
|    time_elapsed         | 382        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.10340509 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.754      |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.296      |
|    value_loss           | 4.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.528      |
| reward_contact          | 0.0274     |
| reward_ctrl             | 0.0512     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0344     |
| reward_position         | 0.00585    |
| reward_rotation         | 0.0526     |
| reward_torque           | 0.0461     |
| reward_velocity         | 0.167      |
| rollout/                |            |
|    ep_len_mean          | 806        |
|    ep_rew_mean          | 514        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 98         |
|    time_elapsed         | 386        |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.16705886 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.307      |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 0.296      |
|    value_loss           | 2.19       |
----------------------------------------
-----------------------------------------
| reward                  | 0.535       |
| reward_contact          | 0.0268      |
| reward_ctrl             | 0.0514      |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0347      |
| reward_position         | 0.00579     |
| reward_rotation         | 0.0529      |
| reward_torque           | 0.0462      |
| reward_velocity         | 0.169       |
| rollout/                |             |
|    ep_len_mean          | 816         |
|    ep_rew_mean          | 522         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 99          |
|    time_elapsed         | 389         |
|    total_timesteps      | 101376      |
| train/                  |             |
|    approx_kl            | 0.046520814 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.3       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 3           |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.296       |
|    value_loss           | 19.5        |
-----------------------------------------
Num timesteps: 102000
Best mean reward: 486.21 - Last mean reward per episode: 522.11
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.528       |
| reward_contact          | 0.0274      |
| reward_ctrl             | 0.0512      |
| reward_motion           | 0.143       |
| reward_orientation      | 0.0349      |
| reward_position         | 0.00579     |
| reward_rotation         | 0.0529      |
| reward_torque           | 0.0461      |
| reward_velocity         | 0.167       |
| rollout/                |             |
|    ep_len_mean          | 815         |
|    ep_rew_mean          | 525         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 100         |
|    time_elapsed         | 393         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.045892596 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0436     |
|    std                  | 0.296       |
|    value_loss           | 34.6        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.534       |
| reward_contact          | 0.0268      |
| reward_ctrl             | 0.0522      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0347      |
| reward_position         | 0.00562     |
| reward_rotation         | 0.0553      |
| reward_torque           | 0.0462      |
| reward_velocity         | 0.17        |
| rollout/                |             |
|    ep_len_mean          | 825         |
|    ep_rew_mean          | 533         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 101         |
|    time_elapsed         | 397         |
|    total_timesteps      | 103424      |
| train/                  |             |
|    approx_kl            | 0.038358767 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.8       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0575     |
|    std                  | 0.296       |
|    value_loss           | 5.2         |
-----------------------------------------
----------------------------------------
| reward                  | 0.543      |
| reward_contact          | 0.0262     |
| reward_ctrl             | 0.0529     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0345     |
| reward_position         | 0.00539    |
| reward_rotation         | 0.0572     |
| reward_torque           | 0.0464     |
| reward_velocity         | 0.172      |
| rollout/                |            |
|    ep_len_mean          | 835        |
|    ep_rew_mean          | 541        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 102        |
|    time_elapsed         | 401        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07268999 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.06       |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.296      |
|    value_loss           | 20.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.542      |
| reward_contact          | 0.0256     |
| reward_ctrl             | 0.0531     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0342     |
| reward_position         | 0.00429    |
| reward_rotation         | 0.0566     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.173      |
| rollout/                |            |
|    ep_len_mean          | 844        |
|    ep_rew_mean          | 549        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 103        |
|    time_elapsed         | 405        |
|    total_timesteps      | 105472     |
| train/                  |            |
|    approx_kl            | 0.10307126 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.221      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.79       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0449    |
|    std                  | 0.296      |
|    value_loss           | 25.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.541      |
| reward_contact          | 0.0256     |
| reward_ctrl             | 0.0535     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.034      |
| reward_position         | 0.00323    |
| reward_rotation         | 0.0566     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.173      |
| rollout/                |            |
|    ep_len_mean          | 852        |
|    ep_rew_mean          | 556        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 104        |
|    time_elapsed         | 410        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.10403752 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.37       |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.296      |
|    value_loss           | 4.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.545      |
| reward_contact          | 0.0251     |
| reward_ctrl             | 0.0549     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0568     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.175      |
| rollout/                |            |
|    ep_len_mean          | 862        |
|    ep_rew_mean          | 562        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 105        |
|    time_elapsed         | 414        |
|    total_timesteps      | 107520     |
| train/                  |            |
|    approx_kl            | 0.06929568 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -32.2      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0874    |
|    std                  | 0.296      |
|    value_loss           | 6.98       |
----------------------------------------
Num timesteps: 108000
Best mean reward: 522.11 - Last mean reward per episode: 569.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.545       |
| reward_contact          | 0.0246      |
| reward_ctrl             | 0.0556      |
| reward_motion           | 0.151       |
| reward_orientation      | 0.0338      |
| reward_position         | 0.00207     |
| reward_rotation         | 0.0563      |
| reward_torque           | 0.0472      |
| reward_velocity         | 0.175       |
| rollout/                |             |
|    ep_len_mean          | 871         |
|    ep_rew_mean          | 569         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 106         |
|    time_elapsed         | 418         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.084463835 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.54        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.116      |
|    std                  | 0.296       |
|    value_loss           | 11.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.552      |
| reward_contact          | 0.0242     |
| reward_ctrl             | 0.057      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0338     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0578     |
| reward_torque           | 0.0475     |
| reward_velocity         | 0.176      |
| rollout/                |            |
|    ep_len_mean          | 881        |
|    ep_rew_mean          | 577        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 107        |
|    time_elapsed         | 422        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.15689455 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.69       |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.296      |
|    value_loss           | 14.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.56       |
| reward_contact          | 0.0236     |
| reward_ctrl             | 0.0585     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.034      |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0602     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.179      |
| rollout/                |            |
|    ep_len_mean          | 889        |
|    ep_rew_mean          | 585        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 108        |
|    time_elapsed         | 426        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.12359036 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.907      |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0778    |
|    std                  | 0.296      |
|    value_loss           | 4.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.558      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0589     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0346     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0615     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.18       |
| rollout/                |            |
|    ep_len_mean          | 892        |
|    ep_rew_mean          | 591        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 109        |
|    time_elapsed         | 429        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.06287424 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.55       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0593    |
|    std                  | 0.296      |
|    value_loss           | 11.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.566      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0596     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00207    |
| reward_rotation         | 0.0616     |
| reward_torque           | 0.0479     |
| reward_velocity         | 0.182      |
| rollout/                |            |
|    ep_len_mean          | 898        |
|    ep_rew_mean          | 598        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 110        |
|    time_elapsed         | 433        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.05106736 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.8      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.27       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.295      |
|    value_loss           | 28.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.567       |
| reward_contact          | 0.0235      |
| reward_ctrl             | 0.0604      |
| reward_motion           | 0.153       |
| reward_orientation      | 0.035       |
| reward_position         | 0.00207     |
| reward_rotation         | 0.0637      |
| reward_torque           | 0.048       |
| reward_velocity         | 0.181       |
| rollout/                |             |
|    ep_len_mean          | 898         |
|    ep_rew_mean          | 600         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 111         |
|    time_elapsed         | 437         |
|    total_timesteps      | 113664      |
| train/                  |             |
|    approx_kl            | 0.085767195 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.41        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.295       |
|    value_loss           | 13.3        |
-----------------------------------------
Num timesteps: 114000
Best mean reward: 569.26 - Last mean reward per episode: 600.00
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.575      |
| reward_contact          | 0.0229     |
| reward_ctrl             | 0.0613     |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0347     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0646     |
| reward_torque           | 0.048      |
| reward_velocity         | 0.183      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 112        |
|    time_elapsed         | 441        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.12279582 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.883      |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.295      |
|    value_loss           | 4.73       |
----------------------------------------
----------------------------------------
| reward                  | 0.583      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0624     |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0348     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0654     |
| reward_torque           | 0.0482     |
| reward_velocity         | 0.185      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 610        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 113        |
|    time_elapsed         | 445        |
|    total_timesteps      | 115712     |
| train/                  |            |
|    approx_kl            | 0.15132055 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.295      |
|    value_loss           | 9.3        |
----------------------------------------
----------------------------------------
| reward                  | 0.583      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.063      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.035      |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0649     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.185      |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 612        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 114        |
|    time_elapsed         | 449        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.07537727 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0585    |
|    std                  | 0.295      |
|    value_loss           | 16.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.577      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0632     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00169    |
| reward_rotation         | 0.0632     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.182      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 613        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 115        |
|    time_elapsed         | 453        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.06908874 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0493    |
|    std                  | 0.294      |
|    value_loss           | 8.22       |
----------------------------------------
----------------------------------------
| reward                  | 0.581      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.063      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0647     |
| reward_torque           | 0.0483     |
| reward_velocity         | 0.181      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 116        |
|    time_elapsed         | 457        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.14941782 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.294      |
|    value_loss           | 4.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.581      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0644     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.067      |
| reward_torque           | 0.0485     |
| reward_velocity         | 0.181      |
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 617        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 117        |
|    time_elapsed         | 460        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.15306525 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.4      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0946    |
|    std                  | 0.294      |
|    value_loss           | 13.5       |
----------------------------------------
Num timesteps: 120000
Best mean reward: 600.00 - Last mean reward per episode: 617.14
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.589      |
| reward_contact          | 0.0228     |
| reward_ctrl             | 0.0656     |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0686     |
| reward_torque           | 0.0487     |
| reward_velocity         | 0.184      |
| rollout/                |            |
|    ep_len_mean          | 914        |
|    ep_rew_mean          | 625        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 118        |
|    time_elapsed         | 464        |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.07449144 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0712    |
|    std                  | 0.294      |
|    value_loss           | 13.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.595       |
| reward_contact          | 0.0222      |
| reward_ctrl             | 0.0668      |
| reward_motion           | 0.163       |
| reward_orientation      | 0.0355      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0701      |
| reward_torque           | 0.0489      |
| reward_velocity         | 0.187       |
| rollout/                |             |
|    ep_len_mean          | 922         |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 119         |
|    time_elapsed         | 468         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.089235544 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.294       |
|    value_loss           | 28.8        |
-----------------------------------------
---------------------------------------
| reward                  | 0.601     |
| reward_contact          | 0.0221    |
| reward_ctrl             | 0.0677    |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0357    |
| reward_position         | 0.00119   |
| reward_rotation         | 0.0724    |
| reward_torque           | 0.049     |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 922       |
|    ep_rew_mean          | 635       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 120       |
|    time_elapsed         | 472       |
|    total_timesteps      | 122880    |
| train/                  |           |
|    approx_kl            | 0.1289843 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.4     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.47      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -0.0779   |
|    std                  | 0.294     |
|    value_loss           | 15.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.603      |
| reward_contact          | 0.0221     |
| reward_ctrl             | 0.0688     |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0357     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0744     |
| reward_torque           | 0.0491     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 922        |
|    ep_rew_mean          | 638        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 121        |
|    time_elapsed         | 476        |
|    total_timesteps      | 123904     |
| train/                  |            |
|    approx_kl            | 0.10008621 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.823      |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.293      |
|    value_loss           | 3.22       |
----------------------------------------
-----------------------------------------
| reward                  | 0.609       |
| reward_contact          | 0.0215      |
| reward_ctrl             | 0.0702      |
| reward_motion           | 0.165       |
| reward_orientation      | 0.0355      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0767      |
| reward_torque           | 0.0493      |
| reward_velocity         | 0.19        |
| rollout/                |             |
|    ep_len_mean          | 925         |
|    ep_rew_mean          | 644         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 122         |
|    time_elapsed         | 480         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.061297253 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.5       |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0504     |
|    std                  | 0.293       |
|    value_loss           | 25.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.61       |
| reward_contact          | 0.0215     |
| reward_ctrl             | 0.0702     |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0355     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0767     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.19       |
| rollout/                |            |
|    ep_len_mean          | 930        |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 123        |
|    time_elapsed         | 483        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.07653153 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.1       |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0706    |
|    std                  | 0.293      |
|    value_loss           | 12.7       |
----------------------------------------
Num timesteps: 126000
Best mean reward: 617.14 - Last mean reward per episode: 648.87
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0215     |
| reward_ctrl             | 0.0708     |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0352     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0782     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 939        |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 124        |
|    time_elapsed         | 487        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.18368107 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.847      |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0928    |
|    std                  | 0.293      |
|    value_loss           | 5.47       |
----------------------------------------
-----------------------------------------
| reward                  | 0.62        |
| reward_contact          | 0.0209      |
| reward_ctrl             | 0.0717      |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0352      |
| reward_position         | 0.00119     |
| reward_rotation         | 0.0782      |
| reward_torque           | 0.0495      |
| reward_velocity         | 0.192       |
| rollout/                |             |
|    ep_len_mean          | 948         |
|    ep_rew_mean          | 664         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 125         |
|    time_elapsed         | 491         |
|    total_timesteps      | 128000      |
| train/                  |             |
|    approx_kl            | 0.049489558 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.1       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.27        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.065      |
|    std                  | 0.293       |
|    value_loss           | 10.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.616      |
| reward_contact          | 0.0214     |
| reward_ctrl             | 0.0713     |
| reward_motion           | 0.169      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0777     |
| reward_torque           | 0.0494     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 666        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 126        |
|    time_elapsed         | 495        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.07996367 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.815      |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0959    |
|    std                  | 0.293      |
|    value_loss           | 3.91       |
----------------------------------------
----------------------------------------
| reward                  | 0.619      |
| reward_contact          | 0.0216     |
| reward_ctrl             | 0.0727     |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0786     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 667        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 127        |
|    time_elapsed         | 499        |
|    total_timesteps      | 130048     |
| train/                  |            |
|    approx_kl            | 0.07502734 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.087     |
|    std                  | 0.293      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.619      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0733     |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0356     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.0789     |
| reward_torque           | 0.0498     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 670        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 128        |
|    time_elapsed         | 503        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.20551392 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.292      |
|    value_loss           | 2.04       |
----------------------------------------
Num timesteps: 132000
Best mean reward: 648.87 - Last mean reward per episode: 677.19
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0211     |
| reward_ctrl             | 0.0735     |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0353     |
| reward_position         | 0.00119    |
| reward_rotation         | 0.08       |
| reward_torque           | 0.05       |
| reward_velocity         | 0.193      |
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | 677        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 129        |
|    time_elapsed         | 507        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.13319708 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.864      |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.292      |
|    value_loss           | 3.72       |
----------------------------------------
----------------------------------------
| reward                  | 0.634      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0747     |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0356     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.082      |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.194      |
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 686        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 130        |
|    time_elapsed         | 510        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.37947002 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.992      |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.292      |
|    value_loss           | 5.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.634      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0738     |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0356     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0825     |
| reward_torque           | 0.0502     |
| reward_velocity         | 0.195      |
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 687        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 131        |
|    time_elapsed         | 514        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.18544862 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.9      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.9        |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0916    |
|    std                  | 0.292      |
|    value_loss           | 3.4        |
----------------------------------------
---------------------------------------
| reward                  | 0.636     |
| reward_contact          | 0.0204    |
| reward_ctrl             | 0.0741    |
| reward_motion           | 0.175     |
| reward_orientation      | 0.036     |
| reward_position         | 6.24e-05  |
| reward_rotation         | 0.0845    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.195     |
| rollout/                |           |
|    ep_len_mean          | 968       |
|    ep_rew_mean          | 690       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 132       |
|    time_elapsed         | 518       |
|    total_timesteps      | 135168    |
| train/                  |           |
|    approx_kl            | 0.0858034 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.5     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.612     |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.0785   |
|    std                  | 0.292     |
|    value_loss           | 3.7       |
---------------------------------------
-----------------------------------------
| reward                  | 0.632       |
| reward_contact          | 0.0204      |
| reward_ctrl             | 0.075       |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0363      |
| reward_position         | 6.24e-05    |
| reward_rotation         | 0.0845      |
| reward_torque           | 0.0504      |
| reward_velocity         | 0.194       |
| rollout/                |             |
|    ep_len_mean          | 968         |
|    ep_rew_mean          | 691         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 133         |
|    time_elapsed         | 522         |
|    total_timesteps      | 136192      |
| train/                  |             |
|    approx_kl            | 0.061183065 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.291       |
|    value_loss           | 15.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.0204     |
| reward_ctrl             | 0.0761     |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0366     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0849     |
| reward_torque           | 0.0506     |
| reward_velocity         | 0.195      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 692        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 134        |
|    time_elapsed         | 526        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.10948655 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.801      |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.291      |
|    value_loss           | 5.23       |
----------------------------------------
Num timesteps: 138000
Best mean reward: 677.19 - Last mean reward per episode: 692.62
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.633      |
| reward_contact          | 0.0209     |
| reward_ctrl             | 0.0755     |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0369     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0837     |
| reward_torque           | 0.0505     |
| reward_velocity         | 0.193      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 693        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 135        |
|    time_elapsed         | 529        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.06692734 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.24       |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.291      |
|    value_loss           | 8          |
----------------------------------------
----------------------------------------
| reward                  | 0.635      |
| reward_contact          | 0.0209     |
| reward_ctrl             | 0.0768     |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0371     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0838     |
| reward_torque           | 0.0507     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 694        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 136        |
|    time_elapsed         | 533        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.06882158 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0725    |
|    std                  | 0.291      |
|    value_loss           | 8.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.623       |
| reward_contact          | 0.0214      |
| reward_ctrl             | 0.0763      |
| reward_motion           | 0.164       |
| reward_orientation      | 0.0373      |
| reward_position         | 6.24e-05    |
| reward_rotation         | 0.083       |
| reward_torque           | 0.0506      |
| reward_velocity         | 0.19        |
| rollout/                |             |
|    ep_len_mean          | 960         |
|    ep_rew_mean          | 693         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 137         |
|    time_elapsed         | 537         |
|    total_timesteps      | 140288      |
| train/                  |             |
|    approx_kl            | 0.082028516 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.34        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.0757     |
|    std                  | 0.291       |
|    value_loss           | 12.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.628      |
| reward_contact          | 0.0214     |
| reward_ctrl             | 0.0777     |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0373     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0844     |
| reward_torque           | 0.0509     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 699        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 138        |
|    time_elapsed         | 541        |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.06576674 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.92       |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.291      |
|    value_loss           | 7.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.623      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0784     |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0374     |
| reward_position         | 6.24e-05   |
| reward_rotation         | 0.0843     |
| reward_torque           | 0.0511     |
| reward_velocity         | 0.191      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 139        |
|    time_elapsed         | 545        |
|    total_timesteps      | 142336     |
| train/                  |            |
|    approx_kl            | 0.05558774 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 9.81       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.291      |
|    value_loss           | 13.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0202     |
| reward_ctrl             | 0.0798     |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0377     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0866     |
| reward_torque           | 0.0513     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 703        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 140        |
|    time_elapsed         | 549        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.11320653 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.908      |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.29       |
|    value_loss           | 3.67       |
----------------------------------------
Num timesteps: 144000
Best mean reward: 692.62 - Last mean reward per episode: 703.24
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.622       |
| reward_contact          | 0.0202      |
| reward_ctrl             | 0.0805      |
| reward_motion           | 0.152       |
| reward_orientation      | 0.0377      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.0881      |
| reward_torque           | 0.0514      |
| reward_velocity         | 0.192       |
| rollout/                |             |
|    ep_len_mean          | 965         |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 141         |
|    time_elapsed         | 553         |
|    total_timesteps      | 144384      |
| train/                  |             |
|    approx_kl            | 0.057991542 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0409     |
|    std                  | 0.29        |
|    value_loss           | 37.1        |
-----------------------------------------
--------------------------------------
| reward                  | 0.617    |
| reward_contact          | 0.0207   |
| reward_ctrl             | 0.0812   |
| reward_motion           | 0.147    |
| reward_orientation      | 0.0381   |
| reward_position         | 6.25e-05 |
| reward_rotation         | 0.088    |
| reward_torque           | 0.0516   |
| reward_velocity         | 0.19     |
| rollout/                |          |
|    ep_len_mean          | 961      |
|    ep_rew_mean          | 704      |
| time/                   |          |
|    fps                  | 261      |
|    iterations           | 142      |
|    time_elapsed         | 556      |
|    total_timesteps      | 145408   |
| train/                  |          |
|    approx_kl            | 0.166669 |
|    clip_fraction        | 0.261    |
|    clip_range           | 0.4      |
|    entropy_loss         | -34.5    |
|    explained_variance   | 0.972    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.784    |
|    n_updates            | 2820     |
|    policy_gradient_loss | -0.0859  |
|    std                  | 0.29     |
|    value_loss           | 3.48     |
--------------------------------------
----------------------------------------
| reward                  | 0.621      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0825     |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0384     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0897     |
| reward_torque           | 0.0517     |
| reward_velocity         | 0.19       |
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 706        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 143        |
|    time_elapsed         | 560        |
|    total_timesteps      | 146432     |
| train/                  |            |
|    approx_kl            | 0.14389788 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.343      |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.29       |
|    value_loss           | 2.6        |
----------------------------------------
----------------------------------------
| reward                  | 0.616      |
| reward_contact          | 0.0212     |
| reward_ctrl             | 0.0832     |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0385     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0894     |
| reward_torque           | 0.0518     |
| reward_velocity         | 0.189      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 705        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 144        |
|    time_elapsed         | 564        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.08992606 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.063     |
|    std                  | 0.29       |
|    value_loss           | 34.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.617     |
| reward_contact          | 0.021     |
| reward_ctrl             | 0.0845    |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0388    |
| reward_position         | 6.25e-05  |
| reward_rotation         | 0.0897    |
| reward_torque           | 0.052     |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 959       |
|    ep_rew_mean          | 706       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 145       |
|    time_elapsed         | 568       |
|    total_timesteps      | 148480    |
| train/                  |           |
|    approx_kl            | 0.0778404 |
|    clip_fraction        | 0.212     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.5     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.77      |
|    n_updates            | 2880      |
|    policy_gradient_loss | -0.0762   |
|    std                  | 0.29      |
|    value_loss           | 5.67      |
---------------------------------------
---------------------------------------
| reward                  | 0.621     |
| reward_contact          | 0.021     |
| reward_ctrl             | 0.086     |
| reward_motion           | 0.145     |
| reward_orientation      | 0.0391    |
| reward_position         | 6.25e-05  |
| reward_rotation         | 0.0889    |
| reward_torque           | 0.0522    |
| reward_velocity         | 0.188     |
| rollout/                |           |
|    ep_len_mean          | 959       |
|    ep_rew_mean          | 709       |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 146       |
|    time_elapsed         | 572       |
|    total_timesteps      | 149504    |
| train/                  |           |
|    approx_kl            | 0.1071461 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.8     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.902     |
|    n_updates            | 2900      |
|    policy_gradient_loss | -0.0905   |
|    std                  | 0.29      |
|    value_loss           | 4.88      |
---------------------------------------
Num timesteps: 150000
Best mean reward: 703.24 - Last mean reward per episode: 710.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.623       |
| reward_contact          | 0.0209      |
| reward_ctrl             | 0.0871      |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0394      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.0875      |
| reward_torque           | 0.0524      |
| reward_velocity         | 0.188       |
| rollout/                |             |
|    ep_len_mean          | 959         |
|    ep_rew_mean          | 711         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 147         |
|    time_elapsed         | 576         |
|    total_timesteps      | 150528      |
| train/                  |             |
|    approx_kl            | 0.052659698 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.56        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0503     |
|    std                  | 0.29        |
|    value_loss           | 34.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.623      |
| reward_contact          | 0.0207     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0895     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 148        |
|    time_elapsed         | 580        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.06989728 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.29       |
|    value_loss           | 5.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0877     |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0889     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 149        |
|    time_elapsed         | 584        |
|    total_timesteps      | 152576     |
| train/                  |            |
|    approx_kl            | 0.15838864 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.289      |
|    value_loss           | 4.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.624      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0882     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0397     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0905     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.188      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 715        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 150        |
|    time_elapsed         | 588        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.13472316 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.47       |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.289      |
|    value_loss           | 3.8        |
----------------------------------------
-----------------------------------------
| reward                  | 0.618       |
| reward_contact          | 0.0208      |
| reward_ctrl             | 0.0863      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.04        |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.09        |
| reward_torque           | 0.0522      |
| reward_velocity         | 0.185       |
| rollout/                |             |
|    ep_len_mean          | 957         |
|    ep_rew_mean          | 716         |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 151         |
|    time_elapsed         | 592         |
|    total_timesteps      | 154624      |
| train/                  |             |
|    approx_kl            | 0.103915185 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.526       |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.1        |
|    std                  | 0.289       |
|    value_loss           | 2.62        |
-----------------------------------------
----------------------------------------
| reward                  | 0.625      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0403     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0919     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 718        |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 152        |
|    time_elapsed         | 596        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.12535065 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.8        |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0994    |
|    std                  | 0.289      |
|    value_loss           | 4.2        |
----------------------------------------
Num timesteps: 156000
Best mean reward: 710.70 - Last mean reward per episode: 717.54
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.625      |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0878     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0403     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0919     |
| reward_torque           | 0.0525     |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 720        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 153        |
|    time_elapsed         | 600        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.06462793 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.19       |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.288      |
|    value_loss           | 8.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0208     |
| reward_ctrl             | 0.0884     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0407     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.0925     |
| reward_torque           | 0.0526     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 721        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 154        |
|    time_elapsed         | 604        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.14862442 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.611      |
|    n_updates            | 3060       |
|    policy_gradient_loss | -0.0879    |
|    std                  | 0.288      |
|    value_loss           | 3.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.622      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0903     |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0412     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.093      |
| reward_torque           | 0.0529     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 723        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 155        |
|    time_elapsed         | 608        |
|    total_timesteps      | 158720     |
| train/                  |            |
|    approx_kl            | 0.14422327 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.495      |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.288      |
|    value_loss           | 3.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.621      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0905     |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0412     |
| reward_position         | 6.25e-05   |
| reward_rotation         | 0.094      |
| reward_torque           | 0.0529     |
| reward_velocity         | 0.186      |
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 727        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 156        |
|    time_elapsed         | 612        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.06450029 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.3       |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.0593    |
|    std                  | 0.288      |
|    value_loss           | 22.7       |
----------------------------------------
-----------------------------------------
| reward                  | 0.621       |
| reward_contact          | 0.0206      |
| reward_ctrl             | 0.0905      |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0412      |
| reward_position         | 6.25e-05    |
| reward_rotation         | 0.094       |
| reward_torque           | 0.0529      |
| reward_velocity         | 0.186       |
| rollout/                |             |
|    ep_len_mean          | 957         |
|    ep_rew_mean          | 731         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 157         |
|    time_elapsed         | 616         |
|    total_timesteps      | 160768      |
| train/                  |             |
|    approx_kl            | 0.054567218 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -34.2       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.053      |
|    std                  | 0.288       |
|    value_loss           | 26.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.62       |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0915     |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0415     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.096      |
| reward_torque           | 0.053      |
| reward_velocity         | 0.187      |
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 735        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 158        |
|    time_elapsed         | 620        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.08203598 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.202      |
|    learning_rate        | 0.0003     |
|    loss                 | 19         |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.288      |
|    value_loss           | 21.5       |
----------------------------------------
Num timesteps: 162000
Best mean reward: 717.54 - Last mean reward per episode: 734.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.632     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0932    |
| reward_motion           | 0.132     |
| reward_orientation      | 0.0417    |
| reward_position         | 9.19e-05  |
| reward_rotation         | 0.101     |
| reward_torque           | 0.0532    |
| reward_velocity         | 0.192     |
| rollout/                |           |
|    ep_len_mean          | 963       |
|    ep_rew_mean          | 740       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 159       |
|    time_elapsed         | 624       |
|    total_timesteps      | 162816    |
| train/                  |           |
|    approx_kl            | 0.0555567 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.4       |
|    entropy_loss         | -34.4     |
|    explained_variance   | 0.248     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.03      |
|    n_updates            | 3160      |
|    policy_gradient_loss | -0.0532   |
|    std                  | 0.288     |
|    value_loss           | 21.7      |
---------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.02       |
| reward_ctrl             | 0.0938     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0417     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.1        |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 742        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 160        |
|    time_elapsed         | 627        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.42139512 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.87       |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0815    |
|    std                  | 0.288      |
|    value_loss           | 5.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.64       |
| reward_contact          | 0.02       |
| reward_ctrl             | 0.0938     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0417     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.1        |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.192      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 749        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 161        |
|    time_elapsed         | 631        |
|    total_timesteps      | 164864     |
| train/                  |            |
|    approx_kl            | 0.05712959 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.287      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.288      |
|    value_loss           | 25.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.648     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0941    |
| reward_motion           | 0.142     |
| reward_orientation      | 0.0418    |
| reward_position         | 9.19e-05  |
| reward_rotation         | 0.102     |
| reward_torque           | 0.0533    |
| reward_velocity         | 0.195     |
| rollout/                |           |
|    ep_len_mean          | 969       |
|    ep_rew_mean          | 749       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 162       |
|    time_elapsed         | 635       |
|    total_timesteps      | 165888    |
| train/                  |           |
|    approx_kl            | 0.0988584 |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.4       |
|    entropy_loss         | -35.8     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.591     |
|    n_updates            | 3220      |
|    policy_gradient_loss | -0.0841   |
|    std                  | 0.287     |
|    value_loss           | 9.28      |
---------------------------------------
----------------------------------------
| reward                  | 0.652      |
| reward_contact          | 0.0188     |
| reward_ctrl             | 0.0939     |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0414     |
| reward_position         | 9.19e-05   |
| reward_rotation         | 0.104      |
| reward_torque           | 0.0533     |
| reward_velocity         | 0.198      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 752        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 163        |
|    time_elapsed         | 639        |
|    total_timesteps      | 166912     |
| train/                  |            |
|    approx_kl            | 0.20166099 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.699      |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.287      |
|    value_loss           | 4.39       |
----------------------------------------
-----------------------------------------
| reward                  | 0.651       |
| reward_contact          | 0.0188      |
| reward_ctrl             | 0.0951      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0415      |
| reward_position         | 9.19e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 753         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 164         |
|    time_elapsed         | 643         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.089540504 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.529       |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.07       |
|    std                  | 0.287       |
|    value_loss           | 3.14        |
-----------------------------------------
Num timesteps: 168000
Best mean reward: 734.72 - Last mean reward per episode: 753.25
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.651       |
| reward_contact          | 0.0188      |
| reward_ctrl             | 0.0951      |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0415      |
| reward_position         | 9.19e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 754         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 165         |
|    time_elapsed         | 647         |
|    total_timesteps      | 168960      |
| train/                  |             |
|    approx_kl            | 0.081179164 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.287       |
|    value_loss           | 3.06        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.653       |
| reward_contact          | 0.0187      |
| reward_ctrl             | 0.095       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0419      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.105       |
| reward_torque           | 0.0534      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 756         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 166         |
|    time_elapsed         | 651         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.096646935 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.0938     |
|    std                  | 0.287       |
|    value_loss           | 2.52        |
-----------------------------------------
----------------------------------------
| reward                  | 0.647      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.0961     |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0418     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.105      |
| reward_torque           | 0.0535     |
| reward_velocity         | 0.197      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 755        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 167        |
|    time_elapsed         | 655        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.10382956 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -35.5      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.287      |
|    value_loss           | 2          |
----------------------------------------
-----------------------------------------
| reward                  | 0.648       |
| reward_contact          | 0.0186      |
| reward_ctrl             | 0.097       |
| reward_motion           | 0.132       |
| reward_orientation      | 0.0419      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.107       |
| reward_torque           | 0.0536      |
| reward_velocity         | 0.198       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 756         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 168         |
|    time_elapsed         | 659         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.050349392 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.4         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95        |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.058      |
|    std                  | 0.287       |
|    value_loss           | 9.67        |
-----------------------------------------
----------------------------------------
| reward                  | 0.65       |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.0984     |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0419     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.109      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.197      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 758        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 169        |
|    time_elapsed         | 663        |
|    total_timesteps      | 173056     |
| train/                  |            |
|    approx_kl            | 0.14275405 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.335      |
|    n_updates            | 3360       |
|    policy_gradient_loss | -0.0883    |
|    std                  | 0.286      |
|    value_loss           | 2.09       |
----------------------------------------
Num timesteps: 174000
Best mean reward: 753.25 - Last mean reward per episode: 760.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.658      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.0992     |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0416     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.11       |
| reward_torque           | 0.054      |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 761        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 170        |
|    time_elapsed         | 667        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.06053085 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.637      |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0768    |
|    std                  | 0.286      |
|    value_loss           | 5.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.658      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.0997     |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0416     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.111      |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 761        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 171        |
|    time_elapsed         | 671        |
|    total_timesteps      | 175104     |
| train/                  |            |
|    approx_kl            | 0.40313876 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.358      |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.286      |
|    value_loss           | 2.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.659      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.136      |
| reward_orientation      | 0.042      |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.11       |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.198      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 760        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 172        |
|    time_elapsed         | 675        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.09005588 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0336     |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.286      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.66       |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0423     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.112      |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 762        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 173        |
|    time_elapsed         | 679        |
|    total_timesteps      | 177152     |
| train/                  |            |
|    approx_kl            | 0.09291803 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.92       |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.286      |
|    value_loss           | 13.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.655      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0425     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.113      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.199      |
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 174        |
|    time_elapsed         | 682        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.12863752 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.36       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.285      |
|    value_loss           | 2.08       |
----------------------------------------
----------------------------------------
| reward                  | 0.663      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.129      |
| reward_orientation      | 0.0425     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.115      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 767        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 175        |
|    time_elapsed         | 686        |
|    total_timesteps      | 179200     |
| train/                  |            |
|    approx_kl            | 0.21886471 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.606      |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.285      |
|    value_loss           | 2.26       |
----------------------------------------
Num timesteps: 180000
Best mean reward: 760.72 - Last mean reward per episode: 768.74
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.665      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0426     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 176        |
|    time_elapsed         | 690        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.08834024 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.7        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.285      |
|    value_loss           | 16.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.656      |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0427     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 177        |
|    time_elapsed         | 694        |
|    total_timesteps      | 181248     |
| train/                  |            |
|    approx_kl            | 0.19440275 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.524      |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.0883    |
|    std                  | 0.285      |
|    value_loss           | 4.03       |
----------------------------------------
---------------------------------------
| reward                  | 0.658     |
| reward_contact          | 0.0183    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.12      |
| reward_orientation      | 0.0427    |
| reward_position         | 9.18e-05  |
| reward_rotation         | 0.117     |
| reward_torque           | 0.0547    |
| reward_velocity         | 0.202     |
| rollout/                |           |
|    ep_len_mean          | 966       |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 178       |
|    time_elapsed         | 698       |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.1197983 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -36.7     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 3540      |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.285     |
|    value_loss           | 3.81      |
---------------------------------------
-----------------------------------------
| reward                  | 0.657       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.121       |
| reward_orientation      | 0.0427      |
| reward_position         | 9.18e-05    |
| reward_rotation         | 0.115       |
| reward_torque           | 0.0546      |
| reward_velocity         | 0.202       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 179         |
|    time_elapsed         | 702         |
|    total_timesteps      | 183296      |
| train/                  |             |
|    approx_kl            | 0.048548646 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.056      |
|    std                  | 0.285       |
|    value_loss           | 12.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.649      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.043      |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.114      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 771        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 180        |
|    time_elapsed         | 706        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.22106248 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.883      |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.285      |
|    value_loss           | 2.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0429     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.114      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.2        |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 181        |
|    time_elapsed         | 710        |
|    total_timesteps      | 185344     |
| train/                  |            |
|    approx_kl            | 0.10555091 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.284      |
|    value_loss           | 6.93       |
----------------------------------------
Num timesteps: 186000
Best mean reward: 768.74 - Last mean reward per episode: 773.48
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.662      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0432     |
| reward_position         | 9.18e-05   |
| reward_rotation         | 0.116      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 182        |
|    time_elapsed         | 714        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.10725984 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.48       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.284      |
|    value_loss           | 3.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.663      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0435     |
| reward_position         | 7.12e-05   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.202      |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 183        |
|    time_elapsed         | 718        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.09395177 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.284      |
|    value_loss           | 5.48       |
----------------------------------------
-----------------------------------------
| reward                  | 0.664       |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.124       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000124    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.203       |
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 184         |
|    time_elapsed         | 722         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.067573786 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.2       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58        |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0786     |
|    std                  | 0.284       |
|    value_loss           | 5.75        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.656       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.118       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000124    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0548      |
| reward_velocity         | 0.201       |
| rollout/                |             |
|    ep_len_mean          | 959         |
|    ep_rew_mean          | 771         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 185         |
|    time_elapsed         | 726         |
|    total_timesteps      | 189440      |
| train/                  |             |
|    approx_kl            | 0.084478304 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.5       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.67        |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.066      |
|    std                  | 0.284       |
|    value_loss           | 23.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.661      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000124   |
| reward_rotation         | 0.118      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.203      |
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 186        |
|    time_elapsed         | 730        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.09867191 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.6      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.657      |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.284      |
|    value_loss           | 12.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.656       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.117       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000357    |
| reward_rotation         | 0.116       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.203       |
| rollout/                |             |
|    ep_len_mean          | 951         |
|    ep_rew_mean          | 765         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 187         |
|    time_elapsed         | 734         |
|    total_timesteps      | 191488      |
| train/                  |             |
|    approx_kl            | 0.081378326 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.4         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.284       |
|    value_loss           | 16.3        |
-----------------------------------------
Num timesteps: 192000
Best mean reward: 773.48 - Last mean reward per episode: 764.06
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0184     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.204      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 188        |
|    time_elapsed         | 738        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.08671241 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.38       |
|    n_updates            | 3740       |
|    policy_gradient_loss | -0.088     |
|    std                  | 0.283      |
|    value_loss           | 17.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.654      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.204      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 764        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 189        |
|    time_elapsed         | 741        |
|    total_timesteps      | 193536     |
| train/                  |            |
|    approx_kl            | 0.06937146 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.2       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.0762    |
|    std                  | 0.283      |
|    value_loss           | 22.7       |
----------------------------------------
--------------------------------------
| reward                  | 0.654    |
| reward_contact          | 0.0191   |
| reward_ctrl             | 0.106    |
| reward_motion           | 0.111    |
| reward_orientation      | 0.0428   |
| reward_position         | 0.000357 |
| reward_rotation         | 0.117    |
| reward_torque           | 0.0551   |
| reward_velocity         | 0.203    |
| rollout/                |          |
|    ep_len_mean          | 951      |
|    ep_rew_mean          | 765      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 190      |
|    time_elapsed         | 745      |
|    total_timesteps      | 194560   |
| train/                  |          |
|    approx_kl            | 0.26849  |
|    clip_fraction        | 0.31     |
|    clip_range           | 0.4      |
|    entropy_loss         | -37.3    |
|    explained_variance   | 0.989    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.375    |
|    n_updates            | 3780     |
|    policy_gradient_loss | -0.116   |
|    std                  | 0.283    |
|    value_loss           | 2.76     |
--------------------------------------
----------------------------------------
| reward                  | 0.653      |
| reward_contact          | 0.0191     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.108      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.117      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.205      |
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 765        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 191        |
|    time_elapsed         | 749        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.24904266 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0953    |
|    std                  | 0.283      |
|    value_loss           | 3.14       |
----------------------------------------
-----------------------------------------
| reward                  | 0.66        |
| reward_contact          | 0.0185      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.113       |
| reward_orientation      | 0.0429      |
| reward_position         | 0.000357    |
| reward_rotation         | 0.119       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.206       |
| rollout/                |             |
|    ep_len_mean          | 960         |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 192         |
|    time_elapsed         | 753         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.076792315 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.1       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.24        |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.0569     |
|    std                  | 0.283       |
|    value_loss           | 12.3        |
-----------------------------------------
----------------------------------------
| reward                  | 0.661      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0426     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.205      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 771        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 193        |
|    time_elapsed         | 757        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.20254421 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.316      |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.283      |
|    value_loss           | 2.88       |
----------------------------------------
Num timesteps: 198000
Best mean reward: 773.48 - Last mean reward per episode: 770.95
----------------------------------------
| reward                  | 0.668      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0423     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 194        |
|    time_elapsed         | 761        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.09727071 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.5      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.711      |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.282      |
|    value_loss           | 4.74       |
----------------------------------------
----------------------------------------
| reward                  | 0.667      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0422     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.118      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 195        |
|    time_elapsed         | 765        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.22269462 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.686      |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.282      |
|    value_loss           | 3.86       |
----------------------------------------
----------------------------------------
| reward                  | 0.666      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0424     |
| reward_position         | 0.000357   |
| reward_rotation         | 0.119      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.207      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 769        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 196        |
|    time_elapsed         | 769        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.07757417 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0974    |
|    std                  | 0.282      |
|    value_loss           | 3.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.676      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.124      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.12       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.208      |
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 197        |
|    time_elapsed         | 773        |
|    total_timesteps      | 201728     |
| train/                  |            |
|    approx_kl            | 0.12799951 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.718      |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.281      |
|    value_loss           | 2.79       |
----------------------------------------
---------------------------------------
| reward                  | 0.683     |
| reward_contact          | 0.0174    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.125     |
| reward_orientation      | 0.043     |
| reward_position         | 0.000779  |
| reward_rotation         | 0.122     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.211     |
| rollout/                |           |
|    ep_len_mean          | 963       |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 198       |
|    time_elapsed         | 777       |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.0695606 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.1     |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 3940      |
|    policy_gradient_loss | -0.0584   |
|    std                  | 0.281     |
|    value_loss           | 32.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.685      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0433     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.211      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 199        |
|    time_elapsed         | 781        |
|    total_timesteps      | 203776     |
| train/                  |            |
|    approx_kl            | 0.12865774 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.72       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.281      |
|    value_loss           | 2.53       |
----------------------------------------
Num timesteps: 204000
Best mean reward: 773.48 - Last mean reward per episode: 773.24
----------------------------------------
| reward                  | 0.691      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 200        |
|    time_elapsed         | 784        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.09465802 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.281      |
|    value_loss           | 8.06       |
----------------------------------------
----------------------------------------
| reward                  | 0.69       |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 201        |
|    time_elapsed         | 788        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.16142128 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.3      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 19         |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.281      |
|    value_loss           | 26.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.687      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 202        |
|    time_elapsed         | 792        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.13441858 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.0996    |
|    std                  | 0.281      |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.689      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.122      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 774        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 203        |
|    time_elapsed         | 796        |
|    total_timesteps      | 207872     |
| train/                  |            |
|    approx_kl            | 0.31247383 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.281      |
|    value_loss           | 1.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.696       |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.134       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.123       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.212       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 779         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 204         |
|    time_elapsed         | 800         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.074120626 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.6       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.15        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.0708     |
|    std                  | 0.28        |
|    value_loss           | 24.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 205        |
|    time_elapsed         | 804        |
|    total_timesteps      | 209920     |
| train/                  |            |
|    approx_kl            | 0.13207534 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0724    |
|    std                  | 0.28       |
|    value_loss           | 7.46       |
----------------------------------------
Num timesteps: 210000
Best mean reward: 773.48 - Last mean reward per episode: 779.77
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.125      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 206        |
|    time_elapsed         | 808        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.06353199 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.4      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.51       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.28       |
|    value_loss           | 14.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.698      |
| reward_contact          | 0.0183     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.126      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 207        |
|    time_elapsed         | 812        |
|    total_timesteps      | 211968     |
| train/                  |            |
|    approx_kl            | 0.43054584 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.2      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.0909    |
|    std                  | 0.28       |
|    value_loss           | 2.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.69       |
| reward_contact          | 0.0177     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.126      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.212      |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 780        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 208        |
|    time_elapsed         | 816        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.16970724 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.281      |
|    value_loss           | 3.57       |
----------------------------------------
-----------------------------------------
| reward                  | 0.688       |
| reward_contact          | 0.0177      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.122       |
| reward_orientation      | 0.0435      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.127       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.213       |
| rollout/                |             |
|    ep_len_mean          | 969         |
|    ep_rew_mean          | 780         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 209         |
|    time_elapsed         | 820         |
|    total_timesteps      | 214016      |
| train/                  |             |
|    approx_kl            | 0.117287226 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.583       |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.103      |
|    std                  | 0.28        |
|    value_loss           | 2.24        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.695       |
| reward_contact          | 0.0177      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.129       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.000779    |
| reward_rotation         | 0.127       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.214       |
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 781         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 210         |
|    time_elapsed         | 824         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.115168765 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.2       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.0705     |
|    std                  | 0.28        |
|    value_loss           | 12.4        |
-----------------------------------------
Num timesteps: 216000
Best mean reward: 779.77 - Last mean reward per episode: 782.12
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.7        |
| reward_contact          | 0.0182     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0436     |
| reward_position         | 0.000779   |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 971        |
|    ep_rew_mean          | 782        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 211        |
|    time_elapsed         | 828        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.44846207 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.28       |
|    value_loss           | 2.55       |
----------------------------------------
---------------------------------------
| reward                  | 0.705     |
| reward_contact          | 0.0176    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.133     |
| reward_orientation      | 0.0433    |
| reward_position         | 0.000737  |
| reward_rotation         | 0.129     |
| reward_torque           | 0.0555    |
| reward_velocity         | 0.216     |
| rollout/                |           |
|    ep_len_mean          | 981       |
|    ep_rew_mean          | 790       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 212       |
|    time_elapsed         | 832       |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0660864 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.2     |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 4220      |
|    policy_gradient_loss | -0.0559   |
|    std                  | 0.28      |
|    value_loss           | 20.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.704      |
| reward_contact          | 0.0176     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000737   |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 789        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 213        |
|    time_elapsed         | 836        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.15172195 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.73       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.28       |
|    value_loss           | 2.09       |
----------------------------------------
---------------------------------------
| reward                  | 0.705     |
| reward_contact          | 0.0176    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0429    |
| reward_position         | 0.000737  |
| reward_rotation         | 0.129     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.216     |
| rollout/                |           |
|    ep_len_mean          | 981       |
|    ep_rew_mean          | 789       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 214       |
|    time_elapsed         | 840       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.3710906 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -37.8     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.329     |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.116    |
|    std                  | 0.28      |
|    value_loss           | 1.88      |
---------------------------------------
-----------------------------------------
| reward                  | 0.7         |
| reward_contact          | 0.0182      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.134       |
| reward_orientation      | 0.0428      |
| reward_position         | 0.000737    |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.215       |
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 789         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 215         |
|    time_elapsed         | 844         |
|    total_timesteps      | 220160      |
| train/                  |             |
|    approx_kl            | 0.059279088 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.4         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.8         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0657     |
|    std                  | 0.28        |
|    value_loss           | 13.9        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.7         |
| reward_contact          | 0.0183      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0426      |
| reward_position         | 0.000737    |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.215       |
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 216         |
|    time_elapsed         | 848         |
|    total_timesteps      | 221184      |
| train/                  |             |
|    approx_kl            | 0.065936334 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.1       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0892     |
|    std                  | 0.279       |
|    value_loss           | 5.63        |
-----------------------------------------
Num timesteps: 222000
Best mean reward: 782.12 - Last mean reward per episode: 788.09
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.702      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.215      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 217        |
|    time_elapsed         | 852        |
|    total_timesteps      | 222208     |
| train/                  |            |
|    approx_kl            | 0.11577028 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.6      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.663      |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0998    |
|    std                  | 0.279      |
|    value_loss           | 2.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0189     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.124      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 218        |
|    time_elapsed         | 856        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.09963942 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -37.6      |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.279      |
|    value_loss           | 23.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.712       |
| reward_contact          | 0.0189      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.126       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.217       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 790         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 219         |
|    time_elapsed         | 860         |
|    total_timesteps      | 224256      |
| train/                  |             |
|    approx_kl            | 0.066470146 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38         |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.2         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0877     |
|    std                  | 0.279       |
|    value_loss           | 12.2        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.707       |
| reward_contact          | 0.0184      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.125       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 220         |
|    time_elapsed         | 864         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.091230586 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.6       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.279       |
|    value_loss           | 3.45        |
-----------------------------------------
---------------------------------------
| reward                  | 0.714     |
| reward_contact          | 0.0186    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.00074   |
| reward_rotation         | 0.127     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.22      |
| rollout/                |           |
|    ep_len_mean          | 982       |
|    ep_rew_mean          | 788       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 221       |
|    time_elapsed         | 868       |
|    total_timesteps      | 226304    |
| train/                  |           |
|    approx_kl            | 0.1391182 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.4       |
|    entropy_loss         | -40       |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.12      |
|    n_updates            | 4400      |
|    policy_gradient_loss | -0.0994   |
|    std                  | 0.279     |
|    value_loss           | 15.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0182     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 222        |
|    time_elapsed         | 872        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.42598796 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.298      |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.279      |
|    value_loss           | 1.63       |
----------------------------------------
Num timesteps: 228000
Best mean reward: 788.09 - Last mean reward per episode: 787.66
-----------------------------------------
| reward                  | 0.718       |
| reward_contact          | 0.018       |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.145       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.129       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 223         |
|    time_elapsed         | 876         |
|    total_timesteps      | 228352      |
| train/                  |             |
|    approx_kl            | 0.075850934 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.7       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.782       |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0991     |
|    std                  | 0.279       |
|    value_loss           | 2.87        |
-----------------------------------------
----------------------------------------
| reward                  | 0.718      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0433     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.129      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 788        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 224        |
|    time_elapsed         | 880        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.11311416 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.278      |
|    value_loss           | 3.91       |
----------------------------------------
-----------------------------------------
| reward                  | 0.72        |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0432      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.13        |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 789         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 225         |
|    time_elapsed         | 884         |
|    total_timesteps      | 230400      |
| train/                  |             |
|    approx_kl            | 0.075297266 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.56        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.0765     |
|    std                  | 0.278       |
|    value_loss           | 7.57        |
-----------------------------------------
----------------------------------------
| reward                  | 0.718      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.13       |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 226        |
|    time_elapsed         | 888        |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.26172417 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.577      |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.278      |
|    value_loss           | 2.65       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.143       |
| reward_orientation      | 0.0436      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.131       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 790         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 227         |
|    time_elapsed         | 892         |
|    total_timesteps      | 232448      |
| train/                  |             |
|    approx_kl            | 0.047955044 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.2       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0547     |
|    std                  | 0.278       |
|    value_loss           | 23.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.724      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.132      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 982        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 228        |
|    time_elapsed         | 896        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09163224 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.46       |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0777    |
|    std                  | 0.278      |
|    value_loss           | 7.28       |
----------------------------------------
Num timesteps: 234000
Best mean reward: 788.09 - Last mean reward per episode: 789.30
Saving new best model to rl/out_dir/models/exp71/best_model.zip
--------------------------------------
| reward                  | 0.722    |
| reward_contact          | 0.0179   |
| reward_ctrl             | 0.107    |
| reward_motion           | 0.146    |
| reward_orientation      | 0.0432   |
| reward_position         | 0.00074  |
| reward_rotation         | 0.132    |
| reward_torque           | 0.0552   |
| reward_velocity         | 0.221    |
| rollout/                |          |
|    ep_len_mean          | 982      |
|    ep_rew_mean          | 789      |
| time/                   |          |
|    fps                  | 260      |
|    iterations           | 229      |
|    time_elapsed         | 900      |
|    total_timesteps      | 234496   |
| train/                  |          |
|    approx_kl            | 0.119911 |
|    clip_fraction        | 0.32     |
|    clip_range           | 0.4      |
|    entropy_loss         | -39.3    |
|    explained_variance   | 0.975    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.773    |
|    n_updates            | 4560     |
|    policy_gradient_loss | -0.107   |
|    std                  | 0.278    |
|    value_loss           | 3.61     |
--------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0181     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.134      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 984        |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 230        |
|    time_elapsed         | 904        |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.14359546 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.277      |
|    value_loss           | 3.25       |
----------------------------------------
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0175     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.136      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 985        |
|    ep_rew_mean          | 791        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 231        |
|    time_elapsed         | 908        |
|    total_timesteps      | 236544     |
| train/                  |            |
|    approx_kl            | 0.25051856 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40        |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.679      |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.278      |
|    value_loss           | 3.13       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0169     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 986        |
|    ep_rew_mean          | 792        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 232        |
|    time_elapsed         | 912        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.09740679 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.39       |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0931    |
|    std                  | 0.278      |
|    value_loss           | 2.89       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 987        |
|    ep_rew_mean          | 795        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 233        |
|    time_elapsed         | 915        |
|    total_timesteps      | 238592     |
| train/                  |            |
|    approx_kl            | 0.19745895 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0879     |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.277      |
|    value_loss           | 1.85       |
----------------------------------------
-----------------------------------------
| reward                  | 0.744       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.151       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 234         |
|    time_elapsed         | 919         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.054082498 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0552     |
|    std                  | 0.277       |
|    value_loss           | 23.2        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: 789.30 - Last mean reward per episode: 797.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 797        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 235        |
|    time_elapsed         | 923        |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.18764104 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.277      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 798        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 236        |
|    time_elapsed         | 927        |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.09910023 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.7      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.91       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.277      |
|    value_loss           | 12.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 796        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 237        |
|    time_elapsed         | 931        |
|    total_timesteps      | 242688     |
| train/                  |            |
|    approx_kl            | 0.13132326 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.277      |
|    value_loss           | 3.23       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 796        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 238        |
|    time_elapsed         | 935        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.11802027 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.484      |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.277      |
|    value_loss           | 3.05       |
----------------------------------------
---------------------------------------
| reward                  | 0.746     |
| reward_contact          | 0.0154    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.158     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.00074   |
| reward_rotation         | 0.14      |
| reward_torque           | 0.0551    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 994       |
|    ep_rew_mean          | 800       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 239       |
|    time_elapsed         | 939       |
|    total_timesteps      | 244736    |
| train/                  |           |
|    approx_kl            | 0.7266917 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39       |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.353     |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.123    |
|    std                  | 0.277     |
|    value_loss           | 2.64      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 994        |
|    ep_rew_mean          | 799        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 240        |
|    time_elapsed         | 943        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.15365112 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.3      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.276      |
|    value_loss           | 1.71       |
----------------------------------------
Num timesteps: 246000
Best mean reward: 797.26 - Last mean reward per episode: 799.37
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 241        |
|    time_elapsed         | 947        |
|    total_timesteps      | 246784     |
| train/                  |            |
|    approx_kl            | 0.11831712 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.1      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.305      |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.276      |
|    value_loss           | 2.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 242        |
|    time_elapsed         | 951        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.12328817 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.451      |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.276      |
|    value_loss           | 2.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 243        |
|    time_elapsed         | 955        |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.11957366 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.276      |
|    value_loss           | 2.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 244        |
|    time_elapsed         | 959        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.09971682 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.523      |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.276      |
|    value_loss           | 2.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 245        |
|    time_elapsed         | 963        |
|    total_timesteps      | 250880     |
| train/                  |            |
|    approx_kl            | 0.11762114 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39        |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.579      |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.276      |
|    value_loss           | 2.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 246        |
|    time_elapsed         | 967        |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.08995411 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 4900       |
|    policy_gradient_loss | -0.0964    |
|    std                  | 0.275      |
|    value_loss           | 3.59       |
----------------------------------------
Num timesteps: 252000
Best mean reward: 799.37 - Last mean reward per episode: 800.68
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 247        |
|    time_elapsed         | 971        |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.12363757 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0846    |
|    std                  | 0.275      |
|    value_loss           | 2.84       |
----------------------------------------
-----------------------------------------
| reward                  | 0.733       |
| reward_contact          | 0.0154      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0548      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 996         |
|    ep_rew_mean          | 800         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 248         |
|    time_elapsed         | 975         |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.098926604 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0889     |
|    std                  | 0.275       |
|    value_loss           | 8.34        |
-----------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00074    |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 249        |
|    time_elapsed         | 979        |
|    total_timesteps      | 254976     |
| train/                  |            |
|    approx_kl            | 0.10817122 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.447      |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.275      |
|    value_loss           | 2.7        |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 250        |
|    time_elapsed         | 982        |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.07255809 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.47       |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.0477    |
|    std                  | 0.275      |
|    value_loss           | 16.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 804        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 251        |
|    time_elapsed         | 986        |
|    total_timesteps      | 257024     |
| train/                  |            |
|    approx_kl            | 0.06664982 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -38.8      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.275      |
|    value_loss           | 14.6       |
----------------------------------------
Num timesteps: 258000
Best mean reward: 800.68 - Last mean reward per episode: 804.55
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00074    |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 805        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 252        |
|    time_elapsed         | 990        |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.07991566 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.934      |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0674    |
|    std                  | 0.274      |
|    value_loss           | 7.49       |
----------------------------------------
-----------------------------------------
| reward                  | 0.736       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.148       |
| reward_orientation      | 0.0431      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 805         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 253         |
|    time_elapsed         | 995         |
|    total_timesteps      | 259072      |
| train/                  |             |
|    approx_kl            | 0.107865684 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.906       |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.0787     |
|    std                  | 0.274       |
|    value_loss           | 4.68        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.734       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.147       |
| reward_orientation      | 0.0431      |
| reward_position         | 0.00074     |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 804         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 254         |
|    time_elapsed         | 999         |
|    total_timesteps      | 260096      |
| train/                  |             |
|    approx_kl            | 0.084903374 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.4         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.22        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0694     |
|    std                  | 0.274       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 255        |
|    time_elapsed         | 1002       |
|    total_timesteps      | 261120     |
| train/                  |            |
|    approx_kl            | 0.16947773 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.384      |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.0892    |
|    std                  | 0.274      |
|    value_loss           | 1.51       |
----------------------------------------
---------------------------------------
| reward                  | 0.73      |
| reward_contact          | 0.016     |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.149     |
| reward_orientation      | 0.0431    |
| reward_position         | 0.000711  |
| reward_rotation         | 0.142     |
| reward_torque           | 0.0546    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 998       |
|    ep_rew_mean          | 803       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 256       |
|    time_elapsed         | 1006      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.2143392 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39.1     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.452     |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.134    |
|    std                  | 0.274     |
|    value_loss           | 1.86      |
---------------------------------------
----------------------------------------
| reward                  | 0.724      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 257        |
|    time_elapsed         | 1010       |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.08330659 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.1      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.43       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.273      |
|    value_loss           | 7.09       |
----------------------------------------
Num timesteps: 264000
Best mean reward: 804.55 - Last mean reward per episode: 801.95
----------------------------------------
| reward                  | 0.713      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 258        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.06518588 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.3      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 5140       |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.273      |
|    value_loss           | 7.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.711      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.0998     |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 259        |
|    time_elapsed         | 1018       |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.38526678 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.6      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.389      |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.273      |
|    value_loss           | 1.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.709      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 260        |
|    time_elapsed         | 1022       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.08361219 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.808      |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0984    |
|    std                  | 0.273      |
|    value_loss           | 3.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.71       |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.0994     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0433     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 261        |
|    time_elapsed         | 1026       |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.31391382 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.273      |
|    value_loss           | 3.05       |
----------------------------------------
---------------------------------------
| reward                  | 0.71      |
| reward_contact          | 0.016     |
| reward_ctrl             | 0.0993    |
| reward_motion           | 0.142     |
| reward_orientation      | 0.0434    |
| reward_position         | 0.000711  |
| reward_rotation         | 0.137     |
| reward_torque           | 0.0543    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 998       |
|    ep_rew_mean          | 802       |
| time/                   |           |
|    fps                  | 260       |
|    iterations           | 262       |
|    time_elapsed         | 1030      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0733189 |
|    clip_fraction        | 0.282     |
|    clip_range           | 0.4       |
|    entropy_loss         | -39.4     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.29      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.0602   |
|    std                  | 0.273     |
|    value_loss           | 7.89      |
---------------------------------------
-----------------------------------------
| reward                  | 0.709       |
| reward_contact          | 0.016       |
| reward_ctrl             | 0.0987      |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.137       |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 803         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 263         |
|    time_elapsed         | 1034        |
|    total_timesteps      | 269312      |
| train/                  |             |
|    approx_kl            | 0.054735593 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.629       |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.0665     |
|    std                  | 0.272       |
|    value_loss           | 12.7        |
-----------------------------------------
Num timesteps: 270000
Best mean reward: 804.55 - Last mean reward per episode: 802.79
----------------------------------------
| reward                  | 0.715      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.0985     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 803        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 264        |
|    time_elapsed         | 1038       |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.07660645 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.13       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.93       |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.053     |
|    std                  | 0.272      |
|    value_loss           | 24.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0986     |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 805        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 265        |
|    time_elapsed         | 1041       |
|    total_timesteps      | 271360     |
| train/                  |            |
|    approx_kl            | 0.05354607 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.4      |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.5       |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0605    |
|    std                  | 0.272      |
|    value_loss           | 21.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.72       |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0972     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.054      |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 266        |
|    time_elapsed         | 1045       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.20370428 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.1      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.689      |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.091     |
|    std                  | 0.272      |
|    value_loss           | 2.17       |
----------------------------------------
----------------------------------------
| reward                  | 0.712      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.0959     |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.137      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 267        |
|    time_elapsed         | 1049       |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.15861273 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.235      |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.272      |
|    value_loss           | 1.76       |
----------------------------------------
-----------------------------------------
| reward                  | 0.714       |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.0964      |
| reward_motion           | 0.145       |
| reward_orientation      | 0.044       |
| reward_position         | 0.000711    |
| reward_rotation         | 0.138       |
| reward_torque           | 0.0539      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 806         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 268         |
|    time_elapsed         | 1054        |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.048325207 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 5           |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0588     |
|    std                  | 0.272       |
|    value_loss           | 20.6        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.711       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0958      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0442      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.136       |
| reward_torque           | 0.0538      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 806         |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 269         |
|    time_elapsed         | 1058        |
|    total_timesteps      | 275456      |
| train/                  |             |
|    approx_kl            | 0.108048886 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.774       |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.093      |
|    std                  | 0.272       |
|    value_loss           | 2.69        |
-----------------------------------------
Num timesteps: 276000
Best mean reward: 804.55 - Last mean reward per episode: 807.69
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.71       |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0958     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0443     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 270        |
|    time_elapsed         | 1063       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.09814938 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.272      |
|    value_loss           | 3          |
----------------------------------------
-----------------------------------------
| reward                  | 0.71        |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0959      |
| reward_motion           | 0.14        |
| reward_orientation      | 0.0443      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.138       |
| reward_torque           | 0.0539      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 271         |
|    time_elapsed         | 1068        |
|    total_timesteps      | 277504      |
| train/                  |             |
|    approx_kl            | 0.112396814 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.879       |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0869     |
|    std                  | 0.272       |
|    value_loss           | 3.37        |
-----------------------------------------
----------------------------------------
| reward                  | 0.716      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0445     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0539     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 272        |
|    time_elapsed         | 1073       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.10278291 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.9      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.243      |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.272      |
|    value_loss           | 1.45       |
----------------------------------------
-----------------------------------------
| reward                  | 0.714       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.0955      |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.139       |
| reward_torque           | 0.0538      |
| reward_velocity         | 0.22        |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 807         |
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 273         |
|    time_elapsed         | 1078        |
|    total_timesteps      | 279552      |
| train/                  |             |
|    approx_kl            | 0.083422765 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0618     |
|    std                  | 0.271       |
|    value_loss           | 11          |
-----------------------------------------
----------------------------------------
| reward                  | 0.717      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.0947     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0538     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 274        |
|    time_elapsed         | 1082       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.07705648 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.271      |
|    value_loss           | 4.27       |
----------------------------------------
-----------------------------------------
| reward                  | 0.726       |
| reward_contact          | 0.0158      |
| reward_ctrl             | 0.096       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.139       |
| reward_torque           | 0.0541      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 275         |
|    time_elapsed         | 1088        |
|    total_timesteps      | 281600      |
| train/                  |             |
|    approx_kl            | 0.060665432 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.8       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.271       |
|    value_loss           | 8.5         |
-----------------------------------------
Num timesteps: 282000
Best mean reward: 807.69 - Last mean reward per episode: 807.99
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.045      |
| reward_position         | 0.000711   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0541     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 276        |
|    time_elapsed         | 1095       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.08215463 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.7      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.271      |
|    value_loss           | 9.18       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0146      |
| reward_ctrl             | 0.0974      |
| reward_motion           | 0.15        |
| reward_orientation      | 0.0451      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.14        |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 277         |
|    time_elapsed         | 1101        |
|    total_timesteps      | 283648      |
| train/                  |             |
|    approx_kl            | 0.081559576 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.4         |
|    entropy_loss         | -39.8       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.271       |
|    value_loss           | 17.4        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.0987     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 810        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 278        |
|    time_elapsed         | 1107       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.10431288 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0942    |
|    std                  | 0.271      |
|    value_loss           | 3.95       |
----------------------------------------
-----------------------------------------
| reward                  | 0.731       |
| reward_contact          | 0.014       |
| reward_ctrl             | 0.0985      |
| reward_motion           | 0.15        |
| reward_orientation      | 0.0454      |
| reward_position         | 0.000711    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 812         |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 279         |
|    time_elapsed         | 1113        |
|    total_timesteps      | 285696      |
| train/                  |             |
|    approx_kl            | 0.046462562 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40         |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.43        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.0568     |
|    std                  | 0.271       |
|    value_loss           | 20.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0455     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 812        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 280        |
|    time_elapsed         | 1119       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.20781538 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0941    |
|    std                  | 0.271      |
|    value_loss           | 1.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0455     |
| reward_position         | 0.000711   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 812        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 281        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 287744     |
| train/                  |            |
|    approx_kl            | 0.14470401 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.729      |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.0865    |
|    std                  | 0.271      |
|    value_loss           | 2          |
----------------------------------------
Num timesteps: 288000
Best mean reward: 807.99 - Last mean reward per episode: 811.88
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.0978     |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 811        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 282        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.24099842 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.437      |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.27       |
|    value_loss           | 2.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.0976     |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0461     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 811        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 283        |
|    time_elapsed         | 1134       |
|    total_timesteps      | 289792     |
| train/                  |            |
|    approx_kl            | 0.08805196 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -39.7      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.27       |
|    value_loss           | 12.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.097      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000658   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 816        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 284        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.11301737 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.3      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.868      |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.0831    |
|    std                  | 0.27       |
|    value_loss           | 4.37       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.0973     |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000658   |
| reward_rotation         | 0.137      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 815        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 285        |
|    time_elapsed         | 1145       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.17451546 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.624      |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.0797    |
|    std                  | 0.27       |
|    value_loss           | 2.44       |
----------------------------------------
-----------------------------------------
| reward                  | 0.722       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.098       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0461      |
| reward_position         | 0.000658    |
| reward_rotation         | 0.137       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 817         |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 286         |
|    time_elapsed         | 1151        |
|    total_timesteps      | 292864      |
| train/                  |             |
|    approx_kl            | 0.117796354 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41         |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.27        |
|    value_loss           | 2.19        |
-----------------------------------------
----------------------------------------
| reward                  | 0.725      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.0979     |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0462     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 825        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 287        |
|    time_elapsed         | 1156       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.41603482 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0913     |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.27       |
|    value_loss           | 1.43       |
----------------------------------------
Num timesteps: 294000
Best mean reward: 811.88 - Last mean reward per episode: 825.15
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.726     |
| reward_contact          | 0.0145    |
| reward_ctrl             | 0.0969    |
| reward_motion           | 0.157     |
| reward_orientation      | 0.0464    |
| reward_position         | 0.000425  |
| reward_rotation         | 0.139     |
| reward_torque           | 0.0543    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 826       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 288       |
|    time_elapsed         | 1162      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.1540857 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.1     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.315     |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.0925   |
|    std                  | 0.269     |
|    value_loss           | 1.87      |
---------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.0966      |
| reward_motion           | 0.159       |
| reward_orientation      | 0.0465      |
| reward_position         | 0.000425    |
| reward_rotation         | 0.14        |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 826         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 289         |
|    time_elapsed         | 1168        |
|    total_timesteps      | 295936      |
| train/                  |             |
|    approx_kl            | 0.069010586 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.0704     |
|    std                  | 0.269       |
|    value_loss           | 4.79        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.0959      |
| reward_motion           | 0.159       |
| reward_orientation      | 0.0465      |
| reward_position         | 0.000425    |
| reward_rotation         | 0.141       |
| reward_torque           | 0.0543      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 827         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 290         |
|    time_elapsed         | 1173        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.108881816 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.2       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0595     |
|    std                  | 0.269       |
|    value_loss           | 8.26        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.0959     |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0465     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0543     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 827        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 291        |
|    time_elapsed         | 1178       |
|    total_timesteps      | 297984     |
| train/                  |            |
|    approx_kl            | 0.18210863 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.466      |
|    n_updates            | 5800       |
|    policy_gradient_loss | -0.0934    |
|    std                  | 0.269      |
|    value_loss           | 3.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.0951     |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0464     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 828        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 292        |
|    time_elapsed         | 1183       |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.11979908 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 5820       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.269      |
|    value_loss           | 2.66       |
----------------------------------------
Num timesteps: 300000
Best mean reward: 825.15 - Last mean reward per episode: 829.32
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.0952     |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0467     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0542     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 829        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 293        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 300032     |
| train/                  |            |
|    approx_kl            | 0.05434999 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.46       |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.269      |
|    value_loss           | 8.89       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.0964     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0471     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 832        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 294        |
|    time_elapsed         | 1195       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.14454904 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.15       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0964    |
|    std                  | 0.269      |
|    value_loss           | 2.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0966     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0475     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 833        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 295        |
|    time_elapsed         | 1200       |
|    total_timesteps      | 302080     |
| train/                  |            |
|    approx_kl            | 0.12774941 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.04       |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0472    |
|    std                  | 0.269      |
|    value_loss           | 5.16       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0962     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0472     |
| reward_position         | 0.000425   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 296        |
|    time_elapsed         | 1206       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.12712756 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.21       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.269      |
|    value_loss           | 1.46       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.0965     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0472     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 297        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 304128     |
| train/                  |            |
|    approx_kl            | 0.08620262 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.263      |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.0777    |
|    std                  | 0.268      |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.725      |
| reward_contact          | 0.0154     |
| reward_ctrl             | 0.0963     |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0471     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 298        |
|    time_elapsed         | 1218       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.07004221 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.6      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.84       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0427    |
|    std                  | 0.268      |
|    value_loss           | 19         |
----------------------------------------
Num timesteps: 306000
Best mean reward: 829.32 - Last mean reward per episode: 834.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.726       |
| reward_contact          | 0.0148      |
| reward_ctrl             | 0.0963      |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0471      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0544      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 835         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 299         |
|    time_elapsed         | 1223        |
|    total_timesteps      | 306176      |
| train/                  |             |
|    approx_kl            | 0.056079336 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.268       |
|    value_loss           | 6.26        |
-----------------------------------------
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.0962     |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 836        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 300        |
|    time_elapsed         | 1228       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.06719802 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.12       |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0653    |
|    std                  | 0.268      |
|    value_loss           | 8.65       |
----------------------------------------
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0148      |
| reward_ctrl             | 0.0972      |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0474      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0545      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 837         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 301         |
|    time_elapsed         | 1232        |
|    total_timesteps      | 308224      |
| train/                  |             |
|    approx_kl            | 0.061720807 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.37        |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.0639     |
|    std                  | 0.268       |
|    value_loss           | 10.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0967     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 837        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 302        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.09308397 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.268      |
|    value_loss           | 17.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0971     |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0474     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 837        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 303        |
|    time_elapsed         | 1244       |
|    total_timesteps      | 310272     |
| train/                  |            |
|    approx_kl            | 0.15197122 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41        |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.424      |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.268      |
|    value_loss           | 2.61       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.097      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0545     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 838        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 304        |
|    time_elapsed         | 1250       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.13442828 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0722    |
|    std                  | 0.268      |
|    value_loss           | 2.2        |
----------------------------------------
Num timesteps: 312000
Best mean reward: 834.70 - Last mean reward per episode: 837.87
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0137      |
| reward_ctrl             | 0.0971      |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0476      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0545      |
| reward_velocity         | 0.22        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 838         |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 305         |
|    time_elapsed         | 1255        |
|    total_timesteps      | 312320      |
| train/                  |             |
|    approx_kl            | 0.080127835 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.538       |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.0693     |
|    std                  | 0.268       |
|    value_loss           | 8.51        |
-----------------------------------------
---------------------------------------
| reward                  | 0.731     |
| reward_contact          | 0.0137    |
| reward_ctrl             | 0.097     |
| reward_motion           | 0.149     |
| reward_orientation      | 0.0477    |
| reward_position         | 3.3e-06   |
| reward_rotation         | 0.149     |
| reward_torque           | 0.0545    |
| reward_velocity         | 0.22      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 839       |
| time/                   |           |
|    fps                  | 248       |
|    iterations           | 306       |
|    time_elapsed         | 1261      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0966893 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -40.7     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.774     |
|    n_updates            | 6100      |
|    policy_gradient_loss | -0.0759   |
|    std                  | 0.268     |
|    value_loss           | 3.27      |
---------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0964     |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0544     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 840        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 307        |
|    time_elapsed         | 1267       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.07179193 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.5      |
|    explained_variance   | 0.246      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.7       |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.0535    |
|    std                  | 0.268      |
|    value_loss           | 27.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.0977     |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0478     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 841        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 308        |
|    time_elapsed         | 1273       |
|    total_timesteps      | 315392     |
| train/                  |            |
|    approx_kl            | 0.05680317 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 6140       |
|    policy_gradient_loss | -0.0484    |
|    std                  | 0.268      |
|    value_loss           | 13.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.736      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0971     |
| reward_motion           | 0.152      |
| reward_orientation      | 0.048      |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0546     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 842        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 309        |
|    time_elapsed         | 1278       |
|    total_timesteps      | 316416     |
| train/                  |            |
|    approx_kl            | 0.05833668 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.33       |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.0465    |
|    std                  | 0.268      |
|    value_loss           | 17.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0155     |
| reward_ctrl             | 0.098      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0483     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 843        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 310        |
|    time_elapsed         | 1283       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.07074337 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.34       |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.268      |
|    value_loss           | 18.4       |
----------------------------------------
Num timesteps: 318000
Best mean reward: 837.87 - Last mean reward per episode: 843.72
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0483     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 844        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 311        |
|    time_elapsed         | 1288       |
|    total_timesteps      | 318464     |
| train/                  |            |
|    approx_kl            | 0.08706002 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.619      |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.0779    |
|    std                  | 0.267      |
|    value_loss           | 2.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.098      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0486     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0547     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 844        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 312        |
|    time_elapsed         | 1294       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.06661334 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.267      |
|    value_loss           | 13.8       |
----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0155      |
| reward_ctrl             | 0.0973      |
| reward_motion           | 0.145       |
| reward_orientation      | 0.0487      |
| reward_position         | 3.3e-06     |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 844         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 313         |
|    time_elapsed         | 1300        |
|    total_timesteps      | 320512      |
| train/                  |             |
|    approx_kl            | 0.110263005 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.535       |
|    n_updates            | 6240        |
|    policy_gradient_loss | -0.0932     |
|    std                  | 0.267       |
|    value_loss           | 2.31        |
-----------------------------------------
--------------------------------------
| reward                  | 0.729    |
| reward_contact          | 0.0154   |
| reward_ctrl             | 0.0984   |
| reward_motion           | 0.142    |
| reward_orientation      | 0.0488   |
| reward_position         | 3.3e-06  |
| reward_rotation         | 0.147    |
| reward_torque           | 0.0548   |
| reward_velocity         | 0.222    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 844      |
| time/                   |          |
|    fps                  | 246      |
|    iterations           | 314      |
|    time_elapsed         | 1306     |
|    total_timesteps      | 321536   |
| train/                  |          |
|    approx_kl            | 0.265162 |
|    clip_fraction        | 0.369    |
|    clip_range           | 0.4      |
|    entropy_loss         | -41.2    |
|    explained_variance   | 0.992    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.26     |
|    n_updates            | 6260     |
|    policy_gradient_loss | -0.0946  |
|    std                  | 0.267    |
|    value_loss           | 4.89     |
--------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.0997     |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0488     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 845        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 315        |
|    time_elapsed         | 1312       |
|    total_timesteps      | 322560     |
| train/                  |            |
|    approx_kl            | 0.19701514 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.89       |
|    n_updates            | 6280       |
|    policy_gradient_loss | -0.0624    |
|    std                  | 0.267      |
|    value_loss           | 8.08       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0148     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0492     |
| reward_position         | 3.3e-06    |
| reward_rotation         | 0.147      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 846        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 316        |
|    time_elapsed         | 1318       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.14349028 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.48       |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.0806    |
|    std                  | 0.267      |
|    value_loss           | 7.37       |
----------------------------------------
Num timesteps: 324000
Best mean reward: 843.72 - Last mean reward per episode: 846.95
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0142     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0492     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 317        |
|    time_elapsed         | 1324       |
|    total_timesteps      | 324608     |
| train/                  |            |
|    approx_kl            | 0.14883742 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.41       |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.0706    |
|    std                  | 0.267      |
|    value_loss           | 4.31       |
----------------------------------------
-----------------------------------------
| reward                  | 0.725       |
| reward_contact          | 0.0142      |
| reward_ctrl             | 0.0998      |
| reward_motion           | 0.136       |
| reward_orientation      | 0.0493      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0549      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 847         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 318         |
|    time_elapsed         | 1330        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.069601014 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.7       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.0498     |
|    std                  | 0.267       |
|    value_loss           | 17.1        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.725       |
| reward_contact          | 0.0136      |
| reward_ctrl             | 0.101       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0495      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0551      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 319         |
|    time_elapsed         | 1335        |
|    total_timesteps      | 326656      |
| train/                  |             |
|    approx_kl            | 0.082963794 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.0468     |
|    std                  | 0.267       |
|    value_loss           | 21.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0498     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 320        |
|    time_elapsed         | 1340       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.07678221 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -40.8      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.83       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.0498    |
|    std                  | 0.267      |
|    value_loss           | 14.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0502     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 321        |
|    time_elapsed         | 1345       |
|    total_timesteps      | 328704     |
| train/                  |            |
|    approx_kl            | 0.14463776 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.267      |
|    value_loss           | 3.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0506     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 322        |
|    time_elapsed         | 1349       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.58815205 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 6420       |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.267      |
|    value_loss           | 2.39       |
----------------------------------------
Num timesteps: 330000
Best mean reward: 846.95 - Last mean reward per episode: 852.32
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.729      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0506     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 323        |
|    time_elapsed         | 1355       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.15225565 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.391      |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.267      |
|    value_loss           | 2.49       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0505     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 324        |
|    time_elapsed         | 1361       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.27948576 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.301      |
|    n_updates            | 6460       |
|    policy_gradient_loss | -0.0902    |
|    std                  | 0.267      |
|    value_loss           | 1.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0507     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 325        |
|    time_elapsed         | 1367       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.21981703 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.353      |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.0967    |
|    std                  | 0.267      |
|    value_loss           | 2          |
----------------------------------------
---------------------------------------
| reward                  | 0.728     |
| reward_contact          | 0.013     |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0508    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.15      |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.222     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 854       |
| time/                   |           |
|    fps                  | 242       |
|    iterations           | 326       |
|    time_elapsed         | 1373      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.1953329 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.6     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.115     |
|    n_updates            | 6500      |
|    policy_gradient_loss | -0.0941   |
|    std                  | 0.267     |
|    value_loss           | 1.4       |
---------------------------------------
-----------------------------------------
| reward                  | 0.731       |
| reward_contact          | 0.013       |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.138       |
| reward_orientation      | 0.0507      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0554      |
| reward_velocity         | 0.222       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 855         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 327         |
|    time_elapsed         | 1379        |
|    total_timesteps      | 334848      |
| train/                  |             |
|    approx_kl            | 0.089071915 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.267       |
|    value_loss           | 3.48        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.013      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0508     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 328        |
|    time_elapsed         | 1383       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.08858572 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.3      |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.852      |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.0456    |
|    std                  | 0.267      |
|    value_loss           | 30.7       |
----------------------------------------
Num timesteps: 336000
Best mean reward: 852.32 - Last mean reward per episode: 855.59
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.051      |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 329        |
|    time_elapsed         | 1388       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.13890404 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.445      |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.267      |
|    value_loss           | 1.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0514     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 330        |
|    time_elapsed         | 1394       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.12827478 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.9        |
|    n_updates            | 6580       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.267      |
|    value_loss           | 3.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0513     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 331        |
|    time_elapsed         | 1400       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.17206544 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.266      |
|    value_loss           | 2.52       |
----------------------------------------
---------------------------------------
| reward                  | 0.735     |
| reward_contact          | 0.0124    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0512    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 858       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 332       |
|    time_elapsed         | 1405      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.2605111 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.2     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.936     |
|    n_updates            | 6620      |
|    policy_gradient_loss | -0.0836   |
|    std                  | 0.266     |
|    value_loss           | 2.34      |
---------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0512     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 333        |
|    time_elapsed         | 1412       |
|    total_timesteps      | 340992     |
| train/                  |            |
|    approx_kl            | 0.10314956 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.12       |
|    n_updates            | 6640       |
|    policy_gradient_loss | -0.0526    |
|    std                  | 0.266      |
|    value_loss           | 6.26       |
----------------------------------------
Num timesteps: 342000
Best mean reward: 855.59 - Last mean reward per episode: 858.19
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0512     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 334        |
|    time_elapsed         | 1417       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.15646465 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.249      |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.0907    |
|    std                  | 0.266      |
|    value_loss           | 1.82       |
----------------------------------------
-----------------------------------------
| reward                  | 0.738       |
| reward_contact          | 0.0124      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.137       |
| reward_orientation      | 0.0514      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.152       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 859         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 335         |
|    time_elapsed         | 1422        |
|    total_timesteps      | 343040      |
| train/                  |             |
|    approx_kl            | 0.047534503 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.41        |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.266       |
|    value_loss           | 7.15        |
-----------------------------------------
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.0124     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0514     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 336        |
|    time_elapsed         | 1428       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.06308009 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.59       |
|    n_updates            | 6700       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.266      |
|    value_loss           | 6.47       |
----------------------------------------
-----------------------------------------
| reward                  | 0.736       |
| reward_contact          | 0.0122      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.13        |
| reward_orientation      | 0.0518      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.155       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 860         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 337         |
|    time_elapsed         | 1434        |
|    total_timesteps      | 345088      |
| train/                  |             |
|    approx_kl            | 0.073026195 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.5       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 6720        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.265       |
|    value_loss           | 22.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0521     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 338        |
|    time_elapsed         | 1439       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.18715525 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.0698    |
|    std                  | 0.265      |
|    value_loss           | 2.88       |
----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0521     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 339        |
|    time_elapsed         | 1445       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.06211298 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.93       |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.265      |
|    value_loss           | 13.8       |
----------------------------------------
Num timesteps: 348000
Best mean reward: 858.19 - Last mean reward per episode: 862.23
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.755       |
| reward_contact          | 0.0126      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0524      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.161       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.231       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 340         |
|    time_elapsed         | 1450        |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.085877754 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 6780        |
|    policy_gradient_loss | -0.0777     |
|    std                  | 0.265       |
|    value_loss           | 2.83        |
-----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0524     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 341        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.25781357 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.2      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.73       |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.265      |
|    value_loss           | 22.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0525     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 342        |
|    time_elapsed         | 1460       |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.37170777 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.296      |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.0954    |
|    std                  | 0.265      |
|    value_loss           | 1.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0529     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 343        |
|    time_elapsed         | 1466       |
|    total_timesteps      | 351232     |
| train/                  |            |
|    approx_kl            | 0.14535479 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.0921    |
|    std                  | 0.265      |
|    value_loss           | 3.33       |
----------------------------------------
---------------------------------------
| reward                  | 0.768     |
| reward_contact          | 0.0125    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.141     |
| reward_orientation      | 0.0529    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 239       |
|    iterations           | 344       |
|    time_elapsed         | 1470      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.1058441 |
|    clip_fraction        | 0.269     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.775     |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.0707   |
|    std                  | 0.264     |
|    value_loss           | 2.08      |
---------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0125     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0529     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 345        |
|    time_elapsed         | 1476       |
|    total_timesteps      | 353280     |
| train/                  |            |
|    approx_kl            | 0.06330973 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.195      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.9        |
|    n_updates            | 6880       |
|    policy_gradient_loss | -0.0468    |
|    std                  | 0.264      |
|    value_loss           | 26.2       |
----------------------------------------
Num timesteps: 354000
Best mean reward: 862.23 - Last mean reward per episode: 866.09
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.776      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.053      |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 346        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.06405512 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.264      |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.775     |
| reward_contact          | 0.0131    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.146     |
| reward_orientation      | 0.0529    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 347       |
|    time_elapsed         | 1488      |
|    total_timesteps      | 355328    |
| train/                  |           |
|    approx_kl            | 0.2617231 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.4     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.4       |
|    n_updates            | 6920      |
|    policy_gradient_loss | -0.0972   |
|    std                  | 0.264     |
|    value_loss           | 5.08      |
---------------------------------------
---------------------------------------
| reward                  | 0.776     |
| reward_contact          | 0.0131    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.145     |
| reward_orientation      | 0.0532    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.163     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 348       |
|    time_elapsed         | 1494      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.5615964 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0838   |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.0782   |
|    std                  | 0.264     |
|    value_loss           | 1.03      |
---------------------------------------
-----------------------------------------
| reward                  | 0.777       |
| reward_contact          | 0.0131      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0536      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 349         |
|    time_elapsed         | 1500        |
|    total_timesteps      | 357376      |
| train/                  |             |
|    approx_kl            | 0.059288148 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.25        |
|    n_updates            | 6960        |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.264       |
|    value_loss           | 24.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0125     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.147      |
| reward_orientation      | 0.0538     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 350        |
|    time_elapsed         | 1506       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.06062203 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.34       |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.0544    |
|    std                  | 0.264      |
|    value_loss           | 8.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 351        |
|    time_elapsed         | 1512       |
|    total_timesteps      | 359424     |
| train/                  |            |
|    approx_kl            | 0.07820898 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.04       |
|    n_updates            | 7000       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.264      |
|    value_loss           | 9.03       |
----------------------------------------
Num timesteps: 360000
Best mean reward: 866.09 - Last mean reward per episode: 867.76
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.147      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 352        |
|    time_elapsed         | 1518       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.11952571 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.576      |
|    n_updates            | 7020       |
|    policy_gradient_loss | -0.0757    |
|    std                  | 0.264      |
|    value_loss           | 2.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 353        |
|    time_elapsed         | 1523       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.36200026 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0922     |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.0846    |
|    std                  | 0.264      |
|    value_loss           | 1.51       |
----------------------------------------
-----------------------------------------
| reward                  | 0.779       |
| reward_contact          | 0.0125      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0534      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 354         |
|    time_elapsed         | 1528        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.046221856 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.4       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 7060        |
|    policy_gradient_loss | -0.0558     |
|    std                  | 0.264       |
|    value_loss           | 17.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 355        |
|    time_elapsed         | 1533       |
|    total_timesteps      | 363520     |
| train/                  |            |
|    approx_kl            | 0.07663175 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 0.264      |
|    value_loss           | 4.2        |
----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0536     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 356        |
|    time_elapsed         | 1538       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.23710397 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0598     |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.263      |
|    value_loss           | 1.32       |
----------------------------------------
---------------------------------------
| reward                  | 0.79      |
| reward_contact          | 0.0132    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.154     |
| reward_orientation      | 0.0534    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0562    |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 870       |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 357       |
|    time_elapsed         | 1544      |
|    total_timesteps      | 365568    |
| train/                  |           |
|    approx_kl            | 0.0728744 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.5     |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.59      |
|    n_updates            | 7120      |
|    policy_gradient_loss | -0.0474   |
|    std                  | 0.263     |
|    value_loss           | 16.8      |
---------------------------------------
Num timesteps: 366000
Best mean reward: 867.76 - Last mean reward per episode: 870.75
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 358        |
|    time_elapsed         | 1550       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.12093382 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.311      |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 0.263      |
|    value_loss           | 2.76       |
----------------------------------------
----------------------------------------
| reward                  | 0.8        |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0534     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 359        |
|    time_elapsed         | 1556       |
|    total_timesteps      | 367616     |
| train/                  |            |
|    approx_kl            | 0.03586774 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.0327    |
|    std                  | 0.263      |
|    value_loss           | 28.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0537     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 872        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 360        |
|    time_elapsed         | 1560       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.11653953 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.263      |
|    value_loss           | 3.15       |
----------------------------------------
----------------------------------------
| reward                  | 0.802      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0538     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 361        |
|    time_elapsed         | 1565       |
|    total_timesteps      | 369664     |
| train/                  |            |
|    approx_kl            | 0.10203838 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.6      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 7200       |
|    policy_gradient_loss | -0.0758    |
|    std                  | 0.263      |
|    value_loss           | 6.29       |
----------------------------------------
---------------------------------------
| reward                  | 0.803     |
| reward_contact          | 0.0126    |
| reward_ctrl             | 0.112     |
| reward_motion           | 0.159     |
| reward_orientation      | 0.0538    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.169     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.241     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 873       |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 362       |
|    time_elapsed         | 1570      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.4398377 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.7     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.215     |
|    n_updates            | 7220      |
|    policy_gradient_loss | -0.0976   |
|    std                  | 0.263     |
|    value_loss           | 1.47      |
---------------------------------------
-----------------------------------------
| reward                  | 0.806       |
| reward_contact          | 0.0126      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.16        |
| reward_orientation      | 0.0539      |
| reward_position         | 5.42e-07    |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.241       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 873         |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 363         |
|    time_elapsed         | 1576        |
|    total_timesteps      | 371712      |
| train/                  |             |
|    approx_kl            | 0.048497554 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 7240        |
|    policy_gradient_loss | -0.0562     |
|    std                  | 0.263       |
|    value_loss           | 19.6        |
-----------------------------------------
Num timesteps: 372000
Best mean reward: 870.75 - Last mean reward per episode: 873.10
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 364        |
|    time_elapsed         | 1582       |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.08879957 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 7260       |
|    policy_gradient_loss | -0.076     |
|    std                  | 0.263      |
|    value_loss           | 3.63       |
----------------------------------------
---------------------------------------
| reward                  | 0.807     |
| reward_contact          | 0.0132    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.0535    |
| reward_position         | 5.42e-07  |
| reward_rotation         | 0.168     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.241     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 874       |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 365       |
|    time_elapsed         | 1588      |
|    total_timesteps      | 373760    |
| train/                  |           |
|    approx_kl            | 0.1352642 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.4       |
|    entropy_loss         | -41.6     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.75      |
|    n_updates            | 7280      |
|    policy_gradient_loss | -0.0676   |
|    std                  | 0.263     |
|    value_loss           | 3.77      |
---------------------------------------
----------------------------------------
| reward                  | 0.81       |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0535     |
| reward_position         | 5.42e-07   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 366        |
|    time_elapsed         | 1594       |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.08276927 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.11       |
|    n_updates            | 7300       |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.263      |
|    value_loss           | 2.73       |
----------------------------------------
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0537     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 367        |
|    time_elapsed         | 1600       |
|    total_timesteps      | 375808     |
| train/                  |            |
|    approx_kl            | 0.12825412 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.49       |
|    n_updates            | 7320       |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.263      |
|    value_loss           | 1.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.812      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0537     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 368        |
|    time_elapsed         | 1606       |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.12419422 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.891      |
|    n_updates            | 7340       |
|    policy_gradient_loss | -0.0728    |
|    std                  | 0.262      |
|    value_loss           | 4.03       |
----------------------------------------
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0534     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 369        |
|    time_elapsed         | 1612       |
|    total_timesteps      | 377856     |
| train/                  |            |
|    approx_kl            | 0.11390327 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.3      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7360       |
|    policy_gradient_loss | -0.0504    |
|    std                  | 0.262      |
|    value_loss           | 12.1       |
----------------------------------------
Num timesteps: 378000
Best mean reward: 873.10 - Last mean reward per episode: 874.37
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.815      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0533     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 370        |
|    time_elapsed         | 1618       |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.12412284 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.4      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.757      |
|    n_updates            | 7380       |
|    policy_gradient_loss | -0.0939    |
|    std                  | 0.262      |
|    value_loss           | 1.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.816      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0529     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 371        |
|    time_elapsed         | 1623       |
|    total_timesteps      | 379904     |
| train/                  |            |
|    approx_kl            | 0.11696975 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 7400       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.262      |
|    value_loss           | 3.01       |
----------------------------------------
-----------------------------------------
| reward                  | 0.805       |
| reward_contact          | 0.0127      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.158       |
| reward_orientation      | 0.0525      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0565      |
| reward_velocity         | 0.242       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 372         |
|    time_elapsed         | 1629        |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.090974815 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.5       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.46        |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.083      |
|    std                  | 0.262       |
|    value_loss           | 9.29        |
-----------------------------------------
---------------------------------------
| reward                  | 0.804     |
| reward_contact          | 0.0127    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.16      |
| reward_orientation      | 0.0525    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.169     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.242     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 875       |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 373       |
|    time_elapsed         | 1635      |
|    total_timesteps      | 381952    |
| train/                  |           |
|    approx_kl            | 0.2094283 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.2     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0832    |
|    n_updates            | 7440      |
|    policy_gradient_loss | -0.101    |
|    std                  | 0.262     |
|    value_loss           | 1.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0523     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 374        |
|    time_elapsed         | 1641       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.12783518 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 7460       |
|    policy_gradient_loss | -0.0444    |
|    std                  | 0.262      |
|    value_loss           | 5.66       |
----------------------------------------
Num timesteps: 384000
Best mean reward: 874.37 - Last mean reward per episode: 874.61
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.155       |
| reward_orientation      | 0.052       |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.242       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 375         |
|    time_elapsed         | 1646        |
|    total_timesteps      | 384000      |
| train/                  |             |
|    approx_kl            | 0.066565424 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.6       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.18        |
|    n_updates            | 7480        |
|    policy_gradient_loss | -0.0684     |
|    std                  | 0.262       |
|    value_loss           | 7.34        |
-----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0521     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 376        |
|    time_elapsed         | 1651       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.19658199 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.552      |
|    n_updates            | 7500       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.262      |
|    value_loss           | 1.99       |
----------------------------------------
-----------------------------------------
| reward                  | 0.803       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.157       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 377         |
|    time_elapsed         | 1655        |
|    total_timesteps      | 386048      |
| train/                  |             |
|    approx_kl            | 0.096728265 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48        |
|    n_updates            | 7520        |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.261       |
|    value_loss           | 6.09        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.803       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.157       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 378         |
|    time_elapsed         | 1661        |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.074183166 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42         |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.36        |
|    n_updates            | 7540        |
|    policy_gradient_loss | -0.0701     |
|    std                  | 0.261       |
|    value_loss           | 9.93        |
-----------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 379        |
|    time_elapsed         | 1667       |
|    total_timesteps      | 388096     |
| train/                  |            |
|    approx_kl            | 0.05800061 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 7560       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.261      |
|    value_loss           | 26.3       |
----------------------------------------
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.153       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 380         |
|    time_elapsed         | 1674        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.097186156 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.844       |
|    n_updates            | 7580        |
|    policy_gradient_loss | -0.0472     |
|    std                  | 0.261       |
|    value_loss           | 8.06        |
-----------------------------------------
Num timesteps: 390000
Best mean reward: 874.61 - Last mean reward per episode: 874.70
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0129      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 381         |
|    time_elapsed         | 1679        |
|    total_timesteps      | 390144      |
| train/                  |             |
|    approx_kl            | 0.082256764 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.2       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.633       |
|    n_updates            | 7600        |
|    policy_gradient_loss | -0.0791     |
|    std                  | 0.261       |
|    value_loss           | 5.78        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.801       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.153       |
| reward_orientation      | 0.0519      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.172       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 382         |
|    time_elapsed         | 1683        |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.067797236 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.4         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0611     |
|    std                  | 0.261       |
|    value_loss           | 13.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 383        |
|    time_elapsed         | 1689       |
|    total_timesteps      | 392192     |
| train/                  |            |
|    approx_kl            | 0.15364167 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 7640       |
|    policy_gradient_loss | -0.0763    |
|    std                  | 0.261      |
|    value_loss           | 2.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0517     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.245      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 384        |
|    time_elapsed         | 1694       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.35743248 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 7660       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.261      |
|    value_loss           | 2.09       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0519     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 385        |
|    time_elapsed         | 1700       |
|    total_timesteps      | 394240     |
| train/                  |            |
|    approx_kl            | 0.12311756 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.5        |
|    n_updates            | 7680       |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.261      |
|    value_loss           | 6.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0518     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 386        |
|    time_elapsed         | 1705       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.09561122 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.12       |
|    n_updates            | 7700       |
|    policy_gradient_loss | -0.083     |
|    std                  | 0.261      |
|    value_loss           | 4.77       |
----------------------------------------
Num timesteps: 396000
Best mean reward: 874.70 - Last mean reward per episode: 875.26
Saving new best model to rl/out_dir/models/exp71/best_model.zip
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.167     |
| reward_orientation      | 0.0518    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.248     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 875       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 387       |
|    time_elapsed         | 1711      |
|    total_timesteps      | 396288    |
| train/                  |           |
|    approx_kl            | 0.5216126 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.6     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 7720      |
|    policy_gradient_loss | -0.0863   |
|    std                  | 0.261     |
|    value_loss           | 6.2       |
---------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0516     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 388        |
|    time_elapsed         | 1716       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.06653166 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.22       |
|    n_updates            | 7740       |
|    policy_gradient_loss | -0.0592    |
|    std                  | 0.261      |
|    value_loss           | 20.7       |
----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.167       |
| reward_orientation      | 0.0518      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.175       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.248       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 389         |
|    time_elapsed         | 1721        |
|    total_timesteps      | 398336      |
| train/                  |             |
|    approx_kl            | 0.097563505 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77        |
|    n_updates            | 7760        |
|    policy_gradient_loss | -0.0878     |
|    std                  | 0.261       |
|    value_loss           | 4.98        |
-----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0515     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.177      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.25       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 390        |
|    time_elapsed         | 1726       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.15344225 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7780       |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.261      |
|    value_loss           | 2.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0515     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.175      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 391        |
|    time_elapsed         | 1732       |
|    total_timesteps      | 400384     |
| train/                  |            |
|    approx_kl            | 0.06414022 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.44       |
|    n_updates            | 7800       |
|    policy_gradient_loss | -0.0666    |
|    std                  | 0.26       |
|    value_loss           | 12.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.114     |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0515    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.248     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 873       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 392       |
|    time_elapsed         | 1738      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.0929115 |
|    clip_fraction        | 0.225     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.6     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.57      |
|    n_updates            | 7820      |
|    policy_gradient_loss | -0.0835   |
|    std                  | 0.26      |
|    value_loss           | 2.76      |
---------------------------------------
Num timesteps: 402000
Best mean reward: 875.26 - Last mean reward per episode: 872.81
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0512     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 393        |
|    time_elapsed         | 1744       |
|    total_timesteps      | 402432     |
| train/                  |            |
|    approx_kl            | 0.34199274 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.334      |
|    n_updates            | 7840       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.26       |
|    value_loss           | 1.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 394        |
|    time_elapsed         | 1749       |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.12631921 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.454      |
|    n_updates            | 7860       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.26       |
|    value_loss           | 2.23       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 395        |
|    time_elapsed         | 1754       |
|    total_timesteps      | 404480     |
| train/                  |            |
|    approx_kl            | 0.12471636 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7880       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.26       |
|    value_loss           | 7.71       |
----------------------------------------
---------------------------------------
| reward                  | 0.822     |
| reward_contact          | 0.0117    |
| reward_ctrl             | 0.115     |
| reward_motion           | 0.167     |
| reward_orientation      | 0.0512    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.175     |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.247     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 871       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 396       |
|    time_elapsed         | 1759      |
|    total_timesteps      | 405504    |
| train/                  |           |
|    approx_kl            | 0.1383692 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.3     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.916     |
|    n_updates            | 7900      |
|    policy_gradient_loss | -0.061    |
|    std                  | 0.26      |
|    value_loss           | 2.41      |
---------------------------------------
-----------------------------------------
| reward                  | 0.814       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.114       |
| reward_motion           | 0.162       |
| reward_orientation      | 0.0511      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 397         |
|    time_elapsed         | 1764        |
|    total_timesteps      | 406528      |
| train/                  |             |
|    approx_kl            | 0.073663875 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.3       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.18        |
|    n_updates            | 7920        |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.26        |
|    value_loss           | 27.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.176      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.248      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 398        |
|    time_elapsed         | 1770       |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.15638089 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 7940       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.26       |
|    value_loss           | 2.01       |
----------------------------------------
Num timesteps: 408000
Best mean reward: 875.26 - Last mean reward per episode: 869.64
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.169      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 399        |
|    time_elapsed         | 1776       |
|    total_timesteps      | 408576     |
| train/                  |            |
|    approx_kl            | 0.07320256 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.476      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 7960       |
|    policy_gradient_loss | -0.047     |
|    std                  | 0.26       |
|    value_loss           | 20.4       |
----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0511      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.246       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 400         |
|    time_elapsed         | 1782        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.066526555 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.6       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.12        |
|    n_updates            | 7980        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.26        |
|    value_loss           | 9.87        |
-----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.0511     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 401        |
|    time_elapsed         | 1788       |
|    total_timesteps      | 410624     |
| train/                  |            |
|    approx_kl            | 0.59169793 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0158     |
|    n_updates            | 8000       |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.259      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.819      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.051      |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.247      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 402        |
|    time_elapsed         | 1793       |
|    total_timesteps      | 411648     |
| train/                  |            |
|    approx_kl            | 0.07575175 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 8020       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.259      |
|    value_loss           | 8.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 403        |
|    time_elapsed         | 1799       |
|    total_timesteps      | 412672     |
| train/                  |            |
|    approx_kl            | 0.40041345 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.226      |
|    n_updates            | 8040       |
|    policy_gradient_loss | -0.0833    |
|    std                  | 0.259      |
|    value_loss           | 1.69       |
----------------------------------------
----------------------------------------
| reward                  | 0.821      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.172      |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.246      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 404        |
|    time_elapsed         | 1804       |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.10629907 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.223      |
|    n_updates            | 8060       |
|    policy_gradient_loss | -0.0749    |
|    std                  | 0.259      |
|    value_loss           | 1.99       |
----------------------------------------
Num timesteps: 414000
Best mean reward: 875.26 - Last mean reward per episode: 868.80
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.175       |
| reward_orientation      | 0.051       |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.171       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 405         |
|    time_elapsed         | 1809        |
|    total_timesteps      | 414720      |
| train/                  |             |
|    approx_kl            | 0.088129334 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 8080        |
|    policy_gradient_loss | -0.0652     |
|    std                  | 0.259       |
|    value_loss           | 4.27        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.823       |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.176       |
| reward_orientation      | 0.0509      |
| reward_position         | 3.7e-17     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.245       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 406         |
|    time_elapsed         | 1814        |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.058865942 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.076      |
|    std                  | 0.259       |
|    value_loss           | 7.68        |
-----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.175      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.245      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 407        |
|    time_elapsed         | 1820       |
|    total_timesteps      | 416768     |
| train/                  |            |
|    approx_kl            | 0.11264556 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.455      |
|    n_updates            | 8120       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.259      |
|    value_loss           | 1.83       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0509     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 408        |
|    time_elapsed         | 1826       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.20351717 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 8140       |
|    policy_gradient_loss | -0.0875    |
|    std                  | 0.258      |
|    value_loss           | 1.66       |
----------------------------------------
----------------------------------------
| reward                  | 0.824      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0508     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 409        |
|    time_elapsed         | 1832       |
|    total_timesteps      | 418816     |
| train/                  |            |
|    approx_kl            | 0.07331851 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.39       |
|    n_updates            | 8160       |
|    policy_gradient_loss | -0.0576    |
|    std                  | 0.258      |
|    value_loss           | 26.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.823     |
| reward_contact          | 0.0105    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.179     |
| reward_orientation      | 0.0509    |
| reward_position         | 3.7e-17   |
| reward_rotation         | 0.17      |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.244     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 410       |
|    time_elapsed         | 1838      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 1.2654184 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0531    |
|    n_updates            | 8180      |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.259     |
|    value_loss           | 1.57      |
---------------------------------------
Num timesteps: 420000
Best mean reward: 875.26 - Last mean reward per episode: 865.77
----------------------------------------
| reward                  | 0.825      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0506     |
| reward_position         | 3.7e-17    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 411        |
|    time_elapsed         | 1843       |
|    total_timesteps      | 420864     |
| train/                  |            |
|    approx_kl            | 0.09515531 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 8200       |
|    policy_gradient_loss | -0.0884    |
|    std                  | 0.258      |
|    value_loss           | 4.16       |
----------------------------------------
-----------------------------------------
| reward                  | 0.819       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.177       |
| reward_orientation      | 0.0505      |
| reward_position         | 6.1e-11     |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.243       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 412         |
|    time_elapsed         | 1849        |
|    total_timesteps      | 421888      |
| train/                  |             |
|    approx_kl            | 0.113677114 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 8220        |
|    policy_gradient_loss | -0.092      |
|    std                  | 0.258       |
|    value_loss           | 1.67        |
-----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.00993    |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0508     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 413        |
|    time_elapsed         | 1854       |
|    total_timesteps      | 422912     |
| train/                  |            |
|    approx_kl            | 0.06169289 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.2       |
|    n_updates            | 8240       |
|    policy_gradient_loss | -0.0454    |
|    std                  | 0.258      |
|    value_loss           | 17.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.828       |
| reward_contact          | 0.00993     |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0508      |
| reward_position         | 6.1e-11     |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 414         |
|    time_elapsed         | 1859        |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.053849638 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 8260        |
|    policy_gradient_loss | -0.054      |
|    std                  | 0.258       |
|    value_loss           | 8.47        |
-----------------------------------------
----------------------------------------
| reward                  | 0.828      |
| reward_contact          | 0.00992    |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.185      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 415        |
|    time_elapsed         | 1864       |
|    total_timesteps      | 424960     |
| train/                  |            |
|    approx_kl            | 0.13096769 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.993      |
|    n_updates            | 8280       |
|    policy_gradient_loss | -0.0642    |
|    std                  | 0.258      |
|    value_loss           | 7.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.833      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 416        |
|    time_elapsed         | 1869       |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.09408496 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.17       |
|    n_updates            | 8300       |
|    policy_gradient_loss | -0.0506    |
|    std                  | 0.258      |
|    value_loss           | 17.7       |
----------------------------------------
Num timesteps: 426000
Best mean reward: 875.26 - Last mean reward per episode: 867.11
----------------------------------------
| reward                  | 0.837      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 417        |
|    time_elapsed         | 1875       |
|    total_timesteps      | 427008     |
| train/                  |            |
|    approx_kl            | 0.21429619 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.45       |
|    n_updates            | 8320       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.258      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.839      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0511     |
| reward_position         | 6.1e-11    |
| reward_rotation         | 0.174      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 418        |
|    time_elapsed         | 1881       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.12873016 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.672      |
|    n_updates            | 8340       |
|    policy_gradient_loss | -0.0924    |
|    std                  | 0.258      |
|    value_loss           | 4.12       |
----------------------------------------
-----------------------------------------
| reward                  | 0.839       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.192       |
| reward_orientation      | 0.0507      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.174       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 419         |
|    time_elapsed         | 1887        |
|    total_timesteps      | 429056      |
| train/                  |             |
|    approx_kl            | 0.089033976 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.5       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 8360        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.257       |
|    value_loss           | 2.65        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.836       |
| reward_contact          | 0.0105      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.19        |
| reward_orientation      | 0.0504      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.175       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.244       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 420         |
|    time_elapsed         | 1892        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.060670216 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.5       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 8380        |
|    policy_gradient_loss | -0.061      |
|    std                  | 0.257       |
|    value_loss           | 6.78        |
-----------------------------------------
----------------------------------------
| reward                  | 0.838      |
| reward_contact          | 0.0105     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0504     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.174      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.244      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 421        |
|    time_elapsed         | 1898       |
|    total_timesteps      | 431104     |
| train/                  |            |
|    approx_kl            | 0.21317141 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -41.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 8400       |
|    policy_gradient_loss | -0.0818    |
|    std                  | 0.257      |
|    value_loss           | 1.94       |
----------------------------------------
Num timesteps: 432000
Best mean reward: 875.26 - Last mean reward per episode: 864.49
----------------------------------------
| reward                  | 0.832      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 422        |
|    time_elapsed         | 1902       |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.06642513 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 4          |
|    n_updates            | 8420       |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.257      |
|    value_loss           | 7.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.833      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.191      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 423        |
|    time_elapsed         | 1906       |
|    total_timesteps      | 433152     |
| train/                  |            |
|    approx_kl            | 0.19337192 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.671      |
|    n_updates            | 8440       |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.257      |
|    value_loss           | 2.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.835      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.193      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.243      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 424        |
|    time_elapsed         | 1910       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.05366406 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 8460       |
|    policy_gradient_loss | -0.0603    |
|    std                  | 0.257      |
|    value_loss           | 9.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.829      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.242      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 425        |
|    time_elapsed         | 1914       |
|    total_timesteps      | 435200     |
| train/                  |            |
|    approx_kl            | 0.09171594 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 8480       |
|    policy_gradient_loss | -0.0695    |
|    std                  | 0.257      |
|    value_loss           | 4.81       |
----------------------------------------
-----------------------------------------
| reward                  | 0.824       |
| reward_contact          | 0.0111      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.19        |
| reward_orientation      | 0.0506      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 426         |
|    time_elapsed         | 1918        |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.097682305 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.39        |
|    n_updates            | 8500        |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.257       |
|    value_loss           | 3.98        |
-----------------------------------------
----------------------------------------
| reward                  | 0.822      |
| reward_contact          | 0.0111     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.188      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 427        |
|    time_elapsed         | 1922       |
|    total_timesteps      | 437248     |
| train/                  |            |
|    approx_kl            | 0.11022976 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.458      |
|    n_updates            | 8520       |
|    policy_gradient_loss | -0.0926    |
|    std                  | 0.257      |
|    value_loss           | 2.39       |
----------------------------------------
Num timesteps: 438000
Best mean reward: 875.26 - Last mean reward per episode: 862.80
----------------------------------------
| reward                  | 0.823      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 428        |
|    time_elapsed         | 1926       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.49428883 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.436      |
|    n_updates            | 8540       |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.257      |
|    value_loss           | 4.28       |
----------------------------------------
--------------------------------------
| reward                  | 0.824    |
| reward_contact          | 0.0117   |
| reward_ctrl             | 0.108    |
| reward_motion           | 0.19     |
| reward_orientation      | 0.0507   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.168    |
| reward_torque           | 0.0557   |
| reward_velocity         | 0.24     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 863      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 429      |
|    time_elapsed         | 1930     |
|    total_timesteps      | 439296   |
| train/                  |          |
|    approx_kl            | 1.406707 |
|    clip_fraction        | 0.46     |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.5    |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0764   |
|    n_updates            | 8560     |
|    policy_gradient_loss | -0.0918  |
|    std                  | 0.257    |
|    value_loss           | 1.34     |
--------------------------------------
----------------------------------------
| reward                  | 0.824      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.0507     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 430        |
|    time_elapsed         | 1934       |
|    total_timesteps      | 440320     |
| train/                  |            |
|    approx_kl            | 0.11693258 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.707      |
|    n_updates            | 8580       |
|    policy_gradient_loss | -0.0354    |
|    std                  | 0.257      |
|    value_loss           | 4.56       |
----------------------------------------
-----------------------------------------
| reward                  | 0.826       |
| reward_contact          | 0.0115      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.193       |
| reward_orientation      | 0.051       |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 431         |
|    time_elapsed         | 1938        |
|    total_timesteps      | 441344      |
| train/                  |             |
|    approx_kl            | 0.077700645 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.863       |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.0786     |
|    std                  | 0.257       |
|    value_loss           | 3.11        |
-----------------------------------------
----------------------------------------
| reward                  | 0.826      |
| reward_contact          | 0.0121     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.195      |
| reward_orientation      | 0.0511     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 432        |
|    time_elapsed         | 1942       |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.08626939 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.942      |
|    n_updates            | 8620       |
|    policy_gradient_loss | -0.0624    |
|    std                  | 0.257      |
|    value_loss           | 6.76       |
----------------------------------------
-----------------------------------------
| reward                  | 0.828       |
| reward_contact          | 0.0121      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0511      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.166       |
| reward_torque           | 0.0554      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 433         |
|    time_elapsed         | 1947        |
|    total_timesteps      | 443392      |
| train/                  |             |
|    approx_kl            | 0.060638763 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.98        |
|    n_updates            | 8640        |
|    policy_gradient_loss | -0.0559     |
|    std                  | 0.257       |
|    value_loss           | 20.7        |
-----------------------------------------
Num timesteps: 444000
Best mean reward: 875.26 - Last mean reward per episode: 863.12
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0507     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 434        |
|    time_elapsed         | 1952       |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.15423091 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.413      |
|    n_updates            | 8660       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.256      |
|    value_loss           | 2.33       |
----------------------------------------
--------------------------------------
| reward                  | 0.825    |
| reward_contact          | 0.0121   |
| reward_ctrl             | 0.106    |
| reward_motion           | 0.195    |
| reward_orientation      | 0.0507   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.165    |
| reward_torque           | 0.0555   |
| reward_velocity         | 0.24     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 862      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 435      |
|    time_elapsed         | 1957     |
|    total_timesteps      | 445440   |
| train/                  |          |
|    approx_kl            | 0.371701 |
|    clip_fraction        | 0.419    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.5    |
|    explained_variance   | 0.99     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.375    |
|    n_updates            | 8680     |
|    policy_gradient_loss | -0.128   |
|    std                  | 0.256    |
|    value_loss           | 1.72     |
--------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 436        |
|    time_elapsed         | 1962       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.20857248 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 8700       |
|    policy_gradient_loss | -0.0828    |
|    std                  | 0.256      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.829      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.202      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 437        |
|    time_elapsed         | 1968       |
|    total_timesteps      | 447488     |
| train/                  |            |
|    approx_kl            | 0.12473744 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.02       |
|    n_updates            | 8720       |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.256      |
|    value_loss           | 12.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0505     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 438        |
|    time_elapsed         | 1972       |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.09645867 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 8740       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.256      |
|    value_loss           | 3.79       |
----------------------------------------
--------------------------------------
| reward                  | 0.823    |
| reward_contact          | 0.0116   |
| reward_ctrl             | 0.104    |
| reward_motion           | 0.199    |
| reward_orientation      | 0.0504   |
| reward_position         | 1.05e-05 |
| reward_rotation         | 0.164    |
| reward_torque           | 0.0553   |
| reward_velocity         | 0.239    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 862      |
| time/                   |          |
|    fps                  | 227      |
|    iterations           | 439      |
|    time_elapsed         | 1979     |
|    total_timesteps      | 449536   |
| train/                  |          |
|    approx_kl            | 0.14214  |
|    clip_fraction        | 0.292    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43      |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.807    |
|    n_updates            | 8760     |
|    policy_gradient_loss | -0.0723  |
|    std                  | 0.256    |
|    value_loss           | 2.55     |
--------------------------------------
Num timesteps: 450000
Best mean reward: 875.26 - Last mean reward per episode: 861.16
----------------------------------------
| reward                  | 0.818      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0504     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 440        |
|    time_elapsed         | 1985       |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.08733638 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 8780       |
|    policy_gradient_loss | -0.0912    |
|    std                  | 0.256      |
|    value_loss           | 2.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.202      |
| reward_orientation      | 0.0502     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 441        |
|    time_elapsed         | 1990       |
|    total_timesteps      | 451584     |
| train/                  |            |
|    approx_kl            | 0.12661164 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 8800       |
|    policy_gradient_loss | -0.055     |
|    std                  | 0.256      |
|    value_loss           | 5.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.818      |
| reward_contact          | 0.0116     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.197      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 442        |
|    time_elapsed         | 1996       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.10178308 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.26       |
|    n_updates            | 8820       |
|    policy_gradient_loss | -0.0747    |
|    std                  | 0.256      |
|    value_loss           | 3.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.82       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.2        |
| reward_orientation      | 0.05       |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 443        |
|    time_elapsed         | 2002       |
|    total_timesteps      | 453632     |
| train/                  |            |
|    approx_kl            | 0.11130506 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 8840       |
|    policy_gradient_loss | -0.0403    |
|    std                  | 0.256      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.817      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.2        |
| reward_orientation      | 0.0496     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 444        |
|    time_elapsed         | 2008       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.10566397 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.256      |
|    value_loss           | 7.46       |
----------------------------------------
-----------------------------------------
| reward                  | 0.812       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0493      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.158       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 445         |
|    time_elapsed         | 2013        |
|    total_timesteps      | 455680      |
| train/                  |             |
|    approx_kl            | 0.101233244 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 8880        |
|    policy_gradient_loss | -0.0801     |
|    std                  | 0.256       |
|    value_loss           | 2.06        |
-----------------------------------------
Num timesteps: 456000
Best mean reward: 875.26 - Last mean reward per episode: 861.05
----------------------------------------
| reward                  | 0.808      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.196      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 446        |
|    time_elapsed         | 2018       |
|    total_timesteps      | 456704     |
| train/                  |            |
|    approx_kl            | 0.32299775 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.134      |
|    n_updates            | 8900       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.255      |
|    value_loss           | 1.64       |
----------------------------------------
---------------------------------------
| reward                  | 0.811     |
| reward_contact          | 0.0128    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.2       |
| reward_orientation      | 0.0494    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.156     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 863       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 447       |
|    time_elapsed         | 2024      |
|    total_timesteps      | 457728    |
| train/                  |           |
|    approx_kl            | 0.1332501 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.5     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.404     |
|    n_updates            | 8920      |
|    policy_gradient_loss | -0.0634   |
|    std                  | 0.255     |
|    value_loss           | 2.28      |
---------------------------------------
-----------------------------------------
| reward                  | 0.806       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.197       |
| reward_orientation      | 0.0494      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.155       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 448         |
|    time_elapsed         | 2030        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.056530066 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.63        |
|    n_updates            | 8940        |
|    policy_gradient_loss | -0.0535     |
|    std                  | 0.255       |
|    value_loss           | 25.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.803      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.194      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 449        |
|    time_elapsed         | 2035       |
|    total_timesteps      | 459776     |
| train/                  |            |
|    approx_kl            | 0.33013603 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 8960       |
|    policy_gradient_loss | -0.0828    |
|    std                  | 0.255      |
|    value_loss           | 2.17       |
----------------------------------------
---------------------------------------
| reward                  | 0.805     |
| reward_contact          | 0.014     |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.198     |
| reward_orientation      | 0.0491    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.153     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 450       |
|    time_elapsed         | 2040      |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.3178975 |
|    clip_fraction        | 0.419     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.8     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0682    |
|    n_updates            | 8980      |
|    policy_gradient_loss | -0.109    |
|    std                  | 0.255     |
|    value_loss           | 1.46      |
---------------------------------------
----------------------------------------
| reward                  | 0.807      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.203      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 451        |
|    time_elapsed         | 2044       |
|    total_timesteps      | 461824     |
| train/                  |            |
|    approx_kl            | 0.06875076 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.1        |
|    n_updates            | 9000       |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.255      |
|    value_loss           | 18.5       |
----------------------------------------
Num timesteps: 462000
Best mean reward: 875.26 - Last mean reward per episode: 859.62
---------------------------------------
| reward                  | 0.803     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.201     |
| reward_orientation      | 0.0494    |
| reward_position         | 1.05e-05  |
| reward_rotation         | 0.149     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 452       |
|    time_elapsed         | 2049      |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.1651975 |
|    clip_fraction        | 0.346     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.7       |
|    n_updates            | 9020      |
|    policy_gradient_loss | -0.119    |
|    std                  | 0.255     |
|    value_loss           | 1.91      |
---------------------------------------
----------------------------------------
| reward                  | 0.802      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 453        |
|    time_elapsed         | 2053       |
|    total_timesteps      | 463872     |
| train/                  |            |
|    approx_kl            | 0.06466171 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.84       |
|    n_updates            | 9040       |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.255      |
|    value_loss           | 22         |
----------------------------------------
-----------------------------------------
| reward                  | 0.808       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.2         |
| reward_orientation      | 0.0496      |
| reward_position         | 1.05e-05    |
| reward_rotation         | 0.152       |
| reward_torque           | 0.0555      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 858         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 454         |
|    time_elapsed         | 2057        |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.082441345 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.823       |
|    n_updates            | 9060        |
|    policy_gradient_loss | -0.0912     |
|    std                  | 0.254       |
|    value_loss           | 4.63        |
-----------------------------------------
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.199      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.05e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 455        |
|    time_elapsed         | 2061       |
|    total_timesteps      | 465920     |
| train/                  |            |
|    approx_kl            | 0.13231426 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.66       |
|    n_updates            | 9080       |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.254      |
|    value_loss           | 5.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.81       |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.198      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 456        |
|    time_elapsed         | 2065       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.08183888 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 9100       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.254      |
|    value_loss           | 7.1        |
----------------------------------------
-----------------------------------------
| reward                  | 0.809       |
| reward_contact          | 0.014       |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.198       |
| reward_orientation      | 0.0492      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.154       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 859         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 457         |
|    time_elapsed         | 2069        |
|    total_timesteps      | 467968      |
| train/                  |             |
|    approx_kl            | 0.085577965 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.75        |
|    n_updates            | 9120        |
|    policy_gradient_loss | -0.0661     |
|    std                  | 0.254       |
|    value_loss           | 29.6        |
-----------------------------------------
Num timesteps: 468000
Best mean reward: 875.26 - Last mean reward per episode: 858.56
-----------------------------------------
| reward                  | 0.8         |
| reward_contact          | 0.0146      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.193       |
| reward_orientation      | 0.0489      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.153       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 858         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 458         |
|    time_elapsed         | 2073        |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.047817096 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.5       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.08        |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.0509     |
|    std                  | 0.254       |
|    value_loss           | 12.9        |
-----------------------------------------
---------------------------------------
| reward                  | 0.794     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.188     |
| reward_orientation      | 0.0488    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0557    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 459       |
|    time_elapsed         | 2077      |
|    total_timesteps      | 470016    |
| train/                  |           |
|    approx_kl            | 0.1912297 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.457     |
|    n_updates            | 9160      |
|    policy_gradient_loss | -0.105    |
|    std                  | 0.254     |
|    value_loss           | 1.83      |
---------------------------------------
----------------------------------------
| reward                  | 0.795      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0487     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 460        |
|    time_elapsed         | 2081       |
|    total_timesteps      | 471040     |
| train/                  |            |
|    approx_kl            | 0.10118708 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.839      |
|    n_updates            | 9180       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.254      |
|    value_loss           | 2.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.192      |
| reward_orientation      | 0.0486     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 461        |
|    time_elapsed         | 2085       |
|    total_timesteps      | 472064     |
| train/                  |            |
|    approx_kl            | 0.25231016 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 9200       |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.254      |
|    value_loss           | 1.84       |
----------------------------------------
----------------------------------------
| reward                  | 0.788      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0486     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 462        |
|    time_elapsed         | 2089       |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.07194422 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.67       |
|    n_updates            | 9220       |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.254      |
|    value_loss           | 19.3       |
----------------------------------------
Num timesteps: 474000
Best mean reward: 875.26 - Last mean reward per episode: 856.37
-----------------------------------------
| reward                  | 0.785       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0486      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.149       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 463         |
|    time_elapsed         | 2093        |
|    total_timesteps      | 474112      |
| train/                  |             |
|    approx_kl            | 0.104774505 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 9240        |
|    policy_gradient_loss | -0.0747     |
|    std                  | 0.254       |
|    value_loss           | 3.43        |
-----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 464        |
|    time_elapsed         | 2097       |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.06723575 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.8       |
|    n_updates            | 9260       |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.254      |
|    value_loss           | 22.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.786       |
| reward_contact          | 0.0157      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.188       |
| reward_orientation      | 0.0489      |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0556      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 465         |
|    time_elapsed         | 2101        |
|    total_timesteps      | 476160      |
| train/                  |             |
|    approx_kl            | 0.077834636 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.627       |
|    n_updates            | 9280        |
|    policy_gradient_loss | -0.0671     |
|    std                  | 0.254       |
|    value_loss           | 3.78        |
-----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 466        |
|    time_elapsed         | 2105       |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.10053451 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.66       |
|    n_updates            | 9300       |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.254      |
|    value_loss           | 5.46       |
----------------------------------------
-----------------------------------------
| reward                  | 0.782       |
| reward_contact          | 0.0163      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.182       |
| reward_orientation      | 0.049       |
| reward_position         | 1.07e-05    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 857         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 467         |
|    time_elapsed         | 2109        |
|    total_timesteps      | 478208      |
| train/                  |             |
|    approx_kl            | 0.043632876 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.89        |
|    n_updates            | 9320        |
|    policy_gradient_loss | -0.0451     |
|    std                  | 0.253       |
|    value_loss           | 19.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.183      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 468        |
|    time_elapsed         | 2113       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.13748741 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.01       |
|    n_updates            | 9340       |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.253      |
|    value_loss           | 6.14       |
----------------------------------------
Num timesteps: 480000
Best mean reward: 875.26 - Last mean reward per episode: 857.12
---------------------------------------
| reward                  | 0.784     |
| reward_contact          | 0.0169    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0494    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.0557    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 469       |
|    time_elapsed         | 2117      |
|    total_timesteps      | 480256    |
| train/                  |           |
|    approx_kl            | 0.1623579 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.255     |
|    n_updates            | 9360      |
|    policy_gradient_loss | -0.0696   |
|    std                  | 0.253     |
|    value_loss           | 3.16      |
---------------------------------------
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.182      |
| reward_orientation      | 0.0495     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 470        |
|    time_elapsed         | 2121       |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.44838685 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 9380       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.253      |
|    value_loss           | 2.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.0169    |
| reward_ctrl             | 0.105     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0499    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0556    |
| reward_velocity         | 0.221     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 857       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 471       |
|    time_elapsed         | 2124      |
|    total_timesteps      | 482304    |
| train/                  |           |
|    approx_kl            | 0.0549325 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.509     |
|    learning_rate        | 0.0003    |
|    loss                 | 15.4      |
|    n_updates            | 9400      |
|    policy_gradient_loss | -0.0562   |
|    std                  | 0.253     |
|    value_loss           | 29        |
---------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0168     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 472        |
|    time_elapsed         | 2129       |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.10102114 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.63       |
|    n_updates            | 9420       |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.253      |
|    value_loss           | 3.35       |
----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0168     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0502     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 473        |
|    time_elapsed         | 2133       |
|    total_timesteps      | 484352     |
| train/                  |            |
|    approx_kl            | 0.40397963 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 9440       |
|    policy_gradient_loss | -0.0489    |
|    std                  | 0.253      |
|    value_loss           | 2.1        |
----------------------------------------
---------------------------------------
| reward                  | 0.786     |
| reward_contact          | 0.0162    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0505    |
| reward_position         | 1.07e-05  |
| reward_rotation         | 0.148     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 859       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 474       |
|    time_elapsed         | 2136      |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0686019 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.2      |
|    n_updates            | 9460      |
|    policy_gradient_loss | -0.0557   |
|    std                  | 0.253     |
|    value_loss           | 18        |
---------------------------------------
Num timesteps: 486000
Best mean reward: 875.26 - Last mean reward per episode: 859.86
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0508     |
| reward_position         | 1.07e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 475        |
|    time_elapsed         | 2140       |
|    total_timesteps      | 486400     |
| train/                  |            |
|    approx_kl            | 0.05930209 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 9480       |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.253      |
|    value_loss           | 19.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.788      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0508     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 476        |
|    time_elapsed         | 2144       |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.05456622 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.61       |
|    n_updates            | 9500       |
|    policy_gradient_loss | -0.0575    |
|    std                  | 0.253      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.185      |
| reward_orientation      | 0.051      |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 477        |
|    time_elapsed         | 2148       |
|    total_timesteps      | 488448     |
| train/                  |            |
|    approx_kl            | 0.10120793 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.42       |
|    n_updates            | 9520       |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.253      |
|    value_loss           | 39.9       |
----------------------------------------
---------------------------------------
| reward                  | 0.786     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.185     |
| reward_orientation      | 0.0511    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.146     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 478       |
|    time_elapsed         | 2152      |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 0.3738192 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.234     |
|    n_updates            | 9540      |
|    policy_gradient_loss | -0.11     |
|    std                  | 0.253     |
|    value_loss           | 2.28      |
---------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0508     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 479        |
|    time_elapsed         | 2156       |
|    total_timesteps      | 490496     |
| train/                  |            |
|    approx_kl            | 0.09465661 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 9560       |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.253      |
|    value_loss           | 7.13       |
----------------------------------------
-----------------------------------------
| reward                  | 0.788       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.188       |
| reward_orientation      | 0.0508      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 480         |
|    time_elapsed         | 2160        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.091873385 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.655       |
|    n_updates            | 9580        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 0.253       |
|    value_loss           | 5.85        |
-----------------------------------------
Num timesteps: 492000
Best mean reward: 875.26 - Last mean reward per episode: 861.00
-----------------------------------------
| reward                  | 0.787       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.186       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.223       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 481         |
|    time_elapsed         | 2164        |
|    total_timesteps      | 492544      |
| train/                  |             |
|    approx_kl            | 0.116025686 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 9600        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.253       |
|    value_loss           | 6.24        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.785       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.183       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 482         |
|    time_elapsed         | 2167        |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.122703984 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.5       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.725       |
|    n_updates            | 9620        |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.253       |
|    value_loss           | 2.25        |
-----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 483        |
|    time_elapsed         | 2171       |
|    total_timesteps      | 494592     |
| train/                  |            |
|    approx_kl            | 0.32508808 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.64       |
|    n_updates            | 9640       |
|    policy_gradient_loss | -0.0302    |
|    std                  | 0.253      |
|    value_loss           | 39.5       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.178     |
| reward_orientation      | 0.0506    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 484       |
|    time_elapsed         | 2175      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 2.1496882 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.339     |
|    n_updates            | 9660      |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.252     |
|    value_loss           | 1.97      |
---------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 485        |
|    time_elapsed         | 2179       |
|    total_timesteps      | 496640     |
| train/                  |            |
|    approx_kl            | 0.04604215 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.191      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.85       |
|    n_updates            | 9680       |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.252      |
|    value_loss           | 26.2       |
----------------------------------------
-----------------------------------------
| reward                  | 0.78        |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.176       |
| reward_orientation      | 0.0507      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 486         |
|    time_elapsed         | 2183        |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.046984963 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    n_updates            | 9700        |
|    policy_gradient_loss | -0.055      |
|    std                  | 0.252       |
|    value_loss           | 7.37        |
-----------------------------------------
Num timesteps: 498000
Best mean reward: 875.26 - Last mean reward per episode: 861.57
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 487        |
|    time_elapsed         | 2188       |
|    total_timesteps      | 498688     |
| train/                  |            |
|    approx_kl            | 0.21521056 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.435      |
|    n_updates            | 9720       |
|    policy_gradient_loss | -0.0627    |
|    std                  | 0.252      |
|    value_loss           | 1.36       |
----------------------------------------
-----------------------------------------
| reward                  | 0.783       |
| reward_contact          | 0.0179      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.179       |
| reward_orientation      | 0.0506      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 488         |
|    time_elapsed         | 2192        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.089148976 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 9740        |
|    policy_gradient_loss | -0.0686     |
|    std                  | 0.252       |
|    value_loss           | 2.29        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.781       |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.177       |
| reward_orientation      | 0.0506      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 489         |
|    time_elapsed         | 2196        |
|    total_timesteps      | 500736      |
| train/                  |             |
|    approx_kl            | 0.065172076 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.46        |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.0654     |
|    std                  | 0.252       |
|    value_loss           | 3.71        |
-----------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.177      |
| reward_orientation      | 0.0506     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 490        |
|    time_elapsed         | 2200       |
|    total_timesteps      | 501760     |
| train/                  |            |
|    approx_kl            | 0.15233779 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.31       |
|    n_updates            | 9780       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.252      |
|    value_loss           | 2.55       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0504     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 491        |
|    time_elapsed         | 2203       |
|    total_timesteps      | 502784     |
| train/                  |            |
|    approx_kl            | 0.07327686 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.252      |
|    value_loss           | 4.94       |
----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 492        |
|    time_elapsed         | 2207       |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.14566626 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.276      |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.252      |
|    value_loss           | 2.41       |
----------------------------------------
Num timesteps: 504000
Best mean reward: 875.26 - Last mean reward per episode: 863.38
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0504     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 493        |
|    time_elapsed         | 2211       |
|    total_timesteps      | 504832     |
| train/                  |            |
|    approx_kl            | 0.06796846 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.52       |
|    n_updates            | 9840       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 0.252      |
|    value_loss           | 22.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 494        |
|    time_elapsed         | 2215       |
|    total_timesteps      | 505856     |
| train/                  |            |
|    approx_kl            | 0.14200589 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.323      |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.252      |
|    value_loss           | 1.91       |
----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0505     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 495        |
|    time_elapsed         | 2219       |
|    total_timesteps      | 506880     |
| train/                  |            |
|    approx_kl            | 0.06337141 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.252      |
|    value_loss           | 6.03       |
----------------------------------------
-----------------------------------------
| reward                  | 0.787       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.184       |
| reward_orientation      | 0.0505      |
| reward_position         | 2.22e-05    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 496         |
|    time_elapsed         | 2223        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.088687226 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.6        |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.0504     |
|    std                  | 0.252       |
|    value_loss           | 31.6        |
-----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 497        |
|    time_elapsed         | 2227       |
|    total_timesteps      | 508928     |
| train/                  |            |
|    approx_kl            | 0.14227937 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.318      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.252      |
|    value_loss           | 2.72       |
----------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.19       |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 498        |
|    time_elapsed         | 2231       |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.15373136 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.414      |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.252      |
|    value_loss           | 2          |
----------------------------------------
Num timesteps: 510000
Best mean reward: 875.26 - Last mean reward per episode: 867.50
----------------------------------------
| reward                  | 0.792      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0498     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 499        |
|    time_elapsed         | 2234       |
|    total_timesteps      | 510976     |
| train/                  |            |
|    approx_kl            | 0.07176496 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.753      |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.0669    |
|    std                  | 0.252      |
|    value_loss           | 7.9        |
----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0498     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 500        |
|    time_elapsed         | 2238       |
|    total_timesteps      | 512000     |
| train/                  |            |
|    approx_kl            | 0.16078682 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 9980       |
|    policy_gradient_loss | -0.0904    |
|    std                  | 0.252      |
|    value_loss           | 2.32       |
----------------------------------------
---------------------------------------
| reward                  | 0.783     |
| reward_contact          | 0.0161    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.181     |
| reward_orientation      | 0.0498    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 501       |
|    time_elapsed         | 2242      |
|    total_timesteps      | 513024    |
| train/                  |           |
|    approx_kl            | 0.7237595 |
|    clip_fraction        | 0.304     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.332     |
|    n_updates            | 10000     |
|    policy_gradient_loss | -0.0576   |
|    std                  | 0.252     |
|    value_loss           | 2.29      |
---------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0161     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.181      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 502        |
|    time_elapsed         | 2246       |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.07646564 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.252      |
|    value_loss           | 4.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 503        |
|    time_elapsed         | 2250       |
|    total_timesteps      | 515072     |
| train/                  |            |
|    approx_kl            | 0.04668444 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.65       |
|    n_updates            | 10040      |
|    policy_gradient_loss | -0.052     |
|    std                  | 0.252      |
|    value_loss           | 20.6       |
----------------------------------------
Num timesteps: 516000
Best mean reward: 875.26 - Last mean reward per episode: 867.95
----------------------------------------
| reward                  | 0.777      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.174      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 504        |
|    time_elapsed         | 2254       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.14593178 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.543      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.252      |
|    value_loss           | 2.74       |
----------------------------------------
----------------------------------------
| reward                  | 0.778      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 505        |
|    time_elapsed         | 2258       |
|    total_timesteps      | 517120     |
| train/                  |            |
|    approx_kl            | 0.14287704 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.458      |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.252      |
|    value_loss           | 2.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 506        |
|    time_elapsed         | 2262       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.20603453 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.252      |
|    value_loss           | 1.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 507        |
|    time_elapsed         | 2265       |
|    total_timesteps      | 519168     |
| train/                  |            |
|    approx_kl            | 0.34805974 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.283      |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.0618    |
|    std                  | 0.251      |
|    value_loss           | 2.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0499     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 508        |
|    time_elapsed         | 2269       |
|    total_timesteps      | 520192     |
| train/                  |            |
|    approx_kl            | 0.17597699 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 10140      |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.251      |
|    value_loss           | 2.01       |
----------------------------------------
---------------------------------------
| reward                  | 0.77      |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.05      |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.229     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 509       |
|    time_elapsed         | 2273      |
|    total_timesteps      | 521216    |
| train/                  |           |
|    approx_kl            | 0.4200024 |
|    clip_fraction        | 0.456     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0943    |
|    n_updates            | 10160     |
|    policy_gradient_loss | -0.0888   |
|    std                  | 0.251     |
|    value_loss           | 1.27      |
---------------------------------------
Num timesteps: 522000
Best mean reward: 875.26 - Last mean reward per episode: 869.60
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 510        |
|    time_elapsed         | 2277       |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.16386592 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.285      |
|    n_updates            | 10180      |
|    policy_gradient_loss | -0.0355    |
|    std                  | 0.251      |
|    value_loss           | 1.16       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 511        |
|    time_elapsed         | 2281       |
|    total_timesteps      | 523264     |
| train/                  |            |
|    approx_kl            | 0.07005659 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.89       |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.251      |
|    value_loss           | 24         |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.0503     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 512        |
|    time_elapsed         | 2284       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.13509196 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.377      |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.251      |
|    value_loss           | 1.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.773      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0502     |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 513        |
|    time_elapsed         | 2288       |
|    total_timesteps      | 525312     |
| train/                  |            |
|    approx_kl            | 0.07728009 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.34       |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.0472    |
|    std                  | 0.251      |
|    value_loss           | 18.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.774     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.156     |
| reward_orientation      | 0.0501    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.153     |
| reward_torque           | 0.0562    |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 514       |
|    time_elapsed         | 2292      |
|    total_timesteps      | 526336    |
| train/                  |           |
|    approx_kl            | 0.0725341 |
|    clip_fraction        | 0.241     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.3     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.706     |
|    n_updates            | 10260     |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.251     |
|    value_loss           | 4.89      |
---------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 515        |
|    time_elapsed         | 2296       |
|    total_timesteps      | 527360     |
| train/                  |            |
|    approx_kl            | 0.12517308 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.779      |
|    n_updates            | 10280      |
|    policy_gradient_loss | -0.0839    |
|    std                  | 0.251      |
|    value_loss           | 3.99       |
----------------------------------------
Num timesteps: 528000
Best mean reward: 875.26 - Last mean reward per episode: 868.24
----------------------------------------
| reward                  | 0.768      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 516        |
|    time_elapsed         | 2300       |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.22875251 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 10300      |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.251      |
|    value_loss           | 2.1        |
----------------------------------------
---------------------------------------
| reward                  | 0.768     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.153     |
| reward_orientation      | 0.0499    |
| reward_position         | 2.22e-05  |
| reward_rotation         | 0.151     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 517       |
|    time_elapsed         | 2304      |
|    total_timesteps      | 529408    |
| train/                  |           |
|    approx_kl            | 1.1021508 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.254     |
|    n_updates            | 10320     |
|    policy_gradient_loss | -0.0772   |
|    std                  | 0.251     |
|    value_loss           | 2.35      |
---------------------------------------
----------------------------------------
| reward                  | 0.761      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.05       |
| reward_position         | 2.22e-05   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 518        |
|    time_elapsed         | 2307       |
|    total_timesteps      | 530432     |
| train/                  |            |
|    approx_kl            | 0.21016224 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0934     |
|    n_updates            | 10340      |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.251      |
|    value_loss           | 1.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.76       |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 519        |
|    time_elapsed         | 2311       |
|    total_timesteps      | 531456     |
| train/                  |            |
|    approx_kl            | 0.43316764 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.51       |
|    n_updates            | 10360      |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.251      |
|    value_loss           | 1.38       |
----------------------------------------
---------------------------------------
| reward                  | 0.755     |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.151     |
| reward_orientation      | 0.0504    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.145     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 520       |
|    time_elapsed         | 2315      |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 1.1664941 |
|    clip_fraction        | 0.438     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.224     |
|    n_updates            | 10380     |
|    policy_gradient_loss | -0.0867   |
|    std                  | 0.251     |
|    value_loss           | 1.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0184     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0503     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 521        |
|    time_elapsed         | 2319       |
|    total_timesteps      | 533504     |
| train/                  |            |
|    approx_kl            | 0.16124254 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.626      |
|    n_updates            | 10400      |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.251      |
|    value_loss           | 2.07       |
----------------------------------------
Num timesteps: 534000
Best mean reward: 875.26 - Last mean reward per episode: 868.07
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0506     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 522        |
|    time_elapsed         | 2323       |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.05537957 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.462      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 10420      |
|    policy_gradient_loss | -0.0549    |
|    std                  | 0.251      |
|    value_loss           | 45.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.152       |
| reward_orientation      | 0.0503      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.143       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 523         |
|    time_elapsed         | 2327        |
|    total_timesteps      | 535552      |
| train/                  |             |
|    approx_kl            | 0.048085444 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.0785      |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.0413     |
|    std                  | 0.251       |
|    value_loss           | 33.4        |
-----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0501     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 524        |
|    time_elapsed         | 2330       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.07042433 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.57       |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.0765    |
|    std                  | 0.251      |
|    value_loss           | 5.8        |
----------------------------------------
-----------------------------------------
| reward                  | 0.757       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.149       |
| reward_orientation      | 0.0499      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 525         |
|    time_elapsed         | 2334        |
|    total_timesteps      | 537600      |
| train/                  |             |
|    approx_kl            | 0.103684336 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.74        |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.0454     |
|    std                  | 0.251       |
|    value_loss           | 10.8        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.763       |
| reward_contact          | 0.0178      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.155       |
| reward_orientation      | 0.0499      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 526         |
|    time_elapsed         | 2338        |
|    total_timesteps      | 538624      |
| train/                  |             |
|    approx_kl            | 0.066971764 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.512       |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.251       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 527        |
|    time_elapsed         | 2342       |
|    total_timesteps      | 539648     |
| train/                  |            |
|    approx_kl            | 0.28087366 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.576      |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.251      |
|    value_loss           | 2.43       |
----------------------------------------
Num timesteps: 540000
Best mean reward: 875.26 - Last mean reward per episode: 867.22
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0497     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 528        |
|    time_elapsed         | 2346       |
|    total_timesteps      | 540672     |
| train/                  |            |
|    approx_kl            | 0.16075626 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.23       |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.0952    |
|    std                  | 0.251      |
|    value_loss           | 1.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 529        |
|    time_elapsed         | 2350       |
|    total_timesteps      | 541696     |
| train/                  |            |
|    approx_kl            | 0.08056688 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.719      |
|    n_updates            | 10560      |
|    policy_gradient_loss | -0.0685    |
|    std                  | 0.251      |
|    value_loss           | 3.19       |
----------------------------------------
----------------------------------------
| reward                  | 0.758      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0494     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 530        |
|    time_elapsed         | 2353       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.10804114 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.905      |
|    n_updates            | 10580      |
|    policy_gradient_loss | -0.0943    |
|    std                  | 0.25       |
|    value_loss           | 2.63       |
----------------------------------------
---------------------------------------
| reward                  | 0.755     |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.153     |
| reward_orientation      | 0.0495    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.143     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 531       |
|    time_elapsed         | 2357      |
|    total_timesteps      | 543744    |
| train/                  |           |
|    approx_kl            | 0.1454529 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.444     |
|    n_updates            | 10600     |
|    policy_gradient_loss | -0.0883   |
|    std                  | 0.25      |
|    value_loss           | 3.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.755      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 532        |
|    time_elapsed         | 2361       |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.20185016 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.213      |
|    n_updates            | 10620      |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.25       |
|    value_loss           | 2.07       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0172      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.148       |
| reward_orientation      | 0.049       |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 533         |
|    time_elapsed         | 2365        |
|    total_timesteps      | 545792      |
| train/                  |             |
|    approx_kl            | 0.087668404 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.613       |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.25        |
|    value_loss           | 2.68        |
-----------------------------------------
Num timesteps: 546000
Best mean reward: 875.26 - Last mean reward per episode: 864.40
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.0172      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.144       |
| reward_orientation      | 0.0493      |
| reward_position         | 1.17e-05    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 534         |
|    time_elapsed         | 2368        |
|    total_timesteps      | 546816      |
| train/                  |             |
|    approx_kl            | 0.069624566 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.72        |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.059      |
|    std                  | 0.25        |
|    value_loss           | 8.91        |
-----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 535        |
|    time_elapsed         | 2372       |
|    total_timesteps      | 547840     |
| train/                  |            |
|    approx_kl            | 0.48225394 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.053      |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.25       |
|    value_loss           | 1.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.144      |
| reward_orientation      | 0.0494     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 536        |
|    time_elapsed         | 2375       |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.06866059 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.11       |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.0523    |
|    std                  | 0.25       |
|    value_loss           | 27.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 537        |
|    time_elapsed         | 2379       |
|    total_timesteps      | 549888     |
| train/                  |            |
|    approx_kl            | 0.42434525 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.21       |
|    n_updates            | 10720      |
|    policy_gradient_loss | -0.094     |
|    std                  | 0.25       |
|    value_loss           | 1.83       |
----------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | 0.0166    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.143     |
| reward_orientation      | 0.0489    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.226     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 538       |
|    time_elapsed         | 2382      |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 0.3604675 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.452     |
|    n_updates            | 10740     |
|    policy_gradient_loss | -0.0724   |
|    std                  | 0.25      |
|    value_loss           | 2.19      |
---------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 539        |
|    time_elapsed         | 2385       |
|    total_timesteps      | 551936     |
| train/                  |            |
|    approx_kl            | 0.24526112 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.574      |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.25       |
|    value_loss           | 2.11       |
----------------------------------------
Num timesteps: 552000
Best mean reward: 875.26 - Last mean reward per episode: 864.80
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 540        |
|    time_elapsed         | 2389       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.43754292 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 10780      |
|    policy_gradient_loss | -0.0331    |
|    std                  | 0.25       |
|    value_loss           | 1.53       |
----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.016      |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0491     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 541        |
|    time_elapsed         | 2392       |
|    total_timesteps      | 553984     |
| train/                  |            |
|    approx_kl            | 0.13341512 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.602      |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.25       |
|    value_loss           | 3.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0163     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0492     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 542        |
|    time_elapsed         | 2395       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.11006014 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.03       |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.25       |
|    value_loss           | 8.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0493     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 543        |
|    time_elapsed         | 2399       |
|    total_timesteps      | 556032     |
| train/                  |            |
|    approx_kl            | 0.07404641 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.57       |
|    n_updates            | 10840      |
|    policy_gradient_loss | -0.0523    |
|    std                  | 0.25       |
|    value_loss           | 30.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.744     |
| reward_contact          | 0.0157    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.136     |
| reward_orientation      | 0.0496    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.148     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 865       |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 544       |
|    time_elapsed         | 2402      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.2541511 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.405     |
|    n_updates            | 10860     |
|    policy_gradient_loss | -0.0827   |
|    std                  | 0.249     |
|    value_loss           | 3.12      |
---------------------------------------
Num timesteps: 558000
Best mean reward: 875.26 - Last mean reward per episode: 864.70
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0157     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 545        |
|    time_elapsed         | 2405       |
|    total_timesteps      | 558080     |
| train/                  |            |
|    approx_kl            | 0.21288641 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.293      |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.0597    |
|    std                  | 0.249      |
|    value_loss           | 1.65       |
----------------------------------------
---------------------------------------
| reward                  | 0.745     |
| reward_contact          | 0.0151    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.134     |
| reward_orientation      | 0.0499    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.15      |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 864       |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 546       |
|    time_elapsed         | 2409      |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 0.3378772 |
|    clip_fraction        | 0.389     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.171     |
|    n_updates            | 10900     |
|    policy_gradient_loss | -0.0705   |
|    std                  | 0.249     |
|    value_loss           | 1.25      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 547        |
|    time_elapsed         | 2412       |
|    total_timesteps      | 560128     |
| train/                  |            |
|    approx_kl            | 0.11263375 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.492      |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.249      |
|    value_loss           | 2.75       |
----------------------------------------
---------------------------------------
| reward                  | 0.741     |
| reward_contact          | 0.0152    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.131     |
| reward_orientation      | 0.0496    |
| reward_position         | 1.17e-05  |
| reward_rotation         | 0.151     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 864       |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 548       |
|    time_elapsed         | 2416      |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.0918726 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43       |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 10940     |
|    policy_gradient_loss | -0.0801   |
|    std                  | 0.249     |
|    value_loss           | 3.41      |
---------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0499     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 549        |
|    time_elapsed         | 2419       |
|    total_timesteps      | 562176     |
| train/                  |            |
|    approx_kl            | 0.28983003 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0315     |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.0781    |
|    std                  | 0.249      |
|    value_loss           | 1.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.741      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 550        |
|    time_elapsed         | 2422       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.06629478 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 10980      |
|    policy_gradient_loss | -0.0328    |
|    std                  | 0.249      |
|    value_loss           | 10.8       |
----------------------------------------
Num timesteps: 564000
Best mean reward: 875.26 - Last mean reward per episode: 864.73
----------------------------------------
| reward                  | 0.741      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0496     |
| reward_position         | 1.17e-05   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 551        |
|    time_elapsed         | 2426       |
|    total_timesteps      | 564224     |
| train/                  |            |
|    approx_kl            | 0.22828011 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.548      |
|    n_updates            | 11000      |
|    policy_gradient_loss | -0.0967    |
|    std                  | 0.249      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0495     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 552        |
|    time_elapsed         | 2429       |
|    total_timesteps      | 565248     |
| train/                  |            |
|    approx_kl            | 0.08491241 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.96       |
|    n_updates            | 11020      |
|    policy_gradient_loss | -0.0507    |
|    std                  | 0.249      |
|    value_loss           | 5.06       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0495     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 553        |
|    time_elapsed         | 2432       |
|    total_timesteps      | 566272     |
| train/                  |            |
|    approx_kl            | 0.08199858 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 11040      |
|    policy_gradient_loss | -0.0362    |
|    std                  | 0.249      |
|    value_loss           | 29         |
----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0492     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 554        |
|    time_elapsed         | 2436       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.06276042 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.0635    |
|    std                  | 0.249      |
|    value_loss           | 6.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0491     |
| reward_position         | 0.000834   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 555        |
|    time_elapsed         | 2439       |
|    total_timesteps      | 568320     |
| train/                  |            |
|    approx_kl            | 0.13898602 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.34       |
|    n_updates            | 11080      |
|    policy_gradient_loss | -0.0489    |
|    std                  | 0.249      |
|    value_loss           | 22.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 556        |
|    time_elapsed         | 2442       |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.11958004 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.385      |
|    n_updates            | 11100      |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.249      |
|    value_loss           | 2.16       |
----------------------------------------
Num timesteps: 570000
Best mean reward: 875.26 - Last mean reward per episode: 865.23
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0493     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 557        |
|    time_elapsed         | 2446       |
|    total_timesteps      | 570368     |
| train/                  |            |
|    approx_kl            | 0.11998257 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.763      |
|    n_updates            | 11120      |
|    policy_gradient_loss | -0.0933    |
|    std                  | 0.249      |
|    value_loss           | 2.59       |
----------------------------------------
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0129     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0496     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 558        |
|    time_elapsed         | 2449       |
|    total_timesteps      | 571392     |
| train/                  |            |
|    approx_kl            | 0.21293719 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 11140      |
|    policy_gradient_loss | -0.0946    |
|    std                  | 0.249      |
|    value_loss           | 1.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.767       |
| reward_contact          | 0.0123      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.146       |
| reward_orientation      | 0.0494      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.159       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 866         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 559         |
|    time_elapsed         | 2453        |
|    total_timesteps      | 572416      |
| train/                  |             |
|    approx_kl            | 0.097502016 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.387       |
|    n_updates            | 11160       |
|    policy_gradient_loss | -0.0399     |
|    std                  | 0.249       |
|    value_loss           | 2.44        |
-----------------------------------------
----------------------------------------
| reward                  | 0.765      |
| reward_contact          | 0.0123     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 560        |
|    time_elapsed         | 2456       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.14848784 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 11180      |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.249      |
|    value_loss           | 14.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0123     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0494     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 561        |
|    time_elapsed         | 2459       |
|    total_timesteps      | 574464     |
| train/                  |            |
|    approx_kl            | 0.27951145 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.695      |
|    n_updates            | 11200      |
|    policy_gradient_loss | -0.0496    |
|    std                  | 0.249      |
|    value_loss           | 1.3        |
----------------------------------------
-----------------------------------------
| reward                  | 0.77        |
| reward_contact          | 0.0117      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.141       |
| reward_orientation      | 0.0491      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.239       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 562         |
|    time_elapsed         | 2463        |
|    total_timesteps      | 575488      |
| train/                  |             |
|    approx_kl            | 0.066412866 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.76        |
|    n_updates            | 11220       |
|    policy_gradient_loss | -0.0414     |
|    std                  | 0.249       |
|    value_loss           | 7.9         |
-----------------------------------------
Num timesteps: 576000
Best mean reward: 875.26 - Last mean reward per episode: 864.15
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0117     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0489     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 563        |
|    time_elapsed         | 2466       |
|    total_timesteps      | 576512     |
| train/                  |            |
|    approx_kl            | 0.18022248 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.477      |
|    n_updates            | 11240      |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.249      |
|    value_loss           | 2.09       |
----------------------------------------
-----------------------------------------
| reward                  | 0.762       |
| reward_contact          | 0.0122      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.136       |
| reward_orientation      | 0.0485      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 564         |
|    time_elapsed         | 2469        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.098722026 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 11260       |
|    policy_gradient_loss | -0.0584     |
|    std                  | 0.249       |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | 0.0122    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.132     |
| reward_orientation      | 0.0485    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.161     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 862       |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 565       |
|    time_elapsed         | 2473      |
|    total_timesteps      | 578560    |
| train/                  |           |
|    approx_kl            | 1.0474597 |
|    clip_fraction        | 0.539     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.222     |
|    n_updates            | 11280     |
|    policy_gradient_loss | -0.117    |
|    std                  | 0.249     |
|    value_loss           | 1.21      |
---------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0484     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 566        |
|    time_elapsed         | 2476       |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.07186464 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 11300      |
|    policy_gradient_loss | -0.0567    |
|    std                  | 0.248      |
|    value_loss           | 12.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 567        |
|    time_elapsed         | 2479       |
|    total_timesteps      | 580608     |
| train/                  |            |
|    approx_kl            | 0.12718612 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.879      |
|    n_updates            | 11320      |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.248      |
|    value_loss           | 16.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0482      |
| reward_position         | 0.000833    |
| reward_rotation         | 0.159       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 568         |
|    time_elapsed         | 2483        |
|    total_timesteps      | 581632      |
| train/                  |             |
|    approx_kl            | 0.088829204 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.0514     |
|    std                  | 0.248       |
|    value_loss           | 20.5        |
-----------------------------------------
Num timesteps: 582000
Best mean reward: 875.26 - Last mean reward per episode: 861.70
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0481     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 569        |
|    time_elapsed         | 2486       |
|    total_timesteps      | 582656     |
| train/                  |            |
|    approx_kl            | 0.07441252 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.943      |
|    n_updates            | 11360      |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.248      |
|    value_loss           | 7.95       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0481     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 570        |
|    time_elapsed         | 2489       |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.15689853 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    n_updates            | 11380      |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.248      |
|    value_loss           | 19.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0478     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 571        |
|    time_elapsed         | 2493       |
|    total_timesteps      | 584704     |
| train/                  |            |
|    approx_kl            | 0.17300808 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.386      |
|    n_updates            | 11400      |
|    policy_gradient_loss | -0.0914    |
|    std                  | 0.248      |
|    value_loss           | 1.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.749     |
| reward_contact          | 0.0122    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.126     |
| reward_orientation      | 0.0477    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.161     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 234       |
|    iterations           | 572       |
|    time_elapsed         | 2496      |
|    total_timesteps      | 585728    |
| train/                  |           |
|    approx_kl            | 0.0744725 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | 19.3      |
|    n_updates            | 11420     |
|    policy_gradient_loss | -0.0658   |
|    std                  | 0.248     |
|    value_loss           | 17.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0122     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0477     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 860        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 573        |
|    time_elapsed         | 2500       |
|    total_timesteps      | 586752     |
| train/                  |            |
|    approx_kl            | 0.09417897 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.643      |
|    n_updates            | 11440      |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.248      |
|    value_loss           | 6.33       |
----------------------------------------
---------------------------------------
| reward                  | 0.75      |
| reward_contact          | 0.0128    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.126     |
| reward_orientation      | 0.0473    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 234       |
|    iterations           | 574       |
|    time_elapsed         | 2503      |
|    total_timesteps      | 587776    |
| train/                  |           |
|    approx_kl            | 16.094242 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.374     |
|    n_updates            | 11460     |
|    policy_gradient_loss | 0.0396    |
|    std                  | 0.248     |
|    value_loss           | 0.793     |
---------------------------------------
Num timesteps: 588000
Best mean reward: 875.26 - Last mean reward per episode: 858.70
---------------------------------------
| reward                  | 0.746     |
| reward_contact          | 0.0128    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.121     |
| reward_orientation      | 0.0472    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.163     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 859       |
| time/                   |           |
|    fps                  | 234       |
|    iterations           | 575       |
|    time_elapsed         | 2506      |
|    total_timesteps      | 588800    |
| train/                  |           |
|    approx_kl            | 0.0682375 |
|    clip_fraction        | 0.276     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.378     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 11480     |
|    policy_gradient_loss | -0.0515   |
|    std                  | 0.248     |
|    value_loss           | 23.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0473     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 576        |
|    time_elapsed         | 2510       |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.07990752 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.5        |
|    n_updates            | 11500      |
|    policy_gradient_loss | -0.0918    |
|    std                  | 0.248      |
|    value_loss           | 8.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.124      |
| reward_orientation      | 0.0469     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 577        |
|    time_elapsed         | 2513       |
|    total_timesteps      | 590848     |
| train/                  |            |
|    approx_kl            | 0.29955417 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 11520      |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.248      |
|    value_loss           | 1.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0469     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 578        |
|    time_elapsed         | 2516       |
|    total_timesteps      | 591872     |
| train/                  |            |
|    approx_kl            | 0.16365805 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.359      |
|    n_updates            | 11540      |
|    policy_gradient_loss | -0.0871    |
|    std                  | 0.248      |
|    value_loss           | 2          |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0472     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 579        |
|    time_elapsed         | 2520       |
|    total_timesteps      | 592896     |
| train/                  |            |
|    approx_kl            | 0.11165956 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.908      |
|    n_updates            | 11560      |
|    policy_gradient_loss | -0.0768    |
|    std                  | 0.248      |
|    value_loss           | 2.25       |
----------------------------------------
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.014       |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.124       |
| reward_orientation      | 0.047       |
| reward_position         | 0.000822    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 855         |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 580         |
|    time_elapsed         | 2523        |
|    total_timesteps      | 593920      |
| train/                  |             |
|    approx_kl            | 0.057115894 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64        |
|    n_updates            | 11580       |
|    policy_gradient_loss | -0.0644     |
|    std                  | 0.248       |
|    value_loss           | 15.4        |
-----------------------------------------
Num timesteps: 594000
Best mean reward: 875.26 - Last mean reward per episode: 854.87
---------------------------------------
| reward                  | 0.745     |
| reward_contact          | 0.0139    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.121     |
| reward_orientation      | 0.0468    |
| reward_position         | 0.000822  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 854       |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 581       |
|    time_elapsed         | 2526      |
|    total_timesteps      | 594944    |
| train/                  |           |
|    approx_kl            | 0.1644088 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.65      |
|    n_updates            | 11600     |
|    policy_gradient_loss | -0.109    |
|    std                  | 0.248     |
|    value_loss           | 2.55      |
---------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0465     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 582        |
|    time_elapsed         | 2530       |
|    total_timesteps      | 595968     |
| train/                  |            |
|    approx_kl            | 0.10625112 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.75       |
|    n_updates            | 11620      |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.247      |
|    value_loss           | 2.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.121      |
| reward_orientation      | 0.0467     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 583        |
|    time_elapsed         | 2533       |
|    total_timesteps      | 596992     |
| train/                  |            |
|    approx_kl            | 0.14409879 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.368      |
|    n_updates            | 11640      |
|    policy_gradient_loss | -0.084     |
|    std                  | 0.247      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0465     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 584        |
|    time_elapsed         | 2537       |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.07835465 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 11660      |
|    policy_gradient_loss | -0.0816    |
|    std                  | 0.247      |
|    value_loss           | 5.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0461     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 585        |
|    time_elapsed         | 2540       |
|    total_timesteps      | 599040     |
| train/                  |            |
|    approx_kl            | 0.19738376 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 11680      |
|    policy_gradient_loss | -0.0918    |
|    std                  | 0.247      |
|    value_loss           | 1.28       |
----------------------------------------
Num timesteps: 600000
Best mean reward: 875.26 - Last mean reward per episode: 853.66
-----------------------------------------
| reward                  | 0.74        |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.12        |
| reward_orientation      | 0.0463      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 854         |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 586         |
|    time_elapsed         | 2543        |
|    total_timesteps      | 600064      |
| train/                  |             |
|    approx_kl            | 0.083070576 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.583       |
|    n_updates            | 11700       |
|    policy_gradient_loss | -0.0745     |
|    std                  | 0.247       |
|    value_loss           | 4.66        |
-----------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0464     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 587        |
|    time_elapsed         | 2547       |
|    total_timesteps      | 601088     |
| train/                  |            |
|    approx_kl            | 0.08453375 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 11720      |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.247      |
|    value_loss           | 7.86       |
----------------------------------------
-----------------------------------------
| reward                  | 0.741       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.123       |
| reward_orientation      | 0.0464      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 852         |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 588         |
|    time_elapsed         | 2550        |
|    total_timesteps      | 602112      |
| train/                  |             |
|    approx_kl            | 0.076597184 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.79        |
|    n_updates            | 11740       |
|    policy_gradient_loss | -0.074      |
|    std                  | 0.247       |
|    value_loss           | 12.5        |
-----------------------------------------
---------------------------------------
| reward                  | 0.738     |
| reward_contact          | 0.0141    |
| reward_ctrl             | 0.105     |
| reward_motion           | 0.122     |
| reward_orientation      | 0.046     |
| reward_position         | 0.000822  |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 852       |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 589       |
|    time_elapsed         | 2553      |
|    total_timesteps      | 603136    |
| train/                  |           |
|    approx_kl            | 0.5358111 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43       |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.21      |
|    n_updates            | 11760     |
|    policy_gradient_loss | -0.0848   |
|    std                  | 0.247     |
|    value_loss           | 1.05      |
---------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 590        |
|    time_elapsed         | 2557       |
|    total_timesteps      | 604160     |
| train/                  |            |
|    approx_kl            | 0.12181044 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.69       |
|    n_updates            | 11780      |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.247      |
|    value_loss           | 7.13       |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 591        |
|    time_elapsed         | 2560       |
|    total_timesteps      | 605184     |
| train/                  |            |
|    approx_kl            | 0.04930418 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.06       |
|    n_updates            | 11800      |
|    policy_gradient_loss | -0.0572    |
|    std                  | 0.247      |
|    value_loss           | 28.7       |
----------------------------------------
Num timesteps: 606000
Best mean reward: 875.26 - Last mean reward per episode: 851.20
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.117      |
| reward_orientation      | 0.0462     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 592        |
|    time_elapsed         | 2563       |
|    total_timesteps      | 606208     |
| train/                  |            |
|    approx_kl            | 0.22637478 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.323      |
|    n_updates            | 11820      |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.247      |
|    value_loss           | 1.66       |
----------------------------------------
-----------------------------------------
| reward                  | 0.726       |
| reward_contact          | 0.0153      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.116       |
| reward_orientation      | 0.0464      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.161       |
| reward_torque           | 0.0557      |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 852         |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 593         |
|    time_elapsed         | 2567        |
|    total_timesteps      | 607232      |
| train/                  |             |
|    approx_kl            | 0.099774584 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.659       |
|    n_updates            | 11840       |
|    policy_gradient_loss | -0.069      |
|    std                  | 0.247       |
|    value_loss           | 3.84        |
-----------------------------------------
----------------------------------------
| reward                  | 0.729      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0465     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 594        |
|    time_elapsed         | 2570       |
|    total_timesteps      | 608256     |
| train/                  |            |
|    approx_kl            | 0.07392986 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.28       |
|    n_updates            | 11860      |
|    policy_gradient_loss | -0.0284    |
|    std                  | 0.247      |
|    value_loss           | 28.6       |
----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.117       |
| reward_orientation      | 0.0461      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 851         |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 595         |
|    time_elapsed         | 2574        |
|    total_timesteps      | 609280      |
| train/                  |             |
|    approx_kl            | 0.108970195 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.3       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 11880       |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.247       |
|    value_loss           | 27          |
-----------------------------------------
----------------------------------------
| reward                  | 0.729      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 596        |
|    time_elapsed         | 2577       |
|    total_timesteps      | 610304     |
| train/                  |            |
|    approx_kl            | 0.08206263 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.2        |
|    n_updates            | 11900      |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.247      |
|    value_loss           | 9.57       |
----------------------------------------
-----------------------------------------
| reward                  | 0.73        |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.116       |
| reward_orientation      | 0.0461      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 851         |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 597         |
|    time_elapsed         | 2580        |
|    total_timesteps      | 611328      |
| train/                  |             |
|    approx_kl            | 0.064104475 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.14        |
|    n_updates            | 11920       |
|    policy_gradient_loss | -0.0559     |
|    std                  | 0.247       |
|    value_loss           | 22.1        |
-----------------------------------------
Num timesteps: 612000
Best mean reward: 875.26 - Last mean reward per episode: 850.69
----------------------------------------
| reward                  | 0.729      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.113      |
| reward_orientation      | 0.0462     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 598        |
|    time_elapsed         | 2584       |
|    total_timesteps      | 612352     |
| train/                  |            |
|    approx_kl            | 0.31175673 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.333      |
|    n_updates            | 11940      |
|    policy_gradient_loss | -0.0535    |
|    std                  | 0.247      |
|    value_loss           | 1.26       |
----------------------------------------
-----------------------------------------
| reward                  | 0.733       |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.118       |
| reward_orientation      | 0.0463      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.161       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.228       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 851         |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 599         |
|    time_elapsed         | 2587        |
|    total_timesteps      | 613376      |
| train/                  |             |
|    approx_kl            | 0.102702275 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.5         |
|    n_updates            | 11960       |
|    policy_gradient_loss | -0.0619     |
|    std                  | 0.247       |
|    value_loss           | 18.7        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.739       |
| reward_contact          | 0.0159      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.122       |
| reward_orientation      | 0.046       |
| reward_position         | 0.000822    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.23        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 851         |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 600         |
|    time_elapsed         | 2590        |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.080119416 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 11980       |
|    policy_gradient_loss | -0.0788     |
|    std                  | 0.247       |
|    value_loss           | 8.31        |
-----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0456     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 601        |
|    time_elapsed         | 2594       |
|    total_timesteps      | 615424     |
| train/                  |            |
|    approx_kl            | 0.28563464 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.169      |
|    n_updates            | 12000      |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.247      |
|    value_loss           | 1.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0159     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0454     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 850        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 602        |
|    time_elapsed         | 2597       |
|    total_timesteps      | 616448     |
| train/                  |            |
|    approx_kl            | 0.15325323 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 12020      |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.247      |
|    value_loss           | 23.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0452     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 850        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 603        |
|    time_elapsed         | 2600       |
|    total_timesteps      | 617472     |
| train/                  |            |
|    approx_kl            | 0.16424628 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.341      |
|    n_updates            | 12040      |
|    policy_gradient_loss | -0.0695    |
|    std                  | 0.247      |
|    value_loss           | 1.1        |
----------------------------------------
Num timesteps: 618000
Best mean reward: 875.26 - Last mean reward per episode: 850.15
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0452     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 850        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 604        |
|    time_elapsed         | 2604       |
|    total_timesteps      | 618496     |
| train/                  |            |
|    approx_kl            | 0.10108711 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 12060      |
|    policy_gradient_loss | -0.0533    |
|    std                  | 0.247      |
|    value_loss           | 15.6       |
----------------------------------------
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.126       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 605         |
|    time_elapsed         | 2607        |
|    total_timesteps      | 619520      |
| train/                  |             |
|    approx_kl            | 0.065307856 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.74        |
|    n_updates            | 12080       |
|    policy_gradient_loss | -0.0727     |
|    std                  | 0.246       |
|    value_loss           | 15.4        |
-----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.126      |
| reward_orientation      | 0.0448     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 849        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 606        |
|    time_elapsed         | 2610       |
|    total_timesteps      | 620544     |
| train/                  |            |
|    approx_kl            | 0.36003375 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.156      |
|    n_updates            | 12100      |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.246      |
|    value_loss           | 1.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0448     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 849        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 607        |
|    time_elapsed         | 2614       |
|    total_timesteps      | 621568     |
| train/                  |            |
|    approx_kl            | 0.16395114 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.247      |
|    n_updates            | 12120      |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.246      |
|    value_loss           | 1.61       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0448     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 848        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 608        |
|    time_elapsed         | 2617       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.22083484 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.7      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0722     |
|    n_updates            | 12140      |
|    policy_gradient_loss | -0.0965    |
|    std                  | 0.246      |
|    value_loss           | 1.35       |
----------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | 0.0153    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.137     |
| reward_orientation      | 0.0445    |
| reward_position         | 0.000822  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 848       |
| time/                   |           |
|    fps                  | 237       |
|    iterations           | 609       |
|    time_elapsed         | 2621      |
|    total_timesteps      | 623616    |
| train/                  |           |
|    approx_kl            | 0.3059358 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.367     |
|    n_updates            | 12160     |
|    policy_gradient_loss | -0.0714   |
|    std                  | 0.246     |
|    value_loss           | 1.52      |
---------------------------------------
Num timesteps: 624000
Best mean reward: 875.26 - Last mean reward per episode: 847.30
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0153     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0442     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 610        |
|    time_elapsed         | 2624       |
|    total_timesteps      | 624640     |
| train/                  |            |
|    approx_kl            | 0.09146992 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 12180      |
|    policy_gradient_loss | -0.0624    |
|    std                  | 0.246      |
|    value_loss           | 5.57       |
----------------------------------------
--------------------------------------
| reward                  | 0.747    |
| reward_contact          | 0.0153   |
| reward_ctrl             | 0.105    |
| reward_motion           | 0.136    |
| reward_orientation      | 0.0439   |
| reward_position         | 0.000822 |
| reward_rotation         | 0.16     |
| reward_torque           | 0.0558   |
| reward_velocity         | 0.23     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 848      |
| time/                   |          |
|    fps                  | 238      |
|    iterations           | 611      |
|    time_elapsed         | 2627     |
|    total_timesteps      | 625664   |
| train/                  |          |
|    approx_kl            | 0.60271  |
|    clip_fraction        | 0.337    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.9    |
|    explained_variance   | 0.996    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.161    |
|    n_updates            | 12200    |
|    policy_gradient_loss | -0.0464  |
|    std                  | 0.246    |
|    value_loss           | 1.19     |
--------------------------------------
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.0153      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0439      |
| reward_position         | 0.000822    |
| reward_rotation         | 0.16        |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.23        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 847         |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 612         |
|    time_elapsed         | 2631        |
|    total_timesteps      | 626688      |
| train/                  |             |
|    approx_kl            | 0.074621946 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 12220       |
|    policy_gradient_loss | -0.0514     |
|    std                  | 0.246       |
|    value_loss           | 9.51        |
-----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0437     |
| reward_position         | 0.000822   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 613        |
|    time_elapsed         | 2634       |
|    total_timesteps      | 627712     |
| train/                  |            |
|    approx_kl            | 0.10530628 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.97       |
|    n_updates            | 12240      |
|    policy_gradient_loss | -0.0704    |
|    std                  | 0.246      |
|    value_loss           | 13.7       |
----------------------------------------
--------------------------------------
| reward                  | 0.743    |
| reward_contact          | 0.0147   |
| reward_ctrl             | 0.105    |
| reward_motion           | 0.134    |
| reward_orientation      | 0.0437   |
| reward_position         | 0.000822 |
| reward_rotation         | 0.16     |
| reward_torque           | 0.0558   |
| reward_velocity         | 0.229    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 847      |
| time/                   |          |
|    fps                  | 238      |
|    iterations           | 614      |
|    time_elapsed         | 2637     |
|    total_timesteps      | 628736   |
| train/                  |          |
|    approx_kl            | 0.840495 |
|    clip_fraction        | 0.515    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.7    |
|    explained_variance   | 0.991    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.292    |
|    n_updates            | 12260    |
|    policy_gradient_loss | -0.0815  |
|    std                  | 0.246    |
|    value_loss           | 1.6      |
--------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 849        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 615        |
|    time_elapsed         | 2641       |
|    total_timesteps      | 629760     |
| train/                  |            |
|    approx_kl            | 0.24517855 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.41       |
|    n_updates            | 12280      |
|    policy_gradient_loss | -0.0169    |
|    std                  | 0.246      |
|    value_loss           | 4.32       |
----------------------------------------
Num timesteps: 630000
Best mean reward: 875.26 - Last mean reward per episode: 848.14
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0437     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 848        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 616        |
|    time_elapsed         | 2644       |
|    total_timesteps      | 630784     |
| train/                  |            |
|    approx_kl            | 0.16563612 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.4       |
|    n_updates            | 12300      |
|    policy_gradient_loss | -0.0412    |
|    std                  | 0.246      |
|    value_loss           | 31.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 848        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 617        |
|    time_elapsed         | 2647       |
|    total_timesteps      | 631808     |
| train/                  |            |
|    approx_kl            | 0.13977668 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.225      |
|    n_updates            | 12320      |
|    policy_gradient_loss | -0.0972    |
|    std                  | 0.246      |
|    value_loss           | 1.47       |
----------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | 0.0141    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.137     |
| reward_orientation      | 0.0434    |
| reward_position         | 0.00117   |
| reward_rotation         | 0.163     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 849       |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 618       |
|    time_elapsed         | 2651      |
|    total_timesteps      | 632832    |
| train/                  |           |
|    approx_kl            | 0.4399044 |
|    clip_fraction        | 0.351     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.15      |
|    n_updates            | 12340     |
|    policy_gradient_loss | -0.1      |
|    std                  | 0.246     |
|    value_loss           | 1.82      |
---------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 849        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 619        |
|    time_elapsed         | 2654       |
|    total_timesteps      | 633856     |
| train/                  |            |
|    approx_kl            | 0.13874966 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.4        |
|    n_updates            | 12360      |
|    policy_gradient_loss | -0.0507    |
|    std                  | 0.246      |
|    value_loss           | 4.33       |
----------------------------------------
-----------------------------------------
| reward                  | 0.762       |
| reward_contact          | 0.0141      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.14        |
| reward_orientation      | 0.0437      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 850         |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 620         |
|    time_elapsed         | 2657        |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.062068015 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.921       |
|    n_updates            | 12380       |
|    policy_gradient_loss | -0.0638     |
|    std                  | 0.246       |
|    value_loss           | 11.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.76       |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 850        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 621        |
|    time_elapsed         | 2661       |
|    total_timesteps      | 635904     |
| train/                  |            |
|    approx_kl            | 0.04365447 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.35       |
|    n_updates            | 12400      |
|    policy_gradient_loss | -0.0285    |
|    std                  | 0.246      |
|    value_loss           | 23.8       |
----------------------------------------
Num timesteps: 636000
Best mean reward: 875.26 - Last mean reward per episode: 850.25
-----------------------------------------
| reward                  | 0.758       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.136       |
| reward_orientation      | 0.0434      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 850         |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 622         |
|    time_elapsed         | 2664        |
|    total_timesteps      | 636928      |
| train/                  |             |
|    approx_kl            | 0.089331195 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.22        |
|    n_updates            | 12420       |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.246       |
|    value_loss           | 5.74        |
-----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0435     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 623        |
|    time_elapsed         | 2668       |
|    total_timesteps      | 637952     |
| train/                  |            |
|    approx_kl            | 0.28640887 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.284      |
|    n_updates            | 12440      |
|    policy_gradient_loss | -0.0266    |
|    std                  | 0.246      |
|    value_loss           | 1.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0437     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 624        |
|    time_elapsed         | 2671       |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.06533817 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.62       |
|    n_updates            | 12460      |
|    policy_gradient_loss | -0.042     |
|    std                  | 0.246      |
|    value_loss           | 20.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 625        |
|    time_elapsed         | 2674       |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.12670514 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 12480      |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.246      |
|    value_loss           | 3.57       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 626        |
|    time_elapsed         | 2678       |
|    total_timesteps      | 641024     |
| train/                  |            |
|    approx_kl            | 0.25480023 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.498      |
|    n_updates            | 12500      |
|    policy_gradient_loss | -0.0436    |
|    std                  | 0.246      |
|    value_loss           | 1.39       |
----------------------------------------
Num timesteps: 642000
Best mean reward: 875.26 - Last mean reward per episode: 851.57
----------------------------------------
| reward                  | 0.768      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 627        |
|    time_elapsed         | 2681       |
|    total_timesteps      | 642048     |
| train/                  |            |
|    approx_kl            | 0.11513498 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.42       |
|    n_updates            | 12520      |
|    policy_gradient_loss | -0.0448    |
|    std                  | 0.246      |
|    value_loss           | 28.4       |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.145      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 628        |
|    time_elapsed         | 2684       |
|    total_timesteps      | 643072     |
| train/                  |            |
|    approx_kl            | 0.22009042 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.327      |
|    n_updates            | 12540      |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.245      |
|    value_loss           | 1.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.774      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 629        |
|    time_elapsed         | 2688       |
|    total_timesteps      | 644096     |
| train/                  |            |
|    approx_kl            | 0.08919787 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.07       |
|    n_updates            | 12560      |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.245      |
|    value_loss           | 21.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.773      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0441     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 630        |
|    time_elapsed         | 2691       |
|    total_timesteps      | 645120     |
| train/                  |            |
|    approx_kl            | 0.12690836 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.338      |
|    n_updates            | 12580      |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.245      |
|    value_loss           | 3.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.773      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0441     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 631        |
|    time_elapsed         | 2695       |
|    total_timesteps      | 646144     |
| train/                  |            |
|    approx_kl            | 0.14306982 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.714      |
|    n_updates            | 12600      |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.245      |
|    value_loss           | 3.72       |
----------------------------------------
-----------------------------------------
| reward                  | 0.774       |
| reward_contact          | 0.0135      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.147       |
| reward_orientation      | 0.0443      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.167       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.236       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 853         |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 632         |
|    time_elapsed         | 2698        |
|    total_timesteps      | 647168      |
| train/                  |             |
|    approx_kl            | 0.102239944 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.601       |
|    n_updates            | 12620       |
|    policy_gradient_loss | -0.0767     |
|    std                  | 0.245       |
|    value_loss           | 2.86        |
-----------------------------------------
Num timesteps: 648000
Best mean reward: 875.26 - Last mean reward per episode: 853.38
----------------------------------------
| reward                  | 0.775      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.149      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 633        |
|    time_elapsed         | 2701       |
|    total_timesteps      | 648192     |
| train/                  |            |
|    approx_kl            | 0.17675312 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.319      |
|    n_updates            | 12640      |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.245      |
|    value_loss           | 1.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.781      |
| reward_contact          | 0.0135     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.15       |
| reward_orientation      | 0.0445     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 634        |
|    time_elapsed         | 2705       |
|    total_timesteps      | 649216     |
| train/                  |            |
|    approx_kl            | 0.17783561 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.88       |
|    n_updates            | 12660      |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.245      |
|    value_loss           | 2.87       |
----------------------------------------
-----------------------------------------
| reward                  | 0.784       |
| reward_contact          | 0.0141      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0446      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.167       |
| reward_torque           | 0.0567      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 853         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 635         |
|    time_elapsed         | 2708        |
|    total_timesteps      | 650240      |
| train/                  |             |
|    approx_kl            | 0.103053406 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 12680       |
|    policy_gradient_loss | -0.0706     |
|    std                  | 0.245       |
|    value_loss           | 2.21        |
-----------------------------------------
---------------------------------------
| reward                  | 0.782     |
| reward_contact          | 0.0141    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.152     |
| reward_orientation      | 0.0446    |
| reward_position         | 0.00117   |
| reward_rotation         | 0.167     |
| reward_torque           | 0.0566    |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 854       |
| time/                   |           |
|    fps                  | 240       |
|    iterations           | 636       |
|    time_elapsed         | 2711      |
|    total_timesteps      | 651264    |
| train/                  |           |
|    approx_kl            | 0.0566906 |
|    clip_fraction        | 0.23      |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.68      |
|    n_updates            | 12700     |
|    policy_gradient_loss | -0.0457   |
|    std                  | 0.245     |
|    value_loss           | 18        |
---------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0141     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0445     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 637        |
|    time_elapsed         | 2715       |
|    total_timesteps      | 652288     |
| train/                  |            |
|    approx_kl            | 0.06896865 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.05       |
|    n_updates            | 12720      |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.245      |
|    value_loss           | 9.32       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0147     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0448     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 638        |
|    time_elapsed         | 2718       |
|    total_timesteps      | 653312     |
| train/                  |            |
|    approx_kl            | 0.22844768 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.602      |
|    n_updates            | 12740      |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.245      |
|    value_loss           | 2.57       |
----------------------------------------
Num timesteps: 654000
Best mean reward: 875.26 - Last mean reward per episode: 854.51
-----------------------------------------
| reward                  | 0.784       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0448      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.167       |
| reward_torque           | 0.0566      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 855         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 639         |
|    time_elapsed         | 2722        |
|    total_timesteps      | 654336      |
| train/                  |             |
|    approx_kl            | 0.087088294 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 12760       |
|    policy_gradient_loss | -0.0835     |
|    std                  | 0.245       |
|    value_loss           | 5.65        |
-----------------------------------------
--------------------------------------
| reward                  | 0.784    |
| reward_contact          | 0.0146   |
| reward_ctrl             | 0.111    |
| reward_motion           | 0.153    |
| reward_orientation      | 0.0448   |
| reward_position         | 0.00117  |
| reward_rotation         | 0.167    |
| reward_torque           | 0.0567   |
| reward_velocity         | 0.235    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 854      |
| time/                   |          |
|    fps                  | 240      |
|    iterations           | 640      |
|    time_elapsed         | 2725     |
|    total_timesteps      | 655360   |
| train/                  |          |
|    approx_kl            | 0.281296 |
|    clip_fraction        | 0.459    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.9    |
|    explained_variance   | 0.992    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0576  |
|    n_updates            | 12780    |
|    policy_gradient_loss | -0.0942  |
|    std                  | 0.245    |
|    value_loss           | 1.08     |
--------------------------------------
-----------------------------------------
| reward                  | 0.783       |
| reward_contact          | 0.0146      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.155       |
| reward_orientation      | 0.0444      |
| reward_position         | 0.00117     |
| reward_rotation         | 0.166       |
| reward_torque           | 0.0566      |
| reward_velocity         | 0.235       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 854         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 641         |
|    time_elapsed         | 2728        |
|    total_timesteps      | 656384      |
| train/                  |             |
|    approx_kl            | 0.115556836 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.15        |
|    n_updates            | 12800       |
|    policy_gradient_loss | -0.0449     |
|    std                  | 0.245       |
|    value_loss           | 4.03        |
-----------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0441     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 642        |
|    time_elapsed         | 2732       |
|    total_timesteps      | 657408     |
| train/                  |            |
|    approx_kl            | 0.17611285 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.515      |
|    n_updates            | 12820      |
|    policy_gradient_loss | -0.0817    |
|    std                  | 0.245      |
|    value_loss           | 2.41       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 643        |
|    time_elapsed         | 2735       |
|    total_timesteps      | 658432     |
| train/                  |            |
|    approx_kl            | 0.08284518 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.11       |
|    n_updates            | 12840      |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.245      |
|    value_loss           | 17.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.785      |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 644        |
|    time_elapsed         | 2738       |
|    total_timesteps      | 659456     |
| train/                  |            |
|    approx_kl            | 0.06069085 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.98       |
|    n_updates            | 12860      |
|    policy_gradient_loss | -0.0545    |
|    std                  | 0.245      |
|    value_loss           | 15.2       |
----------------------------------------
Num timesteps: 660000
Best mean reward: 875.26 - Last mean reward per episode: 854.79
---------------------------------------
| reward                  | 0.789     |
| reward_contact          | 0.0138    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.156     |
| reward_orientation      | 0.0439    |
| reward_position         | 0.00117   |
| reward_rotation         | 0.17      |
| reward_torque           | 0.0566    |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 855       |
| time/                   |           |
|    fps                  | 240       |
|    iterations           | 645       |
|    time_elapsed         | 2742      |
|    total_timesteps      | 660480    |
| train/                  |           |
|    approx_kl            | 0.0807537 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.8     |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.9      |
|    n_updates            | 12880     |
|    policy_gradient_loss | -0.054    |
|    std                  | 0.245     |
|    value_loss           | 12.5      |
---------------------------------------
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0438     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 646        |
|    time_elapsed         | 2745       |
|    total_timesteps      | 661504     |
| train/                  |            |
|    approx_kl            | 0.11684094 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.41       |
|    n_updates            | 12900      |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.245      |
|    value_loss           | 25.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.044      |
| reward_position         | 0.00117    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 647        |
|    time_elapsed         | 2749       |
|    total_timesteps      | 662528     |
| train/                  |            |
|    approx_kl            | 0.16919407 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.717      |
|    n_updates            | 12920      |
|    policy_gradient_loss | -0.0757    |
|    std                  | 0.245      |
|    value_loss           | 2.39       |
----------------------------------------
---------------------------------------
| reward                  | 0.796     |
| reward_contact          | 0.0136    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.157     |
| reward_orientation      | 0.044     |
| reward_position         | 0.00117   |
| reward_rotation         | 0.173     |
| reward_torque           | 0.0567    |
| reward_velocity         | 0.24      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 856       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 648       |
|    time_elapsed         | 2752      |
|    total_timesteps      | 663552    |
| train/                  |           |
|    approx_kl            | 0.2211178 |
|    clip_fraction        | 0.362     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.124     |
|    n_updates            | 12940     |
|    policy_gradient_loss | -0.0931   |
|    std                  | 0.245     |
|    value_loss           | 1.46      |
---------------------------------------
----------------------------------------
| reward                  | 0.797      |
| reward_contact          | 0.0136     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.172      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 649        |
|    time_elapsed         | 2755       |
|    total_timesteps      | 664576     |
| train/                  |            |
|    approx_kl            | 0.16644016 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.125      |
|    n_updates            | 12960      |
|    policy_gradient_loss | -0.0494    |
|    std                  | 0.245      |
|    value_loss           | 1.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0441     |
| reward_position         | 0.00117    |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 650        |
|    time_elapsed         | 2759       |
|    total_timesteps      | 665600     |
| train/                  |            |
|    approx_kl            | 0.11652274 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.6        |
|    n_updates            | 12980      |
|    policy_gradient_loss | -0.0455    |
|    std                  | 0.245      |
|    value_loss           | 12.4       |
----------------------------------------
Num timesteps: 666000
Best mean reward: 875.26 - Last mean reward per episode: 856.22
---------------------------------------
| reward                  | 0.794     |
| reward_contact          | 0.0143    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.161     |
| reward_orientation      | 0.044     |
| reward_position         | 0.00117   |
| reward_rotation         | 0.17      |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 856       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 651       |
|    time_elapsed         | 2762      |
|    total_timesteps      | 666624    |
| train/                  |           |
|    approx_kl            | 0.1617575 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.703     |
|    n_updates            | 13000     |
|    policy_gradient_loss | -0.102    |
|    std                  | 0.245     |
|    value_loss           | 2.47      |
---------------------------------------
---------------------------------------
| reward                  | 0.791     |
| reward_contact          | 0.0147    |
| reward_ctrl             | 0.109     |
| reward_motion           | 0.158     |
| reward_orientation      | 0.044     |
| reward_position         | 0.00035   |
| reward_rotation         | 0.17      |
| reward_torque           | 0.0565    |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 856       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 652       |
|    time_elapsed         | 2765      |
|    total_timesteps      | 667648    |
| train/                  |           |
|    approx_kl            | 3.5289607 |
|    clip_fraction        | 0.654     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0829    |
|    n_updates            | 13020     |
|    policy_gradient_loss | -0.1      |
|    std                  | 0.245     |
|    value_loss           | 0.914     |
---------------------------------------
-----------------------------------------
| reward                  | 0.783       |
| reward_contact          | 0.0147      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0436      |
| reward_position         | 0.00035     |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 857         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 653         |
|    time_elapsed         | 2769        |
|    total_timesteps      | 668672      |
| train/                  |             |
|    approx_kl            | 0.049742483 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 13040       |
|    policy_gradient_loss | -0.0444     |
|    std                  | 0.245       |
|    value_loss           | 27.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0439     |
| reward_position         | 0.00035    |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 654        |
|    time_elapsed         | 2772       |
|    total_timesteps      | 669696     |
| train/                  |            |
|    approx_kl            | 0.14493123 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 13060      |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.245      |
|    value_loss           | 1.94       |
----------------------------------------
-----------------------------------------
| reward                  | 0.785       |
| reward_contact          | 0.0156      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.157       |
| reward_orientation      | 0.0438      |
| reward_position         | 0.00035     |
| reward_rotation         | 0.167       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.236       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 855         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 655         |
|    time_elapsed         | 2775        |
|    total_timesteps      | 670720      |
| train/                  |             |
|    approx_kl            | 0.112909764 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.5       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47        |
|    n_updates            | 13080       |
|    policy_gradient_loss | -0.0683     |
|    std                  | 0.244       |
|    value_loss           | 3.18        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.783       |
| reward_contact          | 0.0156      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.154       |
| reward_orientation      | 0.0434      |
| reward_position         | 0.00035     |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.236       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 656         |
|    time_elapsed         | 2779        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.072375104 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 13100       |
|    policy_gradient_loss | -0.0733     |
|    std                  | 0.244       |
|    value_loss           | 3.07        |
-----------------------------------------
Num timesteps: 672000
Best mean reward: 875.26 - Last mean reward per episode: 856.07
-----------------------------------------
| reward                  | 0.782       |
| reward_contact          | 0.015       |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.153       |
| reward_orientation      | 0.0433      |
| reward_position         | 0.00035     |
| reward_rotation         | 0.168       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 657         |
|    time_elapsed         | 2782        |
|    total_timesteps      | 672768      |
| train/                  |             |
|    approx_kl            | 0.092583254 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.695       |
|    n_updates            | 13120       |
|    policy_gradient_loss | -0.078      |
|    std                  | 0.244       |
|    value_loss           | 4.19        |
-----------------------------------------
----------------------------------------
| reward                  | 0.783      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.155      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00035    |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 658        |
|    time_elapsed         | 2786       |
|    total_timesteps      | 673792     |
| train/                  |            |
|    approx_kl            | 0.16743581 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 13140      |
|    policy_gradient_loss | -0.0752    |
|    std                  | 0.244      |
|    value_loss           | 2.33       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.015     |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.154     |
| reward_orientation      | 0.0433    |
| reward_position         | 0.00035   |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 855       |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 659       |
|    time_elapsed         | 2789      |
|    total_timesteps      | 674816    |
| train/                  |           |
|    approx_kl            | 0.3880999 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.241     |
|    n_updates            | 13160     |
|    policy_gradient_loss | -0.0803   |
|    std                  | 0.244     |
|    value_loss           | 1.14      |
---------------------------------------
----------------------------------------
| reward                  | 0.779      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.16       |
| reward_orientation      | 0.0431     |
| reward_position         | 0.00035    |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 660        |
|    time_elapsed         | 2792       |
|    total_timesteps      | 675840     |
| train/                  |            |
|    approx_kl            | 0.07653847 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 13180      |
|    policy_gradient_loss | -0.0711    |
|    std                  | 0.244      |
|    value_loss           | 3.09       |
----------------------------------------
-----------------------------------------
| reward                  | 0.782       |
| reward_contact          | 0.0156      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.164       |
| reward_orientation      | 0.0431      |
| reward_position         | 0.00035     |
| reward_rotation         | 0.162       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.233       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 661         |
|    time_elapsed         | 2796        |
|    total_timesteps      | 676864      |
| train/                  |             |
|    approx_kl            | 0.123654574 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 13200       |
|    policy_gradient_loss | -0.104      |
|    std                  | 0.244       |
|    value_loss           | 3.92        |
-----------------------------------------
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00035    |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 662        |
|    time_elapsed         | 2799       |
|    total_timesteps      | 677888     |
| train/                  |            |
|    approx_kl            | 0.26786554 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.157      |
|    n_updates            | 13220      |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.244      |
|    value_loss           | 1.41       |
----------------------------------------
Num timesteps: 678000
Best mean reward: 875.26 - Last mean reward per episode: 856.60
----------------------------------------
| reward                  | 0.782      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0434     |
| reward_position         | 0.00035    |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 663        |
|    time_elapsed         | 2802       |
|    total_timesteps      | 678912     |
| train/                  |            |
|    approx_kl            | 0.08259653 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53       |
|    n_updates            | 13240      |
|    policy_gradient_loss | -0.0519    |
|    std                  | 0.244      |
|    value_loss           | 9.76       |
----------------------------------------
---------------------------------------
| reward                  | 0.787     |
| reward_contact          | 0.015     |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0435    |
| reward_position         | 0.00035   |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 856       |
| time/                   |           |
|    fps                  | 242       |
|    iterations           | 664       |
|    time_elapsed         | 2806      |
|    total_timesteps      | 679936    |
| train/                  |           |
|    approx_kl            | 0.2522325 |
|    clip_fraction        | 0.499     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.552     |
|    n_updates            | 13260     |
|    policy_gradient_loss | 0.0209    |
|    std                  | 0.244     |
|    value_loss           | 1.74      |
---------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0144     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 665        |
|    time_elapsed         | 2809       |
|    total_timesteps      | 680960     |
| train/                  |            |
|    approx_kl            | 0.21088246 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.244      |
|    n_updates            | 13280      |
|    policy_gradient_loss | -0.0788    |
|    std                  | 0.244      |
|    value_loss           | 1.2        |
----------------------------------------
---------------------------------------
| reward                  | 0.795     |
| reward_contact          | 0.0144    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.171     |
| reward_orientation      | 0.043     |
| reward_position         | 0.000355  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 858       |
| time/                   |           |
|    fps                  | 242       |
|    iterations           | 666       |
|    time_elapsed         | 2812      |
|    total_timesteps      | 681984    |
| train/                  |           |
|    approx_kl            | 0.1269041 |
|    clip_fraction        | 0.306     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.2     |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.0003    |
|    loss                 | 25.5      |
|    n_updates            | 13300     |
|    policy_gradient_loss | -0.0534   |
|    std                  | 0.244     |
|    value_loss           | 27.9      |
---------------------------------------
-----------------------------------------
| reward                  | 0.794       |
| reward_contact          | 0.0138      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.171       |
| reward_orientation      | 0.0429      |
| reward_position         | 0.000355    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 857         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 667         |
|    time_elapsed         | 2816        |
|    total_timesteps      | 683008      |
| train/                  |             |
|    approx_kl            | 0.058400966 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.31        |
|    n_updates            | 13320       |
|    policy_gradient_loss | -0.0706     |
|    std                  | 0.244       |
|    value_loss           | 5.18        |
-----------------------------------------
Num timesteps: 684000
Best mean reward: 875.26 - Last mean reward per episode: 854.21
---------------------------------------
| reward                  | 0.784     |
| reward_contact          | 0.0138    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.164     |
| reward_orientation      | 0.0428    |
| reward_position         | 0.000355  |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 854       |
| time/                   |           |
|    fps                  | 242       |
|    iterations           | 668       |
|    time_elapsed         | 2819      |
|    total_timesteps      | 684032    |
| train/                  |           |
|    approx_kl            | 1.0427439 |
|    clip_fraction        | 0.462     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.585     |
|    n_updates            | 13340     |
|    policy_gradient_loss | -0.0542   |
|    std                  | 0.244     |
|    value_loss           | 1.51      |
---------------------------------------
----------------------------------------
| reward                  | 0.792      |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 669        |
|    time_elapsed         | 2823       |
|    total_timesteps      | 685056     |
| train/                  |            |
|    approx_kl            | 0.11488045 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.9       |
|    n_updates            | 13360      |
|    policy_gradient_loss | -0.0414    |
|    std                  | 0.244      |
|    value_loss           | 23.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.792      |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 670        |
|    time_elapsed         | 2826       |
|    total_timesteps      | 686080     |
| train/                  |            |
|    approx_kl            | 0.18176797 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.63       |
|    n_updates            | 13380      |
|    policy_gradient_loss | -0.0921    |
|    std                  | 0.244      |
|    value_loss           | 1.99       |
----------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0132     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 671        |
|    time_elapsed         | 2829       |
|    total_timesteps      | 687104     |
| train/                  |            |
|    approx_kl            | 0.16997609 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 13400      |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.244      |
|    value_loss           | 2.32       |
----------------------------------------
-----------------------------------------
| reward                  | 0.793       |
| reward_contact          | 0.0132      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.173       |
| reward_orientation      | 0.0429      |
| reward_position         | 0.000355    |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.236       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 854         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 672         |
|    time_elapsed         | 2833        |
|    total_timesteps      | 688128      |
| train/                  |             |
|    approx_kl            | 0.054464526 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.36        |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.051      |
|    std                  | 0.244       |
|    value_loss           | 10.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.791      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 673        |
|    time_elapsed         | 2836       |
|    total_timesteps      | 689152     |
| train/                  |            |
|    approx_kl            | 0.13086359 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 13440      |
|    policy_gradient_loss | -0.0345    |
|    std                  | 0.244      |
|    value_loss           | 4.77       |
----------------------------------------
Num timesteps: 690000
Best mean reward: 875.26 - Last mean reward per episode: 853.88
----------------------------------------
| reward                  | 0.791      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 854        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 674        |
|    time_elapsed         | 2839       |
|    total_timesteps      | 690176     |
| train/                  |            |
|    approx_kl            | 0.11041151 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 13460      |
|    policy_gradient_loss | -0.0678    |
|    std                  | 0.244      |
|    value_loss           | 2.41       |
----------------------------------------
----------------------------------------
| reward                  | 0.786      |
| reward_contact          | 0.0126     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.169      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 675        |
|    time_elapsed         | 2843       |
|    total_timesteps      | 691200     |
| train/                  |            |
|    approx_kl            | 0.10705216 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.208      |
|    n_updates            | 13480      |
|    policy_gradient_loss | -0.0788    |
|    std                  | 0.244      |
|    value_loss           | 1.8        |
----------------------------------------
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 676        |
|    time_elapsed         | 2846       |
|    total_timesteps      | 692224     |
| train/                  |            |
|    approx_kl            | 0.03688928 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.48       |
|    n_updates            | 13500      |
|    policy_gradient_loss | -0.0421    |
|    std                  | 0.244      |
|    value_loss           | 23.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.789      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 677        |
|    time_elapsed         | 2849       |
|    total_timesteps      | 693248     |
| train/                  |            |
|    approx_kl            | 0.19019243 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0952     |
|    n_updates            | 13520      |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.244      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.789      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0427     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 856        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 678        |
|    time_elapsed         | 2853       |
|    total_timesteps      | 694272     |
| train/                  |            |
|    approx_kl            | 0.33592263 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.276      |
|    n_updates            | 13540      |
|    policy_gradient_loss | -0.0512    |
|    std                  | 0.244      |
|    value_loss           | 1.21       |
----------------------------------------
----------------------------------------
| reward                  | 0.788      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0426     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 679        |
|    time_elapsed         | 2856       |
|    total_timesteps      | 695296     |
| train/                  |            |
|    approx_kl            | 0.09853952 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.29       |
|    n_updates            | 13560      |
|    policy_gradient_loss | -0.0574    |
|    std                  | 0.243      |
|    value_loss           | 8.15       |
----------------------------------------
Num timesteps: 696000
Best mean reward: 875.26 - Last mean reward per episode: 857.61
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 680        |
|    time_elapsed         | 2860       |
|    total_timesteps      | 696320     |
| train/                  |            |
|    approx_kl            | 0.06667801 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 13580      |
|    policy_gradient_loss | -0.0321    |
|    std                  | 0.244      |
|    value_loss           | 18.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.796      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.172      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 681        |
|    time_elapsed         | 2863       |
|    total_timesteps      | 697344     |
| train/                  |            |
|    approx_kl            | 0.07203387 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 5          |
|    n_updates            | 13600      |
|    policy_gradient_loss | -0.0679    |
|    std                  | 0.243      |
|    value_loss           | 6.24       |
----------------------------------------
---------------------------------------
| reward                  | 0.797     |
| reward_contact          | 0.012     |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.173     |
| reward_orientation      | 0.0428    |
| reward_position         | 0.000355  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 243       |
|    iterations           | 682       |
|    time_elapsed         | 2866      |
|    total_timesteps      | 698368    |
| train/                  |           |
|    approx_kl            | 0.0894849 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 13620     |
|    policy_gradient_loss | -0.0646   |
|    std                  | 0.244     |
|    value_loss           | 3.07      |
---------------------------------------
-----------------------------------------
| reward                  | 0.797       |
| reward_contact          | 0.012       |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.173       |
| reward_orientation      | 0.0428      |
| reward_position         | 0.000355    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.238       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 683         |
|    time_elapsed         | 2870        |
|    total_timesteps      | 699392      |
| train/                  |             |
|    approx_kl            | 0.092221335 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.4         |
|    n_updates            | 13640       |
|    policy_gradient_loss | -0.0351     |
|    std                  | 0.244       |
|    value_loss           | 22.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.176      |
| reward_orientation      | 0.043      |
| reward_position         | 0.000355   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 684        |
|    time_elapsed         | 2873       |
|    total_timesteps      | 700416     |
| train/                  |            |
|    approx_kl            | 0.07831679 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.82       |
|    n_updates            | 13660      |
|    policy_gradient_loss | -0.0563    |
|    std                  | 0.244      |
|    value_loss           | 14.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.795     |
| reward_contact          | 0.0125    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.174     |
| reward_orientation      | 0.0427    |
| reward_position         | 0.000355  |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 243       |
|    iterations           | 685       |
|    time_elapsed         | 2876      |
|    total_timesteps      | 701440    |
| train/                  |           |
|    approx_kl            | 0.0457173 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.866     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.14      |
|    n_updates            | 13680     |
|    policy_gradient_loss | -0.0424   |
|    std                  | 0.244     |
|    value_loss           | 13.1      |
---------------------------------------
Num timesteps: 702000
Best mean reward: 875.26 - Last mean reward per episode: 860.32
-----------------------------------------
| reward                  | 0.793       |
| reward_contact          | 0.0125      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.172       |
| reward_orientation      | 0.0426      |
| reward_position         | 0.000355    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.238       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 686         |
|    time_elapsed         | 2880        |
|    total_timesteps      | 702464      |
| train/                  |             |
|    approx_kl            | 0.121987164 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.9       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 13700       |
|    policy_gradient_loss | -0.0773     |
|    std                  | 0.243       |
|    value_loss           | 3.98        |
-----------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0125     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.172      |
| reward_orientation      | 0.0426     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 687        |
|    time_elapsed         | 2883       |
|    total_timesteps      | 703488     |
| train/                  |            |
|    approx_kl            | 0.07295604 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.233      |
|    learning_rate        | 0.0003     |
|    loss                 | 48.6       |
|    n_updates            | 13720      |
|    policy_gradient_loss | -0.0307    |
|    std                  | 0.243      |
|    value_loss           | 30.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.791      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.171      |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 688        |
|    time_elapsed         | 2886       |
|    total_timesteps      | 704512     |
| train/                  |            |
|    approx_kl            | 0.09333302 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 20.2       |
|    n_updates            | 13740      |
|    policy_gradient_loss | -0.0485    |
|    std                  | 0.243      |
|    value_loss           | 19.8       |
----------------------------------------
--------------------------------------
| reward                  | 0.792    |
| reward_contact          | 0.0119   |
| reward_ctrl             | 0.107    |
| reward_motion           | 0.173    |
| reward_orientation      | 0.0432   |
| reward_position         | 0.000355 |
| reward_rotation         | 0.163    |
| reward_torque           | 0.0562   |
| reward_velocity         | 0.238    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 862      |
| time/                   |          |
|    fps                  | 244      |
|    iterations           | 689      |
|    time_elapsed         | 2890     |
|    total_timesteps      | 705536   |
| train/                  |          |
|    approx_kl            | 1.206279 |
|    clip_fraction        | 0.581    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.6    |
|    explained_variance   | 0.989    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.163    |
|    n_updates            | 13760    |
|    policy_gradient_loss | -0.0235  |
|    std                  | 0.243    |
|    value_loss           | 1.72     |
--------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 690        |
|    time_elapsed         | 2893       |
|    total_timesteps      | 706560     |
| train/                  |            |
|    approx_kl            | 0.27169198 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.267      |
|    n_updates            | 13780      |
|    policy_gradient_loss | -0.0843    |
|    std                  | 0.243      |
|    value_loss           | 2.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 691        |
|    time_elapsed         | 2896       |
|    total_timesteps      | 707584     |
| train/                  |            |
|    approx_kl            | 0.15211803 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28       |
|    n_updates            | 13800      |
|    policy_gradient_loss | -0.0564    |
|    std                  | 0.243      |
|    value_loss           | 2.85       |
----------------------------------------
Num timesteps: 708000
Best mean reward: 875.26 - Last mean reward per episode: 862.09
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | 0.0119     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0429     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 692        |
|    time_elapsed         | 2900       |
|    total_timesteps      | 708608     |
| train/                  |            |
|    approx_kl            | 0.32836914 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.804      |
|    n_updates            | 13820      |
|    policy_gradient_loss | -0.0132    |
|    std                  | 0.243      |
|    value_loss           | 1.95       |
----------------------------------------
-----------------------------------------
| reward                  | 0.806       |
| reward_contact          | 0.0113      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.183       |
| reward_orientation      | 0.043       |
| reward_position         | 0.000355    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.241       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 862         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 693         |
|    time_elapsed         | 2903        |
|    total_timesteps      | 709632      |
| train/                  |             |
|    approx_kl            | 0.064164005 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.34        |
|    n_updates            | 13840       |
|    policy_gradient_loss | -0.0334     |
|    std                  | 0.243       |
|    value_loss           | 8.45        |
-----------------------------------------
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0107     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.043      |
| reward_position         | 0.00118    |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 694        |
|    time_elapsed         | 2906       |
|    total_timesteps      | 710656     |
| train/                  |            |
|    approx_kl            | 0.07858826 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.49       |
|    n_updates            | 13860      |
|    policy_gradient_loss | -0.0551    |
|    std                  | 0.243      |
|    value_loss           | 4.84       |
----------------------------------------
-----------------------------------------
| reward                  | 0.804       |
| reward_contact          | 0.0107      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.179       |
| reward_orientation      | 0.043       |
| reward_position         | 0.00118     |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.241       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 863         |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 695         |
|    time_elapsed         | 2910        |
|    total_timesteps      | 711680      |
| train/                  |             |
|    approx_kl            | 0.054649413 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 13880       |
|    policy_gradient_loss | -0.0506     |
|    std                  | 0.243       |
|    value_loss           | 21.5        |
-----------------------------------------
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0112     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.043      |
| reward_position         | 0.00118    |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.241      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 696        |
|    time_elapsed         | 2913       |
|    total_timesteps      | 712704     |
| train/                  |            |
|    approx_kl            | 0.06586459 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 16.5       |
|    n_updates            | 13900      |
|    policy_gradient_loss | -0.0479    |
|    std                  | 0.243      |
|    value_loss           | 17.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 863        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 697        |
|    time_elapsed         | 2916       |
|    total_timesteps      | 713728     |
| train/                  |            |
|    approx_kl            | 0.18110563 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 13920      |
|    policy_gradient_loss | -0.0616    |
|    std                  | 0.243      |
|    value_loss           | 3.09       |
----------------------------------------
Num timesteps: 714000
Best mean reward: 875.26 - Last mean reward per episode: 863.35
----------------------------------------
| reward                  | 0.796      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 698        |
|    time_elapsed         | 2920       |
|    total_timesteps      | 714752     |
| train/                  |            |
|    approx_kl            | 0.08376184 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.698      |
|    n_updates            | 13940      |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.243      |
|    value_loss           | 5.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.796      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.176      |
| reward_orientation      | 0.0432     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 699        |
|    time_elapsed         | 2923       |
|    total_timesteps      | 715776     |
| train/                  |            |
|    approx_kl            | 0.08644451 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.2       |
|    n_updates            | 13960      |
|    policy_gradient_loss | -0.0424    |
|    std                  | 0.243      |
|    value_loss           | 20.4       |
----------------------------------------
--------------------------------------
| reward                  | 0.795    |
| reward_contact          | 0.0114   |
| reward_ctrl             | 0.107    |
| reward_motion           | 0.173    |
| reward_orientation      | 0.0434   |
| reward_position         | 0.00118  |
| reward_rotation         | 0.163    |
| reward_torque           | 0.0562   |
| reward_velocity         | 0.24     |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 864      |
| time/                   |          |
|    fps                  | 244      |
|    iterations           | 700      |
|    time_elapsed         | 2927     |
|    total_timesteps      | 716800   |
| train/                  |          |
|    approx_kl            | 0.233369 |
|    clip_fraction        | 0.348    |
|    clip_range           | 0.4      |
|    entropy_loss         | -44.6    |
|    explained_variance   | 0.987    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.655    |
|    n_updates            | 13980    |
|    policy_gradient_loss | -0.0913  |
|    std                  | 0.243    |
|    value_loss           | 3.5      |
--------------------------------------
----------------------------------------
| reward                  | 0.794      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.169      |
| reward_orientation      | 0.044      |
| reward_position         | 0.00118    |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 701        |
|    time_elapsed         | 2930       |
|    total_timesteps      | 717824     |
| train/                  |            |
|    approx_kl            | 0.09736862 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.69       |
|    n_updates            | 14000      |
|    policy_gradient_loss | 0.00982    |
|    std                  | 0.243      |
|    value_loss           | 5.33       |
----------------------------------------
-----------------------------------------
| reward                  | 0.793       |
| reward_contact          | 0.0114      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.17        |
| reward_orientation      | 0.0441      |
| reward_position         | 0.00118     |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 864         |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 702         |
|    time_elapsed         | 2933        |
|    total_timesteps      | 718848      |
| train/                  |             |
|    approx_kl            | 0.055033453 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.45        |
|    n_updates            | 14020       |
|    policy_gradient_loss | -0.037      |
|    std                  | 0.243       |
|    value_loss           | 9.09        |
-----------------------------------------
----------------------------------------
| reward                  | 0.793      |
| reward_contact          | 0.0114     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.17       |
| reward_orientation      | 0.0441     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 703        |
|    time_elapsed         | 2937       |
|    total_timesteps      | 719872     |
| train/                  |            |
|    approx_kl            | 0.06407282 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.28       |
|    n_updates            | 14040      |
|    policy_gradient_loss | -0.0544    |
|    std                  | 0.243      |
|    value_loss           | 7.64       |
----------------------------------------
Num timesteps: 720000
Best mean reward: 875.26 - Last mean reward per episode: 864.42
-----------------------------------------
| reward                  | 0.796       |
| reward_contact          | 0.012       |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.173       |
| reward_orientation      | 0.044       |
| reward_position         | 0.00118     |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 704         |
|    time_elapsed         | 2940        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.055367872 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 14060       |
|    policy_gradient_loss | -0.0517     |
|    std                  | 0.243       |
|    value_loss           | 24.7        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.798       |
| reward_contact          | 0.012       |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.176       |
| reward_orientation      | 0.0447      |
| reward_position         | 0.00118     |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0561      |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 866         |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 705         |
|    time_elapsed         | 2943        |
|    total_timesteps      | 721920      |
| train/                  |             |
|    approx_kl            | 0.111499555 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.72        |
|    n_updates            | 14080       |
|    policy_gradient_loss | -0.0668     |
|    std                  | 0.243       |
|    value_loss           | 4.59        |
-----------------------------------------
---------------------------------------
| reward                  | 0.797     |
| reward_contact          | 0.012     |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.173     |
| reward_orientation      | 0.0447    |
| reward_position         | 0.00118   |
| reward_rotation         | 0.164     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.24      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 245       |
|    iterations           | 706       |
|    time_elapsed         | 2947      |
|    total_timesteps      | 722944    |
| train/                  |           |
|    approx_kl            | 0.0514259 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 14100     |
|    policy_gradient_loss | -0.0419   |
|    std                  | 0.243     |
|    value_loss           | 5.85      |
---------------------------------------
-----------------------------------------
| reward                  | 0.795       |
| reward_contact          | 0.012       |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.173       |
| reward_orientation      | 0.0447      |
| reward_position         | 0.00118     |
| reward_rotation         | 0.164       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.24        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 867         |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 707         |
|    time_elapsed         | 2950        |
|    total_timesteps      | 723968      |
| train/                  |             |
|    approx_kl            | 0.055395246 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 14120       |
|    policy_gradient_loss | -0.042      |
|    std                  | 0.243       |
|    value_loss           | 21.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.795      |
| reward_contact          | 0.012      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.173      |
| reward_orientation      | 0.0447     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.164      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 708        |
|    time_elapsed         | 2953       |
|    total_timesteps      | 724992     |
| train/                  |            |
|    approx_kl            | 0.13388622 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.252      |
|    n_updates            | 14140      |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.243      |
|    value_loss           | 2.24       |
----------------------------------------
Num timesteps: 726000
Best mean reward: 875.26 - Last mean reward per episode: 867.24
---------------------------------------
| reward                  | 0.803     |
| reward_contact          | 0.0115    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.181     |
| reward_orientation      | 0.0451    |
| reward_position         | 0.00118   |
| reward_rotation         | 0.162     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.241     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 245       |
|    iterations           | 709       |
|    time_elapsed         | 2957      |
|    total_timesteps      | 726016    |
| train/                  |           |
|    approx_kl            | 0.3182362 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.7     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.61      |
|    n_updates            | 14160     |
|    policy_gradient_loss | -0.0364   |
|    std                  | 0.243     |
|    value_loss           | 5.69      |
---------------------------------------
----------------------------------------
| reward                  | 0.806      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0452     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 710        |
|    time_elapsed         | 2960       |
|    total_timesteps      | 727040     |
| train/                  |            |
|    approx_kl            | 0.10576193 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.62       |
|    n_updates            | 14180      |
|    policy_gradient_loss | -0.062     |
|    std                  | 0.243      |
|    value_loss           | 2.17       |
----------------------------------------
----------------------------------------
| reward                  | 0.807      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.161      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 711        |
|    time_elapsed         | 2963       |
|    total_timesteps      | 728064     |
| train/                  |            |
|    approx_kl            | 0.15984637 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.61       |
|    n_updates            | 14200      |
|    policy_gradient_loss | -0.0677    |
|    std                  | 0.243      |
|    value_loss           | 2.04       |
----------------------------------------
----------------------------------------
| reward                  | 0.807      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.187      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.161      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 712        |
|    time_elapsed         | 2967       |
|    total_timesteps      | 729088     |
| train/                  |            |
|    approx_kl            | 0.11811267 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.276      |
|    n_updates            | 14220      |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.243      |
|    value_loss           | 1.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.808      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.186      |
| reward_orientation      | 0.0456     |
| reward_position         | 0.00118    |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 713        |
|    time_elapsed         | 2970       |
|    total_timesteps      | 730112     |
| train/                  |            |
|    approx_kl            | 0.06200584 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.86       |
|    n_updates            | 14240      |
|    policy_gradient_loss | -0.0648    |
|    std                  | 0.243      |
|    value_loss           | 5.27       |
----------------------------------------
---------------------------------------
| reward                  | 0.799     |
| reward_contact          | 0.0121    |
| reward_ctrl             | 0.105     |
| reward_motion           | 0.182     |
| reward_orientation      | 0.0452    |
| reward_position         | 0.000832  |
| reward_rotation         | 0.159     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.239     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 867       |
| time/                   |           |
|    fps                  | 245       |
|    iterations           | 714       |
|    time_elapsed         | 2974      |
|    total_timesteps      | 731136    |
| train/                  |           |
|    approx_kl            | 0.3095142 |
|    clip_fraction        | 0.362     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.5     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.153     |
|    n_updates            | 14260     |
|    policy_gradient_loss | -0.0838   |
|    std                  | 0.243     |
|    value_loss           | 1.43      |
---------------------------------------
Num timesteps: 732000
Best mean reward: 875.26 - Last mean reward per episode: 866.85
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.186      |
| reward_orientation      | 0.0453     |
| reward_position         | 0.000832   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 715        |
|    time_elapsed         | 2977       |
|    total_timesteps      | 732160     |
| train/                  |            |
|    approx_kl            | 0.13039123 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.169      |
|    n_updates            | 14280      |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.243      |
|    value_loss           | 1.19       |
----------------------------------------
----------------------------------------
| reward                  | 0.804      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.186      |
| reward_orientation      | 0.0453     |
| reward_position         | 0.000832   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 716        |
|    time_elapsed         | 2980       |
|    total_timesteps      | 733184     |
| train/                  |            |
|    approx_kl            | 0.13480744 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.7      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.94       |
|    n_updates            | 14300      |
|    policy_gradient_loss | -0.0539    |
|    std                  | 0.243      |
|    value_loss           | 1.78       |
----------------------------------------
---------------------------------------
| reward                  | 0.801     |
| reward_contact          | 0.0133    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.19      |
| reward_orientation      | 0.0449    |
| reward_position         | 0.000832  |
| reward_rotation         | 0.157     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.236     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 717       |
|    time_elapsed         | 2984      |
|    total_timesteps      | 734208    |
| train/                  |           |
|    approx_kl            | 0.2634437 |
|    clip_fraction        | 0.441     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.9     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.298     |
|    n_updates            | 14320     |
|    policy_gradient_loss | -0.0599   |
|    std                  | 0.243     |
|    value_loss           | 1.92      |
---------------------------------------
----------------------------------------
| reward                  | 0.801      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.189      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.000832   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 718        |
|    time_elapsed         | 2987       |
|    total_timesteps      | 735232     |
| train/                  |            |
|    approx_kl            | 0.34481323 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -45        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.395      |
|    n_updates            | 14340      |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.243      |
|    value_loss           | 2.3        |
----------------------------------------
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.000832   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 719        |
|    time_elapsed         | 2990       |
|    total_timesteps      | 736256     |
| train/                  |            |
|    approx_kl            | 0.12641147 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.669      |
|    n_updates            | 14360      |
|    policy_gradient_loss | -0.0279    |
|    std                  | 0.243      |
|    value_loss           | 2.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0446     |
| reward_position         | 0.000832   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 720        |
|    time_elapsed         | 2994       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.10975237 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.09       |
|    n_updates            | 14380      |
|    policy_gradient_loss | -0.0595    |
|    std                  | 0.243      |
|    value_loss           | 3.44       |
----------------------------------------
Num timesteps: 738000
Best mean reward: 875.26 - Last mean reward per episode: 864.68
----------------------------------------
| reward                  | 0.8        |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.045      |
| reward_position         | 0.000832   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.056      |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 721        |
|    time_elapsed         | 2997       |
|    total_timesteps      | 738304     |
| train/                  |            |
|    approx_kl            | 0.93115354 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.556      |
|    n_updates            | 14400      |
|    policy_gradient_loss | -0.0283    |
|    std                  | 0.243      |
|    value_loss           | 1.25       |
----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 866        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 722        |
|    time_elapsed         | 3000       |
|    total_timesteps      | 739328     |
| train/                  |            |
|    approx_kl            | 0.30416954 |
|    clip_fraction        | 0.699      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.0003     |
|    loss                 | 32         |
|    n_updates            | 14420      |
|    policy_gradient_loss | 0.0284     |
|    std                  | 0.243      |
|    value_loss           | 28.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.798     |
| reward_contact          | 0.0133    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.178     |
| reward_orientation      | 0.0451    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 723       |
|    time_elapsed         | 3004      |
|    total_timesteps      | 740352    |
| train/                  |           |
|    approx_kl            | 0.0698975 |
|    clip_fraction        | 0.217     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.4     |
|    explained_variance   | 0.0781    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.42      |
|    n_updates            | 14440     |
|    policy_gradient_loss | -0.029    |
|    std                  | 0.243     |
|    value_loss           | 36.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.798     |
| reward_contact          | 0.0133    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.178     |
| reward_orientation      | 0.0451    |
| reward_position         | 0.000833  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 724       |
|    time_elapsed         | 3007      |
|    total_timesteps      | 741376    |
| train/                  |           |
|    approx_kl            | 0.0622105 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.122     |
|    learning_rate        | 0.0003    |
|    loss                 | 60.1      |
|    n_updates            | 14460     |
|    policy_gradient_loss | -0.043    |
|    std                  | 0.243     |
|    value_loss           | 30.7      |
---------------------------------------
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.179      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000833   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 725        |
|    time_elapsed         | 3010       |
|    total_timesteps      | 742400     |
| train/                  |            |
|    approx_kl            | 0.10655463 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.0872     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.91       |
|    n_updates            | 14480      |
|    policy_gradient_loss | -0.0202    |
|    std                  | 0.243      |
|    value_loss           | 32.5       |
----------------------------------------
---------------------------------------
| reward                  | 0.8       |
| reward_contact          | 0.0127    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.181     |
| reward_orientation      | 0.0452    |
| reward_position         | 0.000843  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.238     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 868       |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 726       |
|    time_elapsed         | 3014      |
|    total_timesteps      | 743424    |
| train/                  |           |
|    approx_kl            | 0.2242229 |
|    clip_fraction        | 0.267     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.6       |
|    n_updates            | 14500     |
|    policy_gradient_loss | -0.0478   |
|    std                  | 0.243     |
|    value_loss           | 8.39      |
---------------------------------------
Num timesteps: 744000
Best mean reward: 875.26 - Last mean reward per episode: 868.36
-----------------------------------------
| reward                  | 0.799       |
| reward_contact          | 0.0127      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.18        |
| reward_orientation      | 0.0454      |
| reward_position         | 0.000843    |
| reward_rotation         | 0.162       |
| reward_torque           | 0.056       |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 727         |
|    time_elapsed         | 3017        |
|    total_timesteps      | 744448      |
| train/                  |             |
|    approx_kl            | 0.059140276 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 14520       |
|    policy_gradient_loss | -0.0364     |
|    std                  | 0.243       |
|    value_loss           | 22.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0454     |
| reward_position         | 0.000843   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 728        |
|    time_elapsed         | 3021       |
|    total_timesteps      | 745472     |
| train/                  |            |
|    approx_kl            | 0.19772089 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.202      |
|    n_updates            | 14540      |
|    policy_gradient_loss | -0.0848    |
|    std                  | 0.243      |
|    value_loss           | 1.91       |
----------------------------------------
----------------------------------------
| reward                  | 0.8        |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.184      |
| reward_orientation      | 0.0453     |
| reward_position         | 0.000843   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 729        |
|    time_elapsed         | 3024       |
|    total_timesteps      | 746496     |
| train/                  |            |
|    approx_kl            | 0.07938992 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.41       |
|    n_updates            | 14560      |
|    policy_gradient_loss | -0.0668    |
|    std                  | 0.243      |
|    value_loss           | 4.27       |
----------------------------------------
----------------------------------------
| reward                  | 0.798      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.182      |
| reward_orientation      | 0.0449     |
| reward_position         | 0.000843   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 730        |
|    time_elapsed         | 3027       |
|    total_timesteps      | 747520     |
| train/                  |            |
|    approx_kl            | 0.29642633 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 14580      |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.243      |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000843   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.056      |
| reward_velocity         | 0.239      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 731        |
|    time_elapsed         | 3031       |
|    total_timesteps      | 748544     |
| train/                  |            |
|    approx_kl            | 0.16972846 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.263      |
|    n_updates            | 14600      |
|    policy_gradient_loss | -0.0795    |
|    std                  | 0.243      |
|    value_loss           | 2.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.045      |
| reward_position         | 0.000843   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 732        |
|    time_elapsed         | 3034       |
|    total_timesteps      | 749568     |
| train/                  |            |
|    approx_kl            | 0.10728191 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.19       |
|    n_updates            | 14620      |
|    policy_gradient_loss | -0.0415    |
|    std                  | 0.243      |
|    value_loss           | 5.89       |
----------------------------------------
Num timesteps: 750000
Best mean reward: 875.26 - Last mean reward per episode: 870.08
----------------------------------------
| reward                  | 0.799      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.18       |
| reward_orientation      | 0.045      |
| reward_position         | 0.000843   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 733        |
|    time_elapsed         | 3037       |
|    total_timesteps      | 750592     |
| train/                  |            |
|    approx_kl            | 0.07896204 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.24       |
|    n_updates            | 14640      |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.243      |
|    value_loss           | 3.59       |
----------------------------------------
----------------------------------------
| reward                  | 0.791      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.174      |
| reward_orientation      | 0.0449     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.24       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 734        |
|    time_elapsed         | 3041       |
|    total_timesteps      | 751616     |
| train/                  |            |
|    approx_kl            | 0.09232487 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.47       |
|    n_updates            | 14660      |
|    policy_gradient_loss | -0.0401    |
|    std                  | 0.243      |
|    value_loss           | 23.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.794     |
| reward_contact          | 0.0139    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.176     |
| reward_orientation      | 0.0448    |
| reward_position         | 0.000845  |
| reward_rotation         | 0.16      |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.24      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 872       |
| time/                   |           |
|    fps                  | 247       |
|    iterations           | 735       |
|    time_elapsed         | 3044      |
|    total_timesteps      | 752640    |
| train/                  |           |
|    approx_kl            | 0.2195392 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.361     |
|    n_updates            | 14680     |
|    policy_gradient_loss | -0.0647   |
|    std                  | 0.243     |
|    value_loss           | 1.94      |
---------------------------------------
----------------------------------------
| reward                  | 0.787      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.172      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 736        |
|    time_elapsed         | 3047       |
|    total_timesteps      | 753664     |
| train/                  |            |
|    approx_kl            | 0.11777529 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.59       |
|    n_updates            | 14700      |
|    policy_gradient_loss | -0.0422    |
|    std                  | 0.242      |
|    value_loss           | 9.34       |
----------------------------------------
-----------------------------------------
| reward                  | 0.778       |
| reward_contact          | 0.0134      |
| reward_ctrl             | 0.103       |
| reward_motion           | 0.165       |
| reward_orientation      | 0.0451      |
| reward_position         | 0.000845    |
| reward_rotation         | 0.157       |
| reward_torque           | 0.0559      |
| reward_velocity         | 0.237       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 870         |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 737         |
|    time_elapsed         | 3051        |
|    total_timesteps      | 754688      |
| train/                  |             |
|    approx_kl            | 0.074587904 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.03        |
|    n_updates            | 14720       |
|    policy_gradient_loss | -0.0596     |
|    std                  | 0.242       |
|    value_loss           | 8.61        |
-----------------------------------------
----------------------------------------
| reward                  | 0.779      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.238      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 738        |
|    time_elapsed         | 3054       |
|    total_timesteps      | 755712     |
| train/                  |            |
|    approx_kl            | 0.07894513 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.43       |
|    n_updates            | 14740      |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.242      |
|    value_loss           | 5.45       |
----------------------------------------
Num timesteps: 756000
Best mean reward: 875.26 - Last mean reward per episode: 870.94
----------------------------------------
| reward                  | 0.775      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0451     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.237      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 739        |
|    time_elapsed         | 3057       |
|    total_timesteps      | 756736     |
| train/                  |            |
|    approx_kl            | 0.14101674 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.0003     |
|    loss                 | 46.2       |
|    n_updates            | 14760      |
|    policy_gradient_loss | -0.0231    |
|    std                  | 0.242      |
|    value_loss           | 29.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.776     |
| reward_contact          | 0.014     |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.165     |
| reward_orientation      | 0.0453    |
| reward_position         | 0.000845  |
| reward_rotation         | 0.154     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.237     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 871       |
| time/                   |           |
|    fps                  | 247       |
|    iterations           | 740       |
|    time_elapsed         | 3061      |
|    total_timesteps      | 757760    |
| train/                  |           |
|    approx_kl            | 1.0986694 |
|    clip_fraction        | 0.35      |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.7     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.298     |
|    n_updates            | 14780     |
|    policy_gradient_loss | -0.03     |
|    std                  | 0.242     |
|    value_loss           | 2.03      |
---------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0456     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 741        |
|    time_elapsed         | 3064       |
|    total_timesteps      | 758784     |
| train/                  |            |
|    approx_kl            | 0.19613129 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.405      |
|    n_updates            | 14800      |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.242      |
|    value_loss           | 1.96       |
----------------------------------------
--------------------------------------
| reward                  | 0.772    |
| reward_contact          | 0.014    |
| reward_ctrl             | 0.103    |
| reward_motion           | 0.165    |
| reward_orientation      | 0.0459   |
| reward_position         | 0.000845 |
| reward_rotation         | 0.152    |
| reward_torque           | 0.056    |
| reward_velocity         | 0.235    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 871      |
| time/                   |          |
|    fps                  | 247      |
|    iterations           | 742      |
|    time_elapsed         | 3068     |
|    total_timesteps      | 759808   |
| train/                  |          |
|    approx_kl            | 2.586573 |
|    clip_fraction        | 0.485    |
|    clip_range           | 0.4      |
|    entropy_loss         | -45.1    |
|    explained_variance   | 0.991    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.45     |
|    n_updates            | 14820    |
|    policy_gradient_loss | 0.0427   |
|    std                  | 0.242    |
|    value_loss           | 2.03     |
--------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000845   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 743        |
|    time_elapsed         | 3071       |
|    total_timesteps      | 760832     |
| train/                  |            |
|    approx_kl            | 0.14301135 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.59       |
|    n_updates            | 14840      |
|    policy_gradient_loss | -0.0373    |
|    std                  | 0.242      |
|    value_loss           | 2.71       |
----------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000845   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 744        |
|    time_elapsed         | 3074       |
|    total_timesteps      | 761856     |
| train/                  |            |
|    approx_kl            | 0.13127474 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.308      |
|    n_updates            | 14860      |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.242      |
|    value_loss           | 2.83       |
----------------------------------------
Num timesteps: 762000
Best mean reward: 875.26 - Last mean reward per episode: 870.80
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0457     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 745        |
|    time_elapsed         | 3078       |
|    total_timesteps      | 762880     |
| train/                  |            |
|    approx_kl            | 0.04631891 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.32       |
|    n_updates            | 14880      |
|    policy_gradient_loss | -0.0359    |
|    std                  | 0.242      |
|    value_loss           | 36.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 746        |
|    time_elapsed         | 3081       |
|    total_timesteps      | 763904     |
| train/                  |            |
|    approx_kl            | 0.42890948 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.6      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.599      |
|    n_updates            | 14900      |
|    policy_gradient_loss | -0.0758    |
|    std                  | 0.242      |
|    value_loss           | 1.74       |
----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0458     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 747        |
|    time_elapsed         | 3084       |
|    total_timesteps      | 764928     |
| train/                  |            |
|    approx_kl            | 0.07606626 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.21       |
|    n_updates            | 14920      |
|    policy_gradient_loss | -0.048     |
|    std                  | 0.242      |
|    value_loss           | 21.3       |
----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0146     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.168      |
| reward_orientation      | 0.046      |
| reward_position         | 0.000845   |
| reward_rotation         | 0.149      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 748        |
|    time_elapsed         | 3088       |
|    total_timesteps      | 765952     |
| train/                  |            |
|    approx_kl            | 0.04222019 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 14940      |
|    policy_gradient_loss | -0.0439    |
|    std                  | 0.242      |
|    value_loss           | 18.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0461     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 749        |
|    time_elapsed         | 3091       |
|    total_timesteps      | 766976     |
| train/                  |            |
|    approx_kl            | 0.07700365 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.48       |
|    n_updates            | 14960      |
|    policy_gradient_loss | -0.0512    |
|    std                  | 0.242      |
|    value_loss           | 8.22       |
----------------------------------------
Num timesteps: 768000
Best mean reward: 875.26 - Last mean reward per episode: 871.34
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.014      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0461     |
| reward_position         | 0.000845   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 750        |
|    time_elapsed         | 3094       |
|    total_timesteps      | 768000     |
| train/                  |            |
|    approx_kl            | 0.15094067 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.675      |
|    n_updates            | 14980      |
|    policy_gradient_loss | -0.0809    |
|    std                  | 0.242      |
|    value_loss           | 2.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.771      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0462     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 751        |
|    time_elapsed         | 3098       |
|    total_timesteps      | 769024     |
| train/                  |            |
|    approx_kl            | 0.06027574 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.562      |
|    n_updates            | 15000      |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.242      |
|    value_loss           | 9.98       |
----------------------------------------
---------------------------------------
| reward                  | 0.772     |
| reward_contact          | 0.0134    |
| reward_ctrl             | 0.105     |
| reward_motion           | 0.164     |
| reward_orientation      | 0.0465    |
| reward_position         | 0.000844  |
| reward_rotation         | 0.153     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 871       |
| time/                   |           |
|    fps                  | 248       |
|    iterations           | 752       |
|    time_elapsed         | 3101      |
|    total_timesteps      | 770048    |
| train/                  |           |
|    approx_kl            | 0.8857931 |
|    clip_fraction        | 0.408     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.7     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.425     |
|    n_updates            | 15020     |
|    policy_gradient_loss | -0.077    |
|    std                  | 0.242     |
|    value_loss           | 1.63      |
---------------------------------------
----------------------------------------
| reward                  | 0.772      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0464     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 872        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 753        |
|    time_elapsed         | 3104       |
|    total_timesteps      | 771072     |
| train/                  |            |
|    approx_kl            | 0.09720527 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.62       |
|    n_updates            | 15040      |
|    policy_gradient_loss | -0.0485    |
|    std                  | 0.242      |
|    value_loss           | 19.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.765      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.158      |
| reward_orientation      | 0.0468     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 754        |
|    time_elapsed         | 3108       |
|    total_timesteps      | 772096     |
| train/                  |            |
|    approx_kl            | 0.09303187 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.853      |
|    n_updates            | 15060      |
|    policy_gradient_loss | -0.0788    |
|    std                  | 0.242      |
|    value_loss           | 4.62       |
----------------------------------------
---------------------------------------
| reward                  | 0.766     |
| reward_contact          | 0.0137    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.159     |
| reward_orientation      | 0.0471    |
| reward_position         | 0.000844  |
| reward_rotation         | 0.151     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 873       |
| time/                   |           |
|    fps                  | 248       |
|    iterations           | 755       |
|    time_elapsed         | 3111      |
|    total_timesteps      | 773120    |
| train/                  |           |
|    approx_kl            | 0.1441721 |
|    clip_fraction        | 0.332     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.4     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.6       |
|    n_updates            | 15080     |
|    policy_gradient_loss | -0.0835   |
|    std                  | 0.242     |
|    value_loss           | 2.14      |
---------------------------------------
Num timesteps: 774000
Best mean reward: 875.26 - Last mean reward per episode: 873.25
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0471     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 756        |
|    time_elapsed         | 3115       |
|    total_timesteps      | 774144     |
| train/                  |            |
|    approx_kl            | 0.07937231 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.77       |
|    n_updates            | 15100      |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.242      |
|    value_loss           | 8.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.768      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.159      |
| reward_orientation      | 0.047      |
| reward_position         | 0.000844   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 757        |
|    time_elapsed         | 3118       |
|    total_timesteps      | 775168     |
| train/                  |            |
|    approx_kl            | 0.15250716 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.485      |
|    n_updates            | 15120      |
|    policy_gradient_loss | -0.0614    |
|    std                  | 0.242      |
|    value_loss           | 1.75       |
----------------------------------------
-----------------------------------------
| reward                  | 0.774       |
| reward_contact          | 0.0137      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.16        |
| reward_orientation      | 0.0472      |
| reward_position         | 0.000844    |
| reward_rotation         | 0.154       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.236       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 875         |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 758         |
|    time_elapsed         | 3121        |
|    total_timesteps      | 776192      |
| train/                  |             |
|    approx_kl            | 0.091618985 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.662       |
|    n_updates            | 15140       |
|    policy_gradient_loss | -0.0458     |
|    std                  | 0.242       |
|    value_loss           | 8.63        |
-----------------------------------------
----------------------------------------
| reward                  | 0.77       |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.153      |
| reward_orientation      | 0.0475     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 759        |
|    time_elapsed         | 3125       |
|    total_timesteps      | 777216     |
| train/                  |            |
|    approx_kl            | 0.21888998 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 15160      |
|    policy_gradient_loss | 0.0342     |
|    std                  | 0.242      |
|    value_loss           | 2.58       |
----------------------------------------
----------------------------------------
| reward                  | 0.764      |
| reward_contact          | 0.0131     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.148      |
| reward_orientation      | 0.0477     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 760        |
|    time_elapsed         | 3128       |
|    total_timesteps      | 778240     |
| train/                  |            |
|    approx_kl            | 0.30100137 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.153      |
|    n_updates            | 15180      |
|    policy_gradient_loss | -0.0196    |
|    std                  | 0.242      |
|    value_loss           | 1.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.767      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.152      |
| reward_orientation      | 0.0476     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 761        |
|    time_elapsed         | 3131       |
|    total_timesteps      | 779264     |
| train/                  |            |
|    approx_kl            | 0.07321717 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.4       |
|    n_updates            | 15200      |
|    policy_gradient_loss | -0.0326    |
|    std                  | 0.242      |
|    value_loss           | 31.2       |
----------------------------------------
Num timesteps: 780000
Best mean reward: 875.26 - Last mean reward per episode: 875.90
Saving new best model to rl/out_dir/models/exp71/best_model.zip
-----------------------------------------
| reward                  | 0.768       |
| reward_contact          | 0.0143      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.155       |
| reward_orientation      | 0.0475      |
| reward_position         | 0.000844    |
| reward_rotation         | 0.154       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 876         |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 762         |
|    time_elapsed         | 3135        |
|    total_timesteps      | 780288      |
| train/                  |             |
|    approx_kl            | 0.107026644 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.303       |
|    n_updates            | 15220       |
|    policy_gradient_loss | -0.0863     |
|    std                  | 0.242       |
|    value_loss           | 1.6         |
-----------------------------------------
----------------------------------------
| reward                  | 0.764      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0475     |
| reward_position         | 0.000844   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 763        |
|    time_elapsed         | 3138       |
|    total_timesteps      | 781312     |
| train/                  |            |
|    approx_kl            | 0.27922195 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.144      |
|    n_updates            | 15240      |
|    policy_gradient_loss | -0.0499    |
|    std                  | 0.242      |
|    value_loss           | 1.26       |
----------------------------------------
----------------------------------------
| reward                  | 0.763      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.154      |
| reward_orientation      | 0.0479     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 764        |
|    time_elapsed         | 3142       |
|    total_timesteps      | 782336     |
| train/                  |            |
|    approx_kl            | 0.67166805 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.5        |
|    n_updates            | 15260      |
|    policy_gradient_loss | -0.0795    |
|    std                  | 0.242      |
|    value_loss           | 1.68       |
----------------------------------------
-----------------------------------------
| reward                  | 0.762       |
| reward_contact          | 0.0143      |
| reward_ctrl             | 0.105       |
| reward_motion           | 0.152       |
| reward_orientation      | 0.0481      |
| reward_position         | 0.000839    |
| reward_rotation         | 0.153       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.233       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 876         |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 765         |
|    time_elapsed         | 3145        |
|    total_timesteps      | 783360      |
| train/                  |             |
|    approx_kl            | 0.105070874 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47        |
|    n_updates            | 15280       |
|    policy_gradient_loss | -0.0485     |
|    std                  | 0.242       |
|    value_loss           | 3.6         |
-----------------------------------------
----------------------------------------
| reward                  | 0.769      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.157      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 766        |
|    time_elapsed         | 3148       |
|    total_timesteps      | 784384     |
| train/                  |            |
|    approx_kl            | 0.09015407 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 15300      |
|    policy_gradient_loss | -0.0411    |
|    std                  | 0.242      |
|    value_loss           | 6.9        |
----------------------------------------
---------------------------------------
| reward                  | 0.772     |
| reward_contact          | 0.0149    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.161     |
| reward_orientation      | 0.048     |
| reward_position         | 0.000839  |
| reward_rotation         | 0.155     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 876       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 767       |
|    time_elapsed         | 3152      |
|    total_timesteps      | 785408    |
| train/                  |           |
|    approx_kl            | 0.5260749 |
|    clip_fraction        | 0.441     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.123     |
|    n_updates            | 15320     |
|    policy_gradient_loss | -0.114    |
|    std                  | 0.242     |
|    value_loss           | 1.31      |
---------------------------------------
Num timesteps: 786000
Best mean reward: 875.90 - Last mean reward per episode: 876.12
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.778      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.162      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 878        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 768        |
|    time_elapsed         | 3155       |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.15284017 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 15340      |
|    policy_gradient_loss | -0.0503    |
|    std                  | 0.242      |
|    value_loss           | 3.83       |
----------------------------------------
----------------------------------------
| reward                  | 0.779      |
| reward_contact          | 0.0149     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.161      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 880        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 769        |
|    time_elapsed         | 3159       |
|    total_timesteps      | 787456     |
| train/                  |            |
|    approx_kl            | 0.44544396 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.578      |
|    n_updates            | 15360      |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.242      |
|    value_loss           | 1.83       |
----------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.163      |
| reward_orientation      | 0.0478     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 879        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 770        |
|    time_elapsed         | 3162       |
|    total_timesteps      | 788480     |
| train/                  |            |
|    approx_kl            | 0.09110832 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 15380      |
|    policy_gradient_loss | -0.0604    |
|    std                  | 0.241      |
|    value_loss           | 10.4       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.015     |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.0477    |
| reward_position         | 0.000839  |
| reward_rotation         | 0.157     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 880       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 771       |
|    time_elapsed         | 3165      |
|    total_timesteps      | 789504    |
| train/                  |           |
|    approx_kl            | 0.1663746 |
|    clip_fraction        | 0.336     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.417     |
|    n_updates            | 15400     |
|    policy_gradient_loss | -0.0714   |
|    std                  | 0.241     |
|    value_loss           | 1.47      |
---------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.015     |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.162     |
| reward_orientation      | 0.048     |
| reward_position         | 0.000839  |
| reward_rotation         | 0.157     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.235     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 879       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 772       |
|    time_elapsed         | 3169      |
|    total_timesteps      | 790528    |
| train/                  |           |
|    approx_kl            | 0.1116617 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.41      |
|    n_updates            | 15420     |
|    policy_gradient_loss | -0.0466   |
|    std                  | 0.241     |
|    value_loss           | 3.36      |
---------------------------------------
----------------------------------------
| reward                  | 0.779      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.164      |
| reward_orientation      | 0.0476     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 879        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 773        |
|    time_elapsed         | 3172       |
|    total_timesteps      | 791552     |
| train/                  |            |
|    approx_kl            | 0.33554947 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.42       |
|    n_updates            | 15440      |
|    policy_gradient_loss | -0.00407   |
|    std                  | 0.241      |
|    value_loss           | 4.26       |
----------------------------------------
Num timesteps: 792000
Best mean reward: 876.12 - Last mean reward per episode: 879.19
Saving new best model to rl/out_dir/models/exp71/best_model.zip
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.165      |
| reward_orientation      | 0.0478     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.236      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 880        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 774        |
|    time_elapsed         | 3175       |
|    total_timesteps      | 792576     |
| train/                  |            |
|    approx_kl            | 0.18222383 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.659      |
|    n_updates            | 15460      |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.241      |
|    value_loss           | 1.58       |
----------------------------------------
----------------------------------------
| reward                  | 0.784      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.167      |
| reward_orientation      | 0.0479     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.235      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 879        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 775        |
|    time_elapsed         | 3179       |
|    total_timesteps      | 793600     |
| train/                  |            |
|    approx_kl            | 0.05879277 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.55       |
|    n_updates            | 15480      |
|    policy_gradient_loss | -0.0407    |
|    std                  | 0.241      |
|    value_loss           | 15.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.778      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.166      |
| reward_orientation      | 0.0481     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 880        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 776        |
|    time_elapsed         | 3182       |
|    total_timesteps      | 794624     |
| train/                  |            |
|    approx_kl            | 0.11750856 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.162      |
|    n_updates            | 15500      |
|    policy_gradient_loss | -0.0652    |
|    std                  | 0.241      |
|    value_loss           | 1.53       |
----------------------------------------
---------------------------------------
| reward                  | 0.778     |
| reward_contact          | 0.0156    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.165     |
| reward_orientation      | 0.048     |
| reward_position         | 0.000839  |
| reward_rotation         | 0.155     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 880       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 777       |
|    time_elapsed         | 3185      |
|    total_timesteps      | 795648    |
| train/                  |           |
|    approx_kl            | 1.0835961 |
|    clip_fraction        | 0.514     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.4     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.43      |
|    n_updates            | 15520     |
|    policy_gradient_loss | -0.0457   |
|    std                  | 0.241     |
|    value_loss           | 1.79      |
---------------------------------------
---------------------------------------
| reward                  | 0.772     |
| reward_contact          | 0.0162    |
| reward_ctrl             | 0.103     |
| reward_motion           | 0.161     |
| reward_orientation      | 0.0481    |
| reward_position         | 0.000839  |
| reward_rotation         | 0.155     |
| reward_torque           | 0.056     |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 879       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 778       |
|    time_elapsed         | 3189      |
|    total_timesteps      | 796672    |
| train/                  |           |
|    approx_kl            | 0.1067452 |
|    clip_fraction        | 0.293     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.7     |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 15540     |
|    policy_gradient_loss | -0.0482   |
|    std                  | 0.241     |
|    value_loss           | 7.06      |
---------------------------------------
---------------------------------------
| reward                  | 0.769     |
| reward_contact          | 0.0156    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.158     |
| reward_orientation      | 0.0479    |
| reward_position         | 0.000839  |
| reward_rotation         | 0.155     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 879       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 779       |
|    time_elapsed         | 3192      |
|    total_timesteps      | 797696    |
| train/                  |           |
|    approx_kl            | 0.3297547 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.406     |
|    n_updates            | 15560     |
|    policy_gradient_loss | -0.0924   |
|    std                  | 0.241     |
|    value_loss           | 2.13      |
---------------------------------------
Num timesteps: 798000
Best mean reward: 879.19 - Last mean reward per episode: 878.82
----------------------------------------
| reward                  | 0.768      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.156      |
| reward_orientation      | 0.0482     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 879        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 780        |
|    time_elapsed         | 3195       |
|    total_timesteps      | 798720     |
| train/                  |            |
|    approx_kl            | 0.09220305 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.89       |
|    n_updates            | 15580      |
|    policy_gradient_loss | -0.00738   |
|    std                  | 0.241      |
|    value_loss           | 14.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.768     |
| reward_contact          | 0.0156    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.156     |
| reward_orientation      | 0.0483    |
| reward_position         | 0.000839  |
| reward_rotation         | 0.156     |
| reward_torque           | 0.0561    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 878       |
| time/                   |           |
|    fps                  | 249       |
|    iterations           | 781       |
|    time_elapsed         | 3199      |
|    total_timesteps      | 799744    |
| train/                  |           |
|    approx_kl            | 3.9597626 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.7     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.496     |
|    n_updates            | 15600     |
|    policy_gradient_loss | -0.0308   |
|    std                  | 0.241     |
|    value_loss           | 1.33      |
---------------------------------------
----------------------------------------
| reward                  | 0.761      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.151      |
| reward_orientation      | 0.0485     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 878        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 782        |
|    time_elapsed         | 3202       |
|    total_timesteps      | 800768     |
| train/                  |            |
|    approx_kl            | 0.29178438 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.561      |
|    n_updates            | 15620      |
|    policy_gradient_loss | -0.0455    |
|    std                  | 0.241      |
|    value_loss           | 1.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.757      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0488     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 783        |
|    time_elapsed         | 3206       |
|    total_timesteps      | 801792     |
| train/                  |            |
|    approx_kl            | 0.09634355 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.625      |
|    n_updates            | 15640      |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.241      |
|    value_loss           | 4.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0488     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 784        |
|    time_elapsed         | 3209       |
|    total_timesteps      | 802816     |
| train/                  |            |
|    approx_kl            | 0.15117246 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.16       |
|    n_updates            | 15660      |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.241      |
|    value_loss           | 1.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0156     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0489     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 878        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 785        |
|    time_elapsed         | 3212       |
|    total_timesteps      | 803840     |
| train/                  |            |
|    approx_kl            | 0.09309214 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.79       |
|    n_updates            | 15680      |
|    policy_gradient_loss | -0.0531    |
|    std                  | 0.241      |
|    value_loss           | 3.69       |
----------------------------------------
Num timesteps: 804000
Best mean reward: 879.19 - Last mean reward per episode: 877.53
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0489     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 786        |
|    time_elapsed         | 3216       |
|    total_timesteps      | 804864     |
| train/                  |            |
|    approx_kl            | 0.16088974 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.459      |
|    n_updates            | 15700      |
|    policy_gradient_loss | -0.0667    |
|    std                  | 0.241      |
|    value_loss           | 1.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0491     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 787        |
|    time_elapsed         | 3219       |
|    total_timesteps      | 805888     |
| train/                  |            |
|    approx_kl            | 0.05434919 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.93       |
|    n_updates            | 15720      |
|    policy_gradient_loss | -0.0558    |
|    std                  | 0.241      |
|    value_loss           | 8.88       |
----------------------------------------
---------------------------------------
| reward                  | 0.749     |
| reward_contact          | 0.015     |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.141     |
| reward_orientation      | 0.049     |
| reward_position         | 0.000839  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.229     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 878       |
| time/                   |           |
|    fps                  | 250       |
|    iterations           | 788       |
|    time_elapsed         | 3222      |
|    total_timesteps      | 806912    |
| train/                  |           |
|    approx_kl            | 0.0882151 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.3       |
|    n_updates            | 15740     |
|    policy_gradient_loss | -0.034    |
|    std                  | 0.241     |
|    value_loss           | 11.7      |
---------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.015      |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.142      |
| reward_orientation      | 0.0491     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 879        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 789        |
|    time_elapsed         | 3226       |
|    total_timesteps      | 807936     |
| train/                  |            |
|    approx_kl            | 0.08621861 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.55       |
|    n_updates            | 15760      |
|    policy_gradient_loss | -0.0457    |
|    std                  | 0.241      |
|    value_loss           | 15.9       |
----------------------------------------
-----------------------------------------
| reward                  | 0.747       |
| reward_contact          | 0.0144      |
| reward_ctrl             | 0.106       |
| reward_motion           | 0.139       |
| reward_orientation      | 0.0489      |
| reward_position         | 0.000839    |
| reward_rotation         | 0.153       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 878         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 790         |
|    time_elapsed         | 3229        |
|    total_timesteps      | 808960      |
| train/                  |             |
|    approx_kl            | 0.103717655 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.24        |
|    n_updates            | 15780       |
|    policy_gradient_loss | -0.0681     |
|    std                  | 0.241       |
|    value_loss           | 4.13        |
-----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0493     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 878        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 791        |
|    time_elapsed         | 3232       |
|    total_timesteps      | 809984     |
| train/                  |            |
|    approx_kl            | 0.11536622 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.78       |
|    n_updates            | 15800      |
|    policy_gradient_loss | -0.0666    |
|    std                  | 0.241      |
|    value_loss           | 4.15       |
----------------------------------------
Num timesteps: 810000
Best mean reward: 879.19 - Last mean reward per episode: 878.21
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0493     |
| reward_position         | 0.000839   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 792        |
|    time_elapsed         | 3236       |
|    total_timesteps      | 811008     |
| train/                  |            |
|    approx_kl            | 0.44180214 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.39       |
|    n_updates            | 15820      |
|    policy_gradient_loss | -0.0265    |
|    std                  | 0.241      |
|    value_loss           | 1.11       |
----------------------------------------
----------------------------------------
| reward                  | 0.743      |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.049      |
| reward_position         | 0.000839   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 877        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 793        |
|    time_elapsed         | 3239       |
|    total_timesteps      | 812032     |
| train/                  |            |
|    approx_kl            | 0.16540529 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.4        |
|    entropy_loss         | -45        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.429      |
|    n_updates            | 15840      |
|    policy_gradient_loss | -0.0636    |
|    std                  | 0.241      |
|    value_loss           | 2.41       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 876        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 794        |
|    time_elapsed         | 3243       |
|    total_timesteps      | 813056     |
| train/                  |            |
|    approx_kl            | 0.12522605 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 15860      |
|    policy_gradient_loss | -0.0495    |
|    std                  | 0.241      |
|    value_loss           | 2.01       |
----------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0138     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0494     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 795        |
|    time_elapsed         | 3246       |
|    total_timesteps      | 814080     |
| train/                  |            |
|    approx_kl            | 0.20419358 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 15880      |
|    policy_gradient_loss | -0.0854    |
|    std                  | 0.241      |
|    value_loss           | 2.92       |
----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0136     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.132      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 875        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 796        |
|    time_elapsed         | 3249       |
|    total_timesteps      | 815104     |
| train/                  |            |
|    approx_kl            | 0.20573717 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.466      |
|    n_updates            | 15900      |
|    policy_gradient_loss | 0.00641    |
|    std                  | 0.241      |
|    value_loss           | 4.95       |
----------------------------------------
Num timesteps: 816000
Best mean reward: 879.19 - Last mean reward per episode: 873.68
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0492     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 797        |
|    time_elapsed         | 3253       |
|    total_timesteps      | 816128     |
| train/                  |            |
|    approx_kl            | 0.25232887 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.402      |
|    n_updates            | 15920      |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.241      |
|    value_loss           | 1.68       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0491     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 798        |
|    time_elapsed         | 3256       |
|    total_timesteps      | 817152     |
| train/                  |            |
|    approx_kl            | 0.33129087 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 15940      |
|    policy_gradient_loss | -0.0889    |
|    std                  | 0.241      |
|    value_loss           | 1.56       |
----------------------------------------
-----------------------------------------
| reward                  | 0.751       |
| reward_contact          | 0.0134      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.135       |
| reward_orientation      | 0.0491      |
| reward_position         | 1.25e-05    |
| reward_rotation         | 0.158       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.231       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 799         |
|    time_elapsed         | 3259        |
|    total_timesteps      | 818176      |
| train/                  |             |
|    approx_kl            | 0.068436146 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.32        |
|    n_updates            | 15960       |
|    policy_gradient_loss | -0.0495     |
|    std                  | 0.241       |
|    value_loss           | 11.7        |
-----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.049      |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 874        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 800        |
|    time_elapsed         | 3263       |
|    total_timesteps      | 819200     |
| train/                  |            |
|    approx_kl            | 0.10253744 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.583      |
|    n_updates            | 15980      |
|    policy_gradient_loss | -0.0613    |
|    std                  | 0.241      |
|    value_loss           | 2.47       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0489     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 801        |
|    time_elapsed         | 3266       |
|    total_timesteps      | 820224     |
| train/                  |            |
|    approx_kl            | 0.08340804 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 16000      |
|    policy_gradient_loss | -0.0406    |
|    std                  | 0.241      |
|    value_loss           | 5.15       |
----------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0134     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.135      |
| reward_orientation      | 0.0487     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 873        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 802        |
|    time_elapsed         | 3269       |
|    total_timesteps      | 821248     |
| train/                  |            |
|    approx_kl            | 0.22470854 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.792      |
|    n_updates            | 16020      |
|    policy_gradient_loss | -0.0471    |
|    std                  | 0.241      |
|    value_loss           | 3.34       |
----------------------------------------
Num timesteps: 822000
Best mean reward: 879.19 - Last mean reward per episode: 872.11
-----------------------------------------
| reward                  | 0.749       |
| reward_contact          | 0.0128      |
| reward_ctrl             | 0.109       |
| reward_motion           | 0.131       |
| reward_orientation      | 0.0485      |
| reward_position         | 1.25e-05    |
| reward_rotation         | 0.16        |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.231       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 872         |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 803         |
|    time_elapsed         | 3273        |
|    total_timesteps      | 822272      |
| train/                  |             |
|    approx_kl            | 0.084906936 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | 6           |
|    n_updates            | 16040       |
|    policy_gradient_loss | -0.0537     |
|    std                  | 0.241       |
|    value_loss           | 10.9        |
-----------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0485     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.16       |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 872        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 804        |
|    time_elapsed         | 3276       |
|    total_timesteps      | 823296     |
| train/                  |            |
|    approx_kl            | 0.08426065 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.5       |
|    n_updates            | 16060      |
|    policy_gradient_loss | -0.0531    |
|    std                  | 0.241      |
|    value_loss           | 12.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.751      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.129      |
| reward_orientation      | 0.0485     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0565     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 805        |
|    time_elapsed         | 3279       |
|    total_timesteps      | 824320     |
| train/                  |            |
|    approx_kl            | 0.13629754 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 16080      |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.241      |
|    value_loss           | 3.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.129      |
| reward_orientation      | 0.0483     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 806        |
|    time_elapsed         | 3283       |
|    total_timesteps      | 825344     |
| train/                  |            |
|    approx_kl            | 0.34410712 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.475      |
|    n_updates            | 16100      |
|    policy_gradient_loss | -0.056     |
|    std                  | 0.241      |
|    value_loss           | 3.23       |
----------------------------------------
----------------------------------------
| reward                  | 0.75       |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0481     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 807        |
|    time_elapsed         | 3286       |
|    total_timesteps      | 826368     |
| train/                  |            |
|    approx_kl            | 0.26989037 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.585      |
|    n_updates            | 16120      |
|    policy_gradient_loss | -0.0822    |
|    std                  | 0.241      |
|    value_loss           | 1.64       |
----------------------------------------
---------------------------------------
| reward                  | 0.748     |
| reward_contact          | 0.0128    |
| reward_ctrl             | 0.112     |
| reward_motion           | 0.122     |
| reward_orientation      | 0.0482    |
| reward_position         | 1.25e-05  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0568    |
| reward_velocity         | 0.231     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 871       |
| time/                   |           |
|    fps                  | 251       |
|    iterations           | 808       |
|    time_elapsed         | 3290      |
|    total_timesteps      | 827392    |
| train/                  |           |
|    approx_kl            | 0.4597103 |
|    clip_fraction        | 0.508     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.392     |
|    n_updates            | 16140     |
|    policy_gradient_loss | -0.0774   |
|    std                  | 0.241     |
|    value_loss           | 1.45      |
---------------------------------------
Num timesteps: 828000
Best mean reward: 879.19 - Last mean reward per episode: 871.06
----------------------------------------
| reward                  | 0.746      |
| reward_contact          | 0.0128     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.048      |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 871        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 809        |
|    time_elapsed         | 3293       |
|    total_timesteps      | 828416     |
| train/                  |            |
|    approx_kl            | 0.18493669 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.676      |
|    n_updates            | 16160      |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.241      |
|    value_loss           | 5.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.743      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.113      |
| reward_orientation      | 0.0478     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 810        |
|    time_elapsed         | 3296       |
|    total_timesteps      | 829440     |
| train/                  |            |
|    approx_kl            | 0.15868175 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.764      |
|    n_updates            | 16180      |
|    policy_gradient_loss | -0.0338    |
|    std                  | 0.241      |
|    value_loss           | 2.45       |
----------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.11       |
| reward_orientation      | 0.0479     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 811        |
|    time_elapsed         | 3300       |
|    total_timesteps      | 830464     |
| train/                  |            |
|    approx_kl            | 0.17654175 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.416      |
|    n_updates            | 16200      |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.241      |
|    value_loss           | 2.1        |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.0127     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.108      |
| reward_orientation      | 0.0476     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 812        |
|    time_elapsed         | 3303       |
|    total_timesteps      | 831488     |
| train/                  |            |
|    approx_kl            | 0.17637458 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.6        |
|    n_updates            | 16220      |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.241      |
|    value_loss           | 2.85       |
----------------------------------------
-----------------------------------------
| reward                  | 0.734       |
| reward_contact          | 0.0127      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.109       |
| reward_orientation      | 0.0476      |
| reward_position         | 1.25e-05    |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0568      |
| reward_velocity         | 0.23        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 813         |
|    time_elapsed         | 3306        |
|    total_timesteps      | 832512      |
| train/                  |             |
|    approx_kl            | 0.073015675 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 16240       |
|    policy_gradient_loss | -0.058      |
|    std                  | 0.241       |
|    value_loss           | 9.61        |
-----------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0121     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0477     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 814        |
|    time_elapsed         | 3310       |
|    total_timesteps      | 833536     |
| train/                  |            |
|    approx_kl            | 0.20161778 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.304      |
|    n_updates            | 16260      |
|    policy_gradient_loss | -0.0615    |
|    std                  | 0.241      |
|    value_loss           | 3.09       |
----------------------------------------
Num timesteps: 834000
Best mean reward: 879.19 - Last mean reward per episode: 869.05
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.107      |
| reward_orientation      | 0.0474     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 815        |
|    time_elapsed         | 3313       |
|    total_timesteps      | 834560     |
| train/                  |            |
|    approx_kl            | 0.25734472 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.578      |
|    n_updates            | 16280      |
|    policy_gradient_loss | -0.0805    |
|    std                  | 0.241      |
|    value_loss           | 1.92       |
----------------------------------------
---------------------------------------
| reward                  | 0.733     |
| reward_contact          | 0.0115    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.104     |
| reward_orientation      | 0.0474    |
| reward_position         | 1.25e-05  |
| reward_rotation         | 0.166     |
| reward_torque           | 0.0568    |
| reward_velocity         | 0.234     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 251       |
|    iterations           | 816       |
|    time_elapsed         | 3317      |
|    total_timesteps      | 835584    |
| train/                  |           |
|    approx_kl            | 0.4187126 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.159     |
|    n_updates            | 16300     |
|    policy_gradient_loss | -0.0735   |
|    std                  | 0.24      |
|    value_loss           | 1.74      |
---------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.109      |
| reward_orientation      | 0.0473     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 817        |
|    time_elapsed         | 3320       |
|    total_timesteps      | 836608     |
| train/                  |            |
|    approx_kl            | 0.23127255 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.503      |
|    n_updates            | 16320      |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.24       |
|    value_loss           | 2.24       |
----------------------------------------
-----------------------------------------
| reward                  | 0.739       |
| reward_contact          | 0.0115      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.109       |
| reward_orientation      | 0.0474      |
| reward_position         | 1.25e-05    |
| reward_rotation         | 0.167       |
| reward_torque           | 0.0568      |
| reward_velocity         | 0.234       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 869         |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 818         |
|    time_elapsed         | 3323        |
|    total_timesteps      | 837632      |
| train/                  |             |
|    approx_kl            | 0.065741464 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.23        |
|    n_updates            | 16340       |
|    policy_gradient_loss | -0.0699     |
|    std                  | 0.24        |
|    value_loss           | 7.36        |
-----------------------------------------
---------------------------------------
| reward                  | 0.741     |
| reward_contact          | 0.0115    |
| reward_ctrl             | 0.112     |
| reward_motion           | 0.114     |
| reward_orientation      | 0.0473    |
| reward_position         | 1.25e-05  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0567    |
| reward_velocity         | 0.233     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 869       |
| time/                   |           |
|    fps                  | 252       |
|    iterations           | 819       |
|    time_elapsed         | 3327      |
|    total_timesteps      | 838656    |
| train/                  |           |
|    approx_kl            | 0.0559251 |
|    clip_fraction        | 0.192     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.12      |
|    n_updates            | 16360     |
|    policy_gradient_loss | -0.0546   |
|    std                  | 0.24      |
|    value_loss           | 9.74      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0472     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 870        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 820        |
|    time_elapsed         | 3330       |
|    total_timesteps      | 839680     |
| train/                  |            |
|    approx_kl            | 0.18610634 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.855      |
|    n_updates            | 16380      |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.24       |
|    value_loss           | 4.37       |
----------------------------------------
Num timesteps: 840000
Best mean reward: 879.19 - Last mean reward per episode: 869.74
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0472     |
| reward_position         | 1.25e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 821        |
|    time_elapsed         | 3333       |
|    total_timesteps      | 840704     |
| train/                  |            |
|    approx_kl            | 0.15263003 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 16400      |
|    policy_gradient_loss | -0.0289    |
|    std                  | 0.24       |
|    value_loss           | 7.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0472     |
| reward_position         | 1.12e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0567     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 869        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 822        |
|    time_elapsed         | 3337       |
|    total_timesteps      | 841728     |
| train/                  |            |
|    approx_kl            | 0.12889558 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 16420      |
|    policy_gradient_loss | -0.053     |
|    std                  | 0.24       |
|    value_loss           | 5.72       |
----------------------------------------
-----------------------------------------
| reward                  | 0.74        |
| reward_contact          | 0.0109      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.115       |
| reward_orientation      | 0.0468      |
| reward_position         | 1.12e-05    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0568      |
| reward_velocity         | 0.233       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 823         |
|    time_elapsed         | 3340        |
|    total_timesteps      | 842752      |
| train/                  |             |
|    approx_kl            | 0.063300416 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 16440       |
|    policy_gradient_loss | -0.0425     |
|    std                  | 0.24        |
|    value_loss           | 21.8        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.743       |
| reward_contact          | 0.0115      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.117       |
| reward_orientation      | 0.0464      |
| reward_position         | 1.12e-05    |
| reward_rotation         | 0.165       |
| reward_torque           | 0.0568      |
| reward_velocity         | 0.233       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 868         |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 824         |
|    time_elapsed         | 3343        |
|    total_timesteps      | 843776      |
| train/                  |             |
|    approx_kl            | 0.071549326 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.93        |
|    n_updates            | 16460       |
|    policy_gradient_loss | -0.0626     |
|    std                  | 0.24        |
|    value_loss           | 18.1        |
-----------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0464     |
| reward_position         | 1.12e-05   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 868        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 825        |
|    time_elapsed         | 3347       |
|    total_timesteps      | 844800     |
| train/                  |            |
|    approx_kl            | 0.09160429 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.54       |
|    n_updates            | 16480      |
|    policy_gradient_loss | -0.0521    |
|    std                  | 0.24       |
|    value_loss           | 12.8       |
----------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.115      |
| reward_orientation      | 0.0463     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 826        |
|    time_elapsed         | 3350       |
|    total_timesteps      | 845824     |
| train/                  |            |
|    approx_kl            | 0.07619019 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.235      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.4       |
|    n_updates            | 16500      |
|    policy_gradient_loss | -0.0237    |
|    std                  | 0.24       |
|    value_loss           | 29.7       |
----------------------------------------
Num timesteps: 846000
Best mean reward: 879.19 - Last mean reward per episode: 867.03
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.115      |
| reward_orientation      | 0.0463     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.233      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 867        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 827        |
|    time_elapsed         | 3354       |
|    total_timesteps      | 846848     |
| train/                  |            |
|    approx_kl            | 0.32146016 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02       |
|    n_updates            | 16520      |
|    policy_gradient_loss | 0.0522     |
|    std                  | 0.24       |
|    value_loss           | 3.16       |
----------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | 0.0115    |
| reward_ctrl             | 0.114     |
| reward_motion           | 0.116     |
| reward_orientation      | 0.0461    |
| reward_position         | 1.87e-06  |
| reward_rotation         | 0.167     |
| reward_torque           | 0.0569    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 866       |
| time/                   |           |
|    fps                  | 252       |
|    iterations           | 828       |
|    time_elapsed         | 3357      |
|    total_timesteps      | 847872    |
| train/                  |           |
|    approx_kl            | 0.7866008 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.5     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.57      |
|    n_updates            | 16540     |
|    policy_gradient_loss | -0.0365   |
|    std                  | 0.241     |
|    value_loss           | 1.16      |
---------------------------------------
----------------------------------------
| reward                  | 0.742      |
| reward_contact          | 0.0121     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.115      |
| reward_orientation      | 0.0462     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 829        |
|    time_elapsed         | 3360       |
|    total_timesteps      | 848896     |
| train/                  |            |
|    approx_kl            | 0.18174385 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.11       |
|    n_updates            | 16560      |
|    policy_gradient_loss | -0.0667    |
|    std                  | 0.24       |
|    value_loss           | 3.99       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0459     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 830        |
|    time_elapsed         | 3364       |
|    total_timesteps      | 849920     |
| train/                  |            |
|    approx_kl            | 0.17735785 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.3      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.383      |
|    n_updates            | 16580      |
|    policy_gradient_loss | -0.0595    |
|    std                  | 0.24       |
|    value_loss           | 2.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0459     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 865        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 831        |
|    time_elapsed         | 3367       |
|    total_timesteps      | 850944     |
| train/                  |            |
|    approx_kl            | 0.21912554 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.659      |
|    n_updates            | 16600      |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.24       |
|    value_loss           | 2.89       |
----------------------------------------
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0456     |
| reward_position         | 1.87e-06   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 832        |
|    time_elapsed         | 3370       |
|    total_timesteps      | 851968     |
| train/                  |            |
|    approx_kl            | 0.41459358 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.63       |
|    n_updates            | 16620      |
|    policy_gradient_loss | -0.0524    |
|    std                  | 0.24       |
|    value_loss           | 1.88       |
----------------------------------------
Num timesteps: 852000
Best mean reward: 879.19 - Last mean reward per episode: 864.44
---------------------------------------
| reward                  | 0.747     |
| reward_contact          | 0.0115    |
| reward_ctrl             | 0.114     |
| reward_motion           | 0.122     |
| reward_orientation      | 0.0454    |
| reward_position         | 3.95e-13  |
| reward_rotation         | 0.166     |
| reward_torque           | 0.0568    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 863       |
| time/                   |           |
|    fps                  | 252       |
|    iterations           | 833       |
|    time_elapsed         | 3374      |
|    total_timesteps      | 852992    |
| train/                  |           |
|    approx_kl            | 0.0894974 |
|    clip_fraction        | 0.331     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.807     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.37      |
|    n_updates            | 16640     |
|    policy_gradient_loss | -0.0251   |
|    std                  | 0.24      |
|    value_loss           | 18.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.748      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.122      |
| reward_orientation      | 0.0454     |
| reward_position         | 3.95e-13   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 834        |
|    time_elapsed         | 3377       |
|    total_timesteps      | 854016     |
| train/                  |            |
|    approx_kl            | 0.10094504 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.26       |
|    n_updates            | 16660      |
|    policy_gradient_loss | -0.0394    |
|    std                  | 0.24       |
|    value_loss           | 17.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.747      |
| reward_contact          | 0.0109     |
| reward_ctrl             | 0.114      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0452     |
| reward_position         | 3.95e-13   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.231      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 835        |
|    time_elapsed         | 3380       |
|    total_timesteps      | 855040     |
| train/                  |            |
|    approx_kl            | 0.19189264 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.732      |
|    n_updates            | 16680      |
|    policy_gradient_loss | -0.0633    |
|    std                  | 0.24       |
|    value_loss           | 4.93       |
----------------------------------------
----------------------------------------
| reward                  | 0.749      |
| reward_contact          | 0.0115     |
| reward_ctrl             | 0.115      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0452     |
| reward_position         | 3.95e-13   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0569     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 862        |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 836        |
|    time_elapsed         | 3384       |
|    total_timesteps      | 856064     |
| train/                  |            |
|    approx_kl            | 0.16887201 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 16700      |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.24       |
|    value_loss           | 1.64       |
----------------------------------------
--------------------------------------
| reward                  | 0.754    |
| reward_contact          | 0.0121   |
| reward_ctrl             | 0.114    |
| reward_motion           | 0.123    |
| reward_orientation      | 0.0452   |
| reward_position         | 3.95e-13 |
| reward_rotation         | 0.17     |
| reward_torque           | 0.0568   |
| reward_velocity         | 0.233    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 861      |
| time/                   |          |
|    fps                  | 253      |
|    iterations           | 837      |
|    time_elapsed         | 3387     |
|    total_timesteps      | 857088   |
| train/                  |          |
|    approx_kl            | 0.224198 |
|    clip_fraction        | 0.34     |
|    clip_range           | 0.4      |
|    entropy_loss         | -44.1    |
|    explained_variance   | 0.994    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.508    |
|    n_updates            | 16720    |
|    policy_gradient_loss | -0.0852  |
|    std                  | 0.24     |
|    value_loss           | 1.85     |
--------------------------------------
Num timesteps: 858000
Best mean reward: 879.19 - Last mean reward per episode: 860.01
---------------------------------------
| reward                  | 0.748     |
| reward_contact          | 0.0121    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.12      |
| reward_orientation      | 0.0452    |
| reward_position         | 3.87e-13  |
| reward_rotation         | 0.169     |
| reward_torque           | 0.0568    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 860       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 838       |
|    time_elapsed         | 3390      |
|    total_timesteps      | 858112    |
| train/                  |           |
|    approx_kl            | 0.3708393 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.53      |
|    n_updates            | 16740     |
|    policy_gradient_loss | -0.0492   |
|    std                  | 0.24      |
|    value_loss           | 1.71      |
---------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0121     |
| reward_ctrl             | 0.113      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0451     |
| reward_position         | 3.87e-13   |
| reward_rotation         | 0.169      |
| reward_torque           | 0.0568     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 861        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 839        |
|    time_elapsed         | 3394       |
|    total_timesteps      | 859136     |
| train/                  |            |
|    approx_kl            | 0.12292674 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.5      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.249      |
|    n_updates            | 16760      |
|    policy_gradient_loss | -0.0461    |
|    std                  | 0.24       |
|    value_loss           | 2.07       |
----------------------------------------
-----------------------------------------
| reward                  | 0.754       |
| reward_contact          | 0.0121      |
| reward_ctrl             | 0.113       |
| reward_motion           | 0.125       |
| reward_orientation      | 0.045       |
| reward_position         | 3.87e-13    |
| reward_rotation         | 0.169       |
| reward_torque           | 0.0568      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 860         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 840         |
|    time_elapsed         | 3397        |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.050132614 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42        |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0454     |
|    std                  | 0.24        |
|    value_loss           | 14.7        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.753       |
| reward_contact          | 0.0127      |
| reward_ctrl             | 0.112       |
| reward_motion           | 0.125       |
| reward_orientation      | 0.0448      |
| reward_position         | 3.87e-13    |
| reward_rotation         | 0.17        |
| reward_torque           | 0.0566      |
| reward_velocity         | 0.232       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 860         |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 841         |
|    time_elapsed         | 3401        |
|    total_timesteps      | 861184      |
| train/                  |             |
|    approx_kl            | 0.104973555 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.4         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.36        |
|    n_updates            | 16800       |
|    policy_gradient_loss | -0.0651     |
|    std                  | 0.24        |
|    value_loss           | 4.67        |
-----------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0444     |
| reward_position         | 3.87e-13   |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 842        |
|    time_elapsed         | 3404       |
|    total_timesteps      | 862208     |
| train/                  |            |
|    approx_kl            | 0.22508593 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.459      |
|    n_updates            | 16820      |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.24       |
|    value_loss           | 1.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0441     |
| reward_position         | 3.87e-13   |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 859        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 843        |
|    time_elapsed         | 3407       |
|    total_timesteps      | 863232     |
| train/                  |            |
|    approx_kl            | 0.30870292 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.503      |
|    n_updates            | 16840      |
|    policy_gradient_loss | -0.0816    |
|    std                  | 0.24       |
|    value_loss           | 3.92       |
----------------------------------------
Num timesteps: 864000
Best mean reward: 879.19 - Last mean reward per episode: 858.20
----------------------------------------
| reward                  | 0.752      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0438     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.171      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.232      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 844        |
|    time_elapsed         | 3411       |
|    total_timesteps      | 864256     |
| train/                  |            |
|    approx_kl            | 0.18637718 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.42       |
|    n_updates            | 16860      |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.24       |
|    value_loss           | 1.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.762      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.13       |
| reward_orientation      | 0.0441     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 858        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 845        |
|    time_elapsed         | 3414       |
|    total_timesteps      | 865280     |
| train/                  |            |
|    approx_kl            | 0.08759187 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 16880      |
|    policy_gradient_loss | -0.0361    |
|    std                  | 0.24       |
|    value_loss           | 21         |
----------------------------------------
----------------------------------------
| reward                  | 0.759      |
| reward_contact          | 0.0133     |
| reward_ctrl             | 0.112      |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0437     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.173      |
| reward_torque           | 0.0566     |
| reward_velocity         | 0.234      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 857        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 846        |
|    time_elapsed         | 3417       |
|    total_timesteps      | 866304     |
| train/                  |            |
|    approx_kl            | 0.47770572 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.296      |
|    n_updates            | 16900      |
|    policy_gradient_loss | -0.0978    |
|    std                  | 0.24       |
|    value_loss           | 1.57       |
----------------------------------------
---------------------------------------
| reward                  | 0.759     |
| reward_contact          | 0.0133    |
| reward_ctrl             | 0.113     |
| reward_motion           | 0.126     |
| reward_orientation      | 0.0434    |
| reward_position         | 1.23e-11  |
| reward_rotation         | 0.174     |
| reward_torque           | 0.0566    |
| reward_velocity         | 0.233     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 856       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 847       |
|    time_elapsed         | 3421      |
|    total_timesteps      | 867328    |
| train/                  |           |
|    approx_kl            | 0.4408362 |
|    clip_fraction        | 0.512     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.8     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.147     |
|    n_updates            | 16920     |
|    policy_gradient_loss | -0.00179  |
|    std                  | 0.24      |
|    value_loss           | 1.69      |
---------------------------------------
---------------------------------------
| reward                  | 0.751     |
| reward_contact          | 0.0133    |
| reward_ctrl             | 0.112     |
| reward_motion           | 0.121     |
| reward_orientation      | 0.0433    |
| reward_position         | 1.23e-11  |
| reward_rotation         | 0.172     |
| reward_torque           | 0.0566    |
| reward_velocity         | 0.232     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 855       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 848       |
|    time_elapsed         | 3424      |
|    total_timesteps      | 868352    |
| train/                  |           |
|    approx_kl            | 1.1776141 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.5     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.278     |
|    n_updates            | 16940     |
|    policy_gradient_loss | -0.0947   |
|    std                  | 0.24      |
|    value_loss           | 1.41      |
---------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.0431     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.17       |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 855        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 849        |
|    time_elapsed         | 3428       |
|    total_timesteps      | 869376     |
| train/                  |            |
|    approx_kl            | 0.29471558 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.357      |
|    n_updates            | 16960      |
|    policy_gradient_loss | -0.0695    |
|    std                  | 0.24       |
|    value_loss           | 1.81       |
----------------------------------------
Num timesteps: 870000
Best mean reward: 879.19 - Last mean reward per episode: 854.64
----------------------------------------
| reward                  | 0.745      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.124      |
| reward_orientation      | 0.0432     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.168      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 850        |
|    time_elapsed         | 3431       |
|    total_timesteps      | 870400     |
| train/                  |            |
|    approx_kl            | 0.28142685 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.58       |
|    n_updates            | 16980      |
|    policy_gradient_loss | -0.0856    |
|    std                  | 0.24       |
|    value_loss           | 1.78       |
----------------------------------------
----------------------------------------
| reward                  | 0.739      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0428     |
| reward_position         | 1.23e-11   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 853        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 851        |
|    time_elapsed         | 3434       |
|    total_timesteps      | 871424     |
| train/                  |            |
|    approx_kl            | 0.17619362 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.518      |
|    n_updates            | 17000      |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.24       |
|    value_loss           | 2.08       |
----------------------------------------
---------------------------------------
| reward                  | 0.74      |
| reward_contact          | 0.0145    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.123     |
| reward_orientation      | 0.0428    |
| reward_position         | 1.23e-11  |
| reward_rotation         | 0.166     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 852       |
| time/                   |           |
|    fps                  | 253       |
|    iterations           | 852       |
|    time_elapsed         | 3438      |
|    total_timesteps      | 872448    |
| train/                  |           |
|    approx_kl            | 0.3915094 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.3     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.205     |
|    n_updates            | 17020     |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.24      |
|    value_loss           | 1.1       |
---------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0425     |
| reward_position         | 1.33e-11   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 852        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 853        |
|    time_elapsed         | 3441       |
|    total_timesteps      | 873472     |
| train/                  |            |
|    approx_kl            | 0.08215048 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 17040      |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.24       |
|    value_loss           | 8.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.738      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0423     |
| reward_position         | 1.33e-11   |
| reward_rotation         | 0.167      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 854        |
|    time_elapsed         | 3444       |
|    total_timesteps      | 874496     |
| train/                  |            |
|    approx_kl            | 0.04450639 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 17060      |
|    policy_gradient_loss | -0.0452    |
|    std                  | 0.24       |
|    value_loss           | 16.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.12       |
| reward_orientation      | 0.042      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 851        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 855        |
|    time_elapsed         | 3448       |
|    total_timesteps      | 875520     |
| train/                  |            |
|    approx_kl            | 0.23008339 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.69       |
|    n_updates            | 17080      |
|    policy_gradient_loss | -0.0807    |
|    std                  | 0.24       |
|    value_loss           | 7.02       |
----------------------------------------
Num timesteps: 876000
Best mean reward: 879.19 - Last mean reward per episode: 850.51
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0139     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.042      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.166      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 850        |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 856        |
|    time_elapsed         | 3451       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.37047204 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.72       |
|    n_updates            | 17100      |
|    policy_gradient_loss | -0.0338    |
|    std                  | 0.24       |
|    value_loss           | 26.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.729     |
| reward_contact          | 0.0139    |
| reward_ctrl             | 0.111     |
| reward_motion           | 0.113     |
| reward_orientation      | 0.0422    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0564    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 849       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 857       |
|    time_elapsed         | 3454      |
|    total_timesteps      | 877568    |
| train/                  |           |
|    approx_kl            | 0.7438823 |
|    clip_fraction        | 0.51      |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.586     |
|    n_updates            | 17120     |
|    policy_gradient_loss | -0.0611   |
|    std                  | 0.24      |
|    value_loss           | 2.21      |
---------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0421     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0564     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 849        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 858        |
|    time_elapsed         | 3458       |
|    total_timesteps      | 878592     |
| train/                  |            |
|    approx_kl            | 0.16038212 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.565      |
|    n_updates            | 17140      |
|    policy_gradient_loss | -0.0813    |
|    std                  | 0.24       |
|    value_loss           | 2.69       |
----------------------------------------
-----------------------------------------
| reward                  | 0.732       |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.111       |
| reward_motion           | 0.115       |
| reward_orientation      | 0.0422      |
| reward_position         | 1.81e-10    |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0564      |
| reward_velocity         | 0.229       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 848         |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 859         |
|    time_elapsed         | 3461        |
|    total_timesteps      | 879616      |
| train/                  |             |
|    approx_kl            | 0.076848894 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 17160       |
|    policy_gradient_loss | -0.0609     |
|    std                  | 0.24        |
|    value_loss           | 10.8        |
-----------------------------------------
---------------------------------------
| reward                  | 0.737     |
| reward_contact          | 0.0145    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.119     |
| reward_orientation      | 0.0422    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.165     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.23      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 848       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 860       |
|    time_elapsed         | 3465      |
|    total_timesteps      | 880640    |
| train/                  |           |
|    approx_kl            | 13.157052 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.2     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.358     |
|    n_updates            | 17180     |
|    policy_gradient_loss | 0.174     |
|    std                  | 0.24      |
|    value_loss           | 1.1       |
---------------------------------------
----------------------------------------
| reward                  | 0.737      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.119      |
| reward_orientation      | 0.0422     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.23       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 861        |
|    time_elapsed         | 3468       |
|    total_timesteps      | 881664     |
| train/                  |            |
|    approx_kl            | 0.07922312 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.99       |
|    n_updates            | 17200      |
|    policy_gradient_loss | -0.0352    |
|    std                  | 0.24       |
|    value_loss           | 29.1       |
----------------------------------------
Num timesteps: 882000
Best mean reward: 879.19 - Last mean reward per episode: 847.22
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0424     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.165      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.229      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 847        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 862        |
|    time_elapsed         | 3471       |
|    total_timesteps      | 882688     |
| train/                  |            |
|    approx_kl            | 0.24125054 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.673      |
|    n_updates            | 17220      |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.24       |
|    value_loss           | 1.79       |
----------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.114      |
| reward_orientation      | 0.0427     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.164      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.228      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 846        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 863        |
|    time_elapsed         | 3475       |
|    total_timesteps      | 883712     |
| train/                  |            |
|    approx_kl            | 0.08188973 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.41       |
|    n_updates            | 17240      |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.24       |
|    value_loss           | 13.3       |
----------------------------------------
-----------------------------------------
| reward                  | 0.733       |
| reward_contact          | 0.0143      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.12        |
| reward_orientation      | 0.0425      |
| reward_position         | 1.81e-10    |
| reward_rotation         | 0.164       |
| reward_torque           | 0.0562      |
| reward_velocity         | 0.227       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 844         |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 864         |
|    time_elapsed         | 3478        |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.118576325 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 17260       |
|    policy_gradient_loss | -0.091      |
|    std                  | 0.24        |
|    value_loss           | 5.12        |
-----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.11       |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0424     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.163      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 844        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 865        |
|    time_elapsed         | 3481       |
|    total_timesteps      | 885760     |
| train/                  |            |
|    approx_kl            | 0.14627862 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.463      |
|    n_updates            | 17280      |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.239      |
|    value_loss           | 2.44       |
----------------------------------------
--------------------------------------
| reward                  | 0.732    |
| reward_contact          | 0.0137   |
| reward_ctrl             | 0.111    |
| reward_motion           | 0.118    |
| reward_orientation      | 0.0421   |
| reward_position         | 1.81e-10 |
| reward_rotation         | 0.163    |
| reward_torque           | 0.0563   |
| reward_velocity         | 0.227    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 843      |
| time/                   |          |
|    fps                  | 254      |
|    iterations           | 866      |
|    time_elapsed         | 3485     |
|    total_timesteps      | 886784   |
| train/                  |          |
|    approx_kl            | 0.071714 |
|    clip_fraction        | 0.258    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.3    |
|    explained_variance   | 0.919    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.77     |
|    n_updates            | 17300    |
|    policy_gradient_loss | -0.0532  |
|    std                  | 0.239    |
|    value_loss           | 11.6     |
--------------------------------------
--------------------------------------
| reward                  | 0.728    |
| reward_contact          | 0.0137   |
| reward_ctrl             | 0.11     |
| reward_motion           | 0.114    |
| reward_orientation      | 0.0423   |
| reward_position         | 1.81e-10 |
| reward_rotation         | 0.164    |
| reward_torque           | 0.0563   |
| reward_velocity         | 0.228    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 842      |
| time/                   |          |
|    fps                  | 254      |
|    iterations           | 867      |
|    time_elapsed         | 3488     |
|    total_timesteps      | 887808   |
| train/                  |          |
|    approx_kl            | 0.664516 |
|    clip_fraction        | 0.553    |
|    clip_range           | 0.4      |
|    entropy_loss         | -42.9    |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.414    |
|    n_updates            | 17320    |
|    policy_gradient_loss | -0.0716  |
|    std                  | 0.239    |
|    value_loss           | 2.15     |
--------------------------------------
Num timesteps: 888000
Best mean reward: 879.19 - Last mean reward per episode: 842.00
---------------------------------------
| reward                  | 0.729     |
| reward_contact          | 0.0137    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.115     |
| reward_orientation      | 0.0424    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.163     |
| reward_torque           | 0.0563    |
| reward_velocity         | 0.228     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 841       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 868       |
|    time_elapsed         | 3492      |
|    total_timesteps      | 888832    |
| train/                  |           |
|    approx_kl            | 1.5241495 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.6     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.749     |
|    n_updates            | 17340     |
|    policy_gradient_loss | -0.0447   |
|    std                  | 0.239     |
|    value_loss           | 2.71      |
---------------------------------------
---------------------------------------
| reward                  | 0.726     |
| reward_contact          | 0.0137    |
| reward_ctrl             | 0.11      |
| reward_motion           | 0.115     |
| reward_orientation      | 0.0423    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.162     |
| reward_torque           | 0.0562    |
| reward_velocity         | 0.228     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 840       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 869       |
|    time_elapsed         | 3495      |
|    total_timesteps      | 889856    |
| train/                  |           |
|    approx_kl            | 0.1373342 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.212     |
|    n_updates            | 17360     |
|    policy_gradient_loss | -0.104    |
|    std                  | 0.239     |
|    value_loss           | 1.98      |
---------------------------------------
----------------------------------------
| reward                  | 0.73       |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.111      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0425     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.162      |
| reward_torque           | 0.0563     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 840        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 870        |
|    time_elapsed         | 3498       |
|    total_timesteps      | 890880     |
| train/                  |            |
|    approx_kl            | 0.20330465 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.571      |
|    n_updates            | 17380      |
|    policy_gradient_loss | -0.0921    |
|    std                  | 0.239      |
|    value_loss           | 2.41       |
----------------------------------------
-----------------------------------------
| reward                  | 0.727       |
| reward_contact          | 0.0137      |
| reward_ctrl             | 0.11        |
| reward_motion           | 0.117       |
| reward_orientation      | 0.0425      |
| reward_position         | 1.81e-10    |
| reward_rotation         | 0.163       |
| reward_torque           | 0.0563      |
| reward_velocity         | 0.225       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 837         |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 871         |
|    time_elapsed         | 3502        |
|    total_timesteps      | 891904      |
| train/                  |             |
|    approx_kl            | 0.100731835 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 17400       |
|    policy_gradient_loss | -0.0697     |
|    std                  | 0.239       |
|    value_loss           | 6           |
-----------------------------------------
----------------------------------------
| reward                  | 0.723      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0426     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.0562     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 836        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 872        |
|    time_elapsed         | 3505       |
|    total_timesteps      | 892928     |
| train/                  |            |
|    approx_kl            | 0.28454486 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.491      |
|    n_updates            | 17420      |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.239      |
|    value_loss           | 2.28       |
----------------------------------------
----------------------------------------
| reward                  | 0.717      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.109      |
| reward_motion           | 0.113      |
| reward_orientation      | 0.0427     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0561     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 835        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 873        |
|    time_elapsed         | 3508       |
|    total_timesteps      | 893952     |
| train/                  |            |
|    approx_kl            | 0.28826213 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44        |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.57       |
|    n_updates            | 17440      |
|    policy_gradient_loss | -0.0687    |
|    std                  | 0.239      |
|    value_loss           | 3.76       |
----------------------------------------
Num timesteps: 894000
Best mean reward: 879.19 - Last mean reward per episode: 834.69
----------------------------------------
| reward                  | 0.716      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.116      |
| reward_orientation      | 0.0429     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.157      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 834        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 874        |
|    time_elapsed         | 3512       |
|    total_timesteps      | 894976     |
| train/                  |            |
|    approx_kl            | 0.10744329 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.9        |
|    n_updates            | 17460      |
|    policy_gradient_loss | -0.0644    |
|    std                  | 0.239      |
|    value_loss           | 3.58       |
----------------------------------------
----------------------------------------
| reward                  | 0.711      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.112      |
| reward_orientation      | 0.0428     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0559     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 832        |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 875        |
|    time_elapsed         | 3515       |
|    total_timesteps      | 896000     |
| train/                  |            |
|    approx_kl            | 0.12497248 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.67       |
|    n_updates            | 17480      |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.239      |
|    value_loss           | 6.95       |
----------------------------------------
---------------------------------------
| reward                  | 0.718     |
| reward_contact          | 0.0143    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.115     |
| reward_orientation      | 0.0424    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.159     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 831       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 876       |
|    time_elapsed         | 3518      |
|    total_timesteps      | 897024    |
| train/                  |           |
|    approx_kl            | 1.0382175 |
|    clip_fraction        | 0.406     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.9     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.92      |
|    n_updates            | 17500     |
|    policy_gradient_loss | -0.0493   |
|    std                  | 0.239     |
|    value_loss           | 1.71      |
---------------------------------------
---------------------------------------
| reward                  | 0.72      |
| reward_contact          | 0.0143    |
| reward_ctrl             | 0.108     |
| reward_motion           | 0.117     |
| reward_orientation      | 0.0424    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.159     |
| reward_torque           | 0.0559    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 831       |
| time/                   |           |
|    fps                  | 254       |
|    iterations           | 877       |
|    time_elapsed         | 3522      |
|    total_timesteps      | 898048    |
| train/                  |           |
|    approx_kl            | 0.2211769 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.7     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.285     |
|    n_updates            | 17520     |
|    policy_gradient_loss | -0.0625   |
|    std                  | 0.239     |
|    value_loss           | 1.53      |
---------------------------------------
----------------------------------------
| reward                  | 0.726      |
| reward_contact          | 0.0137     |
| reward_ctrl             | 0.108      |
| reward_motion           | 0.118      |
| reward_orientation      | 0.0424     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.161      |
| reward_torque           | 0.056      |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 832        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 878        |
|    time_elapsed         | 3525       |
|    total_timesteps      | 899072     |
| train/                  |            |
|    approx_kl            | 0.07339081 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.86       |
|    n_updates            | 17540      |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.238      |
|    value_loss           | 18         |
----------------------------------------
Num timesteps: 900000
Best mean reward: 879.19 - Last mean reward per episode: 831.12
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0137      |
| reward_ctrl             | 0.108       |
| reward_motion           | 0.123       |
| reward_orientation      | 0.0424      |
| reward_position         | 1.84e-10    |
| reward_rotation         | 0.16        |
| reward_torque           | 0.056       |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 831         |
| time/                   |             |
|    fps                  | 255         |
|    iterations           | 879         |
|    time_elapsed         | 3529        |
|    total_timesteps      | 900096      |
| train/                  |             |
|    approx_kl            | 0.110965624 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.5       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.634       |
|    n_updates            | 17560       |
|    policy_gradient_loss | -0.0794     |
|    std                  | 0.238       |
|    value_loss           | 2.76        |
-----------------------------------------
----------------------------------------
| reward                  | 0.723      |
| reward_contact          | 0.0143     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.122      |
| reward_orientation      | 0.0421     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 830        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 880        |
|    time_elapsed         | 3532       |
|    total_timesteps      | 901120     |
| train/                  |            |
|    approx_kl            | 0.08398123 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.64       |
|    n_updates            | 17580      |
|    policy_gradient_loss | -0.0572    |
|    std                  | 0.238      |
|    value_loss           | 4.01       |
----------------------------------------
---------------------------------------
| reward                  | 0.724     |
| reward_contact          | 0.0145    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.124     |
| reward_orientation      | 0.042     |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.156     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 830       |
| time/                   |           |
|    fps                  | 255       |
|    iterations           | 881       |
|    time_elapsed         | 3535      |
|    total_timesteps      | 902144    |
| train/                  |           |
|    approx_kl            | 0.2102328 |
|    clip_fraction        | 0.361     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.7     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.314     |
|    n_updates            | 17600     |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.238     |
|    value_loss           | 1.84      |
---------------------------------------
-----------------------------------------
| reward                  | 0.729       |
| reward_contact          | 0.0145      |
| reward_ctrl             | 0.107       |
| reward_motion           | 0.125       |
| reward_orientation      | 0.0421      |
| reward_position         | 1.84e-10    |
| reward_rotation         | 0.158       |
| reward_torque           | 0.0558      |
| reward_velocity         | 0.226       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 830         |
| time/                   |             |
|    fps                  | 255         |
|    iterations           | 882         |
|    time_elapsed         | 3539        |
|    total_timesteps      | 903168      |
| train/                  |             |
|    approx_kl            | 0.123585135 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.55        |
|    n_updates            | 17620       |
|    policy_gradient_loss | -0.0526     |
|    std                  | 0.238       |
|    value_loss           | 10.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0151     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0418     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.156      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 830        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 883        |
|    time_elapsed         | 3542       |
|    total_timesteps      | 904192     |
| train/                  |            |
|    approx_kl            | 0.25187668 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.272      |
|    n_updates            | 17640      |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.238      |
|    value_loss           | 2.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.133      |
| reward_orientation      | 0.0416     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.158      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 829        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 884        |
|    time_elapsed         | 3545       |
|    total_timesteps      | 905216     |
| train/                  |            |
|    approx_kl            | 0.32791853 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.227      |
|    n_updates            | 17660      |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.238      |
|    value_loss           | 1.61       |
----------------------------------------
Num timesteps: 906000
Best mean reward: 879.19 - Last mean reward per episode: 828.68
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | 0.0139    |
| reward_ctrl             | 0.107     |
| reward_motion           | 0.14      |
| reward_orientation      | 0.0418    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.158     |
| reward_torque           | 0.0558    |
| reward_velocity         | 0.227     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 829       |
| time/                   |           |
|    fps                  | 255       |
|    iterations           | 885       |
|    time_elapsed         | 3549      |
|    total_timesteps      | 906240    |
| train/                  |           |
|    approx_kl            | 0.1105651 |
|    clip_fraction        | 0.305     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.55      |
|    n_updates            | 17680     |
|    policy_gradient_loss | -0.0588   |
|    std                  | 0.238     |
|    value_loss           | 2.66      |
---------------------------------------
----------------------------------------
| reward                  | 0.744      |
| reward_contact          | 0.0145     |
| reward_ctrl             | 0.107      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0416     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.159      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.227      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 827        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 886        |
|    time_elapsed         | 3552       |
|    total_timesteps      | 907264     |
| train/                  |            |
|    approx_kl            | 0.18523157 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 17700      |
|    policy_gradient_loss | -0.0838    |
|    std                  | 0.238      |
|    value_loss           | 1.93       |
----------------------------------------
---------------------------------------
| reward                  | 0.739     |
| reward_contact          | 0.0151    |
| reward_ctrl             | 0.106     |
| reward_motion           | 0.139     |
| reward_orientation      | 0.0417    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.157     |
| reward_torque           | 0.0557    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 826       |
| time/                   |           |
|    fps                  | 255       |
|    iterations           | 887       |
|    time_elapsed         | 3555      |
|    total_timesteps      | 908288    |
| train/                  |           |
|    approx_kl            | 0.3465631 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.106     |
|    n_updates            | 17720     |
|    policy_gradient_loss | -0.124    |
|    std                  | 0.238     |
|    value_loss           | 1.44      |
---------------------------------------
----------------------------------------
| reward                  | 0.74       |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0415     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.155      |
| reward_torque           | 0.0558     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 825        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 888        |
|    time_elapsed         | 3559       |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.17913556 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.765      |
|    n_updates            | 17740      |
|    policy_gradient_loss | -0.0761    |
|    std                  | 0.238      |
|    value_loss           | 1.87       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0152     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0415     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.154      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 889        |
|    time_elapsed         | 3562       |
|    total_timesteps      | 910336     |
| train/                  |            |
|    approx_kl            | 0.24289721 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.48       |
|    n_updates            | 17760      |
|    policy_gradient_loss | -0.0317    |
|    std                  | 0.238      |
|    value_loss           | 3.52       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0419     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 890        |
|    time_elapsed         | 3566       |
|    total_timesteps      | 911360     |
| train/                  |            |
|    approx_kl            | 0.38418084 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.215      |
|    n_updates            | 17780      |
|    policy_gradient_loss | -0.0673    |
|    std                  | 0.238      |
|    value_loss           | 1.3        |
----------------------------------------
Num timesteps: 912000
Best mean reward: 879.19 - Last mean reward per episode: 823.68
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0158     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.134      |
| reward_orientation      | 0.0419     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 823        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 891        |
|    time_elapsed         | 3569       |
|    total_timesteps      | 912384     |
| train/                  |            |
|    approx_kl            | 0.21928331 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.74       |
|    n_updates            | 17800      |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.237      |
|    value_loss           | 2.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0164     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0418     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 892        |
|    time_elapsed         | 3572       |
|    total_timesteps      | 913408     |
| train/                  |            |
|    approx_kl            | 0.19409281 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.275      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.7       |
|    n_updates            | 17820      |
|    policy_gradient_loss | -0.0172    |
|    std                  | 0.237      |
|    value_loss           | 26.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.735     |
| reward_contact          | 0.0164    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.139     |
| reward_orientation      | 0.0422    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.152     |
| reward_torque           | 0.0556    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 824       |
| time/                   |           |
|    fps                  | 255       |
|    iterations           | 893       |
|    time_elapsed         | 3576      |
|    total_timesteps      | 914432    |
| train/                  |           |
|    approx_kl            | 0.3826961 |
|    clip_fraction        | 0.29      |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.537     |
|    n_updates            | 17840     |
|    policy_gradient_loss | -0.0692   |
|    std                  | 0.237     |
|    value_loss           | 1.82      |
---------------------------------------
----------------------------------------
| reward                  | 0.734      |
| reward_contact          | 0.0164     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0424     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.153      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 823        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 894        |
|    time_elapsed         | 3579       |
|    total_timesteps      | 915456     |
| train/                  |            |
|    approx_kl            | 0.09573186 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.07       |
|    n_updates            | 17860      |
|    policy_gradient_loss | -0.0432    |
|    std                  | 0.237      |
|    value_loss           | 8.54       |
----------------------------------------
----------------------------------------
| reward                  | 0.735      |
| reward_contact          | 0.017      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.141      |
| reward_orientation      | 0.0424     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 895        |
|    time_elapsed         | 3582       |
|    total_timesteps      | 916480     |
| train/                  |            |
|    approx_kl            | 0.09727051 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.228      |
|    n_updates            | 17880      |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.237      |
|    value_loss           | 3.46       |
----------------------------------------
----------------------------------------
| reward                  | 0.732      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0425     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.225      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 896        |
|    time_elapsed         | 3586       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.09202133 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 17900      |
|    policy_gradient_loss | -0.0662    |
|    std                  | 0.237      |
|    value_loss           | 3.95       |
----------------------------------------
Num timesteps: 918000
Best mean reward: 879.19 - Last mean reward per episode: 823.54
----------------------------------------
| reward                  | 0.733      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.138      |
| reward_orientation      | 0.0423     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.152      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 824        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 897        |
|    time_elapsed         | 3589       |
|    total_timesteps      | 918528     |
| train/                  |            |
|    approx_kl            | 0.10691885 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.434      |
|    n_updates            | 17920      |
|    policy_gradient_loss | -0.0456    |
|    std                  | 0.237      |
|    value_loss           | 3.54       |
----------------------------------------
--------------------------------------
| reward                  | 0.731    |
| reward_contact          | 0.0166   |
| reward_ctrl             | 0.103    |
| reward_motion           | 0.135    |
| reward_orientation      | 0.0424   |
| reward_position         | 1.84e-10 |
| reward_rotation         | 0.153    |
| reward_torque           | 0.0554   |
| reward_velocity         | 0.226    |
| rollout/                |          |
|    ep_len_mean          | 1.02e+03 |
|    ep_rew_mean          | 824      |
| time/                   |          |
|    fps                  | 255      |
|    iterations           | 898      |
|    time_elapsed         | 3593     |
|    total_timesteps      | 919552   |
| train/                  |          |
|    approx_kl            | 0.064477 |
|    clip_fraction        | 0.178    |
|    clip_range           | 0.4      |
|    entropy_loss         | -43.5    |
|    explained_variance   | 0.697    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.54     |
|    n_updates            | 17940    |
|    policy_gradient_loss | -0.0443  |
|    std                  | 0.237    |
|    value_loss           | 16.1     |
--------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0166     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.137      |
| reward_orientation      | 0.0426     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.151      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.226      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 823        |
| time/                   |            |
|    fps                  | 255        |
|    iterations           | 899        |
|    time_elapsed         | 3596       |
|    total_timesteps      | 920576     |
| train/                  |            |
|    approx_kl            | 0.08798392 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 17960      |
|    policy_gradient_loss | -0.0426    |
|    std                  | 0.237      |
|    value_loss           | 8.24       |
----------------------------------------
----------------------------------------
| reward                  | 0.728      |
| reward_contact          | 0.0172     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0427     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.15       |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.224      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 823        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 900        |
|    time_elapsed         | 3599       |
|    total_timesteps      | 921600     |
| train/                  |            |
|    approx_kl            | 0.17135438 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.2      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.308      |
|    n_updates            | 17980      |
|    policy_gradient_loss | -0.0708    |
|    std                  | 0.237      |
|    value_loss           | 2.08       |
----------------------------------------
---------------------------------------
| reward                  | 0.729     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.137     |
| reward_orientation      | 0.0426    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.15      |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.225     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 823       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 901       |
|    time_elapsed         | 3603      |
|    total_timesteps      | 922624    |
| train/                  |           |
|    approx_kl            | 1.5301503 |
|    clip_fraction        | 0.418     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.414     |
|    n_updates            | 18000     |
|    policy_gradient_loss | -0.036    |
|    std                  | 0.237     |
|    value_loss           | 1.71      |
---------------------------------------
---------------------------------------
| reward                  | 0.73      |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.138     |
| reward_orientation      | 0.0426    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.149     |
| reward_torque           | 0.0554    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 823       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 902       |
|    time_elapsed         | 3606      |
|    total_timesteps      | 923648    |
| train/                  |           |
|    approx_kl            | 0.3332189 |
|    clip_fraction        | 0.4       |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.58      |
|    n_updates            | 18020     |
|    policy_gradient_loss | -0.0838   |
|    std                  | 0.237     |
|    value_loss           | 1.52      |
---------------------------------------
Num timesteps: 924000
Best mean reward: 879.19 - Last mean reward per episode: 822.63
---------------------------------------
| reward                  | 0.725     |
| reward_contact          | 0.0178    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.137     |
| reward_orientation      | 0.0429    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 822       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 903       |
|    time_elapsed         | 3609      |
|    total_timesteps      | 924672    |
| train/                  |           |
|    approx_kl            | 0.0982338 |
|    clip_fraction        | 0.35      |
|    clip_range           | 0.4       |
|    entropy_loss         | -44       |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.247     |
|    n_updates            | 18040     |
|    policy_gradient_loss | -0.0502   |
|    std                  | 0.237     |
|    value_loss           | 2.45      |
---------------------------------------
---------------------------------------
| reward                  | 0.723     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.136     |
| reward_orientation      | 0.0429    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 822       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 904       |
|    time_elapsed         | 3613      |
|    total_timesteps      | 925696    |
| train/                  |           |
|    approx_kl            | 0.1381906 |
|    clip_fraction        | 0.269     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.2     |
|    explained_variance   | 0.988     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.268     |
|    n_updates            | 18060     |
|    policy_gradient_loss | -0.061    |
|    std                  | 0.237     |
|    value_loss           | 2.09      |
---------------------------------------
---------------------------------------
| reward                  | 0.723     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.136     |
| reward_orientation      | 0.0428    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.147     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 822       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 905       |
|    time_elapsed         | 3616      |
|    total_timesteps      | 926720    |
| train/                  |           |
|    approx_kl            | 0.8817368 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.2     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.175     |
|    n_updates            | 18080     |
|    policy_gradient_loss | -0.0562   |
|    std                  | 0.237     |
|    value_loss           | 1.88      |
---------------------------------------
---------------------------------------
| reward                  | 0.722     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.133     |
| reward_orientation      | 0.043     |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.148     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.224     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 822       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 906       |
|    time_elapsed         | 3620      |
|    total_timesteps      | 927744    |
| train/                  |           |
|    approx_kl            | 1.1477106 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.7     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.104     |
|    n_updates            | 18100     |
|    policy_gradient_loss | 0.0168    |
|    std                  | 0.237     |
|    value_loss           | 0.709     |
---------------------------------------
-----------------------------------------
| reward                  | 0.725       |
| reward_contact          | 0.0172      |
| reward_ctrl             | 0.102       |
| reward_motion           | 0.137       |
| reward_orientation      | 0.0432      |
| reward_position         | 1.84e-10    |
| reward_rotation         | 0.147       |
| reward_torque           | 0.0553      |
| reward_velocity         | 0.224       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 821         |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 907         |
|    time_elapsed         | 3623        |
|    total_timesteps      | 928768      |
| train/                  |             |
|    approx_kl            | 0.078049555 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 18120       |
|    policy_gradient_loss | -0.0436     |
|    std                  | 0.237       |
|    value_loss           | 16          |
-----------------------------------------
----------------------------------------
| reward                  | 0.727      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0434     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 821        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 908        |
|    time_elapsed         | 3626       |
|    total_timesteps      | 929792     |
| train/                  |            |
|    approx_kl            | 0.11208098 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.89       |
|    n_updates            | 18140      |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.237      |
|    value_loss           | 4.82       |
----------------------------------------
Num timesteps: 930000
Best mean reward: 879.19 - Last mean reward per episode: 821.25
----------------------------------------
| reward                  | 0.722      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0435     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 821        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 909        |
|    time_elapsed         | 3630       |
|    total_timesteps      | 930816     |
| train/                  |            |
|    approx_kl            | 0.06683041 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.51       |
|    n_updates            | 18160      |
|    policy_gradient_loss | -0.00688   |
|    std                  | 0.237      |
|    value_loss           | 27.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.722      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.0999     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0439     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 821        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 910        |
|    time_elapsed         | 3633       |
|    total_timesteps      | 931840     |
| train/                  |            |
|    approx_kl            | 0.07078634 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.9      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.41       |
|    n_updates            | 18180      |
|    policy_gradient_loss | -0.0692    |
|    std                  | 0.237      |
|    value_loss           | 8.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.722      |
| reward_contact          | 0.0178     |
| reward_ctrl             | 0.0998     |
| reward_motion           | 0.14       |
| reward_orientation      | 0.0438     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 820        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 911        |
|    time_elapsed         | 3636       |
|    total_timesteps      | 932864     |
| train/                  |            |
|    approx_kl            | 0.23517653 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.355      |
|    n_updates            | 18200      |
|    policy_gradient_loss | -0.0948    |
|    std                  | 0.237      |
|    value_loss           | 1.62       |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.146      |
| reward_orientation      | 0.0441     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 820        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 912        |
|    time_elapsed         | 3640       |
|    total_timesteps      | 933888     |
| train/                  |            |
|    approx_kl            | 0.51942503 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 18220      |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.237      |
|    value_loss           | 0.924      |
----------------------------------------
----------------------------------------
| reward                  | 0.731      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.143      |
| reward_orientation      | 0.0441     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 819        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 913        |
|    time_elapsed         | 3643       |
|    total_timesteps      | 934912     |
| train/                  |            |
|    approx_kl            | 0.18975306 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.619      |
|    n_updates            | 18240      |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.237      |
|    value_loss           | 2.59       |
----------------------------------------
---------------------------------------
| reward                  | 0.725     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.1       |
| reward_motion           | 0.142     |
| reward_orientation      | 0.0443    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0551    |
| reward_velocity         | 0.221     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 816       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 914       |
|    time_elapsed         | 3647      |
|    total_timesteps      | 935936    |
| train/                  |           |
|    approx_kl            | 0.8520432 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.8     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.802     |
|    n_updates            | 18260     |
|    policy_gradient_loss | -0.0563   |
|    std                  | 0.237     |
|    value_loss           | 2.44      |
---------------------------------------
Num timesteps: 936000
Best mean reward: 879.19 - Last mean reward per episode: 816.35
----------------------------------------
| reward                  | 0.724      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.139      |
| reward_orientation      | 0.0446     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 816        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 915        |
|    time_elapsed         | 3650       |
|    total_timesteps      | 936960     |
| train/                  |            |
|    approx_kl            | 0.15257831 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 18280      |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.237      |
|    value_loss           | 4.27       |
----------------------------------------
---------------------------------------
| reward                  | 0.717     |
| reward_contact          | 0.0191    |
| reward_ctrl             | 0.1       |
| reward_motion           | 0.135     |
| reward_orientation      | 0.0446    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0551    |
| reward_velocity         | 0.219     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 813       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 916       |
|    time_elapsed         | 3653      |
|    total_timesteps      | 937984    |
| train/                  |           |
|    approx_kl            | 0.6730663 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.4       |
|    entropy_loss         | -44.1     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.616     |
|    n_updates            | 18300     |
|    policy_gradient_loss | -0.031    |
|    std                  | 0.237     |
|    value_loss           | 2.76      |
---------------------------------------
---------------------------------------
| reward                  | 0.716     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.101     |
| reward_motion           | 0.131     |
| reward_orientation      | 0.045     |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.146     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.219     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 813       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 917       |
|    time_elapsed         | 3657      |
|    total_timesteps      | 939008    |
| train/                  |           |
|    approx_kl            | 0.1552221 |
|    clip_fraction        | 0.266     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.2     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.264     |
|    n_updates            | 18320     |
|    policy_gradient_loss | -0.0888   |
|    std                  | 0.236     |
|    value_loss           | 1.78      |
---------------------------------------
----------------------------------------
| reward                  | 0.718      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.136      |
| reward_orientation      | 0.0451     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 813        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 918        |
|    time_elapsed         | 3660       |
|    total_timesteps      | 940032     |
| train/                  |            |
|    approx_kl            | 0.12270244 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.26       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.4       |
|    n_updates            | 18340      |
|    policy_gradient_loss | -0.0445    |
|    std                  | 0.236      |
|    value_loss           | 28.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.716      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.131      |
| reward_orientation      | 0.0453     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 813        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 919        |
|    time_elapsed         | 3663       |
|    total_timesteps      | 941056     |
| train/                  |            |
|    approx_kl            | 0.07529232 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 18360      |
|    policy_gradient_loss | -0.0489    |
|    std                  | 0.236      |
|    value_loss           | 20.5       |
----------------------------------------
Num timesteps: 942000
Best mean reward: 879.19 - Last mean reward per episode: 812.60
----------------------------------------
| reward                  | 0.709      |
| reward_contact          | 0.0191     |
| reward_ctrl             | 0.1        |
| reward_motion           | 0.128      |
| reward_orientation      | 0.0453     |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.217      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 813        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 920        |
|    time_elapsed         | 3667       |
|    total_timesteps      | 942080     |
| train/                  |            |
|    approx_kl            | 0.12841241 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 18380      |
|    policy_gradient_loss | -0.0744    |
|    std                  | 0.236      |
|    value_loss           | 2.5        |
----------------------------------------
----------------------------------------
| reward                  | 0.708      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0995     |
| reward_motion           | 0.13       |
| reward_orientation      | 0.045      |
| reward_position         | 1.84e-10   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.217      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 812        |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 921        |
|    time_elapsed         | 3670       |
|    total_timesteps      | 943104     |
| train/                  |            |
|    approx_kl            | 0.96431124 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.882      |
|    n_updates            | 18400      |
|    policy_gradient_loss | -0.0504    |
|    std                  | 0.236      |
|    value_loss           | 1.51       |
----------------------------------------
---------------------------------------
| reward                  | 0.704     |
| reward_contact          | 0.0203    |
| reward_ctrl             | 0.0993    |
| reward_motion           | 0.128     |
| reward_orientation      | 0.0449    |
| reward_position         | 1.84e-10  |
| reward_rotation         | 0.141     |
| reward_torque           | 0.055     |
| reward_velocity         | 0.216     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 811       |
| time/                   |           |
|    fps                  | 256       |
|    iterations           | 922       |
|    time_elapsed         | 3674      |
|    total_timesteps      | 944128    |
| train/                  |           |
|    approx_kl            | 0.5599909 |
|    clip_fraction        | 0.378     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.7     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.882     |
|    n_updates            | 18420     |
|    policy_gradient_loss | -0.0591   |
|    std                  | 0.236     |
|    value_loss           | 2.52      |
---------------------------------------
----------------------------------------
| reward                  | 0.703      |
| reward_contact          | 0.0203     |
| reward_ctrl             | 0.0988     |
| reward_motion           | 0.129      |
| reward_orientation      | 0.0453     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 811        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 923        |
|    time_elapsed         | 3677       |
|    total_timesteps      | 945152     |
| train/                  |            |
|    approx_kl            | 0.18018442 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.1      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 18440      |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.236      |
|    value_loss           | 2.65       |
----------------------------------------
----------------------------------------
| reward                  | 0.695      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0982     |
| reward_motion           | 0.125      |
| reward_orientation      | 0.0457     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 810        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 924        |
|    time_elapsed         | 3680       |
|    total_timesteps      | 946176     |
| train/                  |            |
|    approx_kl            | 0.13559449 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.13       |
|    n_updates            | 18460      |
|    policy_gradient_loss | -0.0487    |
|    std                  | 0.236      |
|    value_loss           | 26.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.693      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0978     |
| reward_motion           | 0.123      |
| reward_orientation      | 0.0457     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 925        |
|    time_elapsed         | 3684       |
|    total_timesteps      | 947200     |
| train/                  |            |
|    approx_kl            | 0.47874779 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.997      |
|    n_updates            | 18480      |
|    policy_gradient_loss | -0.0712    |
|    std                  | 0.236      |
|    value_loss           | 3.02       |
----------------------------------------
Num timesteps: 948000
Best mean reward: 879.19 - Last mean reward per episode: 806.79
----------------------------------------
| reward                  | 0.693      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0973     |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0458     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.136      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 926        |
|    time_elapsed         | 3687       |
|    total_timesteps      | 948224     |
| train/                  |            |
|    approx_kl            | 0.47336844 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.791      |
|    n_updates            | 18500      |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.236      |
|    value_loss           | 2.11       |
----------------------------------------
----------------------------------------
| reward                  | 0.693      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0973     |
| reward_motion           | 0.127      |
| reward_orientation      | 0.0458     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.136      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 927        |
|    time_elapsed         | 3691       |
|    total_timesteps      | 949248     |
| train/                  |            |
|    approx_kl            | 0.42776012 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.95       |
|    n_updates            | 18520      |
|    policy_gradient_loss | -0.0827    |
|    std                  | 0.236      |
|    value_loss           | 2.04       |
----------------------------------------
----------------------------------------
| reward                  | 0.691      |
| reward_contact          | 0.0197     |
| reward_ctrl             | 0.0979     |
| reward_motion           | 0.122      |
| reward_orientation      | 0.0462     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.136      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 928        |
|    time_elapsed         | 3694       |
|    total_timesteps      | 950272     |
| train/                  |            |
|    approx_kl            | 0.33715013 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.92       |
|    n_updates            | 18540      |
|    policy_gradient_loss | -0.0579    |
|    std                  | 0.236      |
|    value_loss           | 2.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.685     |
| reward_contact          | 0.0191    |
| reward_ctrl             | 0.0969    |
| reward_motion           | 0.118     |
| reward_orientation      | 0.0464    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.136     |
| reward_torque           | 0.0547    |
| reward_velocity         | 0.213     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 807       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 929       |
|    time_elapsed         | 3697      |
|    total_timesteps      | 951296    |
| train/                  |           |
|    approx_kl            | 0.1292075 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -43       |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.86      |
|    n_updates            | 18560     |
|    policy_gradient_loss | -0.0489   |
|    std                  | 0.236     |
|    value_loss           | 9.27      |
---------------------------------------
-----------------------------------------
| reward                  | 0.685       |
| reward_contact          | 0.0191      |
| reward_ctrl             | 0.0969      |
| reward_motion           | 0.118       |
| reward_orientation      | 0.0468      |
| reward_position         | 6.58e-07    |
| reward_rotation         | 0.136       |
| reward_torque           | 0.0547      |
| reward_velocity         | 0.213       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 807         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 930         |
|    time_elapsed         | 3701        |
|    total_timesteps      | 952320      |
| train/                  |             |
|    approx_kl            | 0.093723945 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 18580       |
|    policy_gradient_loss | -0.0424     |
|    std                  | 0.236       |
|    value_loss           | 10.2        |
-----------------------------------------
----------------------------------------
| reward                  | 0.682      |
| reward_contact          | 0.0191     |
| reward_ctrl             | 0.0976     |
| reward_motion           | 0.113      |
| reward_orientation      | 0.0468     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.137      |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 931        |
|    time_elapsed         | 3704       |
|    total_timesteps      | 953344     |
| train/                  |            |
|    approx_kl            | 0.14667699 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 18600      |
|    policy_gradient_loss | -0.0472    |
|    std                  | 0.236      |
|    value_loss           | 7.54       |
----------------------------------------
Num timesteps: 954000
Best mean reward: 879.19 - Last mean reward per episode: 806.41
----------------------------------------
| reward                  | 0.681      |
| reward_contact          | 0.0191     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.11       |
| reward_orientation      | 0.0471     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.138      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.213      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 932        |
|    time_elapsed         | 3707       |
|    total_timesteps      | 954368     |
| train/                  |            |
|    approx_kl            | 0.64704376 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.7      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.107      |
|    n_updates            | 18620      |
|    policy_gradient_loss | -0.0351    |
|    std                  | 0.236      |
|    value_loss           | 0.682      |
----------------------------------------
---------------------------------------
| reward                  | 0.679     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.0987    |
| reward_motion           | 0.107     |
| reward_orientation      | 0.0475    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.14      |
| reward_torque           | 0.0549    |
| reward_velocity         | 0.213     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 806       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 933       |
|    time_elapsed         | 3711      |
|    total_timesteps      | 955392    |
| train/                  |           |
|    approx_kl            | 0.1416431 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.2     |
|    explained_variance   | 0.887     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.345     |
|    n_updates            | 18640     |
|    policy_gradient_loss | -0.0608   |
|    std                  | 0.236     |
|    value_loss           | 9.16      |
---------------------------------------
----------------------------------------
| reward                  | 0.683      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.0983     |
| reward_motion           | 0.109      |
| reward_orientation      | 0.0475     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.14       |
| reward_torque           | 0.0548     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 934        |
|    time_elapsed         | 3714       |
|    total_timesteps      | 956416     |
| train/                  |            |
|    approx_kl            | 0.13270715 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.948      |
|    n_updates            | 18660      |
|    policy_gradient_loss | -0.0795    |
|    std                  | 0.236      |
|    value_loss           | 4.18       |
----------------------------------------
---------------------------------------
| reward                  | 0.682     |
| reward_contact          | 0.0191    |
| reward_ctrl             | 0.0978    |
| reward_motion           | 0.11      |
| reward_orientation      | 0.0478    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.138     |
| reward_torque           | 0.0548    |
| reward_velocity         | 0.214     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 807       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 935       |
|    time_elapsed         | 3718      |
|    total_timesteps      | 957440    |
| train/                  |           |
|    approx_kl            | 0.2040677 |
|    clip_fraction        | 0.287     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.5     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.742     |
|    n_updates            | 18680     |
|    policy_gradient_loss | -0.0568   |
|    std                  | 0.236     |
|    value_loss           | 4.29      |
---------------------------------------
-----------------------------------------
| reward                  | 0.686       |
| reward_contact          | 0.0185      |
| reward_ctrl             | 0.0977      |
| reward_motion           | 0.113       |
| reward_orientation      | 0.0479      |
| reward_position         | 6.58e-07    |
| reward_rotation         | 0.138       |
| reward_torque           | 0.0548      |
| reward_velocity         | 0.216       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 936         |
|    time_elapsed         | 3721        |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.070395276 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.4         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.737       |
|    n_updates            | 18700       |
|    policy_gradient_loss | -0.041      |
|    std                  | 0.236       |
|    value_loss           | 9.82        |
-----------------------------------------
----------------------------------------
| reward                  | 0.684      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.0984     |
| reward_motion           | 0.11       |
| reward_orientation      | 0.0479     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.139      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.215      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 809        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 937        |
|    time_elapsed         | 3724       |
|    total_timesteps      | 959488     |
| train/                  |            |
|    approx_kl            | 0.10830514 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.74       |
|    n_updates            | 18720      |
|    policy_gradient_loss | -0.0343    |
|    std                  | 0.236      |
|    value_loss           | 9.87       |
----------------------------------------
Num timesteps: 960000
Best mean reward: 879.19 - Last mean reward per episode: 808.97
----------------------------------------
| reward                  | 0.689      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.0993     |
| reward_motion           | 0.111      |
| reward_orientation      | 0.048      |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0549     |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 809        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 938        |
|    time_elapsed         | 3728       |
|    total_timesteps      | 960512     |
| train/                  |            |
|    approx_kl            | 0.08349972 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.747      |
|    n_updates            | 18740      |
|    policy_gradient_loss | -0.0602    |
|    std                  | 0.236      |
|    value_loss           | 8.71       |
----------------------------------------
---------------------------------------
| reward                  | 0.689     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.0996    |
| reward_motion           | 0.11      |
| reward_orientation      | 0.0479    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.142     |
| reward_torque           | 0.055     |
| reward_velocity         | 0.217     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 809       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 939       |
|    time_elapsed         | 3731      |
|    total_timesteps      | 961536    |
| train/                  |           |
|    approx_kl            | 0.2640733 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43       |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.568     |
|    n_updates            | 18760     |
|    policy_gradient_loss | -0.0374   |
|    std                  | 0.236     |
|    value_loss           | 2.21      |
---------------------------------------
----------------------------------------
| reward                  | 0.687      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.0998     |
| reward_motion           | 0.107      |
| reward_orientation      | 0.0482     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.055      |
| reward_velocity         | 0.217      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 809        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 940        |
|    time_elapsed         | 3735       |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.19534539 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.808      |
|    n_updates            | 18780      |
|    policy_gradient_loss | -0.0695    |
|    std                  | 0.236      |
|    value_loss           | 2.31       |
----------------------------------------
-----------------------------------------
| reward                  | 0.692       |
| reward_contact          | 0.0173      |
| reward_ctrl             | 0.101       |
| reward_motion           | 0.108       |
| reward_orientation      | 0.0485      |
| reward_position         | 6.58e-07    |
| reward_rotation         | 0.144       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.218       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 809         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 941         |
|    time_elapsed         | 3738        |
|    total_timesteps      | 963584      |
| train/                  |             |
|    approx_kl            | 0.100587666 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | 2           |
|    n_updates            | 18800       |
|    policy_gradient_loss | -0.0482     |
|    std                  | 0.236       |
|    value_loss           | 10.3        |
-----------------------------------------
---------------------------------------
| reward                  | 0.691     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.101     |
| reward_motion           | 0.107     |
| reward_orientation      | 0.0488    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.219     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 810       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 942       |
|    time_elapsed         | 3741      |
|    total_timesteps      | 964608    |
| train/                  |           |
|    approx_kl            | 1.4233646 |
|    clip_fraction        | 0.496     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.9     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.256     |
|    n_updates            | 18820     |
|    policy_gradient_loss | -0.0255   |
|    std                  | 0.236     |
|    value_loss           | 0.909     |
---------------------------------------
-----------------------------------------
| reward                  | 0.691       |
| reward_contact          | 0.0167      |
| reward_ctrl             | 0.101       |
| reward_motion           | 0.107       |
| reward_orientation      | 0.049       |
| reward_position         | 6.58e-07    |
| reward_rotation         | 0.144       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 809         |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 943         |
|    time_elapsed         | 3745        |
|    total_timesteps      | 965632      |
| train/                  |             |
|    approx_kl            | 0.081390604 |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 18840       |
|    policy_gradient_loss | -0.0281     |
|    std                  | 0.236       |
|    value_loss           | 12          |
-----------------------------------------
Num timesteps: 966000
Best mean reward: 879.19 - Last mean reward per episode: 809.26
----------------------------------------
| reward                  | 0.692      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.108      |
| reward_orientation      | 0.0494     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.142      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 809        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 944        |
|    time_elapsed         | 3748       |
|    total_timesteps      | 966656     |
| train/                  |            |
|    approx_kl            | 0.41658205 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.2      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.194      |
|    n_updates            | 18860      |
|    policy_gradient_loss | -0.0433    |
|    std                  | 0.236      |
|    value_loss           | 1.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.687      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.102      |
| reward_orientation      | 0.0495     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 809        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 945        |
|    time_elapsed         | 3752       |
|    total_timesteps      | 967680     |
| train/                  |            |
|    approx_kl            | 0.10303636 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.8       |
|    n_updates            | 18880      |
|    policy_gradient_loss | -0.0234    |
|    std                  | 0.236      |
|    value_loss           | 18.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.687     |
| reward_contact          | 0.0167    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.102     |
| reward_orientation      | 0.0498    |
| reward_position         | 6.58e-07  |
| reward_rotation         | 0.143     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 808       |
| time/                   |           |
|    fps                  | 257       |
|    iterations           | 946       |
|    time_elapsed         | 3755      |
|    total_timesteps      | 968704    |
| train/                  |           |
|    approx_kl            | 13.440866 |
|    clip_fraction        | 0.701     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.6     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 18900     |
|    policy_gradient_loss | 0.166     |
|    std                  | 0.236     |
|    value_loss           | 5.08      |
---------------------------------------
----------------------------------------
| reward                  | 0.686      |
| reward_contact          | 0.0167     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.101      |
| reward_orientation      | 0.0501     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 808        |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 947        |
|    time_elapsed         | 3758       |
|    total_timesteps      | 969728     |
| train/                  |            |
|    approx_kl            | 0.47302425 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.323      |
|    n_updates            | 18920      |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.236      |
|    value_loss           | 2.83       |
----------------------------------------
----------------------------------------
| reward                  | 0.695      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.106      |
| reward_orientation      | 0.0502     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 807        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 948        |
|    time_elapsed         | 3762       |
|    total_timesteps      | 970752     |
| train/                  |            |
|    approx_kl            | 0.35974693 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.67       |
|    n_updates            | 18940      |
|    policy_gradient_loss | -0.0921    |
|    std                  | 0.236      |
|    value_loss           | 2.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.706      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0504     |
| reward_position         | 6.58e-07   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 949        |
|    time_elapsed         | 3765       |
|    total_timesteps      | 971776     |
| train/                  |            |
|    approx_kl            | 0.14003119 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.39       |
|    n_updates            | 18960      |
|    policy_gradient_loss | -0.0877    |
|    std                  | 0.235      |
|    value_loss           | 7.28       |
----------------------------------------
Num timesteps: 972000
Best mean reward: 879.19 - Last mean reward per episode: 806.04
---------------------------------------
| reward                  | 0.707     |
| reward_contact          | 0.0173    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.111     |
| reward_orientation      | 0.0504    |
| reward_position         | 7.57e-06  |
| reward_rotation         | 0.146     |
| reward_torque           | 0.0555    |
| reward_velocity         | 0.223     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 807       |
| time/                   |           |
|    fps                  | 258       |
|    iterations           | 950       |
|    time_elapsed         | 3768      |
|    total_timesteps      | 972800    |
| train/                  |           |
|    approx_kl            | 0.3561501 |
|    clip_fraction        | 0.406     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.3     |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.712     |
|    n_updates            | 18980     |
|    policy_gradient_loss | -0.0863   |
|    std                  | 0.235     |
|    value_loss           | 2.98      |
---------------------------------------
----------------------------------------
| reward                  | 0.707      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0508     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 951        |
|    time_elapsed         | 3772       |
|    total_timesteps      | 973824     |
| train/                  |            |
|    approx_kl            | 0.08039118 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 19000      |
|    policy_gradient_loss | -0.0569    |
|    std                  | 0.235      |
|    value_loss           | 14.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.708      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0507     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.223      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 806        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 952        |
|    time_elapsed         | 3775       |
|    total_timesteps      | 974848     |
| train/                  |            |
|    approx_kl            | 0.21697475 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.9      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.443      |
|    n_updates            | 19020      |
|    policy_gradient_loss | -0.088     |
|    std                  | 0.235      |
|    value_loss           | 3.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.0508     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 953        |
|    time_elapsed         | 3778       |
|    total_timesteps      | 975872     |
| train/                  |            |
|    approx_kl            | 0.20639475 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 19040      |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.235      |
|    value_loss           | 2.84       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.104      |
| reward_motion           | 0.111      |
| reward_orientation      | 0.051      |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 954        |
|    time_elapsed         | 3782       |
|    total_timesteps      | 976896     |
| train/                  |            |
|    approx_kl            | 0.62439805 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.41       |
|    n_updates            | 19060      |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.235      |
|    value_loss           | 2.87       |
----------------------------------------
---------------------------------------
| reward                  | 0.698     |
| reward_contact          | 0.0179    |
| reward_ctrl             | 0.104     |
| reward_motion           | 0.108     |
| reward_orientation      | 0.0512    |
| reward_position         | 7.57e-06  |
| reward_rotation         | 0.144     |
| reward_torque           | 0.0555    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 799       |
| time/                   |           |
|    fps                  | 258       |
|    iterations           | 955       |
|    time_elapsed         | 3785      |
|    total_timesteps      | 977920    |
| train/                  |           |
|    approx_kl            | 0.3921617 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.9     |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 19080     |
|    policy_gradient_loss | -0.0912   |
|    std                  | 0.235     |
|    value_loss           | 2.9       |
---------------------------------------
Num timesteps: 978000
Best mean reward: 879.19 - Last mean reward per episode: 798.97
----------------------------------------
| reward                  | 0.7        |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.11       |
| reward_orientation      | 0.0516     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0554     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 798        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 956        |
|    time_elapsed         | 3789       |
|    total_timesteps      | 978944     |
| train/                  |            |
|    approx_kl            | 0.91006374 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.8      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.235      |
|    n_updates            | 19100      |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.235      |
|    value_loss           | 1.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0179     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.112      |
| reward_orientation      | 0.0517     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 798        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 957        |
|    time_elapsed         | 3792       |
|    total_timesteps      | 979968     |
| train/                  |            |
|    approx_kl            | 0.08844529 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.47       |
|    n_updates            | 19120      |
|    policy_gradient_loss | -0.0612    |
|    std                  | 0.235      |
|    value_loss           | 29.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.705      |
| reward_contact          | 0.0173     |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.113      |
| reward_orientation      | 0.0513     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0555     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 797        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 958        |
|    time_elapsed         | 3795       |
|    total_timesteps      | 980992     |
| train/                  |            |
|    approx_kl            | 0.19121858 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.551      |
|    n_updates            | 19140      |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.235      |
|    value_loss           | 3.04       |
----------------------------------------
---------------------------------------
| reward                  | 0.7       |
| reward_contact          | 0.0179    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.112     |
| reward_orientation      | 0.0513    |
| reward_position         | 7.57e-06  |
| reward_rotation         | 0.145     |
| reward_torque           | 0.0553    |
| reward_velocity         | 0.217     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 796       |
| time/                   |           |
|    fps                  | 258       |
|    iterations           | 959       |
|    time_elapsed         | 3799      |
|    total_timesteps      | 982016    |
| train/                  |           |
|    approx_kl            | 0.0975886 |
|    clip_fraction        | 0.332     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.1     |
|    explained_variance   | 0.945     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 19160     |
|    policy_gradient_loss | -0.0731   |
|    std                  | 0.235     |
|    value_loss           | 15.7      |
---------------------------------------
----------------------------------------
| reward                  | 0.694      |
| reward_contact          | 0.0185     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.108      |
| reward_orientation      | 0.0509     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 794        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 960        |
|    time_elapsed         | 3802       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.15272434 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -43.3      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.548      |
|    n_updates            | 19180      |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.235      |
|    value_loss           | 2.74       |
----------------------------------------
Num timesteps: 984000
Best mean reward: 879.19 - Last mean reward per episode: 793.02
----------------------------------------
| reward                  | 0.685      |
| reward_contact          | 0.0187     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.103      |
| reward_orientation      | 0.0513     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.141      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.214      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 793        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 961        |
|    time_elapsed         | 3805       |
|    total_timesteps      | 984064     |
| train/                  |            |
|    approx_kl            | 0.21371537 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.2        |
|    n_updates            | 19200      |
|    policy_gradient_loss | -0.0895    |
|    std                  | 0.235      |
|    value_loss           | 2.77       |
----------------------------------------
----------------------------------------
| reward                  | 0.692      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.105      |
| reward_orientation      | 0.0512     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.216      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 793        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 962        |
|    time_elapsed         | 3809       |
|    total_timesteps      | 985088     |
| train/                  |            |
|    approx_kl            | 0.39143264 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 19220      |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.235      |
|    value_loss           | 1.13       |
----------------------------------------
----------------------------------------
| reward                  | 0.691      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.105      |
| reward_orientation      | 0.0512     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.143      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 793        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 963        |
|    time_elapsed         | 3813       |
|    total_timesteps      | 986112     |
| train/                  |            |
|    approx_kl            | 0.22747998 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.5       |
|    n_updates            | 19240      |
|    policy_gradient_loss | -0.0608    |
|    std                  | 0.235      |
|    value_loss           | 27.9       |
----------------------------------------
---------------------------------------
| reward                  | 0.691     |
| reward_contact          | 0.0186    |
| reward_ctrl             | 0.101     |
| reward_motion           | 0.103     |
| reward_orientation      | 0.0511    |
| reward_position         | 7.57e-06  |
| reward_rotation         | 0.143     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 793       |
| time/                   |           |
|    fps                  | 258       |
|    iterations           | 964       |
|    time_elapsed         | 3817      |
|    total_timesteps      | 987136    |
| train/                  |           |
|    approx_kl            | 0.2975154 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.4       |
|    entropy_loss         | -43.4     |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.47      |
|    n_updates            | 19260     |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.235     |
|    value_loss           | 4.78      |
---------------------------------------
----------------------------------------
| reward                  | 0.691      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.102      |
| reward_motion           | 0.102      |
| reward_orientation      | 0.0513     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0552     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 794        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 965        |
|    time_elapsed         | 3820       |
|    total_timesteps      | 988160     |
| train/                  |            |
|    approx_kl            | 0.35632128 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.735      |
|    n_updates            | 19280      |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.235      |
|    value_loss           | 4.17       |
----------------------------------------
----------------------------------------
| reward                  | 0.688      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.0989     |
| reward_orientation      | 0.0513     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 794        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 966        |
|    time_elapsed         | 3824       |
|    total_timesteps      | 989184     |
| train/                  |            |
|    approx_kl            | 0.10759602 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 19300      |
|    policy_gradient_loss | -0.0623    |
|    std                  | 0.235      |
|    value_loss           | 28         |
----------------------------------------
Num timesteps: 990000
Best mean reward: 879.19 - Last mean reward per episode: 794.46
---------------------------------------
| reward                  | 0.686     |
| reward_contact          | 0.0186    |
| reward_ctrl             | 0.102     |
| reward_motion           | 0.0979    |
| reward_orientation      | 0.0514    |
| reward_position         | 7.57e-06  |
| reward_rotation         | 0.143     |
| reward_torque           | 0.0552    |
| reward_velocity         | 0.218     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 794       |
| time/                   |           |
|    fps                  | 258       |
|    iterations           | 967       |
|    time_elapsed         | 3828      |
|    total_timesteps      | 990208    |
| train/                  |           |
|    approx_kl            | 0.1843774 |
|    clip_fraction        | 0.339     |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.7     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.342     |
|    n_updates            | 19320     |
|    policy_gradient_loss | -0.118    |
|    std                  | 0.234     |
|    value_loss           | 1.68      |
---------------------------------------
----------------------------------------
| reward                  | 0.685      |
| reward_contact          | 0.0186     |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.097      |
| reward_orientation      | 0.0514     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.144      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.218      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 795        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 968        |
|    time_elapsed         | 3832       |
|    total_timesteps      | 991232     |
| train/                  |            |
|    approx_kl            | 0.20399033 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.5      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.222      |
|    n_updates            | 19340      |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.234      |
|    value_loss           | 1.68       |
----------------------------------------
-----------------------------------------
| reward                  | 0.683       |
| reward_contact          | 0.018       |
| reward_ctrl             | 0.102       |
| reward_motion           | 0.0924      |
| reward_orientation      | 0.0514      |
| reward_position         | 7.57e-06    |
| reward_rotation         | 0.146       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.219       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 796         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 969         |
|    time_elapsed         | 3836        |
|    total_timesteps      | 992256      |
| train/                  |             |
|    approx_kl            | 0.118522145 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.6       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.716       |
|    n_updates            | 19360       |
|    policy_gradient_loss | -0.0608     |
|    std                  | 0.234       |
|    value_loss           | 7.63        |
-----------------------------------------
----------------------------------------
| reward                  | 0.68       |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.101      |
| reward_motion           | 0.0882     |
| reward_orientation      | 0.0512     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.146      |
| reward_torque           | 0.0551     |
| reward_velocity         | 0.22       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 797        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 970        |
|    time_elapsed         | 3840       |
|    total_timesteps      | 993280     |
| train/                  |            |
|    approx_kl            | 0.15852003 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.1      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.821      |
|    n_updates            | 19380      |
|    policy_gradient_loss | -0.0927    |
|    std                  | 0.234      |
|    value_loss           | 2.65       |
----------------------------------------
-----------------------------------------
| reward                  | 0.684       |
| reward_contact          | 0.018       |
| reward_ctrl             | 0.102       |
| reward_motion           | 0.0916      |
| reward_orientation      | 0.0511      |
| reward_position         | 7.57e-06    |
| reward_rotation         | 0.145       |
| reward_torque           | 0.0552      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 798         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 971         |
|    time_elapsed         | 3844        |
|    total_timesteps      | 994304      |
| train/                  |             |
|    approx_kl            | 0.076543465 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.51        |
|    n_updates            | 19400       |
|    policy_gradient_loss | -0.0618     |
|    std                  | 0.234       |
|    value_loss           | 17.8        |
-----------------------------------------
----------------------------------------
| reward                  | 0.689      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.103      |
| reward_motion           | 0.0926     |
| reward_orientation      | 0.051      |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0553     |
| reward_velocity         | 0.221      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 799        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 972        |
|    time_elapsed         | 3848       |
|    total_timesteps      | 995328     |
| train/                  |            |
|    approx_kl            | 0.13721159 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.457      |
|    n_updates            | 19420      |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.234      |
|    value_loss           | 16.6       |
----------------------------------------
Num timesteps: 996000
Best mean reward: 879.19 - Last mean reward per episode: 800.39
-----------------------------------------
| reward                  | 0.689       |
| reward_contact          | 0.0174      |
| reward_ctrl             | 0.104       |
| reward_motion           | 0.0926      |
| reward_orientation      | 0.0513      |
| reward_position         | 7.57e-06    |
| reward_rotation         | 0.148       |
| reward_torque           | 0.0554      |
| reward_velocity         | 0.221       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 800         |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 973         |
|    time_elapsed         | 3851        |
|    total_timesteps      | 996352      |
| train/                  |             |
|    approx_kl            | 0.123344176 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.83        |
|    n_updates            | 19440       |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.234       |
|    value_loss           | 5.99        |
-----------------------------------------
----------------------------------------
| reward                  | 0.684      |
| reward_contact          | 0.0174     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.0881     |
| reward_orientation      | 0.0512     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.147      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 800        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 974        |
|    time_elapsed         | 3855       |
|    total_timesteps      | 997376     |
| train/                  |            |
|    approx_kl            | 0.30561322 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -43        |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.581      |
|    n_updates            | 19460      |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.234      |
|    value_loss           | 2.16       |
----------------------------------------
----------------------------------------
| reward                  | 0.689      |
| reward_contact          | 0.0168     |
| reward_ctrl             | 0.106      |
| reward_motion           | 0.0891     |
| reward_orientation      | 0.0514     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0557     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 801        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 975        |
|    time_elapsed         | 3858       |
|    total_timesteps      | 998400     |
| train/                  |            |
|    approx_kl            | 0.22583398 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0448     |
|    n_updates            | 19480      |
|    policy_gradient_loss | -0.0983    |
|    std                  | 0.234      |
|    value_loss           | 0.96       |
----------------------------------------
----------------------------------------
| reward                  | 0.69       |
| reward_contact          | 0.0174     |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.0895     |
| reward_orientation      | 0.0518     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.148      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.222      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 802        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 976        |
|    time_elapsed         | 3861       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.23172718 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.8      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.71       |
|    n_updates            | 19500      |
|    policy_gradient_loss | -0.0644    |
|    std                  | 0.234      |
|    value_loss           | 12         |
----------------------------------------
----------------------------------------
| reward                  | 0.678      |
| reward_contact          | 0.018      |
| reward_ctrl             | 0.105      |
| reward_motion           | 0.0846     |
| reward_orientation      | 0.0517     |
| reward_position         | 7.57e-06   |
| reward_rotation         | 0.145      |
| reward_torque           | 0.0556     |
| reward_velocity         | 0.219      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 799        |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 977        |
|    time_elapsed         | 3865       |
|    total_timesteps      | 1000448    |
| train/                  |            |
|    approx_kl            | 0.16412416 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -42.8      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.56       |
|    n_updates            | 19520      |
|    policy_gradient_loss | -0.0989    |
|    std                  | 0.234      |
|    value_loss           | 2.7        |
----------------------------------------
