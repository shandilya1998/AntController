running build_ext
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp73/A2C_1
------------------------------------
| time/                 |          |
|    fps                | 232      |
|    iterations         | 100      |
|    time_elapsed       | 3        |
|    total_timesteps    | 800      |
| train/                |          |
|    entropy_loss       | -19.3    |
|    explained_variance | 1.51e-05 |
|    learning_rate      | 0.00096  |
|    n_updates          | 99       |
|    policy_loss        | 0.0163   |
|    std                | 0.135    |
|    value_loss         | 32.7     |
------------------------------------
------------------------------------
| reward                | -2.03    |
| reward_contact        | -0.00151 |
| reward_motion         | 1.62     |
| reward_torque         | -0.349   |
| reward_velocity       | -3.3     |
| rollout/              |          |
|    ep_len_mean        | 372      |
|    ep_rew_mean        | -752     |
| time/                 |          |
|    fps                | 235      |
|    iterations         | 200      |
|    time_elapsed       | 6        |
|    total_timesteps    | 1600     |
| train/                |          |
|    entropy_loss       | -19.2    |
|    explained_variance | -49.6    |
|    learning_rate      | 0.00096  |
|    n_updates          | 199      |
|    policy_loss        | -0.1     |
|    std                | 0.136    |
|    value_loss         | 63.5     |
------------------------------------
-------------------------------------
| reward                | -2.03     |
| reward_contact        | -0.00151  |
| reward_motion         | 1.62      |
| reward_torque         | -0.349    |
| reward_velocity       | -3.3      |
| rollout/              |           |
|    ep_len_mean        | 372       |
|    ep_rew_mean        | -752      |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 300       |
|    time_elapsed       | 10        |
|    total_timesteps    | 2400      |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | -0.000189 |
|    learning_rate      | 0.00096   |
|    n_updates          | 299       |
|    policy_loss        | 0.0033    |
|    std                | 0.135     |
|    value_loss         | 4.41      |
-------------------------------------
------------------------------------
| reward                | -2.08    |
| reward_contact        | -0.00297 |
| reward_motion         | 1.32     |
| reward_torque         | -0.314   |
| reward_velocity       | -3.08    |
| rollout/              |          |
|    ep_len_mean        | 503      |
|    ep_rew_mean        | -984     |
| time/                 |          |
|    fps                | 232      |
|    iterations         | 400      |
|    time_elapsed       | 13       |
|    total_timesteps    | 3200     |
| train/                |          |
|    entropy_loss       | -19.3    |
|    explained_variance | 0.0357   |
|    learning_rate      | 0.00096  |
|    n_updates          | 399      |
|    policy_loss        | -0.128   |
|    std                | 0.136    |
|    value_loss         | 290      |
------------------------------------
-------------------------------------
| reward                | -2.32     |
| reward_contact        | -0.00562  |
| reward_motion         | 1.33      |
| reward_torque         | -0.33     |
| reward_velocity       | -3.31     |
| rollout/              |           |
|    ep_len_mean        | 590       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 500       |
|    time_elapsed       | 17        |
|    total_timesteps    | 4000      |
| train/                |           |
|    entropy_loss       | -19.5     |
|    explained_variance | 0.00115   |
|    learning_rate      | 0.00096   |
|    n_updates          | 499       |
|    policy_loss        | -0.0269   |
|    std                | 0.136     |
|    value_loss         | 3.41      |
-------------------------------------
-------------------------------------
| reward                | -2.26     |
| reward_contact        | -0.00508  |
| reward_motion         | 1.29      |
| reward_torque         | -0.362    |
| reward_velocity       | -3.18     |
| rollout/              |           |
|    ep_len_mean        | 574       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 600       |
|    time_elapsed       | 20        |
|    total_timesteps    | 4800      |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | -62.1     |
|    learning_rate      | 0.00096   |
|    n_updates          | 599       |
|    policy_loss        | 0.0901    |
|    std                | 0.136     |
|    value_loss         | 665       |
-------------------------------------
-------------------------------------
| reward                | -2.26     |
| reward_contact        | -0.00508  |
| reward_motion         | 1.29      |
| reward_torque         | -0.362    |
| reward_velocity       | -3.18     |
| rollout/              |           |
|    ep_len_mean        | 574       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 700       |
|    time_elapsed       | 24        |
|    total_timesteps    | 5600      |
| train/                |           |
|    entropy_loss       | -19.8     |
|    explained_variance | 0.121     |
|    learning_rate      | 0.00096   |
|    n_updates          | 699       |
|    policy_loss        | 0.204     |
|    std                | 0.136     |
|    value_loss         | 4.97      |
-------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -1289.54
Saving new best model to rl/out_dir/models/exp73/best_model.zip
-------------------------------------
| reward                | -2.22     |
| reward_contact        | -0.00544  |
| reward_motion         | 1.14      |
| reward_torque         | -0.323    |
| reward_velocity       | -3.04     |
| rollout/              |           |
|    ep_len_mean        | 624       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 800       |
|    time_elapsed       | 27        |
|    total_timesteps    | 6400      |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.113     |
|    learning_rate      | 0.00096   |
|    n_updates          | 799       |
|    policy_loss        | 0.0188    |
|    std                | 0.136     |
|    value_loss         | 0.97      |
-------------------------------------
-------------------------------------
| reward                | -2.18     |
| reward_contact        | -0.00595  |
| reward_motion         | 1.03      |
| reward_torque         | -0.291    |
| reward_velocity       | -2.92     |
| rollout/              |           |
|    ep_len_mean        | 664       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 900       |
|    time_elapsed       | 31        |
|    total_timesteps    | 7200      |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | 0.00558   |
|    learning_rate      | 0.00096   |
|    n_updates          | 899       |
|    policy_loss        | 0.174     |
|    std                | 0.136     |
|    value_loss         | 3.09      |
-------------------------------------
-------------------------------------
| reward                | -2        |
| reward_contact        | -0.0049   |
| reward_motion         | 1.25      |
| reward_torque         | -0.315    |
| reward_velocity       | -2.93     |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 1000      |
|    time_elapsed       | 34        |
|    total_timesteps    | 8000      |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0254    |
|    learning_rate      | 0.00096   |
|    n_updates          | 999       |
|    policy_loss        | 0.00333   |
|    std                | 0.136     |
|    value_loss         | 0.442     |
-------------------------------------
-------------------------------------
| reward                | -2        |
| reward_contact        | -0.0049   |
| reward_motion         | 1.25      |
| reward_torque         | -0.315    |
| reward_velocity       | -2.93     |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 1100      |
|    time_elapsed       | 38        |
|    total_timesteps    | 8800      |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.256    |
|    learning_rate      | 0.00096   |
|    n_updates          | 1099      |
|    policy_loss        | -0.285    |
|    std                | 0.136     |
|    value_loss         | 3.88      |
-------------------------------------
-------------------------------------
| reward                | -1.96     |
| reward_contact        | -0.00472  |
| reward_motion         | 1.25      |
| reward_torque         | -0.308    |
| reward_velocity       | -2.89     |
| rollout/              |           |
|    ep_len_mean        | 590       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 1200      |
|    time_elapsed       | 41        |
|    total_timesteps    | 9600      |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.732    |
|    learning_rate      | 0.00096   |
|    n_updates          | 1199      |
|    policy_loss        | 0.0816    |
|    std                | 0.136     |
|    value_loss         | 11.2      |
-------------------------------------
-------------------------------------
| reward                | -1.86     |
| reward_contact        | -0.00441  |
| reward_motion         | 1.22      |
| reward_torque         | -0.307    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 584       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 1300      |
|    time_elapsed       | 45        |
|    total_timesteps    | 10400     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.29     |
|    learning_rate      | 0.00096   |
|    n_updates          | 1299      |
|    policy_loss        | 0.234     |
|    std                | 0.136     |
|    value_loss         | 64.9      |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.00492  |
| reward_motion         | 1.36      |
| reward_torque         | -0.277    |
| reward_velocity       | -2.8      |
| rollout/              |           |
|    ep_len_mean        | 527       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 1400      |
|    time_elapsed       | 48        |
|    total_timesteps    | 11200     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.882     |
|    learning_rate      | 0.00096   |
|    n_updates          | 1399      |
|    policy_loss        | -0.038    |
|    std                | 0.136     |
|    value_loss         | 6.77      |
-------------------------------------
Num timesteps: 12000
Best mean reward: -1289.54 - Last mean reward per episode: -1069.74
Saving new best model to rl/out_dir/models/exp73/best_model.zip
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.00492  |
| reward_motion         | 1.36      |
| reward_torque         | -0.277    |
| reward_velocity       | -2.8      |
| rollout/              |           |
|    ep_len_mean        | 527       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 1500      |
|    time_elapsed       | 52        |
|    total_timesteps    | 12000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.2       |
|    learning_rate      | 0.00096   |
|    n_updates          | 1499      |
|    policy_loss        | -0.109    |
|    std                | 0.136     |
|    value_loss         | 41.3      |
-------------------------------------
-------------------------------------
| reward                | -1.76     |
| reward_contact        | -0.0047   |
| reward_motion         | 1.34      |
| reward_torque         | -0.274    |
| reward_velocity       | -2.82     |
| rollout/              |           |
|    ep_len_mean        | 549       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 1600      |
|    time_elapsed       | 55        |
|    total_timesteps    | 12800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.398     |
|    learning_rate      | 0.00096   |
|    n_updates          | 1599      |
|    policy_loss        | 0.44      |
|    std                | 0.136     |
|    value_loss         | 1.52      |
-------------------------------------
-------------------------------------
| reward                | -1.78     |
| reward_contact        | -0.00534  |
| reward_motion         | 1.29      |
| reward_torque         | -0.27     |
| reward_velocity       | -2.79     |
| rollout/              |           |
|    ep_len_mean        | 570       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 1700      |
|    time_elapsed       | 58        |
|    total_timesteps    | 13600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.689    |
|    learning_rate      | 0.00096   |
|    n_updates          | 1699      |
|    policy_loss        | -0.0169   |
|    std                | 0.136     |
|    value_loss         | 18.5      |
-------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.00496 |
| reward_motion         | 1.41     |
| reward_torque         | -0.296   |
| reward_velocity       | -2.78    |
| rollout/              |          |
|    ep_len_mean        | 546      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 1800     |
|    time_elapsed       | 62       |
|    total_timesteps    | 14400    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0728  |
|    learning_rate      | 0.00096  |
|    n_updates          | 1799     |
|    policy_loss        | -0.0202  |
|    std                | 0.136    |
|    value_loss         | 3.84     |
------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.00496 |
| reward_motion         | 1.41     |
| reward_torque         | -0.296   |
| reward_velocity       | -2.78    |
| rollout/              |          |
|    ep_len_mean        | 546      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 1900     |
|    time_elapsed       | 65       |
|    total_timesteps    | 15200    |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.0246   |
|    learning_rate      | 0.00096  |
|    n_updates          | 1899     |
|    policy_loss        | -0.032   |
|    std                | 0.136    |
|    value_loss         | 5.75     |
------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.00485  |
| reward_motion         | 1.42      |
| reward_torque         | -0.279    |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 2000      |
|    time_elapsed       | 68        |
|    total_timesteps    | 16000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.386     |
|    learning_rate      | 0.00096   |
|    n_updates          | 1999      |
|    policy_loss        | 0.00952   |
|    std                | 0.136     |
|    value_loss         | 0.0835    |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.00501  |
| reward_motion         | 1.38      |
| reward_torque         | -0.275    |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 561       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 2100      |
|    time_elapsed       | 72        |
|    total_timesteps    | 16800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0396    |
|    learning_rate      | 0.00096   |
|    n_updates          | 2099      |
|    policy_loss        | 0.00233   |
|    std                | 0.136     |
|    value_loss         | 1.01      |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.00511  |
| reward_motion         | 1.35      |
| reward_torque         | -0.28     |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 576       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 2200      |
|    time_elapsed       | 76        |
|    total_timesteps    | 17600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0488    |
|    learning_rate      | 0.00096   |
|    n_updates          | 2199      |
|    policy_loss        | -0.015    |
|    std                | 0.136     |
|    value_loss         | 1.96      |
-------------------------------------
Num timesteps: 18000
Best mean reward: -1069.74 - Last mean reward per episode: -1134.01
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.00511  |
| reward_motion         | 1.35      |
| reward_torque         | -0.28     |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 574       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 2300      |
|    time_elapsed       | 79        |
|    total_timesteps    | 18400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.25     |
|    learning_rate      | 0.00096   |
|    n_updates          | 2299      |
|    policy_loss        | 0.12      |
|    std                | 0.136     |
|    value_loss         | 8.13      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00499  |
| reward_motion         | 1.4       |
| reward_torque         | -0.281    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 574       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 2400      |
|    time_elapsed       | 82        |
|    total_timesteps    | 19200     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0183    |
|    learning_rate      | 0.00096   |
|    n_updates          | 2399      |
|    policy_loss        | 7.15e-05  |
|    std                | 0.137     |
|    value_loss         | 9.31      |
-------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.00372 |
| reward_motion         | 1.7      |
| reward_torque         | -0.331   |
| reward_velocity       | -2.84    |
| rollout/              |          |
|    ep_len_mean        | 457      |
|    ep_rew_mean        | -900     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 2500     |
|    time_elapsed       | 86       |
|    total_timesteps    | 20000    |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -7.04    |
|    learning_rate      | 0.00096  |
|    n_updates          | 2499     |
|    policy_loss        | -0.151   |
|    std                | 0.136    |
|    value_loss         | 25.2     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.00379 |
| reward_motion         | 1.67     |
| reward_torque         | -0.323   |
| reward_velocity       | -2.82    |
| rollout/              |          |
|    ep_len_mean        | 470      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 230      |
|    iterations         | 2600     |
|    time_elapsed       | 90       |
|    total_timesteps    | 20800    |
| train/                |          |
|    entropy_loss       | -19.7    |
|    explained_variance | -1.8     |
|    learning_rate      | 0.00096  |
|    n_updates          | 2599     |
|    policy_loss        | 0.685    |
|    std                | 0.136    |
|    value_loss         | 9.67     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.00379 |
| reward_motion         | 1.67     |
| reward_torque         | -0.323   |
| reward_velocity       | -2.82    |
| rollout/              |          |
|    ep_len_mean        | 470      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 2700     |
|    time_elapsed       | 93       |
|    total_timesteps    | 21600    |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -0.0401  |
|    learning_rate      | 0.00096  |
|    n_updates          | 2699     |
|    policy_loss        | 0.00783  |
|    std                | 0.136    |
|    value_loss         | 0.399    |
------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.00387  |
| reward_motion         | 1.63      |
| reward_torque         | -0.316    |
| reward_velocity       | -2.79     |
| rollout/              |           |
|    ep_len_mean        | 482       |
|    ep_rew_mean        | -950      |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 2800      |
|    time_elapsed       | 96        |
|    total_timesteps    | 22400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00175  |
|    learning_rate      | 0.00096   |
|    n_updates          | 2799      |
|    policy_loss        | -0.000792 |
|    std                | 0.137     |
|    value_loss         | 3.01      |
-------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.00438 |
| reward_motion         | 1.67     |
| reward_torque         | -0.307   |
| reward_velocity       | -2.79    |
| rollout/              |          |
|    ep_len_mean        | 484      |
|    ep_rew_mean        | -950     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 2900     |
|    time_elapsed       | 100      |
|    total_timesteps    | 23200    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0226   |
|    learning_rate      | 0.00096  |
|    n_updates          | 2899     |
|    policy_loss        | -0.01    |
|    std                | 0.136    |
|    value_loss         | 1.72     |
------------------------------------
Num timesteps: 24000
Best mean reward: -1069.74 - Last mean reward per episode: -966.99
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.00486 |
| reward_motion         | 1.63     |
| reward_torque         | -0.3     |
| reward_velocity       | -2.77    |
| rollout/              |          |
|    ep_len_mean        | 495      |
|    ep_rew_mean        | -967     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3000     |
|    time_elapsed       | 103      |
|    total_timesteps    | 24000    |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.00788 |
|    learning_rate      | 0.00096  |
|    n_updates          | 2999     |
|    policy_loss        | 0.000734 |
|    std                | 0.137    |
|    value_loss         | 0.626    |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.00486 |
| reward_motion         | 1.63     |
| reward_torque         | -0.3     |
| reward_velocity       | -2.77    |
| rollout/              |          |
|    ep_len_mean        | 506      |
|    ep_rew_mean        | -983     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3100     |
|    time_elapsed       | 107      |
|    total_timesteps    | 24800    |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.0916   |
|    learning_rate      | 0.00096  |
|    n_updates          | 3099     |
|    policy_loss        | -0.29    |
|    std                | 0.136    |
|    value_loss         | 9.93e+03 |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.00546 |
| reward_motion         | 1.61     |
| reward_torque         | -0.295   |
| reward_velocity       | -2.75    |
| rollout/              |          |
|    ep_len_mean        | 506      |
|    ep_rew_mean        | -983     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3200     |
|    time_elapsed       | 110      |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0146  |
|    learning_rate      | 0.00096  |
|    n_updates          | 3199     |
|    policy_loss        | 0.0152   |
|    std                | 0.136    |
|    value_loss         | 0.305    |
------------------------------------
-------------------------------------
| reward                | -1.44     |
| reward_contact        | -0.00619  |
| reward_motion         | 1.58      |
| reward_torque         | -0.289    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 516       |
|    ep_rew_mean        | -1e+03    |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 3300      |
|    time_elapsed       | 114       |
|    total_timesteps    | 26400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.000677 |
|    learning_rate      | 0.00096   |
|    n_updates          | 3299      |
|    policy_loss        | 0.000504  |
|    std                | 0.136     |
|    value_loss         | 1.01      |
-------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.00682 |
| reward_motion         | 1.6      |
| reward_torque         | -0.297   |
| reward_velocity       | -2.7     |
| rollout/              |          |
|    ep_len_mean        | 507      |
|    ep_rew_mean        | -971     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3400     |
|    time_elapsed       | 117      |
|    total_timesteps    | 27200    |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.000532 |
|    learning_rate      | 0.00096  |
|    n_updates          | 3399     |
|    policy_loss        | 7.39e-06 |
|    std                | 0.136    |
|    value_loss         | 0.223    |
------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.00682 |
| reward_motion         | 1.6      |
| reward_torque         | -0.297   |
| reward_velocity       | -2.7     |
| rollout/              |          |
|    ep_len_mean        | 517      |
|    ep_rew_mean        | -984     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3500     |
|    time_elapsed       | 120      |
|    total_timesteps    | 28000    |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -1.91    |
|    learning_rate      | 0.00096  |
|    n_updates          | 3499     |
|    policy_loss        | -0.0806  |
|    std                | 0.136    |
|    value_loss         | 44.8     |
------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.00776 |
| reward_motion         | 1.57     |
| reward_torque         | -0.292   |
| reward_velocity       | -2.67    |
| rollout/              |          |
|    ep_len_mean        | 517      |
|    ep_rew_mean        | -984     |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3600     |
|    time_elapsed       | 124      |
|    total_timesteps    | 28800    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.346   |
|    learning_rate      | 0.00096  |
|    n_updates          | 3599     |
|    policy_loss        | 0.00494  |
|    std                | 0.136    |
|    value_loss         | 2.16     |
------------------------------------
------------------------------------
| reward                | -1.42    |
| reward_contact        | -0.00819 |
| reward_motion         | 1.54     |
| reward_torque         | -0.287   |
| reward_velocity       | -2.67    |
| rollout/              |          |
|    ep_len_mean        | 526      |
|    ep_rew_mean        | -1e+03   |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 3700     |
|    time_elapsed       | 127      |
|    total_timesteps    | 29600    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00331 |
|    learning_rate      | 0.00096  |
|    n_updates          | 3699     |
|    policy_loss        | -0.00434 |
|    std                | 0.136    |
|    value_loss         | 1.37     |
------------------------------------
Num timesteps: 30000
Best mean reward: -966.99 - Last mean reward per episode: -1015.90
-------------------------------------
| reward                | -1.42     |
| reward_contact        | -0.00819  |
| reward_motion         | 1.52      |
| reward_torque         | -0.283    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 535       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 3800      |
|    time_elapsed       | 131       |
|    total_timesteps    | 30400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00274   |
|    learning_rate      | 0.00096   |
|    n_updates          | 3799      |
|    policy_loss        | 0.0395    |
|    std                | 0.136     |
|    value_loss         | 2.53      |
-------------------------------------
-------------------------------------
| reward                | -1.41     |
| reward_contact        | -0.00821  |
| reward_motion         | 1.49      |
| reward_torque         | -0.279    |
| reward_velocity       | -2.62     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 3900      |
|    time_elapsed       | 134       |
|    total_timesteps    | 31200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.33     |
|    learning_rate      | 0.00096   |
|    n_updates          | 3899      |
|    policy_loss        | 0.0314    |
|    std                | 0.136     |
|    value_loss         | 4.04      |
-------------------------------------
-------------------------------------
| reward                | -1.41     |
| reward_contact        | -0.00821  |
| reward_motion         | 1.49      |
| reward_torque         | -0.279    |
| reward_velocity       | -2.62     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4000      |
|    time_elapsed       | 138       |
|    total_timesteps    | 32000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00394  |
|    learning_rate      | 0.00096   |
|    n_updates          | 3999      |
|    policy_loss        | -0.000327 |
|    std                | 0.136     |
|    value_loss         | 1.02      |
-------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.00807  |
| reward_motion         | 1.5       |
| reward_torque         | -0.272    |
| reward_velocity       | -2.66     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4100      |
|    time_elapsed       | 141       |
|    total_timesteps    | 32800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0125    |
|    learning_rate      | 0.00096   |
|    n_updates          | 4099      |
|    policy_loss        | -0.407    |
|    std                | 0.136     |
|    value_loss         | 8.56      |
-------------------------------------
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.00793  |
| reward_motion         | 1.48      |
| reward_torque         | -0.268    |
| reward_velocity       | -2.67     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4200      |
|    time_elapsed       | 145       |
|    total_timesteps    | 33600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.000902 |
|    learning_rate      | 0.00096   |
|    n_updates          | 4199      |
|    policy_loss        | -0.0932   |
|    std                | 0.136     |
|    value_loss         | 3.2       |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.00806  |
| reward_motion         | 1.45      |
| reward_torque         | -0.265    |
| reward_velocity       | -2.71     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4300      |
|    time_elapsed       | 148       |
|    total_timesteps    | 34400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -2        |
|    learning_rate      | 0.00096   |
|    n_updates          | 4299      |
|    policy_loss        | 0.101     |
|    std                | 0.136     |
|    value_loss         | 3.13      |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.00806  |
| reward_motion         | 1.45      |
| reward_torque         | -0.265    |
| reward_velocity       | -2.71     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4400      |
|    time_elapsed       | 152       |
|    total_timesteps    | 35200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.00332  |
|    learning_rate      | 0.00096   |
|    n_updates          | 4399      |
|    policy_loss        | 0.00159   |
|    std                | 0.135     |
|    value_loss         | 0.35      |
-------------------------------------
Num timesteps: 36000
Best mean reward: -966.99 - Last mean reward per episode: -1013.92
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.00782  |
| reward_motion         | 1.45      |
| reward_torque         | -0.268    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 535       |
|    ep_rew_mean        | -1.01e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4500      |
|    time_elapsed       | 155       |
|    total_timesteps    | 36000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -89.4     |
|    learning_rate      | 0.00096   |
|    n_updates          | 4499      |
|    policy_loss        | -0.339    |
|    std                | 0.135     |
|    value_loss         | 6.51      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.00789  |
| reward_motion         | 1.44      |
| reward_torque         | -0.267    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 542       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4600      |
|    time_elapsed       | 158       |
|    total_timesteps    | 36800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -4.18     |
|    learning_rate      | 0.00096   |
|    n_updates          | 4599      |
|    policy_loss        | 0.0266    |
|    std                | 0.135     |
|    value_loss         | 7.41      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0078   |
| reward_motion         | 1.41      |
| reward_torque         | -0.263    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 536       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4700      |
|    time_elapsed       | 162       |
|    total_timesteps    | 37600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.794    |
|    learning_rate      | 0.00096   |
|    n_updates          | 4699      |
|    policy_loss        | -0.177    |
|    std                | 0.135     |
|    value_loss         | 3.38      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.00769  |
| reward_motion         | 1.43      |
| reward_torque         | -0.266    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 536       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4800      |
|    time_elapsed       | 165       |
|    total_timesteps    | 38400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0142   |
|    learning_rate      | 0.00096   |
|    n_updates          | 4799      |
|    policy_loss        | 0.135     |
|    std                | 0.135     |
|    value_loss         | 2.79      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.00815  |
| reward_motion         | 1.42      |
| reward_torque         | -0.264    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 4900      |
|    time_elapsed       | 169       |
|    total_timesteps    | 39200     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.338    |
|    learning_rate      | 0.00096   |
|    n_updates          | 4899      |
|    policy_loss        | -0.0417   |
|    std                | 0.135     |
|    value_loss         | 1.95      |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.00831  |
| reward_motion         | 1.42      |
| reward_torque         | -0.267    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 537       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5000      |
|    time_elapsed       | 172       |
|    total_timesteps    | 40000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.123    |
|    learning_rate      | 0.00096   |
|    n_updates          | 4999      |
|    policy_loss        | -0.0258   |
|    std                | 0.135     |
|    value_loss         | 0.948     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.00831  |
| reward_motion         | 1.42      |
| reward_torque         | -0.267    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5100      |
|    time_elapsed       | 176       |
|    total_timesteps    | 40800     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.452    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5099      |
|    policy_loss        | -0.782    |
|    std                | 0.135     |
|    value_loss         | 11        |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.00851  |
| reward_motion         | 1.4       |
| reward_torque         | -0.264    |
| reward_velocity       | -2.76     |
| rollout/              |           |
|    ep_len_mean        | 543       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5200      |
|    time_elapsed       | 179       |
|    total_timesteps    | 41600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.316     |
|    learning_rate      | 0.00096   |
|    n_updates          | 5199      |
|    policy_loss        | 0.0411    |
|    std                | 0.135     |
|    value_loss         | 0.242     |
-------------------------------------
Num timesteps: 42000
Best mean reward: -966.99 - Last mean reward per episode: -1042.22
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.00846  |
| reward_motion         | 1.39      |
| reward_torque         | -0.261    |
| reward_velocity       | -2.74     |
| rollout/              |           |
|    ep_len_mean        | 550       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5300      |
|    time_elapsed       | 182       |
|    total_timesteps    | 42400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0722   |
|    learning_rate      | 0.00096   |
|    n_updates          | 5299      |
|    policy_loss        | -0.0198   |
|    std                | 0.135     |
|    value_loss         | 2.95      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.00825  |
| reward_motion         | 1.41      |
| reward_torque         | -0.266    |
| reward_velocity       | -2.74     |
| rollout/              |           |
|    ep_len_mean        | 547       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5400      |
|    time_elapsed       | 186       |
|    total_timesteps    | 43200     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.456     |
|    learning_rate      | 0.00096   |
|    n_updates          | 5399      |
|    policy_loss        | -0.0777   |
|    std                | 0.135     |
|    value_loss         | 0.994     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.00834  |
| reward_motion         | 1.39      |
| reward_torque         | -0.265    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 549       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5500      |
|    time_elapsed       | 189       |
|    total_timesteps    | 44000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.835    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5499      |
|    policy_loss        | -0.00181  |
|    std                | 0.135     |
|    value_loss         | 0.737     |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.00813  |
| reward_motion         | 1.4       |
| reward_torque         | -0.268    |
| reward_velocity       | -2.75     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5600      |
|    time_elapsed       | 193       |
|    total_timesteps    | 44800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0501    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5599      |
|    policy_loss        | -0.0144   |
|    std                | 0.135     |
|    value_loss         | 15.7      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0086   |
| reward_motion         | 1.39      |
| reward_torque         | -0.265    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 550       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5700      |
|    time_elapsed       | 196       |
|    total_timesteps    | 45600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.139    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5699      |
|    policy_loss        | -0.00147  |
|    std                | 0.135     |
|    value_loss         | 4.57      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.009    |
| reward_motion         | 1.38      |
| reward_torque         | -0.262    |
| reward_velocity       | -2.71     |
| rollout/              |           |
|    ep_len_mean        | 556       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5800      |
|    time_elapsed       | 200       |
|    total_timesteps    | 46400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.173    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5799      |
|    policy_loss        | -0.153    |
|    std                | 0.135     |
|    value_loss         | 0.545     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.009    |
| reward_motion         | 1.38      |
| reward_torque         | -0.262    |
| reward_velocity       | -2.71     |
| rollout/              |           |
|    ep_len_mean        | 555       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 5900      |
|    time_elapsed       | 203       |
|    total_timesteps    | 47200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -4.75     |
|    learning_rate      | 0.00096   |
|    n_updates          | 5899      |
|    policy_loss        | 0.648     |
|    std                | 0.135     |
|    value_loss         | 4.88      |
-------------------------------------
Num timesteps: 48000
Best mean reward: -966.99 - Last mean reward per episode: -1032.74
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.00949  |
| reward_motion         | 1.36      |
| reward_torque         | -0.258    |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 555       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6000      |
|    time_elapsed       | 207       |
|    total_timesteps    | 48000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.00843   |
|    learning_rate      | 0.00096   |
|    n_updates          | 5999      |
|    policy_loss        | -0.00669  |
|    std                | 0.135     |
|    value_loss         | 0.571     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.00949  |
| reward_motion         | 1.35      |
| reward_torque         | -0.255    |
| reward_velocity       | -2.68     |
| rollout/              |           |
|    ep_len_mean        | 561       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6100      |
|    time_elapsed       | 210       |
|    total_timesteps    | 48800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.818     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6099      |
|    policy_loss        | 0.0224    |
|    std                | 0.135     |
|    value_loss         | 0.362     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.00938  |
| reward_motion         | 1.36      |
| reward_torque         | -0.254    |
| reward_velocity       | -2.68     |
| rollout/              |           |
|    ep_len_mean        | 561       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6200      |
|    time_elapsed       | 214       |
|    total_timesteps    | 49600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00454  |
|    learning_rate      | 0.00096   |
|    n_updates          | 6199      |
|    policy_loss        | -0.105    |
|    std                | 0.134     |
|    value_loss         | 4.35      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.00926  |
| reward_motion         | 1.36      |
| reward_torque         | -0.251    |
| reward_velocity       | -2.68     |
| rollout/              |           |
|    ep_len_mean        | 561       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6300      |
|    time_elapsed       | 217       |
|    total_timesteps    | 50400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.38     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6299      |
|    policy_loss        | 0.0225    |
|    std                | 0.134     |
|    value_loss         | 5.74      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.00928  |
| reward_motion         | 1.37      |
| reward_torque         | -0.255    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 555       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6400      |
|    time_elapsed       | 221       |
|    total_timesteps    | 51200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | 0.368     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6399      |
|    policy_loss        | -0.052    |
|    std                | 0.134     |
|    value_loss         | 4.71      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.00928  |
| reward_motion         | 1.37      |
| reward_torque         | -0.255    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 555       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6500      |
|    time_elapsed       | 224       |
|    total_timesteps    | 52000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.42     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6499      |
|    policy_loss        | -0.0091   |
|    std                | 0.134     |
|    value_loss         | 1.75      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00934  |
| reward_motion         | 1.39      |
| reward_torque         | -0.257    |
| reward_velocity       | -2.66     |
| rollout/              |           |
|    ep_len_mean        | 554       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6600      |
|    time_elapsed       | 227       |
|    total_timesteps    | 52800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00122   |
|    learning_rate      | 0.00096   |
|    n_updates          | 6599      |
|    policy_loss        | 0.000278  |
|    std                | 0.134     |
|    value_loss         | 2.78      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00932  |
| reward_motion         | 1.37      |
| reward_torque         | -0.254    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 559       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6700      |
|    time_elapsed       | 231       |
|    total_timesteps    | 53600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00103   |
|    learning_rate      | 0.00096   |
|    n_updates          | 6699      |
|    policy_loss        | -0.00252  |
|    std                | 0.134     |
|    value_loss         | 1.33      |
-------------------------------------
Num timesteps: 54000
Best mean reward: -966.99 - Last mean reward per episode: -1025.74
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00931  |
| reward_motion         | 1.36      |
| reward_torque         | -0.251    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 564       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6800      |
|    time_elapsed       | 234       |
|    total_timesteps    | 54400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00192  |
|    learning_rate      | 0.00096   |
|    n_updates          | 6799      |
|    policy_loss        | 0.00327   |
|    std                | 0.134     |
|    value_loss         | 0.512     |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00931  |
| reward_motion         | 1.36      |
| reward_torque         | -0.251    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 568       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 6900      |
|    time_elapsed       | 238       |
|    total_timesteps    | 55200     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.452     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6899      |
|    policy_loss        | -0.328    |
|    std                | 0.134     |
|    value_loss         | 0.463     |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.00931  |
| reward_motion         | 1.35      |
| reward_torque         | -0.249    |
| reward_velocity       | -2.63     |
| rollout/              |           |
|    ep_len_mean        | 568       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7000      |
|    time_elapsed       | 242       |
|    total_timesteps    | 56000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000107  |
|    learning_rate      | 0.00096   |
|    n_updates          | 6999      |
|    policy_loss        | 4.41e-05  |
|    std                | 0.134     |
|    value_loss         | 1.32      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0093   |
| reward_motion         | 1.33      |
| reward_torque         | -0.246    |
| reward_velocity       | -2.62     |
| rollout/              |           |
|    ep_len_mean        | 573       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7100      |
|    time_elapsed       | 245       |
|    total_timesteps    | 56800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00312  |
|    learning_rate      | 0.00096   |
|    n_updates          | 7099      |
|    policy_loss        | -0.0278   |
|    std                | 0.134     |
|    value_loss         | 3.09      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.00951  |
| reward_motion         | 1.32      |
| reward_torque         | -0.244    |
| reward_velocity       | -2.61     |
| rollout/              |           |
|    ep_len_mean        | 578       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7200      |
|    time_elapsed       | 249       |
|    total_timesteps    | 57600     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.15     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7199      |
|    policy_loss        | 0.0194    |
|    std                | 0.134     |
|    value_loss         | 9.01      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.00949  |
| reward_motion         | 1.31      |
| reward_torque         | -0.242    |
| reward_velocity       | -2.61     |
| rollout/              |           |
|    ep_len_mean        | 582       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7300      |
|    time_elapsed       | 252       |
|    total_timesteps    | 58400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.613     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7299      |
|    policy_loss        | -0.00294  |
|    std                | 0.134     |
|    value_loss         | 0.155     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.00949  |
| reward_motion         | 1.31      |
| reward_torque         | -0.242    |
| reward_velocity       | -2.61     |
| rollout/              |           |
|    ep_len_mean        | 582       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7400      |
|    time_elapsed       | 255       |
|    total_timesteps    | 59200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0757    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7399      |
|    policy_loss        | 0.0405    |
|    std                | 0.134     |
|    value_loss         | 2.6       |
-------------------------------------
Num timesteps: 60000
Best mean reward: -966.99 - Last mean reward per episode: -1056.45
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.00991  |
| reward_motion         | 1.31      |
| reward_torque         | -0.24     |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 582       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7500      |
|    time_elapsed       | 259       |
|    total_timesteps    | 60000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0282    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7499      |
|    policy_loss        | -0.0926   |
|    std                | 0.134     |
|    value_loss         | 24.3      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0105   |
| reward_motion         | 1.29      |
| reward_torque         | -0.236    |
| reward_velocity       | -2.61     |
| rollout/              |           |
|    ep_len_mean        | 590       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7600      |
|    time_elapsed       | 262       |
|    total_timesteps    | 60800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000107  |
|    learning_rate      | 0.00096   |
|    n_updates          | 7599      |
|    policy_loss        | -0.00471  |
|    std                | 0.134     |
|    value_loss         | 2.95      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0105   |
| reward_motion         | 1.27      |
| reward_torque         | -0.231    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 600       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7700      |
|    time_elapsed       | 266       |
|    total_timesteps    | 61600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.114    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7699      |
|    policy_loss        | 0.0228    |
|    std                | 0.134     |
|    value_loss         | 0.384     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0105   |
| reward_motion         | 1.27      |
| reward_torque         | -0.231    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.08e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7800      |
|    time_elapsed       | 269       |
|    total_timesteps    | 62400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.28     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7799      |
|    policy_loss        | 0.525     |
|    std                | 0.134     |
|    value_loss         | 11.4      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0105   |
| reward_motion         | 1.27      |
| reward_torque         | -0.228    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.08e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 7900      |
|    time_elapsed       | 273       |
|    total_timesteps    | 63200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000677  |
|    learning_rate      | 0.00096   |
|    n_updates          | 7899      |
|    policy_loss        | -0.00123  |
|    std                | 0.134     |
|    value_loss         | 0.312     |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0104   |
| reward_motion         | 1.29      |
| reward_torque         | -0.225    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 588       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8000      |
|    time_elapsed       | 276       |
|    total_timesteps    | 64000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.16      |
|    learning_rate      | 0.00096   |
|    n_updates          | 7999      |
|    policy_loss        | -0.0419   |
|    std                | 0.134     |
|    value_loss         | 0.481     |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0106   |
| reward_motion         | 1.3       |
| reward_torque         | -0.225    |
| reward_velocity       | -2.59     |
| rollout/              |           |
|    ep_len_mean        | 589       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8100      |
|    time_elapsed       | 280       |
|    total_timesteps    | 64800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00265   |
|    learning_rate      | 0.00096   |
|    n_updates          | 8099      |
|    policy_loss        | -0.00345  |
|    std                | 0.134     |
|    value_loss         | 0.232     |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0106   |
| reward_motion         | 1.3       |
| reward_torque         | -0.225    |
| reward_velocity       | -2.59     |
| rollout/              |           |
|    ep_len_mean        | 589       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8200      |
|    time_elapsed       | 283       |
|    total_timesteps    | 65600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -19.3     |
|    learning_rate      | 0.00096   |
|    n_updates          | 8199      |
|    policy_loss        | 0.124     |
|    std                | 0.134     |
|    value_loss         | 44.5      |
-------------------------------------
Num timesteps: 66000
Best mean reward: -966.99 - Last mean reward per episode: -1048.26
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0106   |
| reward_motion         | 1.3       |
| reward_torque         | -0.225    |
| reward_velocity       | -2.59     |
| rollout/              |           |
|    ep_len_mean        | 589       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8300      |
|    time_elapsed       | 287       |
|    total_timesteps    | 66400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0834   |
|    learning_rate      | 0.00096   |
|    n_updates          | 8299      |
|    policy_loss        | 0.00229   |
|    std                | 0.133     |
|    value_loss         | 0.781     |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.011    |
| reward_motion         | 1.31      |
| reward_torque         | -0.228    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 589       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8400      |
|    time_elapsed       | 290       |
|    total_timesteps    | 67200     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00232  |
|    learning_rate      | 0.00096   |
|    n_updates          | 8399      |
|    policy_loss        | -0.000269 |
|    std                | 0.133     |
|    value_loss         | 2.58      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0113   |
| reward_motion         | 1.27      |
| reward_torque         | -0.22     |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8500      |
|    time_elapsed       | 293       |
|    total_timesteps    | 68000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0325    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8499      |
|    policy_loss        | 7.63e-06  |
|    std                | 0.133     |
|    value_loss         | 0.154     |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0113   |
| reward_motion         | 1.27      |
| reward_torque         | -0.219    |
| reward_velocity       | -2.56     |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8600      |
|    time_elapsed       | 297       |
|    total_timesteps    | 68800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00225  |
|    learning_rate      | 0.00096   |
|    n_updates          | 8599      |
|    policy_loss        | -0.00113  |
|    std                | 0.133     |
|    value_loss         | 0.11      |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0113   |
| reward_motion         | 1.27      |
| reward_torque         | -0.219    |
| reward_velocity       | -2.56     |
| rollout/              |           |
|    ep_len_mean        | 598       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8700      |
|    time_elapsed       | 301       |
|    total_timesteps    | 69600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.272    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8699      |
|    policy_loss        | -0.107    |
|    std                | 0.134     |
|    value_loss         | 3.46      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0118   |
| reward_motion         | 1.25      |
| reward_torque         | -0.224    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 608       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8800      |
|    time_elapsed       | 304       |
|    total_timesteps    | 70400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.802    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8799      |
|    policy_loss        | -0.00301  |
|    std                | 0.134     |
|    value_loss         | 3.13      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0118   |
| reward_motion         | 1.25      |
| reward_torque         | -0.223    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 608       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 8900      |
|    time_elapsed       | 308       |
|    total_timesteps    | 71200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.177     |
|    learning_rate      | 0.00096   |
|    n_updates          | 8899      |
|    policy_loss        | -0.0235   |
|    std                | 0.134     |
|    value_loss         | 0.196     |
-------------------------------------
Num timesteps: 72000
Best mean reward: -966.99 - Last mean reward per episode: -1085.65
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0123   |
| reward_motion         | 1.24      |
| reward_torque         | -0.218    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 618       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9000      |
|    time_elapsed       | 311       |
|    total_timesteps    | 72000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00143  |
|    learning_rate      | 0.00096   |
|    n_updates          | 8999      |
|    policy_loss        | 0.000358  |
|    std                | 0.134     |
|    value_loss         | 1.59      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0123   |
| reward_motion         | 1.24      |
| reward_torque         | -0.218    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 618       |
|    ep_rew_mean        | -1.08e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9100      |
|    time_elapsed       | 314       |
|    total_timesteps    | 72800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.81     |
|    learning_rate      | 0.00096   |
|    n_updates          | 9099      |
|    policy_loss        | 0.038     |
|    std                | 0.133     |
|    value_loss         | 3.91      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0123   |
| reward_motion         | 1.23      |
| reward_torque         | -0.216    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 618       |
|    ep_rew_mean        | -1.08e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9200      |
|    time_elapsed       | 318       |
|    total_timesteps    | 73600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00047  |
|    learning_rate      | 0.00096   |
|    n_updates          | 9199      |
|    policy_loss        | 0.000191  |
|    std                | 0.134     |
|    value_loss         | 2.38      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0124   |
| reward_motion         | 1.2       |
| reward_torque         | -0.215    |
| reward_velocity       | -2.56     |
| rollout/              |           |
|    ep_len_mean        | 628       |
|    ep_rew_mean        | -1.1e+03  |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9300      |
|    time_elapsed       | 321       |
|    total_timesteps    | 74400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000409 |
|    learning_rate      | 0.00096   |
|    n_updates          | 9299      |
|    policy_loss        | -9.25e-05 |
|    std                | 0.134     |
|    value_loss         | 0.119     |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0128   |
| reward_motion         | 1.19      |
| reward_torque         | -0.213    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 637       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9400      |
|    time_elapsed       | 325       |
|    total_timesteps    | 75200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00484  |
|    learning_rate      | 0.00096   |
|    n_updates          | 9399      |
|    policy_loss        | 0.0142    |
|    std                | 0.134     |
|    value_loss         | 0.341     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.18      |
| reward_torque         | -0.211    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 637       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9500      |
|    time_elapsed       | 328       |
|    total_timesteps    | 76000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00542  |
|    learning_rate      | 0.00096   |
|    n_updates          | 9499      |
|    policy_loss        | -2.38e-06 |
|    std                | 0.134     |
|    value_loss         | 0.902     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.18      |
| reward_torque         | -0.211    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 637       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9600      |
|    time_elapsed       | 332       |
|    total_timesteps    | 76800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00328  |
|    learning_rate      | 0.00096   |
|    n_updates          | 9599      |
|    policy_loss        | 9.54e-06  |
|    std                | 0.134     |
|    value_loss         | 1.03      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0131   |
| reward_motion         | 1.18      |
| reward_torque         | -0.209    |
| reward_velocity       | -2.5      |
| rollout/              |           |
|    ep_len_mean        | 637       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9700      |
|    time_elapsed       | 335       |
|    total_timesteps    | 77600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.191    |
|    learning_rate      | 0.00096   |
|    n_updates          | 9699      |
|    policy_loss        | 0.206     |
|    std                | 0.134     |
|    value_loss         | 1.4       |
-------------------------------------
Num timesteps: 78000
Best mean reward: -966.99 - Last mean reward per episode: -1103.54
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0131   |
| reward_motion         | 1.17      |
| reward_torque         | -0.208    |
| reward_velocity       | -2.5      |
| rollout/              |           |
|    ep_len_mean        | 637       |
|    ep_rew_mean        | -1.1e+03  |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9800      |
|    time_elapsed       | 339       |
|    total_timesteps    | 78400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -9.78e-06 |
|    learning_rate      | 0.00096   |
|    n_updates          | 9799      |
|    policy_loss        | 0.00428   |
|    std                | 0.134     |
|    value_loss         | 0.485     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.14      |
| reward_torque         | -0.2      |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 647       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 9900      |
|    time_elapsed       | 342       |
|    total_timesteps    | 79200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00529   |
|    learning_rate      | 0.00096   |
|    n_updates          | 9899      |
|    policy_loss        | 0.06      |
|    std                | 0.134     |
|    value_loss         | 1.74      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.14      |
| reward_torque         | -0.2      |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 647       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 10000     |
|    time_elapsed       | 345       |
|    total_timesteps    | 80000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0193    |
|    learning_rate      | 0.00096   |
|    n_updates          | 9999      |
|    policy_loss        | 0.0331    |
|    std                | 0.134     |
|    value_loss         | 2.69      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0134   |
| reward_motion         | 1.14      |
| reward_torque         | -0.199    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 647       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 10100     |
|    time_elapsed       | 349       |
|    total_timesteps    | 80800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.000162  |
|    learning_rate      | 0.00096   |
|    n_updates          | 10099     |
|    policy_loss        | 2.72e-05  |
|    std                | 0.134     |
|    value_loss         | 0.229     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0136   |
| reward_motion         | 1.1       |
| reward_torque         | -0.199    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 657       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 10200     |
|    time_elapsed       | 352       |
|    total_timesteps    | 81600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -23.4     |
|    learning_rate      | 0.00096   |
|    n_updates          | 10199     |
|    policy_loss        | -0.0276   |
|    std                | 0.134     |
|    value_loss         | 2.03e+03  |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0136   |
| reward_motion         | 1.1       |
| reward_torque         | -0.2      |
| reward_velocity       | -2.5      |
| rollout/              |           |
|    ep_len_mean        | 657       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 10300     |
|    time_elapsed       | 355       |
|    total_timesteps    | 82400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -32.1     |
|    learning_rate      | 0.00096   |
|    n_updates          | 10299     |
|    policy_loss        | 0.11      |
|    std                | 0.134     |
|    value_loss         | 183       |
-------------------------------------
------------------------------------
| reward                | -1.61    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.14     |
| reward_torque         | -0.199   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 642      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 232      |
|    iterations         | 10400    |
|    time_elapsed       | 358      |
|    total_timesteps    | 83200    |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 10399    |
|    policy_loss        | 0.00163  |
|    std                | 0.134    |
|    value_loss         | 2.47     |
------------------------------------
Num timesteps: 84000
Best mean reward: -966.99 - Last mean reward per episode: -1099.23
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0139   |
| reward_motion         | 1.1       |
| reward_torque         | -0.194    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 643       |
|    ep_rew_mean        | -1.1e+03  |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 10500     |
|    time_elapsed       | 361       |
|    total_timesteps    | 84000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.26e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 10499     |
|    policy_loss        | -0.00917  |
|    std                | 0.134     |
|    value_loss         | 1.02      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0139   |
| reward_motion         | 1.1       |
| reward_torque         | -0.194    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 653       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 10600     |
|    time_elapsed       | 363       |
|    total_timesteps    | 84800     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -7.71     |
|    learning_rate      | 0.00096   |
|    n_updates          | 10599     |
|    policy_loss        | 0.934     |
|    std                | 0.134     |
|    value_loss         | 4.67      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.014    |
| reward_motion         | 1.08      |
| reward_torque         | -0.186    |
| reward_velocity       | -2.47     |
| rollout/              |           |
|    ep_len_mean        | 653       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 10700     |
|    time_elapsed       | 366       |
|    total_timesteps    | 85600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -4.58     |
|    learning_rate      | 0.00096   |
|    n_updates          | 10699     |
|    policy_loss        | -0.0244   |
|    std                | 0.134     |
|    value_loss         | 1.28      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0141   |
| reward_motion         | 1.04      |
| reward_torque         | -0.181    |
| reward_velocity       | -2.43     |
| rollout/              |           |
|    ep_len_mean        | 663       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 10800     |
|    time_elapsed       | 368       |
|    total_timesteps    | 86400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00269  |
|    learning_rate      | 0.00096   |
|    n_updates          | 10799     |
|    policy_loss        | -0.0268   |
|    std                | 0.134     |
|    value_loss         | 1.03      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0144   |
| reward_motion         | 1.01      |
| reward_torque         | -0.175    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 673       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 10900     |
|    time_elapsed       | 371       |
|    total_timesteps    | 87200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0336   |
|    learning_rate      | 0.00096   |
|    n_updates          | 10899     |
|    policy_loss        | 0.0428    |
|    std                | 0.134     |
|    value_loss         | 0.225     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.979     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 683       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 11000     |
|    time_elapsed       | 374       |
|    total_timesteps    | 88000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.00446   |
|    learning_rate      | 0.00096   |
|    n_updates          | 10999     |
|    policy_loss        | 0.000757  |
|    std                | 0.134     |
|    value_loss         | 1.09      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.979     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 683       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 11100     |
|    time_elapsed       | 376       |
|    total_timesteps    | 88800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00218  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11099     |
|    policy_loss        | 0.00039   |
|    std                | 0.134     |
|    value_loss         | 1.69      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.963     |
| reward_torque         | -0.171    |
| reward_velocity       | -2.36     |
| rollout/              |           |
|    ep_len_mean        | 693       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 11200     |
|    time_elapsed       | 379       |
|    total_timesteps    | 89600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0177   |
|    learning_rate      | 0.00096   |
|    n_updates          | 11199     |
|    policy_loss        | 0.00381   |
|    std                | 0.134     |
|    value_loss         | 3.29      |
-------------------------------------
Num timesteps: 90000
Best mean reward: -966.99 - Last mean reward per episode: -1189.48
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0147   |
| reward_motion         | 0.941     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 703       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 11300     |
|    time_elapsed       | 381       |
|    total_timesteps    | 90400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00404   |
|    learning_rate      | 0.00096   |
|    n_updates          | 11299     |
|    policy_loss        | 0.000571  |
|    std                | 0.134     |
|    value_loss         | 3.04      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0147   |
| reward_motion         | 0.938     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 703       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 11400     |
|    time_elapsed       | 384       |
|    total_timesteps    | 91200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 3.37e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11399     |
|    policy_loss        | 6.2e-06   |
|    std                | 0.134     |
|    value_loss         | 0.617     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0147   |
| reward_motion         | 0.938     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 703       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 11500     |
|    time_elapsed       | 386       |
|    total_timesteps    | 92000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -3.58     |
|    learning_rate      | 0.00096   |
|    n_updates          | 11499     |
|    policy_loss        | -0.206    |
|    std                | 0.134     |
|    value_loss         | 9.32      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.965     |
| reward_torque         | -0.17     |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 238       |
|    iterations         | 11600     |
|    time_elapsed       | 389       |
|    total_timesteps    | 92800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0208   |
|    learning_rate      | 0.00096   |
|    n_updates          | 11599     |
|    policy_loss        | -0.00199  |
|    std                | 0.134     |
|    value_loss         | 2.03      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.948     |
| reward_torque         | -0.169    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 238       |
|    iterations         | 11700     |
|    time_elapsed       | 392       |
|    total_timesteps    | 93600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.025     |
|    learning_rate      | 0.00096   |
|    n_updates          | 11699     |
|    policy_loss        | -0.0491   |
|    std                | 0.134     |
|    value_loss         | 0.302     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0143   |
| reward_motion         | 0.955     |
| reward_torque         | -0.172    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 684       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 11800     |
|    time_elapsed       | 396       |
|    total_timesteps    | 94400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.286    |
|    learning_rate      | 0.00096   |
|    n_updates          | 11799     |
|    policy_loss        | 0.0038    |
|    std                | 0.134     |
|    value_loss         | 3.27      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0143   |
| reward_motion         | 0.955     |
| reward_torque         | -0.172    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 684       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 11900     |
|    time_elapsed       | 400       |
|    total_timesteps    | 95200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -7.88     |
|    learning_rate      | 0.00096   |
|    n_updates          | 11899     |
|    policy_loss        | -0.0302   |
|    std                | 0.134     |
|    value_loss         | 2.92      |
-------------------------------------
Num timesteps: 96000
Best mean reward: -966.99 - Last mean reward per episode: -1144.61
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0141   |
| reward_motion         | 0.954     |
| reward_torque         | -0.172    |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 684       |
|    ep_rew_mean        | -1.14e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12000     |
|    time_elapsed       | 404       |
|    total_timesteps    | 96000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000118 |
|    learning_rate      | 0.00096   |
|    n_updates          | 11999     |
|    policy_loss        | -0.000451 |
|    std                | 0.134     |
|    value_loss         | 4.38      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0145   |
| reward_motion         | 0.918     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12100     |
|    time_elapsed       | 407       |
|    total_timesteps    | 96800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00125  |
|    learning_rate      | 0.00096   |
|    n_updates          | 12099     |
|    policy_loss        | 0.0403    |
|    std                | 0.134     |
|    value_loss         | 2.13      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0147   |
| reward_motion         | 0.898     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 704       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12200     |
|    time_elapsed       | 411       |
|    total_timesteps    | 97600     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.105     |
|    learning_rate      | 0.00096   |
|    n_updates          | 12199     |
|    policy_loss        | -0.0542   |
|    std                | 0.134     |
|    value_loss         | 1.57      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0147   |
| reward_motion         | 0.898     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12300     |
|    time_elapsed       | 414       |
|    total_timesteps    | 98400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0371   |
|    learning_rate      | 0.00096   |
|    n_updates          | 12299     |
|    policy_loss        | 0.0427    |
|    std                | 0.134     |
|    value_loss         | 1.84      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.014    |
| reward_motion         | 0.911     |
| reward_torque         | -0.16     |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12400     |
|    time_elapsed       | 418       |
|    total_timesteps    | 99200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.41     |
|    learning_rate      | 0.00096   |
|    n_updates          | 12399     |
|    policy_loss        | 0.00105   |
|    std                | 0.134     |
|    value_loss         | 1.07      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0142   |
| reward_motion         | 0.909     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 237       |
|    iterations         | 12500     |
|    time_elapsed       | 421       |
|    total_timesteps    | 100000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.161    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12499     |
|    policy_loss        | 0.000167  |
|    std                | 0.134     |
|    value_loss         | 1.93      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0144   |
| reward_motion         | 0.909     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 12600     |
|    time_elapsed       | 425       |
|    total_timesteps    | 100800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.302    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12599     |
|    policy_loss        | -0.0274   |
|    std                | 0.134     |
|    value_loss         | 1.12      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.892     |
| reward_torque         | -0.162    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 12700     |
|    time_elapsed       | 429       |
|    total_timesteps    | 101600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00562  |
|    learning_rate      | 0.00096   |
|    n_updates          | 12699     |
|    policy_loss        | 0.0004    |
|    std                | 0.134     |
|    value_loss         | 0.199     |
-------------------------------------
Num timesteps: 102000
Best mean reward: -966.99 - Last mean reward per episode: -1152.21
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.892     |
| reward_torque         | -0.162    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 12800     |
|    time_elapsed       | 433       |
|    total_timesteps    | 102400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0277    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12799     |
|    policy_loss        | 0.00175   |
|    std                | 0.134     |
|    value_loss         | 0.0741    |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.015    |
| reward_motion         | 0.914     |
| reward_torque         | -0.172    |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 694       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 12900     |
|    time_elapsed       | 436       |
|    total_timesteps    | 103200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00243   |
|    learning_rate      | 0.00096   |
|    n_updates          | 12899     |
|    policy_loss        | 0.00657   |
|    std                | 0.134     |
|    value_loss         | 2.45      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0157   |
| reward_motion         | 0.897     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 703       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 236       |
|    iterations         | 13000     |
|    time_elapsed       | 440       |
|    total_timesteps    | 104000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0219    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12999     |
|    policy_loss        | -0.000456 |
|    std                | 0.134     |
|    value_loss         | 0.448     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0159   |
| reward_motion         | 0.899     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 703       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13100     |
|    time_elapsed       | 444       |
|    total_timesteps    | 104800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0112   |
|    learning_rate      | 0.00096   |
|    n_updates          | 13099     |
|    policy_loss        | -0.0544   |
|    std                | 0.134     |
|    value_loss         | 0.411     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0159   |
| reward_motion         | 0.899     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 713       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13200     |
|    time_elapsed       | 447       |
|    total_timesteps    | 105600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.102    |
|    learning_rate      | 0.00096   |
|    n_updates          | 13199     |
|    policy_loss        | 0.00959   |
|    std                | 0.134     |
|    value_loss         | 3.93      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0162   |
| reward_motion         | 0.889     |
| reward_torque         | -0.174    |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 713       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13300     |
|    time_elapsed       | 451       |
|    total_timesteps    | 106400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.021     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13299     |
|    policy_loss        | -0.00103  |
|    std                | 0.134     |
|    value_loss         | 5.9       |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0164   |
| reward_motion         | 0.887     |
| reward_torque         | -0.172    |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 713       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13400     |
|    time_elapsed       | 454       |
|    total_timesteps    | 107200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.678     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13399     |
|    policy_loss        | -0.27     |
|    std                | 0.134     |
|    value_loss         | 7.24      |
-------------------------------------
Num timesteps: 108000
Best mean reward: -966.99 - Last mean reward per episode: -1192.51
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0165   |
| reward_motion         | 0.866     |
| reward_torque         | -0.167    |
| reward_velocity       | -2.24     |
| rollout/              |           |
|    ep_len_mean        | 723       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13500     |
|    time_elapsed       | 458       |
|    total_timesteps    | 108000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.268     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13499     |
|    policy_loss        | 0.000654  |
|    std                | 0.134     |
|    value_loss         | 1.85      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0168   |
| reward_motion         | 0.846     |
| reward_torque         | -0.165    |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13600     |
|    time_elapsed       | 462       |
|    total_timesteps    | 108800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.47     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13599     |
|    policy_loss        | 0.0675    |
|    std                | 0.134     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0168   |
| reward_motion         | 0.846     |
| reward_torque         | -0.165    |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13700     |
|    time_elapsed       | 465       |
|    total_timesteps    | 109600    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -16.2     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13699     |
|    policy_loss        | -0.297    |
|    std                | 0.134     |
|    value_loss         | 48.3      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0167   |
| reward_motion         | 0.874     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.24     |
| rollout/              |           |
|    ep_len_mean        | 724       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13800     |
|    time_elapsed       | 469       |
|    total_timesteps    | 110400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00734   |
|    learning_rate      | 0.00096   |
|    n_updates          | 13799     |
|    policy_loss        | 0.032     |
|    std                | 0.134     |
|    value_loss         | 0.539     |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.864     |
| reward_torque         | -0.171    |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 235       |
|    iterations         | 13900     |
|    time_elapsed       | 473       |
|    total_timesteps    | 111200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00198   |
|    learning_rate      | 0.00096   |
|    n_updates          | 13899     |
|    policy_loss        | -0.00263  |
|    std                | 0.134     |
|    value_loss         | 1.26      |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0172   |
| reward_motion         | 0.841     |
| reward_torque         | -0.167    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 743       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14000     |
|    time_elapsed       | 476       |
|    total_timesteps    | 112000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.189     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13999     |
|    policy_loss        | -0.0483   |
|    std                | 0.134     |
|    value_loss         | 2.03      |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0172   |
| reward_motion         | 0.841     |
| reward_torque         | -0.167    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 743       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14100     |
|    time_elapsed       | 480       |
|    total_timesteps    | 112800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.25      |
|    learning_rate      | 0.00096   |
|    n_updates          | 14099     |
|    policy_loss        | 0.153     |
|    std                | 0.134     |
|    value_loss         | 1.13      |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0168   |
| reward_motion         | 0.843     |
| reward_torque         | -0.166    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 743       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14200     |
|    time_elapsed       | 483       |
|    total_timesteps    | 113600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000264  |
|    learning_rate      | 0.00096   |
|    n_updates          | 14199     |
|    policy_loss        | -0.00015  |
|    std                | 0.134     |
|    value_loss         | 2.14      |
-------------------------------------
Num timesteps: 114000
Best mean reward: -966.99 - Last mean reward per episode: -1213.48
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0168   |
| reward_motion         | 0.839     |
| reward_torque         | -0.164    |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 743       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14300     |
|    time_elapsed       | 487       |
|    total_timesteps    | 114400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00242  |
|    learning_rate      | 0.00096   |
|    n_updates          | 14299     |
|    policy_loss        | 0.00264   |
|    std                | 0.134     |
|    value_loss         | 1.44      |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0168   |
| reward_motion         | 0.831     |
| reward_torque         | -0.16     |
| reward_velocity       | -2.13     |
| rollout/              |           |
|    ep_len_mean        | 752       |
|    ep_rew_mean        | -1.23e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14400     |
|    time_elapsed       | 491       |
|    total_timesteps    | 115200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00865   |
|    learning_rate      | 0.00096   |
|    n_updates          | 14399     |
|    policy_loss        | -0.226    |
|    std                | 0.134     |
|    value_loss         | 0.446     |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0171   |
| reward_motion         | 0.811     |
| reward_torque         | -0.157    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 762       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14500     |
|    time_elapsed       | 495       |
|    total_timesteps    | 116000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -2.84     |
|    learning_rate      | 0.00096   |
|    n_updates          | 14499     |
|    policy_loss        | -0.161    |
|    std                | 0.134     |
|    value_loss         | 1.29      |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0171   |
| reward_motion         | 0.811     |
| reward_torque         | -0.157    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 762       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 234       |
|    iterations         | 14600     |
|    time_elapsed       | 498       |
|    total_timesteps    | 116800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -7.24     |
|    learning_rate      | 0.00096   |
|    n_updates          | 14599     |
|    policy_loss        | 0.154     |
|    std                | 0.134     |
|    value_loss         | 4.13      |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0171   |
| reward_motion         | 0.814     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 762       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 14700     |
|    time_elapsed       | 502       |
|    total_timesteps    | 117600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0676   |
|    learning_rate      | 0.00096   |
|    n_updates          | 14699     |
|    policy_loss        | -0.0469   |
|    std                | 0.134     |
|    value_loss         | 0.166     |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0175   |
| reward_motion         | 0.813     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.13     |
| rollout/              |           |
|    ep_len_mean        | 762       |
|    ep_rew_mean        | -1.23e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 14800     |
|    time_elapsed       | 506       |
|    total_timesteps    | 118400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 14799     |
|    policy_loss        | -1.91e-06 |
|    std                | 0.134     |
|    value_loss         | 0.958     |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0179   |
| reward_motion         | 0.778     |
| reward_torque         | -0.154    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 764       |
|    ep_rew_mean        | -1.23e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 14900     |
|    time_elapsed       | 510       |
|    total_timesteps    | 119200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 14899     |
|    policy_loss        | 4.77e-07  |
|    std                | 0.134     |
|    value_loss         | 2.86      |
-------------------------------------
Num timesteps: 120000
Best mean reward: -966.99 - Last mean reward per episode: -1245.71
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0179   |
| reward_motion         | 0.778     |
| reward_torque         | -0.154    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 774       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 15000     |
|    time_elapsed       | 513       |
|    total_timesteps    | 120000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 14999     |
|    policy_loss        | -6.2e-06  |
|    std                | 0.134     |
|    value_loss         | 9.68      |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.769     |
| reward_torque         | -0.15     |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 774       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 15100     |
|    time_elapsed       | 517       |
|    total_timesteps    | 120800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.178    |
|    learning_rate      | 0.00096   |
|    n_updates          | 15099     |
|    policy_loss        | 0.329     |
|    std                | 0.133     |
|    value_loss         | 0.612     |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0185   |
| reward_motion         | 0.77      |
| reward_torque         | -0.148    |
| reward_velocity       | -2.11     |
| rollout/              |           |
|    ep_len_mean        | 774       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 15200     |
|    time_elapsed       | 521       |
|    total_timesteps    | 121600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00468  |
|    learning_rate      | 0.00096   |
|    n_updates          | 15199     |
|    policy_loss        | 0.00195   |
|    std                | 0.133     |
|    value_loss         | 2.55      |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0189   |
| reward_motion         | 0.748     |
| reward_torque         | -0.141    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 781       |
|    ep_rew_mean        | -1.26e+03 |
| time/                 |           |
|    fps                | 233       |
|    iterations         | 15300     |
|    time_elapsed       | 525       |
|    total_timesteps    | 122400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 15299     |
|    policy_loss        | -9.54e-07 |
|    std                | 0.133     |
|    value_loss         | 1.09      |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0192   |
| reward_motion         | 0.732     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15400     |
|    time_elapsed       | 528       |
|    total_timesteps    | 123200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 15399     |
|    policy_loss        | 9.54e-07  |
|    std                | 0.133     |
|    value_loss         | 0.0819    |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0192   |
| reward_motion         | 0.732     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15500     |
|    time_elapsed       | 532       |
|    total_timesteps    | 124000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00355  |
|    learning_rate      | 0.00096   |
|    n_updates          | 15499     |
|    policy_loss        | -4.77e-07 |
|    std                | 0.133     |
|    value_loss         | 9.14e-07  |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0192   |
| reward_motion         | 0.732     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.07     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.26e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15600     |
|    time_elapsed       | 536       |
|    total_timesteps    | 124800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 15599     |
|    policy_loss        | 1.67e-06  |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0192   |
| reward_motion         | 0.728     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.07     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.26e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15700     |
|    time_elapsed       | 540       |
|    total_timesteps    | 125600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 1.67e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 15699     |
|    policy_loss        | 2.15e-06  |
|    std                | 0.133     |
|    value_loss         | 0.000531  |
-------------------------------------
Num timesteps: 126000
Best mean reward: -966.99 - Last mean reward per episode: -1264.28
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.019    |
| reward_motion         | 0.727     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15800     |
|    time_elapsed       | 543       |
|    total_timesteps    | 126400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00274  |
|    learning_rate      | 0.00096   |
|    n_updates          | 15799     |
|    policy_loss        | -0.00455  |
|    std                | 0.133     |
|    value_loss         | 20.8      |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.019    |
| reward_motion         | 0.727     |
| reward_torque         | -0.14     |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 800       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 15900     |
|    time_elapsed       | 547       |
|    total_timesteps    | 127200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -3.52     |
|    learning_rate      | 0.00096   |
|    n_updates          | 15899     |
|    policy_loss        | -0.185    |
|    std                | 0.133     |
|    value_loss         | 23.4      |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0193   |
| reward_motion         | 0.71      |
| reward_torque         | -0.138    |
| reward_velocity       | -2.06     |
| rollout/              |           |
|    ep_len_mean        | 800       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 16000     |
|    time_elapsed       | 551       |
|    total_timesteps    | 128000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 15999     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0196   |
| reward_motion         | 0.71      |
| reward_torque         | -0.138    |
| reward_velocity       | -2.06     |
| rollout/              |           |
|    ep_len_mean        | 800       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 16100     |
|    time_elapsed       | 554       |
|    total_timesteps    | 128800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16099     |
|    policy_loss        | -1.91e-06 |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.684     |
| reward_torque         | -0.136    |
| reward_velocity       | -2.05     |
| rollout/              |           |
|    ep_len_mean        | 804       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 232       |
|    iterations         | 16200     |
|    time_elapsed       | 558       |
|    total_timesteps    | 129600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 16199     |
|    policy_loss        | -4.53e-06 |
|    std                | 0.133     |
|    value_loss         | 1.48      |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.684     |
| reward_torque         | -0.136    |
| reward_velocity       | -2.06     |
| rollout/              |           |
|    ep_len_mean        | 804       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 16300     |
|    time_elapsed       | 562       |
|    total_timesteps    | 130400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 16299     |
|    policy_loss        | 4.77e-07  |
|    std                | 0.133     |
|    value_loss         | 3.64      |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.684     |
| reward_torque         | -0.136    |
| reward_velocity       | -2.06     |
| rollout/              |           |
|    ep_len_mean        | 804       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 16400     |
|    time_elapsed       | 565       |
|    total_timesteps    | 131200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16399     |
|    policy_loss        | -1.19e-07 |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
Num timesteps: 132000
Best mean reward: -966.99 - Last mean reward per episode: -1297.09
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0206  |
| reward_motion         | 0.657    |
| reward_torque         | -0.134   |
| reward_velocity       | -2.02    |
| rollout/              |          |
|    ep_len_mean        | 814      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 16500    |
|    time_elapsed       | 569      |
|    total_timesteps    | 132000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 16499    |
|    policy_loss        | 4.77e-07 |
|    std                | 0.133    |
|    value_loss         | 0        |
------------------------------------
------------------------------------
| reward                | -1.53    |
| reward_contact        | -0.021   |
| reward_motion         | 0.657    |
| reward_torque         | -0.134   |
| reward_velocity       | -2.03    |
| rollout/              |          |
|    ep_len_mean        | 814      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 231      |
|    iterations         | 16600    |
|    time_elapsed       | 573      |
|    total_timesteps    | 132800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 16599    |
|    policy_loss        | 1.19e-06 |
|    std                | 0.133    |
|    value_loss         | 10.4     |
------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.648     |
| reward_torque         | -0.127    |
| reward_velocity       | -2.02     |
| rollout/              |           |
|    ep_len_mean        | 823       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 16700     |
|    time_elapsed       | 576       |
|    total_timesteps    | 133600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 16699     |
|    policy_loss        | 1.19e-06  |
|    std                | 0.133     |
|    value_loss         | 2.39      |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.648     |
| reward_torque         | -0.127    |
| reward_velocity       | -2.02     |
| rollout/              |           |
|    ep_len_mean        | 833       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 16800     |
|    time_elapsed       | 580       |
|    total_timesteps    | 134400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.368     |
|    learning_rate      | 0.00096   |
|    n_updates          | 16799     |
|    policy_loss        | 0.0239    |
|    std                | 0.133     |
|    value_loss         | 1.06      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.613     |
| reward_torque         | -0.123    |
| reward_velocity       | -2.01     |
| rollout/              |           |
|    ep_len_mean        | 833       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 16900     |
|    time_elapsed       | 584       |
|    total_timesteps    | 135200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16899     |
|    policy_loss        | -1.43e-06 |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0217   |
| reward_motion         | 0.599     |
| reward_torque         | -0.12     |
| reward_velocity       | -1.99     |
| rollout/              |           |
|    ep_len_mean        | 833       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 17000     |
|    time_elapsed       | 588       |
|    total_timesteps    | 136000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 16999     |
|    policy_loss        | 2.38e-06  |
|    std                | 0.133     |
|    value_loss         | 0.0269    |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.022    |
| reward_motion         | 0.565     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.98     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 17100     |
|    time_elapsed       | 591       |
|    total_timesteps    | 136800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 17099     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.133     |
|    value_loss         | 3         |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0224   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.97     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 17200     |
|    time_elapsed       | 595       |
|    total_timesteps    | 137600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 17199     |
|    policy_loss        | -1.67e-06 |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
Num timesteps: 138000
Best mean reward: -966.99 - Last mean reward per episode: -1342.19
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0224   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.97     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 231       |
|    iterations         | 17300     |
|    time_elapsed       | 599       |
|    total_timesteps    | 138400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.322     |
|    learning_rate      | 0.00096   |
|    n_updates          | 17299     |
|    policy_loss        | -9.94e-05 |
|    std                | 0.133     |
|    value_loss         | 1.4       |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0228   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.97     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17400     |
|    time_elapsed       | 602       |
|    total_timesteps    | 139200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 17399     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.133     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.023    |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.96     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17500     |
|    time_elapsed       | 606       |
|    total_timesteps    | 140000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 17499     |
|    policy_loss        | 9.54e-07  |
|    std                | 0.133     |
|    value_loss         | 1.1       |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0233   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.97     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17600     |
|    time_elapsed       | 610       |
|    total_timesteps    | 140800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 3.58e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 17599     |
|    policy_loss        | 1.91e-06  |
|    std                | 0.133     |
|    value_loss         | 0.00108   |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.96     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17700     |
|    time_elapsed       | 614       |
|    total_timesteps    | 141600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -6.76e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 17699     |
|    policy_loss        | 9.54e-07  |
|    std                | 0.133     |
|    value_loss         | 3.6       |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.96     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17800     |
|    time_elapsed       | 617       |
|    total_timesteps    | 142400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 17799     |
|    policy_loss        | -0        |
|    std                | 0.133     |
|    value_loss         | 0.374     |
-------------------------------------
-------------------------------------
| reward                | -1.53     |
| reward_contact        | -0.0237   |
| reward_motion         | 0.564     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.96     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 17900     |
|    time_elapsed       | 621       |
|    total_timesteps    | 143200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 17899     |
|    policy_loss        | -0        |
|    std                | 0.133     |
|    value_loss         | 0.23      |
-------------------------------------
Num timesteps: 144000
Best mean reward: -966.99 - Last mean reward per episode: -1330.79
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0235   |
| reward_motion         | 0.562     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.94     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 18000     |
|    time_elapsed       | 625       |
|    total_timesteps    | 144000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 17999     |
|    policy_loss        | 2.38e-06  |
|    std                | 0.133     |
|    value_loss         | 0.585     |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0233   |
| reward_motion         | 0.561     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.94     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 18100     |
|    time_elapsed       | 628       |
|    total_timesteps    | 144800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0121    |
|    learning_rate      | 0.00096   |
|    n_updates          | 18099     |
|    policy_loss        | 0.000139  |
|    std                | 0.133     |
|    value_loss         | 1.14      |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0233   |
| reward_motion         | 0.561     |
| reward_torque         | -0.116    |
| reward_velocity       | -1.94     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 18200     |
|    time_elapsed       | 632       |
|    total_timesteps    | 145600    |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -1.85     |
|    learning_rate      | 0.00096   |
|    n_updates          | 18199     |
|    policy_loss        | -0.0615   |
|    std                | 0.133     |
|    value_loss         | 7.93      |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.562     |
| reward_torque         | -0.118    |
| reward_velocity       | -1.94     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 18300     |
|    time_elapsed       | 635       |
|    total_timesteps    | 146400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 18299     |
|    policy_loss        | -4.77e-07 |
|    std                | 0.133     |
|    value_loss         | 0.17      |
-------------------------------------
-------------------------------------
| reward                | -1.51     |
| reward_contact        | -0.0237   |
| reward_motion         | 0.562     |
| reward_torque         | -0.118    |
| reward_velocity       | -1.93     |
| rollout/              |           |
|    ep_len_mean        | 843       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 230       |
|    iterations         | 18400     |
|    time_elapsed       | 639       |
|    total_timesteps    | 147200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 18399     |
|    policy_loss        | -1.67e-06 |
|    std                | 0.133     |
|    value_loss         | 0.316     |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0241   |
| reward_motion         | 0.532     |
| reward_torque         | -0.117    |
| reward_velocity       | -1.88     |
| rollout/              |           |
|    ep_len_mean        | 853       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 18500     |
|    time_elapsed       | 643       |
|    total_timesteps    | 148000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000153 |
|    learning_rate      | 0.00096   |
|    n_updates          | 18499     |
|    policy_loss        | 3.81e-06  |
|    std                | 0.133     |
|    value_loss         | 0.00101   |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0244   |
| reward_motion         | 0.532     |
| reward_torque         | -0.117    |
| reward_velocity       | -1.88     |
| rollout/              |           |
|    ep_len_mean        | 853       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 18600     |
|    time_elapsed       | 647       |
|    total_timesteps    | 148800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 5.96e-08  |
|    learning_rate      | 0.00096   |
|    n_updates          | 18599     |
|    policy_loss        | 3.81e-06  |
|    std                | 0.133     |
|    value_loss         | 4.86      |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0244   |
| reward_motion         | 0.532     |
| reward_torque         | -0.117    |
| reward_velocity       | -1.88     |
| rollout/              |           |
|    ep_len_mean        | 853       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 18700     |
|    time_elapsed       | 650       |
|    total_timesteps    | 149600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.336     |
|    learning_rate      | 0.00096   |
|    n_updates          | 18699     |
|    policy_loss        | 0.457     |
|    std                | 0.133     |
|    value_loss         | 0.384     |
-------------------------------------
Num timesteps: 150000
Best mean reward: -966.99 - Last mean reward per episode: -1350.61
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.0246   |
| reward_motion         | 0.503     |
| reward_torque         | -0.111    |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 863       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 18800     |
|    time_elapsed       | 654       |
|    total_timesteps    | 150400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.272    |
|    learning_rate      | 0.00096   |
|    n_updates          | 18799     |
|    policy_loss        | -0.00374  |
|    std                | 0.133     |
|    value_loss         | 1.64      |
-------------------------------------
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.0248   |
| reward_motion         | 0.503     |
| reward_torque         | -0.111    |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 863       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 18900     |
|    time_elapsed       | 658       |
|    total_timesteps    | 151200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 18899     |
|    policy_loss        | 3.58e-07  |
|    std                | 0.133     |
|    value_loss         | 0.331     |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0251   |
| reward_motion         | 0.471     |
| reward_torque         | -0.108    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 19000     |
|    time_elapsed       | 662       |
|    total_timesteps    | 152000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 18999     |
|    policy_loss        | -6.34e-05 |
|    std                | 0.133     |
|    value_loss         | 0.00908   |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0251   |
| reward_motion         | 0.471     |
| reward_torque         | -0.108    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 19100     |
|    time_elapsed       | 666       |
|    total_timesteps    | 152800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -15.7     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19099     |
|    policy_loss        | 1.4       |
|    std                | 0.133     |
|    value_loss         | 85.5      |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0253   |
| reward_motion         | 0.471     |
| reward_torque         | -0.107    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 19200     |
|    time_elapsed       | 670       |
|    total_timesteps    | 153600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 19199     |
|    policy_loss        | -1.67e-06 |
|    std                | 0.133     |
|    value_loss         | 2.8       |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0253   |
| reward_motion         | 0.466     |
| reward_torque         | -0.104    |
| reward_velocity       | -1.82     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 19300     |
|    time_elapsed       | 673       |
|    total_timesteps    | 154400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 19299     |
|    policy_loss        | 2.62e-06  |
|    std                | 0.133     |
|    value_loss         | 0.0918    |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0257   |
| reward_motion         | 0.466     |
| reward_torque         | -0.104    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 229       |
|    iterations         | 19400     |
|    time_elapsed       | 677       |
|    total_timesteps    | 155200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -3.93     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19399     |
|    policy_loss        | -0.0403   |
|    std                | 0.133     |
|    value_loss         | 0.0623    |
-------------------------------------
Num timesteps: 156000
Best mean reward: -966.99 - Last mean reward per episode: -1377.71
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.026    |
| reward_motion         | 0.443     |
| reward_torque         | -0.1      |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 19500     |
|    time_elapsed       | 681       |
|    total_timesteps    | 156000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.97     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19499     |
|    policy_loss        | 0.114     |
|    std                | 0.133     |
|    value_loss         | 5.97      |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.026    |
| reward_motion         | 0.443     |
| reward_torque         | -0.1      |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 19600     |
|    time_elapsed       | 684       |
|    total_timesteps    | 156800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0635   |
|    learning_rate      | 0.00096   |
|    n_updates          | 19599     |
|    policy_loss        | 0.00606   |
|    std                | 0.133     |
|    value_loss         | 1.84      |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0262   |
| reward_motion         | 0.443     |
| reward_torque         | -0.1      |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 19700     |
|    time_elapsed       | 688       |
|    total_timesteps    | 157600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0264    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19699     |
|    policy_loss        | -0.000316 |
|    std                | 0.133     |
|    value_loss         | 4.44      |
-------------------------------------
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.026    |
| reward_motion         | 0.437     |
| reward_torque         | -0.0915   |
| reward_velocity       | -1.79     |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 19800     |
|    time_elapsed       | 692       |
|    total_timesteps    | 158400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.74     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19799     |
|    policy_loss        | 0.000481  |
|    std                | 0.133     |
|    value_loss         | 28.2      |
-------------------------------------
-------------------------------------
| reward                | -1.46     |
| reward_contact        | -0.0262   |
| reward_motion         | 0.437     |
| reward_torque         | -0.0912   |
| reward_velocity       | -1.78     |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 19900     |
|    time_elapsed       | 696       |
|    total_timesteps    | 159200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.103    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19899     |
|    policy_loss        | -8.08e-05 |
|    std                | 0.133     |
|    value_loss         | 0.0224    |
-------------------------------------
-------------------------------------
| reward                | -1.46     |
| reward_contact        | -0.0257   |
| reward_motion         | 0.443     |
| reward_torque         | -0.0922   |
| reward_velocity       | -1.79     |
| rollout/              |           |
|    ep_len_mean        | 879       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20000     |
|    time_elapsed       | 700       |
|    total_timesteps    | 160000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.554    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19999     |
|    policy_loss        | 0.0505    |
|    std                | 0.133     |
|    value_loss         | 14.5      |
-------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.0256   |
| reward_motion         | 0.467     |
| reward_torque         | -0.0956   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 876       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20100     |
|    time_elapsed       | 703       |
|    total_timesteps    | 160800    |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | 0.743     |
|    learning_rate      | 0.00096   |
|    n_updates          | 20099     |
|    policy_loss        | -0.485    |
|    std                | 0.133     |
|    value_loss         | 18.3      |
-------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.0256   |
| reward_motion         | 0.465     |
| reward_torque         | -0.0977   |
| reward_velocity       | -1.79     |
| rollout/              |           |
|    ep_len_mean        | 876       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20200     |
|    time_elapsed       | 707       |
|    total_timesteps    | 161600    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.000617 |
|    learning_rate      | 0.00096   |
|    n_updates          | 20199     |
|    policy_loss        | -0.0107   |
|    std                | 0.133     |
|    value_loss         | 5.71      |
-------------------------------------
Num timesteps: 162000
Best mean reward: -966.99 - Last mean reward per episode: -1360.06
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.0256   |
| reward_motion         | 0.465     |
| reward_torque         | -0.0977   |
| reward_velocity       | -1.79     |
| rollout/              |           |
|    ep_len_mean        | 876       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20300     |
|    time_elapsed       | 711       |
|    total_timesteps    | 162400    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.515    |
|    learning_rate      | 0.00096   |
|    n_updates          | 20299     |
|    policy_loss        | -0.26     |
|    std                | 0.133     |
|    value_loss         | 1.92      |
-------------------------------------
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0251   |
| reward_motion         | 0.451     |
| reward_torque         | -0.104    |
| reward_velocity       | -1.82     |
| rollout/              |           |
|    ep_len_mean        | 876       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20400     |
|    time_elapsed       | 714       |
|    total_timesteps    | 163200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.379     |
|    learning_rate      | 0.00096   |
|    n_updates          | 20399     |
|    policy_loss        | -0.222    |
|    std                | 0.133     |
|    value_loss         | 29.2      |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0248   |
| reward_motion         | 0.493     |
| reward_torque         | -0.111    |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 868       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20500     |
|    time_elapsed       | 718       |
|    total_timesteps    | 164000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0424   |
|    learning_rate      | 0.00096   |
|    n_updates          | 20499     |
|    policy_loss        | -0.00612  |
|    std                | 0.133     |
|    value_loss         | 0.542     |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0248   |
| reward_motion         | 0.493     |
| reward_torque         | -0.112    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 868       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20600     |
|    time_elapsed       | 721       |
|    total_timesteps    | 164800    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.0976   |
|    learning_rate      | 0.00096   |
|    n_updates          | 20599     |
|    policy_loss        | 0.1       |
|    std                | 0.134     |
|    value_loss         | 0.572     |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0248   |
| reward_motion         | 0.493     |
| reward_torque         | -0.112    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 868       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 20700     |
|    time_elapsed       | 725       |
|    total_timesteps    | 165600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0242   |
|    learning_rate      | 0.00096   |
|    n_updates          | 20699     |
|    policy_loss        | -6.87e-05 |
|    std                | 0.134     |
|    value_loss         | 0.114     |
-------------------------------------
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0242  |
| reward_motion         | 0.55     |
| reward_torque         | -0.127   |
| reward_velocity       | -1.89    |
| rollout/              |          |
|    ep_len_mean        | 837      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 228      |
|    iterations         | 20800    |
|    time_elapsed       | 729      |
|    total_timesteps    | 166400   |
| train/                |          |
|    entropy_loss       | -19.8    |
|    explained_variance | -0.011   |
|    learning_rate      | 0.00096  |
|    n_updates          | 20799    |
|    policy_loss        | 1.51     |
|    std                | 0.134    |
|    value_loss         | 8.03     |
------------------------------------
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0242  |
| reward_motion         | 0.55     |
| reward_torque         | -0.127   |
| reward_velocity       | -1.89    |
| rollout/              |          |
|    ep_len_mean        | 837      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 228      |
|    iterations         | 20900    |
|    time_elapsed       | 733      |
|    total_timesteps    | 167200   |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | 0.0465   |
|    learning_rate      | 0.00096  |
|    n_updates          | 20899    |
|    policy_loss        | 0.0456   |
|    std                | 0.134    |
|    value_loss         | 6.67     |
------------------------------------
Num timesteps: 168000
Best mean reward: -966.99 - Last mean reward per episode: -1312.10
-------------------------------------
| reward                | -1.5      |
| reward_contact        | -0.0243   |
| reward_motion         | 0.522     |
| reward_torque         | -0.125    |
| reward_velocity       | -1.88     |
| rollout/              |           |
|    ep_len_mean        | 844       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 21000     |
|    time_elapsed       | 736       |
|    total_timesteps    | 168000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -143      |
|    learning_rate      | 0.00096   |
|    n_updates          | 20999     |
|    policy_loss        | -0.403    |
|    std                | 0.134     |
|    value_loss         | 69.9      |
-------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.0238   |
| reward_motion         | 0.555     |
| reward_torque         | -0.134    |
| reward_velocity       | -1.92     |
| rollout/              |           |
|    ep_len_mean        | 848       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21100     |
|    time_elapsed       | 740       |
|    total_timesteps    | 168800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -21.8     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21099     |
|    policy_loss        | -0.131    |
|    std                | 0.134     |
|    value_loss         | 10.5      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0238   |
| reward_motion         | 0.55      |
| reward_torque         | -0.137    |
| reward_velocity       | -1.93     |
| rollout/              |           |
|    ep_len_mean        | 852       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 21200     |
|    time_elapsed       | 743       |
|    total_timesteps    | 169600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0228   |
|    learning_rate      | 0.00096   |
|    n_updates          | 21199     |
|    policy_loss        | -0.265    |
|    std                | 0.134     |
|    value_loss         | 3.64      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.604     |
| reward_torque         | -0.146    |
| reward_velocity       | -2.01     |
| rollout/              |           |
|    ep_len_mean        | 824       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 228       |
|    iterations         | 21300     |
|    time_elapsed       | 747       |
|    total_timesteps    | 170400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -2.45     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21299     |
|    policy_loss        | -0.427    |
|    std                | 0.134     |
|    value_loss         | 2.48      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0235   |
| reward_motion         | 0.613     |
| reward_torque         | -0.15     |
| reward_velocity       | -2.04     |
| rollout/              |           |
|    ep_len_mean        | 824       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21400     |
|    time_elapsed       | 751       |
|    total_timesteps    | 171200    |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -149      |
|    learning_rate      | 0.00096   |
|    n_updates          | 21399     |
|    policy_loss        | 0.0478    |
|    std                | 0.134     |
|    value_loss         | 23.4      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.64      |
| reward_torque         | -0.153    |
| reward_velocity       | -2.05     |
| rollout/              |           |
|    ep_len_mean        | 814       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21500     |
|    time_elapsed       | 754       |
|    total_timesteps    | 172000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0045    |
|    learning_rate      | 0.00096   |
|    n_updates          | 21499     |
|    policy_loss        | 0.0969    |
|    std                | 0.134     |
|    value_loss         | 0.512     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0234   |
| reward_motion         | 0.64      |
| reward_torque         | -0.153    |
| reward_velocity       | -2.04     |
| rollout/              |           |
|    ep_len_mean        | 814       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21600     |
|    time_elapsed       | 758       |
|    total_timesteps    | 172800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00492   |
|    learning_rate      | 0.00096   |
|    n_updates          | 21599     |
|    policy_loss        | -2.15e-06 |
|    std                | 0.134     |
|    value_loss         | 1.25e-07  |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0231   |
| reward_motion         | 0.64      |
| reward_torque         | -0.153    |
| reward_velocity       | -2.04     |
| rollout/              |           |
|    ep_len_mean        | 814       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21700     |
|    time_elapsed       | 762       |
|    total_timesteps    | 173600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.734    |
|    learning_rate      | 0.00096   |
|    n_updates          | 21699     |
|    policy_loss        | 0.028     |
|    std                | 0.134     |
|    value_loss         | 3.03      |
-------------------------------------
Num timesteps: 174000
Best mean reward: -966.99 - Last mean reward per episode: -1270.67
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0231   |
| reward_motion         | 0.64      |
| reward_torque         | -0.153    |
| reward_velocity       | -2.04     |
| rollout/              |           |
|    ep_len_mean        | 814       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21800     |
|    time_elapsed       | 765       |
|    total_timesteps    | 174400    |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.654    |
|    learning_rate      | 0.00096   |
|    n_updates          | 21799     |
|    policy_loss        | 0.295     |
|    std                | 0.134     |
|    value_loss         | 2.87      |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0232   |
| reward_motion         | 0.629     |
| reward_torque         | -0.157    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 818       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 21900     |
|    time_elapsed       | 769       |
|    total_timesteps    | 175200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.247    |
|    learning_rate      | 0.00096   |
|    n_updates          | 21899     |
|    policy_loss        | -0.00372  |
|    std                | 0.134     |
|    value_loss         | 0.834     |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0232   |
| reward_motion         | 0.629     |
| reward_torque         | -0.157    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 818       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22000     |
|    time_elapsed       | 772       |
|    total_timesteps    | 176000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -2.88     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21999     |
|    policy_loss        | 0.0877    |
|    std                | 0.134     |
|    value_loss         | 58.8      |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0225   |
| reward_motion         | 0.624     |
| reward_torque         | -0.159    |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 818       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22100     |
|    time_elapsed       | 776       |
|    total_timesteps    | 176800    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.717     |
|    learning_rate      | 0.00096   |
|    n_updates          | 22099     |
|    policy_loss        | -0.00706  |
|    std                | 0.134     |
|    value_loss         | 1.04      |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0222   |
| reward_motion         | 0.634     |
| reward_torque         | -0.164    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 812       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22200     |
|    time_elapsed       | 780       |
|    total_timesteps    | 177600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -15.4     |
|    learning_rate      | 0.00096   |
|    n_updates          | 22199     |
|    policy_loss        | 0.209     |
|    std                | 0.134     |
|    value_loss         | 53.5      |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0222   |
| reward_motion         | 0.634     |
| reward_torque         | -0.164    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 812       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22300     |
|    time_elapsed       | 784       |
|    total_timesteps    | 178400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -3.5e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 22299     |
|    policy_loss        | 4.77e-06  |
|    std                | 0.134     |
|    value_loss         | 0.946     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.022    |
| reward_motion         | 0.634     |
| reward_torque         | -0.164    |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 812       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22400     |
|    time_elapsed       | 787       |
|    total_timesteps    | 179200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.844    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22399     |
|    policy_loss        | 0.196     |
|    std                | 0.134     |
|    value_loss         | 0.442     |
-------------------------------------
Num timesteps: 180000
Best mean reward: -966.99 - Last mean reward per episode: -1278.33
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.637     |
| reward_torque         | -0.167    |
| reward_velocity       | -2.1      |
| rollout/              |           |
|    ep_len_mean        | 812       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22500     |
|    time_elapsed       | 791       |
|    total_timesteps    | 180000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.495     |
|    learning_rate      | 0.00096   |
|    n_updates          | 22499     |
|    policy_loss        | -0.059    |
|    std                | 0.134     |
|    value_loss         | 0.919     |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.022    |
| reward_motion         | 0.624     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 822       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22600     |
|    time_elapsed       | 794       |
|    total_timesteps    | 180800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0195   |
|    learning_rate      | 0.00096   |
|    n_updates          | 22599     |
|    policy_loss        | 0.0176    |
|    std                | 0.133     |
|    value_loss         | 0.97      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.022    |
| reward_motion         | 0.624     |
| reward_torque         | -0.168    |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 822       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22700     |
|    time_elapsed       | 798       |
|    total_timesteps    | 181600    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.772     |
|    learning_rate      | 0.00096   |
|    n_updates          | 22699     |
|    policy_loss        | 0.501     |
|    std                | 0.133     |
|    value_loss         | 2.63      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0203   |
| reward_motion         | 0.711     |
| reward_torque         | -0.173    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 783       |
|    ep_rew_mean        | -1.23e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22800     |
|    time_elapsed       | 802       |
|    total_timesteps    | 182400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.003    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22799     |
|    policy_loss        | 0.000837  |
|    std                | 0.133     |
|    value_loss         | 0.418     |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0195   |
| reward_motion         | 0.773     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.12     |
| rollout/              |           |
|    ep_len_mean        | 754       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 22900     |
|    time_elapsed       | 805       |
|    total_timesteps    | 183200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.203    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22899     |
|    policy_loss        | -0.249    |
|    std                | 0.133     |
|    value_loss         | 49.9      |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.804     |
| reward_torque         | -0.187    |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 23000     |
|    time_elapsed       | 809       |
|    total_timesteps    | 184000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0136    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22999     |
|    policy_loss        | -3.62e-05 |
|    std                | 0.133     |
|    value_loss         | 0.129     |
-------------------------------------
-------------------------------------
| reward                | -1.54     |
| reward_contact        | -0.0191   |
| reward_motion         | 0.809     |
| reward_torque         | -0.186    |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 23100     |
|    time_elapsed       | 813       |
|    total_timesteps    | 184800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 23099     |
|    policy_loss        | 5.48e-06  |
|    std                | 0.133     |
|    value_loss         | 6.29      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0189   |
| reward_motion         | 0.79      |
| reward_torque         | -0.182    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 23200     |
|    time_elapsed       | 817       |
|    total_timesteps    | 185600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00581  |
|    learning_rate      | 0.00096   |
|    n_updates          | 23199     |
|    policy_loss        | 0.0303    |
|    std                | 0.133     |
|    value_loss         | 0.255     |
-------------------------------------
Num timesteps: 186000
Best mean reward: -966.99 - Last mean reward per episode: -1163.70
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0189   |
| reward_motion         | 0.79      |
| reward_torque         | -0.182    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 23300     |
|    time_elapsed       | 820       |
|    total_timesteps    | 186400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.009    |
|    learning_rate      | 0.00096   |
|    n_updates          | 23299     |
|    policy_loss        | 0.0136    |
|    std                | 0.133     |
|    value_loss         | 2.26      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.788     |
| reward_torque         | -0.182    |
| reward_velocity       | -2.17     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 227       |
|    iterations         | 23400     |
|    time_elapsed       | 824       |
|    total_timesteps    | 187200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.194     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23399     |
|    policy_loss        | 0.198     |
|    std                | 0.133     |
|    value_loss         | 5.94      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.019    |
| reward_motion         | 0.783     |
| reward_torque         | -0.181    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 23500     |
|    time_elapsed       | 828       |
|    total_timesteps    | 188000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0666    |
|    learning_rate      | 0.00096   |
|    n_updates          | 23499     |
|    policy_loss        | 0.0134    |
|    std                | 0.133     |
|    value_loss         | 0.215     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.789     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 23600     |
|    time_elapsed       | 831       |
|    total_timesteps    | 188800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.667     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23599     |
|    policy_loss        | -0.0126   |
|    std                | 0.132     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.019    |
| reward_motion         | 0.781     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 23700     |
|    time_elapsed       | 835       |
|    total_timesteps    | 189600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.649     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23699     |
|    policy_loss        | 0.22      |
|    std                | 0.133     |
|    value_loss         | 0.394     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.019    |
| reward_motion         | 0.781     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.17e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 23800     |
|    time_elapsed       | 839       |
|    total_timesteps    | 190400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.00782  |
|    learning_rate      | 0.00096   |
|    n_updates          | 23799     |
|    policy_loss        | -0.0339   |
|    std                | 0.133     |
|    value_loss         | 13.5      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.019    |
| reward_motion         | 0.784     |
| reward_torque         | -0.186    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 23900     |
|    time_elapsed       | 843       |
|    total_timesteps    | 191200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.315     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23899     |
|    policy_loss        | 0.227     |
|    std                | 0.133     |
|    value_loss         | 11        |
-------------------------------------
Num timesteps: 192000
Best mean reward: -966.99 - Last mean reward per episode: -1179.63
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0189   |
| reward_motion         | 0.777     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24000     |
|    time_elapsed       | 846       |
|    total_timesteps    | 192000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.027     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23999     |
|    policy_loss        | -0.00113  |
|    std                | 0.133     |
|    value_loss         | 3.96      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.777     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24100     |
|    time_elapsed       | 850       |
|    total_timesteps    | 192800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0823    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24099     |
|    policy_loss        | -0.0278   |
|    std                | 0.132     |
|    value_loss         | 0.271     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.777     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24200     |
|    time_elapsed       | 854       |
|    total_timesteps    | 193600    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.0757    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24199     |
|    policy_loss        | -0.0819   |
|    std                | 0.132     |
|    value_loss         | 14.8      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0186   |
| reward_motion         | 0.777     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24300     |
|    time_elapsed       | 857       |
|    total_timesteps    | 194400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.393     |
|    learning_rate      | 0.00096   |
|    n_updates          | 24299     |
|    policy_loss        | -3.55e-05 |
|    std                | 0.132     |
|    value_loss         | 0.395     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.777     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24400     |
|    time_elapsed       | 861       |
|    total_timesteps    | 195200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.00378   |
|    learning_rate      | 0.00096   |
|    n_updates          | 24399     |
|    policy_loss        | 0.000736  |
|    std                | 0.133     |
|    value_loss         | 1.01      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.774     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24500     |
|    time_elapsed       | 865       |
|    total_timesteps    | 196000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -10.7     |
|    learning_rate      | 0.00096   |
|    n_updates          | 24499     |
|    policy_loss        | 0.03      |
|    std                | 0.133     |
|    value_loss         | 0.426     |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.774     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24600     |
|    time_elapsed       | 869       |
|    total_timesteps    | 196800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0097    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24599     |
|    policy_loss        | 0.00395   |
|    std                | 0.132     |
|    value_loss         | 6.48      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.774     |
| reward_torque         | -0.183    |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24700     |
|    time_elapsed       | 873       |
|    total_timesteps    | 197600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.00789   |
|    learning_rate      | 0.00096   |
|    n_updates          | 24699     |
|    policy_loss        | -0.0027   |
|    std                | 0.132     |
|    value_loss         | 1.27      |
-------------------------------------
Num timesteps: 198000
Best mean reward: -966.99 - Last mean reward per episode: -1190.87
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0182   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 24800     |
|    time_elapsed       | 876       |
|    total_timesteps    | 198400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -3.01     |
|    learning_rate      | 0.00096   |
|    n_updates          | 24799     |
|    policy_loss        | -0.00395  |
|    std                | 0.132     |
|    value_loss         | 5.87      |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0179  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.16    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 226      |
|    iterations         | 24900    |
|    time_elapsed       | 880      |
|    total_timesteps    | 199200   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.274   |
|    learning_rate      | 0.00096  |
|    n_updates          | 24899    |
|    policy_loss        | 0.000703 |
|    std                | 0.132    |
|    value_loss         | 8.69     |
------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0177   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.17     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.2e+03  |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 25000     |
|    time_elapsed       | 883       |
|    total_timesteps    | 200000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.21     |
|    learning_rate      | 0.00096   |
|    n_updates          | 24999     |
|    policy_loss        | -0.000186 |
|    std                | 0.132     |
|    value_loss         | 1.93      |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0177  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 226      |
|    iterations         | 25100    |
|    time_elapsed       | 887      |
|    total_timesteps    | 200800   |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -0.447   |
|    learning_rate      | 0.00096  |
|    n_updates          | 25099    |
|    policy_loss        | 0.0557   |
|    std                | 0.132    |
|    value_loss         | 2.44     |
------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0175  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.16    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 226      |
|    iterations         | 25200    |
|    time_elapsed       | 891      |
|    total_timesteps    | 201600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.00111 |
|    learning_rate      | 0.00096  |
|    n_updates          | 25199    |
|    policy_loss        | 4.67e-05 |
|    std                | 0.132    |
|    value_loss         | 0.541    |
------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0173  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 226      |
|    iterations         | 25300    |
|    time_elapsed       | 895      |
|    total_timesteps    | 202400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.157    |
|    learning_rate      | 0.00096  |
|    n_updates          | 25299    |
|    policy_loss        | -0.0142  |
|    std                | 0.132    |
|    value_loss         | 3.96     |
------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0172   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.15     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 226       |
|    iterations         | 25400     |
|    time_elapsed       | 898       |
|    total_timesteps    | 203200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0174   |
|    learning_rate      | 0.00096   |
|    n_updates          | 25399     |
|    policy_loss        | -2.43e-05 |
|    std                | 0.132     |
|    value_loss         | 0.00229   |
-------------------------------------
Num timesteps: 204000
Best mean reward: -966.99 - Last mean reward per episode: -1197.43
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0171  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.15    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 25500    |
|    time_elapsed       | 902      |
|    total_timesteps    | 204000   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.000125 |
|    learning_rate      | 0.00096  |
|    n_updates          | 25499    |
|    policy_loss        | 0.0225   |
|    std                | 0.132    |
|    value_loss         | 1.7      |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0171  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.15    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 25600    |
|    time_elapsed       | 906      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.00327 |
|    learning_rate      | 0.00096  |
|    n_updates          | 25599    |
|    policy_loss        | 3.12e-05 |
|    std                | 0.132    |
|    value_loss         | 0.274    |
------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.017   |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.16    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 25700    |
|    time_elapsed       | 910      |
|    total_timesteps    | 205600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -1.39    |
|    learning_rate      | 0.00096  |
|    n_updates          | 25699    |
|    policy_loss        | 0.00126  |
|    std                | 0.132    |
|    value_loss         | 2.13     |
------------------------------------
------------------------------------
| reward                | -1.6     |
| reward_contact        | -0.0168  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 25800    |
|    time_elapsed       | 914      |
|    total_timesteps    | 206400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.166    |
|    learning_rate      | 0.00096  |
|    n_updates          | 25799    |
|    policy_loss        | -0.984   |
|    std                | 0.132    |
|    value_loss         | 21.3     |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0166  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.16    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 25900    |
|    time_elapsed       | 917      |
|    total_timesteps    | 207200   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.00493 |
|    learning_rate      | 0.00096  |
|    n_updates          | 25899    |
|    policy_loss        | 0.000537 |
|    std                | 0.132    |
|    value_loss         | 0.614    |
------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0166   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26000     |
|    time_elapsed       | 921       |
|    total_timesteps    | 208000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.475     |
|    learning_rate      | 0.00096   |
|    n_updates          | 25999     |
|    policy_loss        | -0.0176   |
|    std                | 0.132     |
|    value_loss         | 0.787     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0165   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26100     |
|    time_elapsed       | 925       |
|    total_timesteps    | 208800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.191     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26099     |
|    policy_loss        | -0.0128   |
|    std                | 0.132     |
|    value_loss         | 2.73      |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0164  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 26200    |
|    time_elapsed       | 928      |
|    total_timesteps    | 209600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.624    |
|    learning_rate      | 0.00096  |
|    n_updates          | 26199    |
|    policy_loss        | 0.00632  |
|    std                | 0.133    |
|    value_loss         | 0.0795   |
------------------------------------
Num timesteps: 210000
Best mean reward: -966.99 - Last mean reward per episode: -1204.79
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0163  |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 26300    |
|    time_elapsed       | 932      |
|    total_timesteps    | 210400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.177   |
|    learning_rate      | 0.00096  |
|    n_updates          | 26299    |
|    policy_loss        | -0.00115 |
|    std                | 0.132    |
|    value_loss         | 0.341    |
------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0162   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.17     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26400     |
|    time_elapsed       | 936       |
|    total_timesteps    | 211200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.268     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26399     |
|    policy_loss        | 0.00192   |
|    std                | 0.132     |
|    value_loss         | 2.74      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0162   |
| reward_motion         | 0.774     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.17     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26500     |
|    time_elapsed       | 940       |
|    total_timesteps    | 212000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -4.06     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26499     |
|    policy_loss        | -0.0256   |
|    std                | 0.133     |
|    value_loss         | 17.1      |
-------------------------------------
------------------------------------
| reward                | -1.6     |
| reward_contact        | -0.016   |
| reward_motion         | 0.774    |
| reward_torque         | -0.184   |
| reward_velocity       | -2.17    |
| rollout/              |          |
|    ep_len_mean        | 746      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 225      |
|    iterations         | 26600    |
|    time_elapsed       | 943      |
|    total_timesteps    | 212800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -2.05    |
|    learning_rate      | 0.00096  |
|    n_updates          | 26599    |
|    policy_loss        | 0.00209  |
|    std                | 0.133    |
|    value_loss         | 7.66     |
------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0157   |
| reward_motion         | 0.774     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26700     |
|    time_elapsed       | 947       |
|    total_timesteps    | 213600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.131    |
|    learning_rate      | 0.00096   |
|    n_updates          | 26699     |
|    policy_loss        | 0.000159  |
|    std                | 0.133     |
|    value_loss         | 0.677     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0156   |
| reward_motion         | 0.774     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26800     |
|    time_elapsed       | 951       |
|    total_timesteps    | 214400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0137    |
|    learning_rate      | 0.00096   |
|    n_updates          | 26799     |
|    policy_loss        | 2.1e-05   |
|    std                | 0.133     |
|    value_loss         | 2.99      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0155   |
| reward_motion         | 0.774     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.19     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 26900     |
|    time_elapsed       | 955       |
|    total_timesteps    | 215200    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.127     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26899     |
|    policy_loss        | -0.02     |
|    std                | 0.133     |
|    value_loss         | 3.24      |
-------------------------------------
Num timesteps: 216000
Best mean reward: -966.99 - Last mean reward per episode: -1211.45
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0155   |
| reward_motion         | 0.774     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.19     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27000     |
|    time_elapsed       | 959       |
|    total_timesteps    | 216000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.02     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26999     |
|    policy_loss        | -0.00804  |
|    std                | 0.133     |
|    value_loss         | 5.35      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0152   |
| reward_motion         | 0.774     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27100     |
|    time_elapsed       | 962       |
|    total_timesteps    | 216800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0613    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27099     |
|    policy_loss        | 9.97e-05  |
|    std                | 0.133     |
|    value_loss         | 1.21      |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0148   |
| reward_motion         | 0.793     |
| reward_torque         | -0.19     |
| reward_velocity       | -2.23     |
| rollout/              |           |
|    ep_len_mean        | 736       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27200     |
|    time_elapsed       | 966       |
|    total_timesteps    | 217600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.57     |
|    learning_rate      | 0.00096   |
|    n_updates          | 27199     |
|    policy_loss        | -0.000158 |
|    std                | 0.133     |
|    value_loss         | 0.0909    |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.806     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27300     |
|    time_elapsed       | 970       |
|    total_timesteps    | 218400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00305  |
|    learning_rate      | 0.00096   |
|    n_updates          | 27299     |
|    policy_loss        | 0.000203  |
|    std                | 0.133     |
|    value_loss         | 0.235     |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.806     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.18e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27400     |
|    time_elapsed       | 973       |
|    total_timesteps    | 219200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00189   |
|    learning_rate      | 0.00096   |
|    n_updates          | 27399     |
|    policy_loss        | -2.91e-05 |
|    std                | 0.133     |
|    value_loss         | 0.378     |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.806     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 225       |
|    iterations         | 27500     |
|    time_elapsed       | 977       |
|    total_timesteps    | 220000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00183  |
|    learning_rate      | 0.00096   |
|    n_updates          | 27499     |
|    policy_loss        | -0.00738  |
|    std                | 0.133     |
|    value_loss         | 0.319     |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0141   |
| reward_motion         | 0.806     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 27600     |
|    time_elapsed       | 981       |
|    total_timesteps    | 220800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.134     |
|    learning_rate      | 0.00096   |
|    n_updates          | 27599     |
|    policy_loss        | -0.000128 |
|    std                | 0.133     |
|    value_loss         | 1.64      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0139   |
| reward_motion         | 0.806     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 27700     |
|    time_elapsed       | 985       |
|    total_timesteps    | 221600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -3.06     |
|    learning_rate      | 0.00096   |
|    n_updates          | 27699     |
|    policy_loss        | 0.000969  |
|    std                | 0.133     |
|    value_loss         | 5.34      |
-------------------------------------
Num timesteps: 222000
Best mean reward: -966.99 - Last mean reward per episode: -1186.96
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.014    |
| reward_motion         | 0.798     |
| reward_torque         | -0.191    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 27800     |
|    time_elapsed       | 988       |
|    total_timesteps    | 222400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.816     |
|    learning_rate      | 0.00096   |
|    n_updates          | 27799     |
|    policy_loss        | -0.0183   |
|    std                | 0.133     |
|    value_loss         | 0.365     |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.014    |
| reward_motion         | 0.798     |
| reward_torque         | -0.191    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 27900     |
|    time_elapsed       | 992       |
|    total_timesteps    | 223200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.59     |
|    learning_rate      | 0.00096   |
|    n_updates          | 27899     |
|    policy_loss        | -0.000482 |
|    std                | 0.133     |
|    value_loss         | 7.01      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0138   |
| reward_motion         | 0.798     |
| reward_torque         | -0.191    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28000     |
|    time_elapsed       | 996       |
|    total_timesteps    | 224000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0503    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27999     |
|    policy_loss        | 5.91e-05  |
|    std                | 0.133     |
|    value_loss         | 0.325     |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0137   |
| reward_motion         | 0.798     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.19e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28100     |
|    time_elapsed       | 999       |
|    total_timesteps    | 224800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.00223   |
|    learning_rate      | 0.00096   |
|    n_updates          | 28099     |
|    policy_loss        | 0.000518  |
|    std                | 0.133     |
|    value_loss         | 4.74      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0135   |
| reward_motion         | 0.798     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.2e+03  |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28200     |
|    time_elapsed       | 1003      |
|    total_timesteps    | 225600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.095    |
|    learning_rate      | 0.00096   |
|    n_updates          | 28199     |
|    policy_loss        | -7.68e-05 |
|    std                | 0.133     |
|    value_loss         | 4.57      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0135   |
| reward_motion         | 0.798     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.2e+03  |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28300     |
|    time_elapsed       | 1007      |
|    total_timesteps    | 226400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.155     |
|    learning_rate      | 0.00096   |
|    n_updates          | 28299     |
|    policy_loss        | -3.81e-06 |
|    std                | 0.133     |
|    value_loss         | 0.955     |
-------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0132  |
| reward_motion         | 0.798    |
| reward_torque         | -0.192   |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 726      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 28400    |
|    time_elapsed       | 1010     |
|    total_timesteps    | 227200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.143   |
|    learning_rate      | 0.00096  |
|    n_updates          | 28399    |
|    policy_loss        | 0.000111 |
|    std                | 0.133    |
|    value_loss         | 5.41     |
------------------------------------
Num timesteps: 228000
Best mean reward: -966.99 - Last mean reward per episode: -1198.69
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.013    |
| reward_motion         | 0.798     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.2e+03  |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28500     |
|    time_elapsed       | 1014      |
|    total_timesteps    | 228000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000752 |
|    learning_rate      | 0.00096   |
|    n_updates          | 28499     |
|    policy_loss        | -0.000451 |
|    std                | 0.133     |
|    value_loss         | 0.212     |
-------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0128  |
| reward_motion         | 0.798    |
| reward_torque         | -0.192   |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 726      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 28600    |
|    time_elapsed       | 1018     |
|    total_timesteps    | 228800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.235    |
|    learning_rate      | 0.00096  |
|    n_updates          | 28599    |
|    policy_loss        | 0.00397  |
|    std                | 0.134    |
|    value_loss         | 0.847    |
------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0127  |
| reward_motion         | 0.797    |
| reward_torque         | -0.192   |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 726      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 28700    |
|    time_elapsed       | 1021     |
|    total_timesteps    | 229600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.0258  |
|    learning_rate      | 0.00096  |
|    n_updates          | 28699    |
|    policy_loss        | -0.673   |
|    std                | 0.134    |
|    value_loss         | 8.5      |
------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0127  |
| reward_motion         | 0.797    |
| reward_torque         | -0.192   |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 726      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 28800    |
|    time_elapsed       | 1025     |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.00024 |
|    learning_rate      | 0.00096  |
|    n_updates          | 28799    |
|    policy_loss        | -4.6e-05 |
|    std                | 0.134    |
|    value_loss         | 0.377    |
------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0127   |
| reward_motion         | 0.797     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 28900     |
|    time_elapsed       | 1029      |
|    total_timesteps    | 231200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.522     |
|    learning_rate      | 0.00096   |
|    n_updates          | 28899     |
|    policy_loss        | 3.05e-05  |
|    std                | 0.134     |
|    value_loss         | 0.5       |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0126   |
| reward_motion         | 0.797     |
| reward_torque         | -0.192    |
| reward_velocity       | -2.26     |
| rollout/              |           |
|    ep_len_mean        | 726       |
|    ep_rew_mean        | -1.21e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29000     |
|    time_elapsed       | 1032      |
|    total_timesteps    | 232000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.429     |
|    learning_rate      | 0.00096   |
|    n_updates          | 28999     |
|    policy_loss        | -7.99e-05 |
|    std                | 0.134     |
|    value_loss         | 0.699     |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0129   |
| reward_motion         | 0.791     |
| reward_torque         | -0.191    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 730       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29100     |
|    time_elapsed       | 1036      |
|    total_timesteps    | 232800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -1.65     |
|    learning_rate      | 0.00096   |
|    n_updates          | 29099     |
|    policy_loss        | 0.00449   |
|    std                | 0.134     |
|    value_loss         | 0.577     |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0129   |
| reward_motion         | 0.791     |
| reward_torque         | -0.191    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29200     |
|    time_elapsed       | 1040      |
|    total_timesteps    | 233600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0246   |
|    learning_rate      | 0.00096   |
|    n_updates          | 29199     |
|    policy_loss        | -0.0187   |
|    std                | 0.134     |
|    value_loss         | 16.9      |
-------------------------------------
Num timesteps: 234000
Best mean reward: -966.99 - Last mean reward per episode: -1224.01
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0132   |
| reward_motion         | 0.767     |
| reward_torque         | -0.187    |
| reward_velocity       | -2.24     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29300     |
|    time_elapsed       | 1044      |
|    total_timesteps    | 234400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.799    |
|    learning_rate      | 0.00096   |
|    n_updates          | 29299     |
|    policy_loss        | -0.000962 |
|    std                | 0.134     |
|    value_loss         | 4.42      |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0133   |
| reward_motion         | 0.764     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.23     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29400     |
|    time_elapsed       | 1047      |
|    total_timesteps    | 235200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0298   |
|    learning_rate      | 0.00096   |
|    n_updates          | 29399     |
|    policy_loss        | -0.000762 |
|    std                | 0.134     |
|    value_loss         | 3.71      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0134   |
| reward_motion         | 0.764     |
| reward_torque         | -0.185    |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 733       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29500     |
|    time_elapsed       | 1051      |
|    total_timesteps    | 236000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0285   |
|    learning_rate      | 0.00096   |
|    n_updates          | 29499     |
|    policy_loss        | -0.00356  |
|    std                | 0.134     |
|    value_loss         | 4.33      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0134   |
| reward_motion         | 0.786     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 742       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29600     |
|    time_elapsed       | 1055      |
|    total_timesteps    | 236800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.218     |
|    learning_rate      | 0.00096   |
|    n_updates          | 29599     |
|    policy_loss        | -0.0206   |
|    std                | 0.134     |
|    value_loss         | 7.67      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0134   |
| reward_motion         | 0.786     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 742       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29700     |
|    time_elapsed       | 1059      |
|    total_timesteps    | 237600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.056    |
|    learning_rate      | 0.00096   |
|    n_updates          | 29699     |
|    policy_loss        | 0.0279    |
|    std                | 0.134     |
|    value_loss         | 7.34      |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0136   |
| reward_motion         | 0.784     |
| reward_torque         | -0.184    |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 742       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29800     |
|    time_elapsed       | 1062      |
|    total_timesteps    | 238400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.223     |
|    learning_rate      | 0.00096   |
|    n_updates          | 29799     |
|    policy_loss        | 0.00312   |
|    std                | 0.134     |
|    value_loss         | 0.712     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0134   |
| reward_motion         | 0.828     |
| reward_torque         | -0.194    |
| reward_velocity       | -2.25     |
| rollout/              |           |
|    ep_len_mean        | 732       |
|    ep_rew_mean        | -1.22e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 29900     |
|    time_elapsed       | 1066      |
|    total_timesteps    | 239200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0047    |
|    learning_rate      | 0.00096   |
|    n_updates          | 29899     |
|    policy_loss        | 0.6       |
|    std                | 0.134     |
|    value_loss         | 2.74      |
-------------------------------------
Num timesteps: 240000
Best mean reward: -966.99 - Last mean reward per episode: -1227.69
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0137   |
| reward_motion         | 0.836     |
| reward_torque         | -0.188    |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 736       |
|    ep_rew_mean        | -1.23e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30000     |
|    time_elapsed       | 1070      |
|    total_timesteps    | 240000    |
| train/                |           |
|    entropy_loss       | -19.8     |
|    explained_variance | 0.00446   |
|    learning_rate      | 0.00096   |
|    n_updates          | 29999     |
|    policy_loss        | -0.0393   |
|    std                | 0.134     |
|    value_loss         | 50.8      |
-------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0136  |
| reward_motion         | 0.888    |
| reward_torque         | -0.194   |
| reward_velocity       | -2.35    |
| rollout/              |          |
|    ep_len_mean        | 718      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 30100    |
|    time_elapsed       | 1074     |
|    total_timesteps    | 240800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.00588 |
|    learning_rate      | 0.00096  |
|    n_updates          | 30099    |
|    policy_loss        | 2.55     |
|    std                | 0.134    |
|    value_loss         | 1.85e+04 |
------------------------------------
------------------------------------
| reward                | -1.72    |
| reward_contact        | -0.0138  |
| reward_motion         | 0.899    |
| reward_torque         | -0.205   |
| reward_velocity       | -2.4     |
| rollout/              |          |
|    ep_len_mean        | 714      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 30200    |
|    time_elapsed       | 1077     |
|    total_timesteps    | 241600   |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -0.00667 |
|    learning_rate      | 0.00096  |
|    n_updates          | 30199    |
|    policy_loss        | 0.116    |
|    std                | 0.134    |
|    value_loss         | 7.72     |
------------------------------------
------------------------------------
| reward                | -1.75    |
| reward_contact        | -0.0135  |
| reward_motion         | 0.882    |
| reward_torque         | -0.203   |
| reward_velocity       | -2.41    |
| rollout/              |          |
|    ep_len_mean        | 710      |
|    ep_rew_mean        | -1.2e+03 |
| time/                 |          |
|    fps                | 224      |
|    iterations         | 30300    |
|    time_elapsed       | 1081     |
|    total_timesteps    | 242400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -1.95    |
|    learning_rate      | 0.00096  |
|    n_updates          | 30299    |
|    policy_loss        | -0.176   |
|    std                | 0.134    |
|    value_loss         | 36.7     |
------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0137   |
| reward_motion         | 1.02      |
| reward_torque         | -0.235    |
| reward_velocity       | -2.47     |
| rollout/              |           |
|    ep_len_mean        | 667       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30400     |
|    time_elapsed       | 1084      |
|    total_timesteps    | 243200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.21     |
|    learning_rate      | 0.00096   |
|    n_updates          | 30399     |
|    policy_loss        | 0.886     |
|    std                | 0.134     |
|    value_loss         | 21.9      |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0137   |
| reward_motion         | 1         |
| reward_torque         | -0.235    |
| reward_velocity       | -2.46     |
| rollout/              |           |
|    ep_len_mean        | 677       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30500     |
|    time_elapsed       | 1088      |
|    total_timesteps    | 244000    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.11      |
|    learning_rate      | 0.00096   |
|    n_updates          | 30499     |
|    policy_loss        | 0.147     |
|    std                | 0.134     |
|    value_loss         | 1.05      |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0137   |
| reward_motion         | 1         |
| reward_torque         | -0.235    |
| reward_velocity       | -2.46     |
| rollout/              |           |
|    ep_len_mean        | 687       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30600     |
|    time_elapsed       | 1092      |
|    total_timesteps    | 244800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -12.9     |
|    learning_rate      | 0.00096   |
|    n_updates          | 30599     |
|    policy_loss        | -0.284    |
|    std                | 0.134     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0136   |
| reward_motion         | 1.04      |
| reward_torque         | -0.241    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 640       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30700     |
|    time_elapsed       | 1095      |
|    total_timesteps    | 245600    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0961   |
|    learning_rate      | 0.00096   |
|    n_updates          | 30699     |
|    policy_loss        | -0.00926  |
|    std                | 0.134     |
|    value_loss         | 19.8      |
-------------------------------------
Num timesteps: 246000
Best mean reward: -966.99 - Last mean reward per episode: -1090.40
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0134   |
| reward_motion         | 1.08      |
| reward_torque         | -0.248    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 640       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30800     |
|    time_elapsed       | 1099      |
|    total_timesteps    | 246400    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0957   |
|    learning_rate      | 0.00096   |
|    n_updates          | 30799     |
|    policy_loss        | 0.433     |
|    std                | 0.134     |
|    value_loss         | 9.4       |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0133   |
| reward_motion         | 1.05      |
| reward_torque         | -0.248    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 650       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 224       |
|    iterations         | 30900     |
|    time_elapsed       | 1103      |
|    total_timesteps    | 247200    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.298     |
|    learning_rate      | 0.00096   |
|    n_updates          | 30899     |
|    policy_loss        | -0.051    |
|    std                | 0.134     |
|    value_loss         | 5.47      |
-------------------------------------
-------------------------------------
| reward                | -1.86     |
| reward_contact        | -0.0134   |
| reward_motion         | 1.01      |
| reward_torque         | -0.239    |
| reward_velocity       | -2.63     |
| rollout/              |           |
|    ep_len_mean        | 660       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 31000     |
|    time_elapsed       | 1107      |
|    total_timesteps    | 248000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000142 |
|    learning_rate      | 0.00096   |
|    n_updates          | 30999     |
|    policy_loss        | -0.0365   |
|    std                | 0.134     |
|    value_loss         | 23.4      |
-------------------------------------
-------------------------------------
| reward                | -1.91     |
| reward_contact        | -0.0134   |
| reward_motion         | 1.03      |
| reward_torque         | -0.243    |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 657       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 31100     |
|    time_elapsed       | 1110      |
|    total_timesteps    | 248800    |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.783    |
|    learning_rate      | 0.00096   |
|    n_updates          | 31099     |
|    policy_loss        | -0.683    |
|    std                | 0.134     |
|    value_loss         | 22.2      |
-------------------------------------
-------------------------------------
| reward                | -1.91     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.05      |
| reward_torque         | -0.243    |
| reward_velocity       | -2.7      |
| rollout/              |           |
|    ep_len_mean        | 657       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 31200     |
|    time_elapsed       | 1114      |
|    total_timesteps    | 249600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000197  |
|    learning_rate      | 0.00096   |
|    n_updates          | 31199     |
|    policy_loss        | -0.000404 |
|    std                | 0.134     |
|    value_loss         | 2.67      |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.013    |
| reward_motion         | 1.1       |
| reward_torque         | -0.265    |
| reward_velocity       | -2.74     |
| rollout/              |           |
|    ep_len_mean        | 628       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 31300     |
|    time_elapsed       | 1118      |
|    total_timesteps    | 250400    |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.109     |
|    learning_rate      | 0.00096   |
|    n_updates          | 31299     |
|    policy_loss        | -0.0126   |
|    std                | 0.134     |
|    value_loss         | 0.762     |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.013    |
| reward_motion         | 1.1       |
| reward_torque         | -0.265    |
| reward_velocity       | -2.74     |
| rollout/              |           |
|    ep_len_mean        | 628       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 31400     |
|    time_elapsed       | 1122      |
|    total_timesteps    | 251200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.00849  |
|    learning_rate      | 0.00096   |
|    n_updates          | 31399     |
|    policy_loss        | 0.00974   |
|    std                | 0.135     |
|    value_loss         | 0.807     |
-------------------------------------
Num timesteps: 252000
Best mean reward: -966.99 - Last mean reward per episode: -996.62
------------------------------------
| reward                | -1.91    |
| reward_contact        | -0.0116  |
| reward_motion         | 1.24     |
| reward_torque         | -0.301   |
| reward_velocity       | -2.84    |
| rollout/              |          |
|    ep_len_mean        | 571      |
|    ep_rew_mean        | -997     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 31500    |
|    time_elapsed       | 1125     |
|    total_timesteps    | 252000   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -5.12    |
|    learning_rate      | 0.00096  |
|    n_updates          | 31499    |
|    policy_loss        | -0.0242  |
|    std                | 0.135    |
|    value_loss         | 2.84     |
------------------------------------
------------------------------------
| reward                | -1.91    |
| reward_contact        | -0.0116  |
| reward_motion         | 1.24     |
| reward_torque         | -0.301   |
| reward_velocity       | -2.84    |
| rollout/              |          |
|    ep_len_mean        | 571      |
|    ep_rew_mean        | -998     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 31600    |
|    time_elapsed       | 1129     |
|    total_timesteps    | 252800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -2.61    |
|    learning_rate      | 0.00096  |
|    n_updates          | 31599    |
|    policy_loss        | -0.176   |
|    std                | 0.135    |
|    value_loss         | 2.38     |
------------------------------------
------------------------------------
| reward                | -1.92    |
| reward_contact        | -0.0117  |
| reward_motion         | 1.24     |
| reward_torque         | -0.301   |
| reward_velocity       | -2.84    |
| rollout/              |          |
|    ep_len_mean        | 571      |
|    ep_rew_mean        | -998     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 31700    |
|    time_elapsed       | 1133     |
|    total_timesteps    | 253600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00114  |
|    learning_rate      | 0.00096  |
|    n_updates          | 31699    |
|    policy_loss        | -0.00284 |
|    std                | 0.135    |
|    value_loss         | 1.43     |
------------------------------------
------------------------------------
| reward                | -1.9     |
| reward_contact        | -0.0113  |
| reward_motion         | 1.29     |
| reward_torque         | -0.309   |
| reward_velocity       | -2.87    |
| rollout/              |          |
|    ep_len_mean        | 552      |
|    ep_rew_mean        | -961     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 31800    |
|    time_elapsed       | 1137     |
|    total_timesteps    | 254400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.0185   |
|    learning_rate      | 0.00096  |
|    n_updates          | 31799    |
|    policy_loss        | -0.219   |
|    std                | 0.135    |
|    value_loss         | 4.46     |
------------------------------------
------------------------------------
| reward                | -1.89    |
| reward_contact        | -0.0112  |
| reward_motion         | 1.29     |
| reward_torque         | -0.309   |
| reward_velocity       | -2.86    |
| rollout/              |          |
|    ep_len_mean        | 552      |
|    ep_rew_mean        | -962     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 31900    |
|    time_elapsed       | 1140     |
|    total_timesteps    | 255200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.12    |
|    learning_rate      | 0.00096  |
|    n_updates          | 31899    |
|    policy_loss        | -0.0763  |
|    std                | 0.135    |
|    value_loss         | 25.6     |
------------------------------------
------------------------------------
| reward                | -1.87    |
| reward_contact        | -0.011   |
| reward_motion         | 1.33     |
| reward_torque         | -0.317   |
| reward_velocity       | -2.87    |
| rollout/              |          |
|    ep_len_mean        | 541      |
|    ep_rew_mean        | -948     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32000    |
|    time_elapsed       | 1144     |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.697   |
|    learning_rate      | 0.00096  |
|    n_updates          | 31999    |
|    policy_loss        | 1.22     |
|    std                | 0.135    |
|    value_loss         | 4.9      |
------------------------------------
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.0106  |
| reward_motion         | 1.41     |
| reward_torque         | -0.324   |
| reward_velocity       | -2.91    |
| rollout/              |          |
|    ep_len_mean        | 533      |
|    ep_rew_mean        | -939     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32100    |
|    time_elapsed       | 1148     |
|    total_timesteps    | 256800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.581    |
|    learning_rate      | 0.00096  |
|    n_updates          | 32099    |
|    policy_loss        | -0.0667  |
|    std                | 0.135    |
|    value_loss         | 0.697    |
------------------------------------
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.0101  |
| reward_motion         | 1.45     |
| reward_torque         | -0.329   |
| reward_velocity       | -2.94    |
| rollout/              |          |
|    ep_len_mean        | 523      |
|    ep_rew_mean        | -924     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32200    |
|    time_elapsed       | 1151     |
|    total_timesteps    | 257600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.0773  |
|    learning_rate      | 0.00096  |
|    n_updates          | 32199    |
|    policy_loss        | -0.103   |
|    std                | 0.135    |
|    value_loss         | 9.02     |
------------------------------------
Num timesteps: 258000
Best mean reward: -966.99 - Last mean reward per episode: -924.24
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00994 |
| reward_motion         | 1.45     |
| reward_torque         | -0.329   |
| reward_velocity       | -2.93    |
| rollout/              |          |
|    ep_len_mean        | 523      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32300    |
|    time_elapsed       | 1155     |
|    total_timesteps    | 258400   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.00256 |
|    learning_rate      | 0.00096  |
|    n_updates          | 32299    |
|    policy_loss        | -0.00152 |
|    std                | 0.135    |
|    value_loss         | 0.51     |
------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.00994  |
| reward_motion         | 1.45      |
| reward_torque         | -0.329    |
| reward_velocity       | -2.93     |
| rollout/              |           |
|    ep_len_mean        | 523       |
|    ep_rew_mean        | -928      |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 32400     |
|    time_elapsed       | 1159      |
|    total_timesteps    | 259200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 32399     |
|    policy_loss        | -1.43e-06 |
|    std                | 0.135     |
|    value_loss         | 0.237     |
-------------------------------------
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.00999 |
| reward_motion         | 1.45     |
| reward_torque         | -0.329   |
| reward_velocity       | -2.93    |
| rollout/              |          |
|    ep_len_mean        | 523      |
|    ep_rew_mean        | -926     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32500    |
|    time_elapsed       | 1163     |
|    total_timesteps    | 260000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00224  |
|    learning_rate      | 0.00096  |
|    n_updates          | 32499    |
|    policy_loss        | 0.00106  |
|    std                | 0.135    |
|    value_loss         | 1.01     |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00978 |
| reward_motion         | 1.48     |
| reward_torque         | -0.333   |
| reward_velocity       | -2.95    |
| rollout/              |          |
|    ep_len_mean        | 513      |
|    ep_rew_mean        | -911     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32600    |
|    time_elapsed       | 1167     |
|    total_timesteps    | 260800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0655  |
|    learning_rate      | 0.00096  |
|    n_updates          | 32599    |
|    policy_loss        | -0.00168 |
|    std                | 0.135    |
|    value_loss         | 0.0515   |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00953 |
| reward_motion         | 1.48     |
| reward_torque         | -0.333   |
| reward_velocity       | -2.95    |
| rollout/              |          |
|    ep_len_mean        | 513      |
|    ep_rew_mean        | -912     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32700    |
|    time_elapsed       | 1170     |
|    total_timesteps    | 261600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.583   |
|    learning_rate      | 0.00096  |
|    n_updates          | 32699    |
|    policy_loss        | 0.0889   |
|    std                | 0.135    |
|    value_loss         | 0.198    |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00953 |
| reward_motion         | 1.48     |
| reward_torque         | -0.333   |
| reward_velocity       | -2.95    |
| rollout/              |          |
|    ep_len_mean        | 513      |
|    ep_rew_mean        | -910     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32800    |
|    time_elapsed       | 1174     |
|    total_timesteps    | 262400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.0606  |
|    learning_rate      | 0.00096  |
|    n_updates          | 32799    |
|    policy_loss        | 0.188    |
|    std                | 0.135    |
|    value_loss         | 1.53     |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00935 |
| reward_motion         | 1.48     |
| reward_torque         | -0.334   |
| reward_velocity       | -2.95    |
| rollout/              |          |
|    ep_len_mean        | 513      |
|    ep_rew_mean        | -910     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 32900    |
|    time_elapsed       | 1178     |
|    total_timesteps    | 263200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.000442 |
|    learning_rate      | 0.00096  |
|    n_updates          | 32899    |
|    policy_loss        | 0.00454  |
|    std                | 0.135    |
|    value_loss         | 1.08     |
------------------------------------
Num timesteps: 264000
Best mean reward: -924.24 - Last mean reward per episode: -902.00
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.84    |
| reward_contact        | -0.00917 |
| reward_motion         | 1.5      |
| reward_torque         | -0.339   |
| reward_velocity       | -2.99    |
| rollout/              |          |
|    ep_len_mean        | 505      |
|    ep_rew_mean        | -902     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33000    |
|    time_elapsed       | 1181     |
|    total_timesteps    | 264000   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.531   |
|    learning_rate      | 0.00096  |
|    n_updates          | 32999    |
|    policy_loss        | -0.0573  |
|    std                | 0.135    |
|    value_loss         | 1.22     |
------------------------------------
------------------------------------
| reward                | -1.86    |
| reward_contact        | -0.00879 |
| reward_motion         | 1.51     |
| reward_torque         | -0.345   |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 495      |
|    ep_rew_mean        | -886     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33100    |
|    time_elapsed       | 1185     |
|    total_timesteps    | 264800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.137    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33099    |
|    policy_loss        | 0.00103  |
|    std                | 0.135    |
|    value_loss         | 1.11     |
------------------------------------
------------------------------------
| reward                | -1.86    |
| reward_contact        | -0.00879 |
| reward_motion         | 1.51     |
| reward_torque         | -0.345   |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 495      |
|    ep_rew_mean        | -886     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33200    |
|    time_elapsed       | 1189     |
|    total_timesteps    | 265600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.425   |
|    learning_rate      | 0.00096  |
|    n_updates          | 33199    |
|    policy_loss        | -0.091   |
|    std                | 0.136    |
|    value_loss         | 2.33     |
------------------------------------
------------------------------------
| reward                | -1.86    |
| reward_contact        | -0.00847 |
| reward_motion         | 1.54     |
| reward_torque         | -0.35    |
| reward_velocity       | -3.05    |
| rollout/              |          |
|    ep_len_mean        | 485      |
|    ep_rew_mean        | -874     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33300    |
|    time_elapsed       | 1193     |
|    total_timesteps    | 266400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 8.26e-05 |
|    learning_rate      | 0.00096  |
|    n_updates          | 33299    |
|    policy_loss        | 0.000187 |
|    std                | 0.135    |
|    value_loss         | 0.132    |
------------------------------------
------------------------------------
| reward                | -1.84    |
| reward_contact        | -0.00888 |
| reward_motion         | 1.53     |
| reward_torque         | -0.345   |
| reward_velocity       | -3.01    |
| rollout/              |          |
|    ep_len_mean        | 495      |
|    ep_rew_mean        | -890     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33400    |
|    time_elapsed       | 1196     |
|    total_timesteps    | 267200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.348    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33399    |
|    policy_loss        | 0.252    |
|    std                | 0.135    |
|    value_loss         | 3.19     |
------------------------------------
------------------------------------
| reward                | -1.79    |
| reward_contact        | -0.00884 |
| reward_motion         | 1.56     |
| reward_torque         | -0.352   |
| reward_velocity       | -2.98    |
| rollout/              |          |
|    ep_len_mean        | 486      |
|    ep_rew_mean        | -872     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33500    |
|    time_elapsed       | 1200     |
|    total_timesteps    | 268000   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.156    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33499    |
|    policy_loss        | -0.302   |
|    std                | 0.135    |
|    value_loss         | 4.63     |
------------------------------------
------------------------------------
| reward                | -1.79    |
| reward_contact        | -0.00862 |
| reward_motion         | 1.57     |
| reward_torque         | -0.355   |
| reward_velocity       | -3       |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -864     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33600    |
|    time_elapsed       | 1204     |
|    total_timesteps    | 268800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -3.18    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33599    |
|    policy_loss        | 0.128    |
|    std                | 0.135    |
|    value_loss         | 57.5     |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00856 |
| reward_motion         | 1.57     |
| reward_torque         | -0.358   |
| reward_velocity       | -3.01    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -862     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33700    |
|    time_elapsed       | 1207     |
|    total_timesteps    | 269600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.157    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33699    |
|    policy_loss        | 0.0104   |
|    std                | 0.135    |
|    value_loss         | 14.7     |
------------------------------------
Num timesteps: 270000
Best mean reward: -902.00 - Last mean reward per episode: -862.39
Saving new best model to rl/out_dir/models/exp73/best_model.zip
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.00856  |
| reward_motion         | 1.57      |
| reward_torque         | -0.358    |
| reward_velocity       | -3.01     |
| rollout/              |           |
|    ep_len_mean        | 482       |
|    ep_rew_mean        | -862      |
| time/                 |           |
|    fps                | 223       |
|    iterations         | 33800     |
|    time_elapsed       | 1211      |
|    total_timesteps    | 270400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 33799     |
|    policy_loss        | 2.38e-07  |
|    std                | 0.135     |
|    value_loss         | 0.182     |
-------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00869 |
| reward_motion         | 1.57     |
| reward_torque         | -0.358   |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -861     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 33900    |
|    time_elapsed       | 1215     |
|    total_timesteps    | 271200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 33899    |
|    policy_loss        | 1.79e-06 |
|    std                | 0.135    |
|    value_loss         | 20.6     |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00888 |
| reward_motion         | 1.57     |
| reward_torque         | -0.358   |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -856     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 34000    |
|    time_elapsed       | 1219     |
|    total_timesteps    | 272000   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -5.84    |
|    learning_rate      | 0.00096  |
|    n_updates          | 33999    |
|    policy_loss        | -0.193   |
|    std                | 0.135    |
|    value_loss         | 9.72     |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00931 |
| reward_motion         | 1.58     |
| reward_torque         | -0.359   |
| reward_velocity       | -3.03    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -859     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 34100    |
|    time_elapsed       | 1223     |
|    total_timesteps    | 272800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.312    |
|    learning_rate      | 0.00096  |
|    n_updates          | 34099    |
|    policy_loss        | 0.609    |
|    std                | 0.135    |
|    value_loss         | 0.909    |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.00931 |
| reward_motion         | 1.58     |
| reward_torque         | -0.359   |
| reward_velocity       | -3.03    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -861     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 34200    |
|    time_elapsed       | 1226     |
|    total_timesteps    | 273600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -27.8    |
|    learning_rate      | 0.00096  |
|    n_updates          | 34199    |
|    policy_loss        | -0.316   |
|    std                | 0.135    |
|    value_loss         | 75.1     |
------------------------------------
------------------------------------
| reward                | -1.8     |
| reward_contact        | -0.00922 |
| reward_motion         | 1.58     |
| reward_torque         | -0.36    |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -861     |
| time/                 |          |
|    fps                | 223      |
|    iterations         | 34300    |
|    time_elapsed       | 1230     |
|    total_timesteps    | 274400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0709   |
|    learning_rate      | 0.00096  |
|    n_updates          | 34299    |
|    policy_loss        | 0.00474  |
|    std                | 0.135    |
|    value_loss         | 2.26     |
------------------------------------
-------------------------------------
| reward                | -1.8      |
| reward_contact        | -0.00948  |
| reward_motion         | 1.58      |
| reward_torque         | -0.359    |
| reward_velocity       | -3.01     |
| rollout/              |           |
|    ep_len_mean        | 482       |
|    ep_rew_mean        | -853      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 34400     |
|    time_elapsed       | 1234      |
|    total_timesteps    | 275200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -2.55e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 34399     |
|    policy_loss        | 0.0375    |
|    std                | 0.135     |
|    value_loss         | 0.407     |
-------------------------------------
Num timesteps: 276000
Best mean reward: -862.39 - Last mean reward per episode: -854.57
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00956 |
| reward_motion         | 1.58     |
| reward_torque         | -0.36    |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 482      |
|    ep_rew_mean        | -855     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 34500    |
|    time_elapsed       | 1238     |
|    total_timesteps    | 276000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.05    |
|    learning_rate      | 0.00096  |
|    n_updates          | 34499    |
|    policy_loss        | 0.000261 |
|    std                | 0.135    |
|    value_loss         | 0.261    |
------------------------------------
------------------------------------
| reward                | -1.8     |
| reward_contact        | -0.00955 |
| reward_motion         | 1.59     |
| reward_torque         | -0.363   |
| reward_velocity       | -3.01    |
| rollout/              |          |
|    ep_len_mean        | 473      |
|    ep_rew_mean        | -837     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 34600    |
|    time_elapsed       | 1242     |
|    total_timesteps    | 276800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -1.74    |
|    learning_rate      | 0.00096  |
|    n_updates          | 34599    |
|    policy_loss        | 0.167    |
|    std                | 0.135    |
|    value_loss         | 3.18     |
------------------------------------
------------------------------------
| reward                | -1.79    |
| reward_contact        | -0.00936 |
| reward_motion         | 1.61     |
| reward_torque         | -0.366   |
| reward_velocity       | -3.02    |
| rollout/              |          |
|    ep_len_mean        | 473      |
|    ep_rew_mean        | -837     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 34700    |
|    time_elapsed       | 1245     |
|    total_timesteps    | 277600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.114   |
|    learning_rate      | 0.00096  |
|    n_updates          | 34699    |
|    policy_loss        | 0.187    |
|    std                | 0.135    |
|    value_loss         | 8.84     |
------------------------------------
-------------------------------------
| reward                | -1.78     |
| reward_contact        | -0.00911  |
| reward_motion         | 1.63      |
| reward_torque         | -0.37     |
| reward_velocity       | -3.03     |
| rollout/              |           |
|    ep_len_mean        | 473       |
|    ep_rew_mean        | -834      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 34800     |
|    time_elapsed       | 1249      |
|    total_timesteps    | 278400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.000441 |
|    learning_rate      | 0.00096   |
|    n_updates          | 34799     |
|    policy_loss        | 2.38e-06  |
|    std                | 0.136     |
|    value_loss         | 0.00765   |
-------------------------------------
------------------------------------
| reward                | -1.77    |
| reward_contact        | -0.00897 |
| reward_motion         | 1.66     |
| reward_torque         | -0.374   |
| reward_velocity       | -3.05    |
| rollout/              |          |
|    ep_len_mean        | 463      |
|    ep_rew_mean        | -814     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 34900    |
|    time_elapsed       | 1253     |
|    total_timesteps    | 279200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.00464 |
|    learning_rate      | 0.00096  |
|    n_updates          | 34899    |
|    policy_loss        | 0.011    |
|    std                | 0.136    |
|    value_loss         | 0.675    |
------------------------------------
------------------------------------
| reward                | -1.77    |
| reward_contact        | -0.00908 |
| reward_motion         | 1.66     |
| reward_torque         | -0.374   |
| reward_velocity       | -3.04    |
| rollout/              |          |
|    ep_len_mean        | 463      |
|    ep_rew_mean        | -814     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35000    |
|    time_elapsed       | 1257     |
|    total_timesteps    | 280000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.792    |
|    learning_rate      | 0.00096  |
|    n_updates          | 34999    |
|    policy_loss        | -0.0374  |
|    std                | 0.135    |
|    value_loss         | 0.075    |
------------------------------------
------------------------------------
| reward                | -1.8     |
| reward_contact        | -0.00885 |
| reward_motion         | 1.68     |
| reward_torque         | -0.381   |
| reward_velocity       | -3.08    |
| rollout/              |          |
|    ep_len_mean        | 459      |
|    ep_rew_mean        | -807     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35100    |
|    time_elapsed       | 1260     |
|    total_timesteps    | 280800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.206    |
|    learning_rate      | 0.00096  |
|    n_updates          | 35099    |
|    policy_loss        | -0.0263  |
|    std                | 0.136    |
|    value_loss         | 0.54     |
------------------------------------
------------------------------------
| reward                | -1.8     |
| reward_contact        | -0.00885 |
| reward_motion         | 1.68     |
| reward_torque         | -0.381   |
| reward_velocity       | -3.08    |
| rollout/              |          |
|    ep_len_mean        | 449      |
|    ep_rew_mean        | -792     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35200    |
|    time_elapsed       | 1264     |
|    total_timesteps    | 281600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.692   |
|    learning_rate      | 0.00096  |
|    n_updates          | 35199    |
|    policy_loss        | -0.253   |
|    std                | 0.135    |
|    value_loss         | 0.469    |
------------------------------------
Num timesteps: 282000
Best mean reward: -854.57 - Last mean reward per episode: -792.37
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.84    |
| reward_contact        | -0.00889 |
| reward_motion         | 1.67     |
| reward_torque         | -0.378   |
| reward_velocity       | -3.12    |
| rollout/              |          |
|    ep_len_mean        | 449      |
|    ep_rew_mean        | -792     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35300    |
|    time_elapsed       | 1268     |
|    total_timesteps    | 282400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.0212  |
|    learning_rate      | 0.00096  |
|    n_updates          | 35299    |
|    policy_loss        | -0.00818 |
|    std                | 0.136    |
|    value_loss         | 0.394    |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00865 |
| reward_motion         | 1.67     |
| reward_torque         | -0.378   |
| reward_velocity       | -3.09    |
| rollout/              |          |
|    ep_len_mean        | 443      |
|    ep_rew_mean        | -787     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35400    |
|    time_elapsed       | 1272     |
|    total_timesteps    | 283200   |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -5.34    |
|    learning_rate      | 0.00096  |
|    n_updates          | 35399    |
|    policy_loss        | -0.6     |
|    std                | 0.136    |
|    value_loss         | 23.9     |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00865 |
| reward_motion         | 1.67     |
| reward_torque         | -0.378   |
| reward_velocity       | -3.09    |
| rollout/              |          |
|    ep_len_mean        | 443      |
|    ep_rew_mean        | -787     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35500    |
|    time_elapsed       | 1276     |
|    total_timesteps    | 284000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.0081   |
|    learning_rate      | 0.00096  |
|    n_updates          | 35499    |
|    policy_loss        | -0.00255 |
|    std                | 0.136    |
|    value_loss         | 2.81     |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00874 |
| reward_motion         | 1.64     |
| reward_torque         | -0.372   |
| reward_velocity       | -3.07    |
| rollout/              |          |
|    ep_len_mean        | 452      |
|    ep_rew_mean        | -807     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35600    |
|    time_elapsed       | 1279     |
|    total_timesteps    | 284800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0976  |
|    learning_rate      | 0.00096  |
|    n_updates          | 35599    |
|    policy_loss        | 0.0833   |
|    std                | 0.136    |
|    value_loss         | 0.25     |
------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.00922  |
| reward_motion         | 1.63      |
| reward_torque         | -0.371    |
| reward_velocity       | -3.06     |
| rollout/              |           |
|    ep_len_mean        | 456       |
|    ep_rew_mean        | -809      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 35700     |
|    time_elapsed       | 1283      |
|    total_timesteps    | 285600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.000101 |
|    learning_rate      | 0.00096   |
|    n_updates          | 35699     |
|    policy_loss        | -0.0215   |
|    std                | 0.136     |
|    value_loss         | 2.29      |
-------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00944 |
| reward_motion         | 1.6      |
| reward_torque         | -0.368   |
| reward_velocity       | -3.04    |
| rollout/              |          |
|    ep_len_mean        | 466      |
|    ep_rew_mean        | -825     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35800    |
|    time_elapsed       | 1286     |
|    total_timesteps    | 286400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00118 |
|    learning_rate      | 0.00096  |
|    n_updates          | 35799    |
|    policy_loss        | -0.00116 |
|    std                | 0.136    |
|    value_loss         | 0.226    |
------------------------------------
------------------------------------
| reward                | -1.81    |
| reward_contact        | -0.00944 |
| reward_motion         | 1.6      |
| reward_torque         | -0.368   |
| reward_velocity       | -3.04    |
| rollout/              |          |
|    ep_len_mean        | 476      |
|    ep_rew_mean        | -844     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 35900    |
|    time_elapsed       | 1290     |
|    total_timesteps    | 287200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.596    |
|    learning_rate      | 0.00096  |
|    n_updates          | 35899    |
|    policy_loss        | -0.0286  |
|    std                | 0.136    |
|    value_loss         | 2.01     |
------------------------------------
Num timesteps: 288000
Best mean reward: -792.37 - Last mean reward per episode: -843.82
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.00949  |
| reward_motion         | 1.58      |
| reward_torque         | -0.365    |
| reward_velocity       | -3.04     |
| rollout/              |           |
|    ep_len_mean        | 476       |
|    ep_rew_mean        | -844      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 36000     |
|    time_elapsed       | 1294      |
|    total_timesteps    | 288000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 1.54e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 35999     |
|    policy_loss        | -0.000268 |
|    std                | 0.136     |
|    value_loss         | 0.0372    |
-------------------------------------
------------------------------------
| reward                | -1.8     |
| reward_contact        | -0.0105  |
| reward_motion         | 1.57     |
| reward_torque         | -0.358   |
| reward_velocity       | -3       |
| rollout/              |          |
|    ep_len_mean        | 485      |
|    ep_rew_mean        | -861     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36100    |
|    time_elapsed       | 1298     |
|    total_timesteps    | 288800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00476 |
|    learning_rate      | 0.00096  |
|    n_updates          | 36099    |
|    policy_loss        | 0.0465   |
|    std                | 0.136    |
|    value_loss         | 1.44     |
------------------------------------
------------------------------------
| reward                | -1.75    |
| reward_contact        | -0.0107  |
| reward_motion         | 1.53     |
| reward_torque         | -0.352   |
| reward_velocity       | -2.91    |
| rollout/              |          |
|    ep_len_mean        | 494      |
|    ep_rew_mean        | -873     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36200    |
|    time_elapsed       | 1301     |
|    total_timesteps    | 289600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00915  |
|    learning_rate      | 0.00096  |
|    n_updates          | 36199    |
|    policy_loss        | 0.822    |
|    std                | 0.137    |
|    value_loss         | 1.18     |
------------------------------------
------------------------------------
| reward                | -1.74    |
| reward_contact        | -0.011   |
| reward_motion         | 1.51     |
| reward_torque         | -0.349   |
| reward_velocity       | -2.89    |
| rollout/              |          |
|    ep_len_mean        | 501      |
|    ep_rew_mean        | -882     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36300    |
|    time_elapsed       | 1305     |
|    total_timesteps    | 290400   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 8.62e-05 |
|    learning_rate      | 0.00096  |
|    n_updates          | 36299    |
|    policy_loss        | -0.0202  |
|    std                | 0.137    |
|    value_loss         | 0.336    |
------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.011    |
| reward_motion         | 1.51      |
| reward_torque         | -0.349    |
| reward_velocity       | -2.89     |
| rollout/              |           |
|    ep_len_mean        | 501       |
|    ep_rew_mean        | -882      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 36400     |
|    time_elapsed       | 1309      |
|    total_timesteps    | 291200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000776 |
|    learning_rate      | 0.00096   |
|    n_updates          | 36399     |
|    policy_loss        | 0.0241    |
|    std                | 0.137     |
|    value_loss         | 0.68      |
-------------------------------------
------------------------------------
| reward                | -1.77    |
| reward_contact        | -0.011   |
| reward_motion         | 1.48     |
| reward_torque         | -0.344   |
| reward_velocity       | -2.89    |
| rollout/              |          |
|    ep_len_mean        | 510      |
|    ep_rew_mean        | -898     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36500    |
|    time_elapsed       | 1313     |
|    total_timesteps    | 292000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.211    |
|    learning_rate      | 0.00096  |
|    n_updates          | 36499    |
|    policy_loss        | -2.12    |
|    std                | 0.137    |
|    value_loss         | 6.03     |
------------------------------------
-------------------------------------
| reward                | -1.78     |
| reward_contact        | -0.0111   |
| reward_motion         | 1.44      |
| reward_torque         | -0.339    |
| reward_velocity       | -2.88     |
| rollout/              |           |
|    ep_len_mean        | 518       |
|    ep_rew_mean        | -911      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 36600     |
|    time_elapsed       | 1316      |
|    total_timesteps    | 292800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 36599     |
|    policy_loss        | -2.15e-06 |
|    std                | 0.137     |
|    value_loss         | 0.0315    |
-------------------------------------
------------------------------------
| reward                | -1.75    |
| reward_contact        | -0.0114  |
| reward_motion         | 1.42     |
| reward_torque         | -0.334   |
| reward_velocity       | -2.83    |
| rollout/              |          |
|    ep_len_mean        | 527      |
|    ep_rew_mean        | -922     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36700    |
|    time_elapsed       | 1320     |
|    total_timesteps    | 293600   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.00254  |
|    learning_rate      | 0.00096  |
|    n_updates          | 36699    |
|    policy_loss        | -0.00413 |
|    std                | 0.137    |
|    value_loss         | 0.503    |
------------------------------------
Num timesteps: 294000
Best mean reward: -792.37 - Last mean reward per episode: -921.92
------------------------------------
| reward                | -1.75    |
| reward_contact        | -0.0114  |
| reward_motion         | 1.42     |
| reward_torque         | -0.334   |
| reward_velocity       | -2.83    |
| rollout/              |          |
|    ep_len_mean        | 536      |
|    ep_rew_mean        | -933     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 36800    |
|    time_elapsed       | 1324     |
|    total_timesteps    | 294400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -2.13    |
|    learning_rate      | 0.00096  |
|    n_updates          | 36799    |
|    policy_loss        | -0.214   |
|    std                | 0.137    |
|    value_loss         | 9.59     |
------------------------------------
-------------------------------------
| reward                | -1.73     |
| reward_contact        | -0.0117   |
| reward_motion         | 1.39      |
| reward_torque         | -0.329    |
| reward_velocity       | -2.78     |
| rollout/              |           |
|    ep_len_mean        | 536       |
|    ep_rew_mean        | -933      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 36900     |
|    time_elapsed       | 1328      |
|    total_timesteps    | 295200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 36899     |
|    policy_loss        | -2.86e-06 |
|    std                | 0.137     |
|    value_loss         | 1.86      |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0122   |
| reward_motion         | 1.37      |
| reward_torque         | -0.318    |
| reward_velocity       | -2.73     |
| rollout/              |           |
|    ep_len_mean        | 544       |
|    ep_rew_mean        | -948      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 37000     |
|    time_elapsed       | 1331      |
|    total_timesteps    | 296000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.136     |
|    learning_rate      | 0.00096   |
|    n_updates          | 36999     |
|    policy_loss        | -8.34e-07 |
|    std                | 0.137     |
|    value_loss         | 3.64e-09  |
-------------------------------------
------------------------------------
| reward                | -1.68    |
| reward_contact        | -0.0126  |
| reward_motion         | 1.35     |
| reward_torque         | -0.311   |
| reward_velocity       | -2.71    |
| rollout/              |          |
|    ep_len_mean        | 552      |
|    ep_rew_mean        | -956     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 37100    |
|    time_elapsed       | 1335     |
|    total_timesteps    | 296800   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.0144  |
|    learning_rate      | 0.00096  |
|    n_updates          | 37099    |
|    policy_loss        | -0.0065  |
|    std                | 0.137    |
|    value_loss         | 3.32     |
------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.013   |
| reward_motion         | 1.34     |
| reward_torque         | -0.306   |
| reward_velocity       | -2.69    |
| rollout/              |          |
|    ep_len_mean        | 560      |
|    ep_rew_mean        | -969     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 37200    |
|    time_elapsed       | 1339     |
|    total_timesteps    | 297600   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.755    |
|    learning_rate      | 0.00096  |
|    n_updates          | 37199    |
|    policy_loss        | -0.349   |
|    std                | 0.138    |
|    value_loss         | 2.06     |
------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.013    |
| reward_motion         | 1.34      |
| reward_torque         | -0.306    |
| reward_velocity       | -2.69     |
| rollout/              |           |
|    ep_len_mean        | 560       |
|    ep_rew_mean        | -969      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 37300     |
|    time_elapsed       | 1343      |
|    total_timesteps    | 298400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 5.96e-08  |
|    learning_rate      | 0.00096   |
|    n_updates          | 37299     |
|    policy_loss        | -4.77e-07 |
|    std                | 0.138     |
|    value_loss         | 6.56      |
-------------------------------------
------------------------------------
| reward                | -1.68    |
| reward_contact        | -0.0138  |
| reward_motion         | 1.32     |
| reward_torque         | -0.303   |
| reward_velocity       | -2.68    |
| rollout/              |          |
|    ep_len_mean        | 568      |
|    ep_rew_mean        | -981     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 37400    |
|    time_elapsed       | 1346     |
|    total_timesteps    | 299200   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.00678 |
|    learning_rate      | 0.00096  |
|    n_updates          | 37399    |
|    policy_loss        | 0.01     |
|    std                | 0.137    |
|    value_loss         | 7.5      |
------------------------------------
Num timesteps: 300000
Best mean reward: -792.37 - Last mean reward per episode: -985.13
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0143   |
| reward_motion         | 1.31      |
| reward_torque         | -0.298    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 574       |
|    ep_rew_mean        | -985      |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 37500     |
|    time_elapsed       | 1350      |
|    total_timesteps    | 300000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 37499     |
|    policy_loss        | -0        |
|    std                | 0.137     |
|    value_loss         | 0.0621    |
-------------------------------------
------------------------------------
| reward                | -1.65    |
| reward_contact        | -0.0146  |
| reward_motion         | 1.31     |
| reward_torque         | -0.296   |
| reward_velocity       | -2.65    |
| rollout/              |          |
|    ep_len_mean        | 582      |
|    ep_rew_mean        | -995     |
| time/                 |          |
|    fps                | 222      |
|    iterations         | 37600    |
|    time_elapsed       | 1354     |
|    total_timesteps    | 300800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.155   |
|    learning_rate      | 0.00096  |
|    n_updates          | 37599    |
|    policy_loss        | 0.116    |
|    std                | 0.137    |
|    value_loss         | 65.6     |
------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0146   |
| reward_motion         | 1.31      |
| reward_torque         | -0.296    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 592       |
|    ep_rew_mean        | -1.01e+03 |
| time/                 |           |
|    fps                | 222       |
|    iterations         | 37700     |
|    time_elapsed       | 1358      |
|    total_timesteps    | 301600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.182     |
|    learning_rate      | 0.00096   |
|    n_updates          | 37699     |
|    policy_loss        | -0.0611   |
|    std                | 0.137     |
|    value_loss         | 56.2      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.015    |
| reward_motion         | 1.27      |
| reward_torque         | -0.288    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 592       |
|    ep_rew_mean        | -1.01e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 37800     |
|    time_elapsed       | 1362      |
|    total_timesteps    | 302400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000144 |
|    learning_rate      | 0.00096   |
|    n_updates          | 37799     |
|    policy_loss        | -0.00011  |
|    std                | 0.137     |
|    value_loss         | 5.14      |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.0152   |
| reward_motion         | 1.22      |
| reward_torque         | -0.275    |
| reward_velocity       | -2.67     |
| rollout/              |           |
|    ep_len_mean        | 603       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 37900     |
|    time_elapsed       | 1365      |
|    total_timesteps    | 303200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.000197  |
|    learning_rate      | 0.00096   |
|    n_updates          | 37899     |
|    policy_loss        | -3.03e-05 |
|    std                | 0.138     |
|    value_loss         | 0.786     |
-------------------------------------
-------------------------------------
| reward                | -1.75     |
| reward_contact        | -0.0156   |
| reward_motion         | 1.2       |
| reward_torque         | -0.266    |
| reward_velocity       | -2.67     |
| rollout/              |           |
|    ep_len_mean        | 613       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38000     |
|    time_elapsed       | 1369      |
|    total_timesteps    | 304000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00206   |
|    learning_rate      | 0.00096   |
|    n_updates          | 37999     |
|    policy_loss        | 9.49e-05  |
|    std                | 0.137     |
|    value_loss         | 0.612     |
-------------------------------------
-------------------------------------
| reward                | -1.75     |
| reward_contact        | -0.0156   |
| reward_motion         | 1.2       |
| reward_torque         | -0.266    |
| reward_velocity       | -2.67     |
| rollout/              |           |
|    ep_len_mean        | 613       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38100     |
|    time_elapsed       | 1373      |
|    total_timesteps    | 304800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 38099     |
|    policy_loss        | 3.34e-06  |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.73     |
| reward_contact        | -0.0157   |
| reward_motion         | 1.2       |
| reward_torque         | -0.261    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 613       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38200     |
|    time_elapsed       | 1377      |
|    total_timesteps    | 305600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 38199     |
|    policy_loss        | -9.54e-07 |
|    std                | 0.138     |
|    value_loss         | 0.648     |
-------------------------------------
Num timesteps: 306000
Best mean reward: -792.37 - Last mean reward per episode: -1031.41
-------------------------------------
| reward                | -1.73     |
| reward_contact        | -0.016    |
| reward_motion         | 1.21      |
| reward_torque         | -0.266    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 613       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38300     |
|    time_elapsed       | 1381      |
|    total_timesteps    | 306400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0122    |
|    learning_rate      | 0.00096   |
|    n_updates          | 38299     |
|    policy_loss        | 0.165     |
|    std                | 0.138     |
|    value_loss         | 3.91      |
-------------------------------------
-------------------------------------
| reward                | -1.71     |
| reward_contact        | -0.016    |
| reward_motion         | 1.2       |
| reward_torque         | -0.265    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 619       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38400     |
|    time_elapsed       | 1384      |
|    total_timesteps    | 307200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00315   |
|    learning_rate      | 0.00096   |
|    n_updates          | 38399     |
|    policy_loss        | 0.000653  |
|    std                | 0.138     |
|    value_loss         | 1.12      |
-------------------------------------
-------------------------------------
| reward                | -1.71     |
| reward_contact        | -0.016    |
| reward_motion         | 1.2       |
| reward_torque         | -0.265    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 627       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38500     |
|    time_elapsed       | 1388      |
|    total_timesteps    | 308000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.319    |
|    learning_rate      | 0.00096   |
|    n_updates          | 38499     |
|    policy_loss        | -0.0409   |
|    std                | 0.137     |
|    value_loss         | 1.31      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0161   |
| reward_motion         | 1.18      |
| reward_torque         | -0.263    |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 632       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38600     |
|    time_elapsed       | 1392      |
|    total_timesteps    | 308800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.521     |
|    learning_rate      | 0.00096   |
|    n_updates          | 38599     |
|    policy_loss        | 0.378     |
|    std                | 0.137     |
|    value_loss         | 6.71      |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0161   |
| reward_motion         | 1.17      |
| reward_torque         | -0.258    |
| reward_velocity       | -2.56     |
| rollout/              |           |
|    ep_len_mean        | 633       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38700     |
|    time_elapsed       | 1396      |
|    total_timesteps    | 309600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.215    |
|    learning_rate      | 0.00096   |
|    n_updates          | 38699     |
|    policy_loss        | -0.0502   |
|    std                | 0.137     |
|    value_loss         | 0.21      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.016    |
| reward_motion         | 1.17      |
| reward_torque         | -0.258    |
| reward_velocity       | -2.55     |
| rollout/              |           |
|    ep_len_mean        | 633       |
|    ep_rew_mean        | -1.04e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38800     |
|    time_elapsed       | 1399      |
|    total_timesteps    | 310400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00966  |
|    learning_rate      | 0.00096   |
|    n_updates          | 38799     |
|    policy_loss        | -0.00591  |
|    std                | 0.137     |
|    value_loss         | 1.32      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0162   |
| reward_motion         | 1.15      |
| reward_torque         | -0.253    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 642       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 38900     |
|    time_elapsed       | 1403      |
|    total_timesteps    | 311200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0308   |
|    learning_rate      | 0.00096   |
|    n_updates          | 38899     |
|    policy_loss        | -0.0245   |
|    std                | 0.137     |
|    value_loss         | 0.836     |
-------------------------------------
Num timesteps: 312000
Best mean reward: -792.37 - Last mean reward per episode: -1052.96
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0162   |
| reward_motion         | 1.15      |
| reward_torque         | -0.253    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 642       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39000     |
|    time_elapsed       | 1407      |
|    total_timesteps    | 312000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -7.11     |
|    learning_rate      | 0.00096   |
|    n_updates          | 38999     |
|    policy_loss        | -0.259    |
|    std                | 0.137     |
|    value_loss         | 2.86      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0162   |
| reward_motion         | 1.15      |
| reward_torque         | -0.253    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 642       |
|    ep_rew_mean        | -1.05e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39100     |
|    time_elapsed       | 1410      |
|    total_timesteps    | 312800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0274   |
|    learning_rate      | 0.00096   |
|    n_updates          | 39099     |
|    policy_loss        | -0.0204   |
|    std                | 0.137     |
|    value_loss         | 11.3      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0166   |
| reward_motion         | 1.15      |
| reward_torque         | -0.255    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 649       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39200     |
|    time_elapsed       | 1414      |
|    total_timesteps    | 313600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00426  |
|    learning_rate      | 0.00096   |
|    n_updates          | 39199     |
|    policy_loss        | 0.000589  |
|    std                | 0.137     |
|    value_loss         | 0.0523    |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0166   |
| reward_motion         | 1.15      |
| reward_torque         | -0.255    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 649       |
|    ep_rew_mean        | -1.06e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39300     |
|    time_elapsed       | 1418      |
|    total_timesteps    | 314400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00138   |
|    learning_rate      | 0.00096   |
|    n_updates          | 39299     |
|    policy_loss        | -0.0109   |
|    std                | 0.137     |
|    value_loss         | 0.257     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0165   |
| reward_motion         | 1.13      |
| reward_torque         | -0.252    |
| reward_velocity       | -2.46     |
| rollout/              |           |
|    ep_len_mean        | 658       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39400     |
|    time_elapsed       | 1422      |
|    total_timesteps    | 315200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00782  |
|    learning_rate      | 0.00096   |
|    n_updates          | 39399     |
|    policy_loss        | 0.0639    |
|    std                | 0.137     |
|    value_loss         | 1.37      |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.13      |
| reward_torque         | -0.248    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 660       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39500     |
|    time_elapsed       | 1425      |
|    total_timesteps    | 316000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.417     |
|    learning_rate      | 0.00096   |
|    n_updates          | 39499     |
|    policy_loss        | 0.156     |
|    std                | 0.137     |
|    value_loss         | 6.81      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.11      |
| reward_torque         | -0.242    |
| reward_velocity       | -2.5      |
| rollout/              |           |
|    ep_len_mean        | 659       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39600     |
|    time_elapsed       | 1429      |
|    total_timesteps    | 316800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0165   |
|    learning_rate      | 0.00096   |
|    n_updates          | 39599     |
|    policy_loss        | -0.00111  |
|    std                | 0.137     |
|    value_loss         | 0.149     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.1       |
| reward_torque         | -0.234    |
| reward_velocity       | -2.48     |
| rollout/              |           |
|    ep_len_mean        | 668       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39700     |
|    time_elapsed       | 1433      |
|    total_timesteps    | 317600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00535   |
|    learning_rate      | 0.00096   |
|    n_updates          | 39699     |
|    policy_loss        | -0.000521 |
|    std                | 0.137     |
|    value_loss         | 0.611     |
-------------------------------------
Num timesteps: 318000
Best mean reward: -792.37 - Last mean reward per episode: -1086.69
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.1       |
| reward_torque         | -0.234    |
| reward_velocity       | -2.48     |
| rollout/              |           |
|    ep_len_mean        | 668       |
|    ep_rew_mean        | -1.08e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 39800     |
|    time_elapsed       | 1437      |
|    total_timesteps    | 318400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.214     |
|    learning_rate      | 0.00096   |
|    n_updates          | 39799     |
|    policy_loss        | 0.149     |
|    std                | 0.137     |
|    value_loss         | 18.2      |
-------------------------------------
------------------------------------
| reward                | -1.64    |
| reward_contact        | -0.0165  |
| reward_motion         | 1.07     |
| reward_torque         | -0.229   |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 676      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 221      |
|    iterations         | 39900    |
|    time_elapsed       | 1441     |
|    total_timesteps    | 319200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.398   |
|    learning_rate      | 0.00096  |
|    n_updates          | 39899    |
|    policy_loss        | 0.137    |
|    std                | 0.137    |
|    value_loss         | 12.8     |
------------------------------------
------------------------------------
| reward                | -1.66    |
| reward_contact        | -0.0165  |
| reward_motion         | 1.06     |
| reward_torque         | -0.225   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 676      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 221      |
|    iterations         | 40000    |
|    time_elapsed       | 1444     |
|    total_timesteps    | 320000   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.00375  |
|    learning_rate      | 0.00096  |
|    n_updates          | 39999    |
|    policy_loss        | 0.0107   |
|    std                | 0.137    |
|    value_loss         | 2.81     |
------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0167   |
| reward_motion         | 1.03      |
| reward_torque         | -0.218    |
| reward_velocity       | -2.44     |
| rollout/              |           |
|    ep_len_mean        | 686       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40100     |
|    time_elapsed       | 1448      |
|    total_timesteps    | 320800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 40099     |
|    policy_loss        | 3.58e-07  |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0171   |
| reward_motion         | 1.01      |
| reward_torque         | -0.212    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 696       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40200     |
|    time_elapsed       | 1452      |
|    total_timesteps    | 321600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -7.86     |
|    learning_rate      | 0.00096   |
|    n_updates          | 40199     |
|    policy_loss        | 0.107     |
|    std                | 0.137     |
|    value_loss         | 8.85      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0174   |
| reward_motion         | 0.988     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 705       |
|    ep_rew_mean        | -1.14e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40300     |
|    time_elapsed       | 1455      |
|    total_timesteps    | 322400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.49     |
|    learning_rate      | 0.00096   |
|    n_updates          | 40299     |
|    policy_loss        | -0.14     |
|    std                | 0.137     |
|    value_loss         | 3.13      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0174   |
| reward_motion         | 0.988     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 705       |
|    ep_rew_mean        | -1.14e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40400     |
|    time_elapsed       | 1459      |
|    total_timesteps    | 323200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.498    |
|    learning_rate      | 0.00096   |
|    n_updates          | 40399     |
|    policy_loss        | -0.167    |
|    std                | 0.137     |
|    value_loss         | 0.177     |
-------------------------------------
Num timesteps: 324000
Best mean reward: -792.37 - Last mean reward per episode: -1156.55
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0179   |
| reward_motion         | 0.976     |
| reward_torque         | -0.202    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 715       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40500     |
|    time_elapsed       | 1463      |
|    total_timesteps    | 324000    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.75     |
|    learning_rate      | 0.00096   |
|    n_updates          | 40499     |
|    policy_loss        | -0.747    |
|    std                | 0.137     |
|    value_loss         | 2.34      |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0172   |
| reward_motion         | 1         |
| reward_torque         | -0.208    |
| reward_velocity       | -2.44     |
| rollout/              |           |
|    ep_len_mean        | 705       |
|    ep_rew_mean        | -1.14e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40600     |
|    time_elapsed       | 1466      |
|    total_timesteps    | 324800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.469    |
|    learning_rate      | 0.00096   |
|    n_updates          | 40599     |
|    policy_loss        | 0.22      |
|    std                | 0.137     |
|    value_loss         | 1.7       |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0175   |
| reward_motion         | 0.994     |
| reward_torque         | -0.203    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 714       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40700     |
|    time_elapsed       | 1470      |
|    total_timesteps    | 325600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 40699     |
|    policy_loss        | -1.91e-06 |
|    std                | 0.137     |
|    value_loss         | 2.79      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0175   |
| reward_motion         | 0.994     |
| reward_torque         | -0.203    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 714       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40800     |
|    time_elapsed       | 1474      |
|    total_timesteps    | 326400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0635    |
|    learning_rate      | 0.00096   |
|    n_updates          | 40799     |
|    policy_loss        | -0.864    |
|    std                | 0.137     |
|    value_loss         | 11        |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0179   |
| reward_motion         | 0.994     |
| reward_torque         | -0.203    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 714       |
|    ep_rew_mean        | -1.15e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 40900     |
|    time_elapsed       | 1477      |
|    total_timesteps    | 327200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0189   |
|    learning_rate      | 0.00096   |
|    n_updates          | 40899     |
|    policy_loss        | -0.00111  |
|    std                | 0.137     |
|    value_loss         | 0.0538    |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.951     |
| reward_torque         | -0.196    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 720       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41000     |
|    time_elapsed       | 1481      |
|    total_timesteps    | 328000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.445    |
|    learning_rate      | 0.00096   |
|    n_updates          | 40999     |
|    policy_loss        | 0.577     |
|    std                | 0.137     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0183   |
| reward_motion         | 0.905     |
| reward_torque         | -0.193    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 725       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41100     |
|    time_elapsed       | 1485      |
|    total_timesteps    | 328800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00957   |
|    learning_rate      | 0.00096   |
|    n_updates          | 41099     |
|    policy_loss        | 0.0716    |
|    std                | 0.137     |
|    value_loss         | 1.37      |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.895     |
| reward_torque         | -0.195    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 723       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41200     |
|    time_elapsed       | 1488      |
|    total_timesteps    | 329600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0379   |
|    learning_rate      | 0.00096   |
|    n_updates          | 41199     |
|    policy_loss        | 0.0408    |
|    std                | 0.138     |
|    value_loss         | 4.72      |
-------------------------------------
Num timesteps: 330000
Best mean reward: -792.37 - Last mean reward per episode: -1161.23
-------------------------------------
| reward                | -1.73     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.876     |
| reward_torque         | -0.197    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 723       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41300     |
|    time_elapsed       | 1492      |
|    total_timesteps    | 330400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000583 |
|    learning_rate      | 0.00096   |
|    n_updates          | 41299     |
|    policy_loss        | -0.0273   |
|    std                | 0.138     |
|    value_loss         | 2.58      |
-------------------------------------
-------------------------------------
| reward                | -1.73     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.876     |
| reward_torque         | -0.196    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 723       |
|    ep_rew_mean        | -1.16e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41400     |
|    time_elapsed       | 1496      |
|    total_timesteps    | 331200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 41399     |
|    policy_loss        | -1.43e-06 |
|    std                | 0.137     |
|    value_loss         | 0.392     |
-------------------------------------
-------------------------------------
| reward                | -1.76     |
| reward_contact        | -0.0185   |
| reward_motion         | 0.912     |
| reward_torque         | -0.204    |
| reward_velocity       | -2.45     |
| rollout/              |           |
|    ep_len_mean        | 705       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41500     |
|    time_elapsed       | 1500      |
|    total_timesteps    | 332000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -1.82     |
|    learning_rate      | 0.00096   |
|    n_updates          | 41499     |
|    policy_loss        | -0.204    |
|    std                | 0.138     |
|    value_loss         | 2.88      |
-------------------------------------
-------------------------------------
| reward                | -1.76     |
| reward_contact        | -0.0185   |
| reward_motion         | 0.912     |
| reward_torque         | -0.204    |
| reward_velocity       | -2.45     |
| rollout/              |           |
|    ep_len_mean        | 705       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41600     |
|    time_elapsed       | 1503      |
|    total_timesteps    | 332800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0197   |
|    learning_rate      | 0.00096   |
|    n_updates          | 41599     |
|    policy_loss        | -0.0102   |
|    std                | 0.138     |
|    value_loss         | 0.711     |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.931     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.43     |
| rollout/              |           |
|    ep_len_mean        | 697       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41700     |
|    time_elapsed       | 1507      |
|    total_timesteps    | 333600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 5.38e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 41699     |
|    policy_loss        | -1.87e-05 |
|    std                | 0.138     |
|    value_loss         | 0.746     |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0186   |
| reward_motion         | 0.947     |
| reward_torque         | -0.204    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 697       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41800     |
|    time_elapsed       | 1511      |
|    total_timesteps    | 334400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.221    |
|    learning_rate      | 0.00096   |
|    n_updates          | 41799     |
|    policy_loss        | 0.0754    |
|    std                | 0.137     |
|    value_loss         | 11.6      |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0186   |
| reward_motion         | 0.947     |
| reward_torque         | -0.204    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 697       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 41900     |
|    time_elapsed       | 1514      |
|    total_timesteps    | 335200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -11.5     |
|    learning_rate      | 0.00096   |
|    n_updates          | 41899     |
|    policy_loss        | 0.05      |
|    std                | 0.137     |
|    value_loss         | 25.9      |
-------------------------------------
Num timesteps: 336000
Best mean reward: -792.37 - Last mean reward per episode: -1114.26
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.019    |
| reward_motion         | 0.949     |
| reward_torque         | -0.202    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 697       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42000     |
|    time_elapsed       | 1518      |
|    total_timesteps    | 336000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.803    |
|    learning_rate      | 0.00096   |
|    n_updates          | 41999     |
|    policy_loss        | -0.379    |
|    std                | 0.137     |
|    value_loss         | 0.111     |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0189   |
| reward_motion         | 0.945     |
| reward_torque         | -0.212    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 688       |
|    ep_rew_mean        | -1.1e+03  |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42100     |
|    time_elapsed       | 1522      |
|    total_timesteps    | 336800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 42099     |
|    policy_loss        | -9.54e-07 |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0189   |
| reward_motion         | 0.945     |
| reward_torque         | -0.212    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 688       |
|    ep_rew_mean        | -1.1e+03  |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42200     |
|    time_elapsed       | 1526      |
|    total_timesteps    | 337600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -2.44e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 42199     |
|    policy_loss        | -4.77e-07 |
|    std                | 0.137     |
|    value_loss         | 1.84      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0194   |
| reward_motion         | 0.932     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 698       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42300     |
|    time_elapsed       | 1529      |
|    total_timesteps    | 338400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 5.96e-08  |
|    learning_rate      | 0.00096   |
|    n_updates          | 42299     |
|    policy_loss        | -2.74e-06 |
|    std                | 0.137     |
|    value_loss         | 1.66      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.917     |
| reward_torque         | -0.203    |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 702       |
|    ep_rew_mean        | -1.12e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42400     |
|    time_elapsed       | 1533      |
|    total_timesteps    | 339200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00354   |
|    learning_rate      | 0.00096   |
|    n_updates          | 42399     |
|    policy_loss        | -0.000486 |
|    std                | 0.137     |
|    value_loss         | 0.1       |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.912     |
| reward_torque         | -0.2      |
| reward_velocity       | -2.37     |
| rollout/              |           |
|    ep_len_mean        | 702       |
|    ep_rew_mean        | -1.13e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42500     |
|    time_elapsed       | 1537      |
|    total_timesteps    | 340000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -4.52e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 42499     |
|    policy_loss        | 6.08e-06  |
|    std                | 0.137     |
|    value_loss         | 0.947     |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.02     |
| reward_motion         | 0.912     |
| reward_torque         | -0.2      |
| reward_velocity       | -2.37     |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42600     |
|    time_elapsed       | 1541      |
|    total_timesteps    | 340800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.205    |
|    learning_rate      | 0.00096   |
|    n_updates          | 42599     |
|    policy_loss        | -0.793    |
|    std                | 0.137     |
|    value_loss         | 14.1      |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0195   |
| reward_motion         | 0.945     |
| reward_torque         | -0.205    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42700     |
|    time_elapsed       | 1544      |
|    total_timesteps    | 341600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -86.7     |
|    learning_rate      | 0.00096   |
|    n_updates          | 42699     |
|    policy_loss        | -0.722    |
|    std                | 0.137     |
|    value_loss         | 5.18      |
-------------------------------------
Num timesteps: 342000
Best mean reward: -792.37 - Last mean reward per episode: -1109.36
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0189   |
| reward_motion         | 0.945     |
| reward_torque         | -0.205    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42800     |
|    time_elapsed       | 1548      |
|    total_timesteps    | 342400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.191     |
|    learning_rate      | 0.00096   |
|    n_updates          | 42799     |
|    policy_loss        | 0.00135   |
|    std                | 0.137     |
|    value_loss         | 0.0184    |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.945     |
| reward_torque         | -0.205    |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 42900     |
|    time_elapsed       | 1552      |
|    total_timesteps    | 343200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 42899     |
|    policy_loss        | 4.77e-07  |
|    std                | 0.137     |
|    value_loss         | 3.72      |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.945     |
| reward_torque         | -0.205    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 221       |
|    iterations         | 43000     |
|    time_elapsed       | 1556      |
|    total_timesteps    | 344000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.188    |
|    learning_rate      | 0.00096   |
|    n_updates          | 42999     |
|    policy_loss        | -0.261    |
|    std                | 0.137     |
|    value_loss         | 40        |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.945     |
| reward_torque         | -0.205    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 692       |
|    ep_rew_mean        | -1.11e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 43100     |
|    time_elapsed       | 1560      |
|    total_timesteps    | 344800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 43099     |
|    policy_loss        | -2.5e-06  |
|    std                | 0.137     |
|    value_loss         | 3.55      |
-------------------------------------
------------------------------------
| reward                | -1.65    |
| reward_contact        | -0.019   |
| reward_motion         | 0.963    |
| reward_torque         | -0.209   |
| reward_velocity       | -2.39    |
| rollout/              |          |
|    ep_len_mean        | 685      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 43200    |
|    time_elapsed       | 1563     |
|    total_timesteps    | 345600   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.012    |
|    learning_rate      | 0.00096  |
|    n_updates          | 43199    |
|    policy_loss        | 0.00167  |
|    std                | 0.137    |
|    value_loss         | 0.501    |
------------------------------------
------------------------------------
| reward                | -1.67    |
| reward_contact        | -0.0191  |
| reward_motion         | 0.941    |
| reward_torque         | -0.206   |
| reward_velocity       | -2.38    |
| rollout/              |          |
|    ep_len_mean        | 685      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 43300    |
|    time_elapsed       | 1567     |
|    total_timesteps    | 346400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -12.5    |
|    learning_rate      | 0.00096  |
|    n_updates          | 43299    |
|    policy_loss        | 0.029    |
|    std                | 0.137    |
|    value_loss         | 35.7     |
------------------------------------
------------------------------------
| reward                | -1.7     |
| reward_contact        | -0.0191  |
| reward_motion         | 0.931    |
| reward_torque         | -0.206   |
| reward_velocity       | -2.4     |
| rollout/              |          |
|    ep_len_mean        | 685      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 43400    |
|    time_elapsed       | 1571     |
|    total_timesteps    | 347200   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.0124   |
|    learning_rate      | 0.00096  |
|    n_updates          | 43399    |
|    policy_loss        | 0.00171  |
|    std                | 0.137    |
|    value_loss         | 4.51     |
------------------------------------
Num timesteps: 348000
Best mean reward: -792.37 - Last mean reward per episode: -1100.54
------------------------------------
| reward                | -1.71    |
| reward_contact        | -0.0187  |
| reward_motion         | 0.931    |
| reward_torque         | -0.206   |
| reward_velocity       | -2.42    |
| rollout/              |          |
|    ep_len_mean        | 685      |
|    ep_rew_mean        | -1.1e+03 |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 43500    |
|    time_elapsed       | 1575     |
|    total_timesteps    | 348000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00791  |
|    learning_rate      | 0.00096  |
|    n_updates          | 43499    |
|    policy_loss        | 0.002    |
|    std                | 0.137    |
|    value_loss         | 0.303    |
------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.933     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 681       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 43600     |
|    time_elapsed       | 1579      |
|    total_timesteps    | 348800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.00103   |
|    learning_rate      | 0.00096   |
|    n_updates          | 43599     |
|    policy_loss        | -8.95e-05 |
|    std                | 0.136     |
|    value_loss         | 2.95e-07  |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.933     |
| reward_torque         | -0.206    |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 681       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 43700     |
|    time_elapsed       | 1582      |
|    total_timesteps    | 349600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0363   |
|    learning_rate      | 0.00096   |
|    n_updates          | 43699     |
|    policy_loss        | -0.000314 |
|    std                | 0.137     |
|    value_loss         | 0.0779    |
-------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0182   |
| reward_motion         | 0.936     |
| reward_torque         | -0.207    |
| reward_velocity       | -2.37     |
| rollout/              |           |
|    ep_len_mean        | 681       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 43800     |
|    time_elapsed       | 1586      |
|    total_timesteps    | 350400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 43799     |
|    policy_loss        | 1.79e-07  |
|    std                | 0.137     |
|    value_loss         | 2.96      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.953     |
| reward_torque         | -0.217    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 670       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 43900     |
|    time_elapsed       | 1590      |
|    total_timesteps    | 351200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.428    |
|    learning_rate      | 0.00096   |
|    n_updates          | 43899     |
|    policy_loss        | -0.258    |
|    std                | 0.136     |
|    value_loss         | 0.725     |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.953     |
| reward_torque         | -0.217    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 670       |
|    ep_rew_mean        | -1.07e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44000     |
|    time_elapsed       | 1594      |
|    total_timesteps    | 352000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00706   |
|    learning_rate      | 0.00096   |
|    n_updates          | 43999     |
|    policy_loss        | -0.000434 |
|    std                | 0.136     |
|    value_loss         | 0.605     |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.018    |
| reward_motion         | 1.02      |
| reward_torque         | -0.222    |
| reward_velocity       | -2.42     |
| rollout/              |           |
|    ep_len_mean        | 650       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44100     |
|    time_elapsed       | 1597      |
|    total_timesteps    | 352800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.531    |
|    learning_rate      | 0.00096   |
|    n_updates          | 44099     |
|    policy_loss        | -0.158    |
|    std                | 0.136     |
|    value_loss         | 0.443     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0171   |
| reward_motion         | 1.01      |
| reward_torque         | -0.222    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 650       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44200     |
|    time_elapsed       | 1601      |
|    total_timesteps    | 353600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0241    |
|    learning_rate      | 0.00096   |
|    n_updates          | 44199     |
|    policy_loss        | 6.91e-06  |
|    std                | 0.136     |
|    value_loss         | 0.943     |
-------------------------------------
Num timesteps: 354000
Best mean reward: -792.37 - Last mean reward per episode: -1027.43
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0167   |
| reward_motion         | 1.01      |
| reward_torque         | -0.222    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 650       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44300     |
|    time_elapsed       | 1605      |
|    total_timesteps    | 354400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.508    |
|    learning_rate      | 0.00096   |
|    n_updates          | 44299     |
|    policy_loss        | -0.148    |
|    std                | 0.136     |
|    value_loss         | 0.609     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0167   |
| reward_motion         | 1.01      |
| reward_torque         | -0.219    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 653       |
|    ep_rew_mean        | -1.03e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44400     |
|    time_elapsed       | 1608      |
|    total_timesteps    | 355200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0572    |
|    learning_rate      | 0.00096   |
|    n_updates          | 44399     |
|    policy_loss        | -0.0185   |
|    std                | 0.136     |
|    value_loss         | 0.291     |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.01      |
| reward_torque         | -0.222    |
| reward_velocity       | -2.43     |
| rollout/              |           |
|    ep_len_mean        | 646       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44500     |
|    time_elapsed       | 1612      |
|    total_timesteps    | 356000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000976 |
|    learning_rate      | 0.00096   |
|    n_updates          | 44499     |
|    policy_loss        | 0.000493  |
|    std                | 0.137     |
|    value_loss         | 4.24      |
-------------------------------------
-------------------------------------
| reward                | -1.65     |
| reward_contact        | -0.0164   |
| reward_motion         | 1.01      |
| reward_torque         | -0.222    |
| reward_velocity       | -2.43     |
| rollout/              |           |
|    ep_len_mean        | 646       |
|    ep_rew_mean        | -1.02e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44600     |
|    time_elapsed       | 1616      |
|    total_timesteps    | 356800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0912    |
|    learning_rate      | 0.00096   |
|    n_updates          | 44599     |
|    policy_loss        | 0.00392   |
|    std                | 0.137     |
|    value_loss         | 0.826     |
-------------------------------------
------------------------------------
| reward                | -1.64    |
| reward_contact        | -0.0164  |
| reward_motion         | 1.05     |
| reward_torque         | -0.223   |
| reward_velocity       | -2.45    |
| rollout/              |          |
|    ep_len_mean        | 636      |
|    ep_rew_mean        | -1e+03   |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 44700    |
|    time_elapsed       | 1620     |
|    total_timesteps    | 357600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.326    |
|    learning_rate      | 0.00096  |
|    n_updates          | 44699    |
|    policy_loss        | 0.246    |
|    std                | 0.137    |
|    value_loss         | 5.81     |
------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0159   |
| reward_motion         | 1.05      |
| reward_torque         | -0.223    |
| reward_velocity       | -2.45     |
| rollout/              |           |
|    ep_len_mean        | 636       |
|    ep_rew_mean        | -1.01e+03 |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 44800     |
|    time_elapsed       | 1624      |
|    total_timesteps    | 358400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0393   |
|    learning_rate      | 0.00096   |
|    n_updates          | 44799     |
|    policy_loss        | -0.0121   |
|    std                | 0.137     |
|    value_loss         | 0.691     |
-------------------------------------
------------------------------------
| reward                | -1.6     |
| reward_contact        | -0.0147  |
| reward_motion         | 1.13     |
| reward_torque         | -0.24    |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 616      |
|    ep_rew_mean        | -974     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 44900    |
|    time_elapsed       | 1627     |
|    total_timesteps    | 359200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -23.9    |
|    learning_rate      | 0.00096  |
|    n_updates          | 44899    |
|    policy_loss        | -0.0536  |
|    std                | 0.137    |
|    value_loss         | 151      |
------------------------------------
Num timesteps: 360000
Best mean reward: -792.37 - Last mean reward per episode: -945.96
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.2      |
| reward_torque         | -0.249   |
| reward_velocity       | -2.52    |
| rollout/              |          |
|    ep_len_mean        | 598      |
|    ep_rew_mean        | -946     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45000    |
|    time_elapsed       | 1630     |
|    total_timesteps    | 360000   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | 0.0549   |
|    learning_rate      | 0.00096  |
|    n_updates          | 44999    |
|    policy_loss        | 0.00147  |
|    std                | 0.137    |
|    value_loss         | 3.84     |
------------------------------------
------------------------------------
| reward                | -1.57    |
| reward_contact        | -0.0125  |
| reward_motion         | 1.26     |
| reward_torque         | -0.255   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 579      |
|    ep_rew_mean        | -921     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45100    |
|    time_elapsed       | 1634     |
|    total_timesteps    | 360800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0854   |
|    learning_rate      | 0.00096  |
|    n_updates          | 45099    |
|    policy_loss        | 0.588    |
|    std                | 0.137    |
|    value_loss         | 2.65     |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.25     |
| reward_torque         | -0.253   |
| reward_velocity       | -2.49    |
| rollout/              |          |
|    ep_len_mean        | 577      |
|    ep_rew_mean        | -915     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45200    |
|    time_elapsed       | 1638     |
|    total_timesteps    | 361600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.418    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45199    |
|    policy_loss        | -0.403   |
|    std                | 0.137    |
|    value_loss         | 2.71     |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0124  |
| reward_motion         | 1.28     |
| reward_torque         | -0.258   |
| reward_velocity       | -2.52    |
| rollout/              |          |
|    ep_len_mean        | 577      |
|    ep_rew_mean        | -915     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45300    |
|    time_elapsed       | 1641     |
|    total_timesteps    | 362400   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.0736  |
|    learning_rate      | 0.00096  |
|    n_updates          | 45299    |
|    policy_loss        | 0.00011  |
|    std                | 0.137    |
|    value_loss         | 1.32     |
------------------------------------
------------------------------------
| reward                | -1.55    |
| reward_contact        | -0.0121  |
| reward_motion         | 1.26     |
| reward_torque         | -0.259   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 577      |
|    ep_rew_mean        | -917     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45400    |
|    time_elapsed       | 1644     |
|    total_timesteps    | 363200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -28.1    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45399    |
|    policy_loss        | -0.0301  |
|    std                | 0.137    |
|    value_loss         | 36       |
------------------------------------
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0121  |
| reward_motion         | 1.28     |
| reward_torque         | -0.257   |
| reward_velocity       | -2.53    |
| rollout/              |          |
|    ep_len_mean        | 583      |
|    ep_rew_mean        | -926     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45500    |
|    time_elapsed       | 1648     |
|    total_timesteps    | 364000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.942   |
|    learning_rate      | 0.00096  |
|    n_updates          | 45499    |
|    policy_loss        | -0.00603 |
|    std                | 0.137    |
|    value_loss         | 11.2     |
------------------------------------
------------------------------------
| reward                | -1.54    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.29     |
| reward_torque         | -0.26    |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 583      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45600    |
|    time_elapsed       | 1652     |
|    total_timesteps    | 364800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.139    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45599    |
|    policy_loss        | -0.00934 |
|    std                | 0.137    |
|    value_loss         | 0.189    |
------------------------------------
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.0116  |
| reward_motion         | 1.27     |
| reward_torque         | -0.254   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 586      |
|    ep_rew_mean        | -939     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45700    |
|    time_elapsed       | 1656     |
|    total_timesteps    | 365600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.91    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45699    |
|    policy_loss        | -0.624   |
|    std                | 0.137    |
|    value_loss         | 10.4     |
------------------------------------
Num timesteps: 366000
Best mean reward: -792.37 - Last mean reward per episode: -939.12
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.29     |
| reward_torque         | -0.259   |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 582      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45800    |
|    time_elapsed       | 1659     |
|    total_timesteps    | 366400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.49    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45799    |
|    policy_loss        | -0.126   |
|    std                | 0.137    |
|    value_loss         | 0.413    |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.29     |
| reward_torque         | -0.259   |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 590      |
|    ep_rew_mean        | -940     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 45900    |
|    time_elapsed       | 1663     |
|    total_timesteps    | 367200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.347   |
|    learning_rate      | 0.00096  |
|    n_updates          | 45899    |
|    policy_loss        | -0.297   |
|    std                | 0.137    |
|    value_loss         | 13.1     |
------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.0112  |
| reward_motion         | 1.39     |
| reward_torque         | -0.271   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 570      |
|    ep_rew_mean        | -912     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46000    |
|    time_elapsed       | 1667     |
|    total_timesteps    | 368000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.45    |
|    learning_rate      | 0.00096  |
|    n_updates          | 45999    |
|    policy_loss        | -0.674   |
|    std                | 0.137    |
|    value_loss         | 18.2     |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0108  |
| reward_motion         | 1.43     |
| reward_torque         | -0.277   |
| reward_velocity       | -2.65    |
| rollout/              |          |
|    ep_len_mean        | 551      |
|    ep_rew_mean        | -883     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46100    |
|    time_elapsed       | 1670     |
|    total_timesteps    | 368800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.419   |
|    learning_rate      | 0.00096  |
|    n_updates          | 46099    |
|    policy_loss        | -0.0797  |
|    std                | 0.137    |
|    value_loss         | 0.225    |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0108  |
| reward_motion         | 1.43     |
| reward_torque         | -0.277   |
| reward_velocity       | -2.65    |
| rollout/              |          |
|    ep_len_mean        | 551      |
|    ep_rew_mean        | -880     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46200    |
|    time_elapsed       | 1674     |
|    total_timesteps    | 369600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.514    |
|    learning_rate      | 0.00096  |
|    n_updates          | 46199    |
|    policy_loss        | -0.723   |
|    std                | 0.137    |
|    value_loss         | 5.18     |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.011   |
| reward_motion         | 1.43     |
| reward_torque         | -0.276   |
| reward_velocity       | -2.65    |
| rollout/              |          |
|    ep_len_mean        | 551      |
|    ep_rew_mean        | -880     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46300    |
|    time_elapsed       | 1677     |
|    total_timesteps    | 370400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.427    |
|    learning_rate      | 0.00096  |
|    n_updates          | 46299    |
|    policy_loss        | -0.00463 |
|    std                | 0.137    |
|    value_loss         | 0.873    |
------------------------------------
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.011   |
| reward_motion         | 1.43     |
| reward_torque         | -0.275   |
| reward_velocity       | -2.64    |
| rollout/              |          |
|    ep_len_mean        | 551      |
|    ep_rew_mean        | -883     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46400    |
|    time_elapsed       | 1681     |
|    total_timesteps    | 371200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00801  |
|    learning_rate      | 0.00096  |
|    n_updates          | 46399    |
|    policy_loss        | -0.00359 |
|    std                | 0.136    |
|    value_loss         | 0.133    |
------------------------------------
Num timesteps: 372000
Best mean reward: -792.37 - Last mean reward per episode: -896.85
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.41     |
| reward_torque         | -0.277   |
| reward_velocity       | -2.62    |
| rollout/              |          |
|    ep_len_mean        | 562      |
|    ep_rew_mean        | -897     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46500    |
|    time_elapsed       | 1685     |
|    total_timesteps    | 372000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -2.65    |
|    learning_rate      | 0.00096  |
|    n_updates          | 46499    |
|    policy_loss        | -0.0835  |
|    std                | 0.137    |
|    value_loss         | 6.08     |
------------------------------------
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.41     |
| reward_torque         | -0.277   |
| reward_velocity       | -2.62    |
| rollout/              |          |
|    ep_len_mean        | 562      |
|    ep_rew_mean        | -897     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46600    |
|    time_elapsed       | 1689     |
|    total_timesteps    | 372800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0136   |
|    learning_rate      | 0.00096  |
|    n_updates          | 46599    |
|    policy_loss        | 0.0119   |
|    std                | 0.136    |
|    value_loss         | 0.833    |
------------------------------------
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.0116  |
| reward_motion         | 1.4      |
| reward_torque         | -0.274   |
| reward_velocity       | -2.61    |
| rollout/              |          |
|    ep_len_mean        | 571      |
|    ep_rew_mean        | -914     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46700    |
|    time_elapsed       | 1692     |
|    total_timesteps    | 373600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.000359 |
|    learning_rate      | 0.00096  |
|    n_updates          | 46699    |
|    policy_loss        | 0.121    |
|    std                | 0.136    |
|    value_loss         | 3        |
------------------------------------
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.012   |
| reward_motion         | 1.38     |
| reward_torque         | -0.272   |
| reward_velocity       | -2.6     |
| rollout/              |          |
|    ep_len_mean        | 579      |
|    ep_rew_mean        | -927     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46800    |
|    time_elapsed       | 1696     |
|    total_timesteps    | 374400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.337   |
|    learning_rate      | 0.00096  |
|    n_updates          | 46799    |
|    policy_loss        | 0.302    |
|    std                | 0.136    |
|    value_loss         | 2.57     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.012   |
| reward_motion         | 1.4      |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -941     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 46900    |
|    time_elapsed       | 1700     |
|    total_timesteps    | 375200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -1.71    |
|    learning_rate      | 0.00096  |
|    n_updates          | 46899    |
|    policy_loss        | -0.2     |
|    std                | 0.136    |
|    value_loss         | 0.79     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.012   |
| reward_motion         | 1.4      |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -940     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47000    |
|    time_elapsed       | 1704     |
|    total_timesteps    | 376000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -2.19    |
|    learning_rate      | 0.00096  |
|    n_updates          | 46999    |
|    policy_loss        | 0.887    |
|    std                | 0.136    |
|    value_loss         | 38       |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.012   |
| reward_motion         | 1.4      |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -940     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47100    |
|    time_elapsed       | 1707     |
|    total_timesteps    | 376800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00405 |
|    learning_rate      | 0.00096  |
|    n_updates          | 47099    |
|    policy_loss        | 0.00392  |
|    std                | 0.136    |
|    value_loss         | 3.08     |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0119  |
| reward_motion         | 1.4      |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -937     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47200    |
|    time_elapsed       | 1711     |
|    total_timesteps    | 377600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.611   |
|    learning_rate      | 0.00096  |
|    n_updates          | 47199    |
|    policy_loss        | 0.132    |
|    std                | 0.136    |
|    value_loss         | 2.33     |
------------------------------------
Num timesteps: 378000
Best mean reward: -792.37 - Last mean reward per episode: -933.44
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0127  |
| reward_motion         | 1.4      |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -933     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47300    |
|    time_elapsed       | 1715     |
|    total_timesteps    | 378400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -8.29    |
|    learning_rate      | 0.00096  |
|    n_updates          | 47299    |
|    policy_loss        | -0.17    |
|    std                | 0.136    |
|    value_loss         | 2.78     |
------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0129  |
| reward_motion         | 1.39     |
| reward_torque         | -0.267   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 599      |
|    ep_rew_mean        | -947     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47400    |
|    time_elapsed       | 1718     |
|    total_timesteps    | 379200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 6.49e-05 |
|    learning_rate      | 0.00096  |
|    n_updates          | 47399    |
|    policy_loss        | -0.00779 |
|    std                | 0.136    |
|    value_loss         | 14.6     |
------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0129  |
| reward_motion         | 1.39     |
| reward_torque         | -0.267   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 599      |
|    ep_rew_mean        | -946     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47500    |
|    time_elapsed       | 1722     |
|    total_timesteps    | 380000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00319 |
|    learning_rate      | 0.00096  |
|    n_updates          | 47499    |
|    policy_loss        | -0.772   |
|    std                | 0.136    |
|    value_loss         | 6.57e+03 |
------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.0128   |
| reward_motion         | 1.41      |
| reward_torque         | -0.269    |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 589       |
|    ep_rew_mean        | -931      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 47600     |
|    time_elapsed       | 1726      |
|    total_timesteps    | 380800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 47599     |
|    policy_loss        | -4.77e-07 |
|    std                | 0.136     |
|    value_loss         | 11.4      |
-------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.0126  |
| reward_motion         | 1.41     |
| reward_torque         | -0.278   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 572      |
|    ep_rew_mean        | -905     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47700    |
|    time_elapsed       | 1730     |
|    total_timesteps    | 381600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0209   |
|    learning_rate      | 0.00096  |
|    n_updates          | 47699    |
|    policy_loss        | -0.00583 |
|    std                | 0.136    |
|    value_loss         | 0.0929   |
------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.0126  |
| reward_motion         | 1.41     |
| reward_torque         | -0.278   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 572      |
|    ep_rew_mean        | -905     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 47800    |
|    time_elapsed       | 1733     |
|    total_timesteps    | 382400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 47799    |
|    policy_loss        | -0       |
|    std                | 0.136    |
|    value_loss         | 0.618    |
------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.013    |
| reward_motion         | 1.39      |
| reward_torque         | -0.272    |
| reward_velocity       | -2.56     |
| rollout/              |           |
|    ep_len_mean        | 581       |
|    ep_rew_mean        | -920      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 47900     |
|    time_elapsed       | 1737      |
|    total_timesteps    | 383200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 47899     |
|    policy_loss        | -0        |
|    std                | 0.136     |
|    value_loss         | 1.65      |
-------------------------------------
Num timesteps: 384000
Best mean reward: -792.37 - Last mean reward per episode: -920.74
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0131  |
| reward_motion         | 1.39     |
| reward_torque         | -0.272   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 581      |
|    ep_rew_mean        | -921     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48000    |
|    time_elapsed       | 1741     |
|    total_timesteps    | 384000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00493 |
|    learning_rate      | 0.00096  |
|    n_updates          | 47999    |
|    policy_loss        | 6.02e-05 |
|    std                | 0.136    |
|    value_loss         | 13       |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0135  |
| reward_motion         | 1.35     |
| reward_torque         | -0.269   |
| reward_velocity       | -2.53    |
| rollout/              |          |
|    ep_len_mean        | 591      |
|    ep_rew_mean        | -931     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48100    |
|    time_elapsed       | 1745     |
|    total_timesteps    | 384800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.45    |
|    learning_rate      | 0.00096  |
|    n_updates          | 48099    |
|    policy_loss        | -0.00183 |
|    std                | 0.136    |
|    value_loss         | 1.08     |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0131  |
| reward_motion         | 1.35     |
| reward_torque         | -0.269   |
| reward_velocity       | -2.53    |
| rollout/              |          |
|    ep_len_mean        | 591      |
|    ep_rew_mean        | -935     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48200    |
|    time_elapsed       | 1749     |
|    total_timesteps    | 385600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.00451  |
|    learning_rate      | 0.00096  |
|    n_updates          | 48199    |
|    policy_loss        | -0.00461 |
|    std                | 0.136    |
|    value_loss         | 2.31     |
------------------------------------
-------------------------------------
| reward                | -1.46     |
| reward_contact        | -0.0131   |
| reward_motion         | 1.35      |
| reward_torque         | -0.269    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 591       |
|    ep_rew_mean        | -935      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 48300     |
|    time_elapsed       | 1752      |
|    total_timesteps    | 386400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 3.99e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 48299     |
|    policy_loss        | -3.48e-05 |
|    std                | 0.136     |
|    value_loss         | 11.1      |
-------------------------------------
-------------------------------------
| reward                | -1.46     |
| reward_contact        | -0.013    |
| reward_motion         | 1.36      |
| reward_torque         | -0.272    |
| reward_velocity       | -2.54     |
| rollout/              |           |
|    ep_len_mean        | 591       |
|    ep_rew_mean        | -937      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 48400     |
|    time_elapsed       | 1756      |
|    total_timesteps    | 387200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 48399     |
|    policy_loss        | -2.86e-06 |
|    std                | 0.136     |
|    value_loss         | 2.5       |
-------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0133  |
| reward_motion         | 1.39     |
| reward_torque         | -0.273   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 581      |
|    ep_rew_mean        | -914     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48500    |
|    time_elapsed       | 1759     |
|    total_timesteps    | 388000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.31    |
|    learning_rate      | 0.00096  |
|    n_updates          | 48499    |
|    policy_loss        | -0.518   |
|    std                | 0.136    |
|    value_loss         | 9.86     |
------------------------------------
------------------------------------
| reward                | -1.42    |
| reward_contact        | -0.0138  |
| reward_motion         | 1.38     |
| reward_torque         | -0.267   |
| reward_velocity       | -2.52    |
| rollout/              |          |
|    ep_len_mean        | 591      |
|    ep_rew_mean        | -930     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48600    |
|    time_elapsed       | 1763     |
|    total_timesteps    | 388800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.172    |
|    learning_rate      | 0.00096  |
|    n_updates          | 48599    |
|    policy_loss        | 0.0137   |
|    std                | 0.136    |
|    value_loss         | 13.1     |
------------------------------------
-------------------------------------
| reward                | -1.42     |
| reward_contact        | -0.0138   |
| reward_motion         | 1.38      |
| reward_torque         | -0.267    |
| reward_velocity       | -2.52     |
| rollout/              |           |
|    ep_len_mean        | 591       |
|    ep_rew_mean        | -930      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 48700     |
|    time_elapsed       | 1767      |
|    total_timesteps    | 389600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0345    |
|    learning_rate      | 0.00096   |
|    n_updates          | 48699     |
|    policy_loss        | -0.000481 |
|    std                | 0.136     |
|    value_loss         | 0.113     |
-------------------------------------
Num timesteps: 390000
Best mean reward: -792.37 - Last mean reward per episode: -925.85
------------------------------------
| reward                | -1.42    |
| reward_contact        | -0.0138  |
| reward_motion         | 1.38     |
| reward_torque         | -0.267   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 591      |
|    ep_rew_mean        | -926     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48800    |
|    time_elapsed       | 1770     |
|    total_timesteps    | 390400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.112   |
|    learning_rate      | 0.00096  |
|    n_updates          | 48799    |
|    policy_loss        | 0.0321   |
|    std                | 0.136    |
|    value_loss         | 3.94     |
------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.013   |
| reward_motion         | 1.39     |
| reward_torque         | -0.272   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 594      |
|    ep_rew_mean        | -923     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 48900    |
|    time_elapsed       | 1774     |
|    total_timesteps    | 391200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.08    |
|    learning_rate      | 0.00096  |
|    n_updates          | 48899    |
|    policy_loss        | 0.69     |
|    std                | 0.137    |
|    value_loss         | 42.5     |
------------------------------------
------------------------------------
| reward                | -1.4     |
| reward_contact        | -0.013   |
| reward_motion         | 1.38     |
| reward_torque         | -0.269   |
| reward_velocity       | -2.49    |
| rollout/              |          |
|    ep_len_mean        | 594      |
|    ep_rew_mean        | -923     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49000    |
|    time_elapsed       | 1778     |
|    total_timesteps    | 392000   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -2.45    |
|    learning_rate      | 0.00096  |
|    n_updates          | 48999    |
|    policy_loss        | 0.108    |
|    std                | 0.137    |
|    value_loss         | 1.32     |
------------------------------------
------------------------------------
| reward                | -1.39    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.36     |
| reward_torque         | -0.263   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 604      |
|    ep_rew_mean        | -935     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49100    |
|    time_elapsed       | 1782     |
|    total_timesteps    | 392800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -2.38    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49099    |
|    policy_loss        | 0.577    |
|    std                | 0.137    |
|    value_loss         | 0.985    |
------------------------------------
------------------------------------
| reward                | -1.38    |
| reward_contact        | -0.0138  |
| reward_motion         | 1.34     |
| reward_torque         | -0.262   |
| reward_velocity       | -2.45    |
| rollout/              |          |
|    ep_len_mean        | 613      |
|    ep_rew_mean        | -949     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49200    |
|    time_elapsed       | 1785     |
|    total_timesteps    | 393600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.14    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49199    |
|    policy_loss        | 0.385    |
|    std                | 0.137    |
|    value_loss         | 0.744    |
------------------------------------
------------------------------------
| reward                | -1.39    |
| reward_contact        | -0.0135  |
| reward_motion         | 1.35     |
| reward_torque         | -0.263   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 608      |
|    ep_rew_mean        | -936     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49300    |
|    time_elapsed       | 1789     |
|    total_timesteps    | 394400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0199   |
|    learning_rate      | 0.00096  |
|    n_updates          | 49299    |
|    policy_loss        | -0.00112 |
|    std                | 0.137    |
|    value_loss         | 1.34     |
------------------------------------
------------------------------------
| reward                | -1.4     |
| reward_contact        | -0.0135  |
| reward_motion         | 1.33     |
| reward_torque         | -0.26    |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 617      |
|    ep_rew_mean        | -950     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49400    |
|    time_elapsed       | 1793     |
|    total_timesteps    | 395200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.108   |
|    learning_rate      | 0.00096  |
|    n_updates          | 49399    |
|    policy_loss        | 0.0265   |
|    std                | 0.137    |
|    value_loss         | 42.7     |
------------------------------------
Num timesteps: 396000
Best mean reward: -792.37 - Last mean reward per episode: -947.87
------------------------------------
| reward                | -1.4     |
| reward_contact        | -0.0135  |
| reward_motion         | 1.33     |
| reward_torque         | -0.26    |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 617      |
|    ep_rew_mean        | -948     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49500    |
|    time_elapsed       | 1797     |
|    total_timesteps    | 396000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.278    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49499    |
|    policy_loss        | -0.145   |
|    std                | 0.136    |
|    value_loss         | 22.5     |
------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.0137  |
| reward_motion         | 1.33     |
| reward_torque         | -0.258   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 621      |
|    ep_rew_mean        | -953     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49600    |
|    time_elapsed       | 1800     |
|    total_timesteps    | 396800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.196    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49599    |
|    policy_loss        | 0.0281   |
|    std                | 0.136    |
|    value_loss         | 0.495    |
------------------------------------
------------------------------------
| reward                | -1.41    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.33     |
| reward_torque         | -0.257   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -941     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49700    |
|    time_elapsed       | 1804     |
|    total_timesteps    | 397600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.327   |
|    learning_rate      | 0.00096  |
|    n_updates          | 49699    |
|    policy_loss        | 0.615    |
|    std                | 0.137    |
|    value_loss         | 72.1     |
------------------------------------
------------------------------------
| reward                | -1.37    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.36     |
| reward_torque         | -0.256   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 616      |
|    ep_rew_mean        | -944     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49800    |
|    time_elapsed       | 1808     |
|    total_timesteps    | 398400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -4.23    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49799    |
|    policy_loss        | 0.834    |
|    std                | 0.136    |
|    value_loss         | 9.43     |
------------------------------------
------------------------------------
| reward                | -1.37    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.36     |
| reward_torque         | -0.256   |
| reward_velocity       | -2.47    |
| rollout/              |          |
|    ep_len_mean        | 625      |
|    ep_rew_mean        | -953     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 49900    |
|    time_elapsed       | 1812     |
|    total_timesteps    | 399200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.446    |
|    learning_rate      | 0.00096  |
|    n_updates          | 49899    |
|    policy_loss        | 0.203    |
|    std                | 0.136    |
|    value_loss         | 23.8     |
------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0117  |
| reward_motion         | 1.4      |
| reward_torque         | -0.261   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -929     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50000    |
|    time_elapsed       | 1815     |
|    total_timesteps    | 400000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.054   |
|    learning_rate      | 0.00096  |
|    n_updates          | 49999    |
|    policy_loss        | -0.0194  |
|    std                | 0.136    |
|    value_loss         | 5.58     |
------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0117  |
| reward_motion         | 1.4      |
| reward_torque         | -0.261   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -929     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50100    |
|    time_elapsed       | 1819     |
|    total_timesteps    | 400800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0556   |
|    learning_rate      | 0.00096  |
|    n_updates          | 50099    |
|    policy_loss        | 0.00131  |
|    std                | 0.136    |
|    value_loss         | 0.0837   |
------------------------------------
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0116  |
| reward_motion         | 1.4      |
| reward_torque         | -0.261   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50200    |
|    time_elapsed       | 1823     |
|    total_timesteps    | 401600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.942   |
|    learning_rate      | 0.00096  |
|    n_updates          | 50199    |
|    policy_loss        | -0.0646  |
|    std                | 0.136    |
|    value_loss         | 3.35     |
------------------------------------
Num timesteps: 402000
Best mean reward: -792.37 - Last mean reward per episode: -928.11
------------------------------------
| reward                | -1.43    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.4      |
| reward_torque         | -0.261   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -928     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50300    |
|    time_elapsed       | 1827     |
|    total_timesteps    | 402400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.21     |
|    learning_rate      | 0.00096  |
|    n_updates          | 50299    |
|    policy_loss        | -0.244   |
|    std                | 0.136    |
|    value_loss         | 0.601    |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.37     |
| reward_torque         | -0.257   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 622      |
|    ep_rew_mean        | -940     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50400    |
|    time_elapsed       | 1831     |
|    total_timesteps    | 403200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -37.8    |
|    learning_rate      | 0.00096  |
|    n_updates          | 50399    |
|    policy_loss        | 0.924    |
|    std                | 0.136    |
|    value_loss         | 6.35     |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0115  |
| reward_motion         | 1.39     |
| reward_torque         | -0.259   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 615      |
|    ep_rew_mean        | -930     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50500    |
|    time_elapsed       | 1834     |
|    total_timesteps    | 404000   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.045    |
|    learning_rate      | 0.00096  |
|    n_updates          | 50499    |
|    policy_loss        | -0.0219  |
|    std                | 0.136    |
|    value_loss         | 2.95     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0111  |
| reward_motion         | 1.4      |
| reward_torque         | -0.263   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -922     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50600    |
|    time_elapsed       | 1838     |
|    total_timesteps    | 404800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0143  |
|    learning_rate      | 0.00096  |
|    n_updates          | 50599    |
|    policy_loss        | 0.00474  |
|    std                | 0.136    |
|    value_loss         | 7.85     |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0107  |
| reward_motion         | 1.4      |
| reward_torque         | -0.263   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -922     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50700    |
|    time_elapsed       | 1842     |
|    total_timesteps    | 405600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.139   |
|    learning_rate      | 0.00096  |
|    n_updates          | 50699    |
|    policy_loss        | 0.577    |
|    std                | 0.136    |
|    value_loss         | 2.88     |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0107  |
| reward_motion         | 1.4      |
| reward_torque         | -0.263   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 612      |
|    ep_rew_mean        | -920     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50800    |
|    time_elapsed       | 1845     |
|    total_timesteps    | 406400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 50799    |
|    policy_loss        | 7.63e-06 |
|    std                | 0.136    |
|    value_loss         | 13.4     |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0107  |
| reward_motion         | 1.4      |
| reward_torque         | -0.263   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 610      |
|    ep_rew_mean        | -914     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 50900    |
|    time_elapsed       | 1849     |
|    total_timesteps    | 407200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.983   |
|    learning_rate      | 0.00096  |
|    n_updates          | 50899    |
|    policy_loss        | 0.499    |
|    std                | 0.136    |
|    value_loss         | 40.3     |
------------------------------------
Num timesteps: 408000
Best mean reward: -792.37 - Last mean reward per episode: -913.85
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.0111   |
| reward_motion         | 1.4       |
| reward_torque         | -0.256    |
| reward_velocity       | -2.6      |
| rollout/              |           |
|    ep_len_mean        | 610       |
|    ep_rew_mean        | -914      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 51000     |
|    time_elapsed       | 1853      |
|    total_timesteps    | 408000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 1.19e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 50999     |
|    policy_loss        | -1.67e-06 |
|    std                | 0.136     |
|    value_loss         | 0.000172  |
-------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.0115   |
| reward_motion         | 1.39      |
| reward_torque         | -0.252    |
| reward_velocity       | -2.57     |
| rollout/              |           |
|    ep_len_mean        | 618       |
|    ep_rew_mean        | -925      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 51100     |
|    time_elapsed       | 1856      |
|    total_timesteps    | 408800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.000495  |
|    learning_rate      | 0.00096   |
|    n_updates          | 51099     |
|    policy_loss        | -3.29e-05 |
|    std                | 0.136     |
|    value_loss         | 0.562     |
-------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.39     |
| reward_torque         | -0.252   |
| reward_velocity       | -2.57    |
| rollout/              |          |
|    ep_len_mean        | 618      |
|    ep_rew_mean        | -922     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51200    |
|    time_elapsed       | 1860     |
|    total_timesteps    | 409600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -3.44    |
|    learning_rate      | 0.00096  |
|    n_updates          | 51199    |
|    policy_loss        | -0.29    |
|    std                | 0.136    |
|    value_loss         | 16.6     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.39     |
| reward_torque         | -0.253   |
| reward_velocity       | -2.57    |
| rollout/              |          |
|    ep_len_mean        | 618      |
|    ep_rew_mean        | -914     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51300    |
|    time_elapsed       | 1864     |
|    total_timesteps    | 410400   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.95    |
|    learning_rate      | 0.00096  |
|    n_updates          | 51299    |
|    policy_loss        | -0.0563  |
|    std                | 0.136    |
|    value_loss         | 25.4     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.39     |
| reward_torque         | -0.253   |
| reward_velocity       | -2.57    |
| rollout/              |          |
|    ep_len_mean        | 618      |
|    ep_rew_mean        | -914     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51400    |
|    time_elapsed       | 1868     |
|    total_timesteps    | 411200   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.0698  |
|    learning_rate      | 0.00096  |
|    n_updates          | 51399    |
|    policy_loss        | 0.121    |
|    std                | 0.136    |
|    value_loss         | 16.4     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.37     |
| reward_torque         | -0.246   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 632      |
|    ep_rew_mean        | -926     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51500    |
|    time_elapsed       | 1871     |
|    total_timesteps    | 412000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.0534   |
|    learning_rate      | 0.00096  |
|    n_updates          | 51499    |
|    policy_loss        | -0.00907 |
|    std                | 0.136    |
|    value_loss         | 27.7     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0118  |
| reward_motion         | 1.37     |
| reward_torque         | -0.246   |
| reward_velocity       | -2.59    |
| rollout/              |          |
|    ep_len_mean        | 632      |
|    ep_rew_mean        | -926     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51600    |
|    time_elapsed       | 1875     |
|    total_timesteps    | 412800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0478   |
|    learning_rate      | 0.00096  |
|    n_updates          | 51599    |
|    policy_loss        | -0.0284  |
|    std                | 0.136    |
|    value_loss         | 1.37     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0122  |
| reward_motion         | 1.37     |
| reward_torque         | -0.246   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 632      |
|    ep_rew_mean        | -918     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51700    |
|    time_elapsed       | 1879     |
|    total_timesteps    | 413600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.461   |
|    learning_rate      | 0.00096  |
|    n_updates          | 51699    |
|    policy_loss        | 0.0764   |
|    std                | 0.136    |
|    value_loss         | 3.19     |
------------------------------------
Num timesteps: 414000
Best mean reward: -792.37 - Last mean reward per episode: -928.76
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0122  |
| reward_motion         | 1.35     |
| reward_torque         | -0.243   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 642      |
|    ep_rew_mean        | -929     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 51800    |
|    time_elapsed       | 1883     |
|    total_timesteps    | 414400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -1.04    |
|    learning_rate      | 0.00096  |
|    n_updates          | 51799    |
|    policy_loss        | -0.0089  |
|    std                | 0.136    |
|    value_loss         | 0.542    |
------------------------------------
-------------------------------------
| reward                | -1.45     |
| reward_contact        | -0.012    |
| reward_motion         | 1.35      |
| reward_torque         | -0.243    |
| reward_velocity       | -2.55     |
| rollout/              |           |
|    ep_len_mean        | 642       |
|    ep_rew_mean        | -928      |
| time/                 |           |
|    fps                | 220       |
|    iterations         | 51900     |
|    time_elapsed       | 1886      |
|    total_timesteps    | 415200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -3.22e-06 |
|    learning_rate      | 0.00096   |
|    n_updates          | 51899     |
|    policy_loss        | 0.000176  |
|    std                | 0.135     |
|    value_loss         | 17.6      |
-------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.012   |
| reward_motion         | 1.35     |
| reward_torque         | -0.243   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 652      |
|    ep_rew_mean        | -942     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 52000    |
|    time_elapsed       | 1890     |
|    total_timesteps    | 416000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -6.21    |
|    learning_rate      | 0.00096  |
|    n_updates          | 51999    |
|    policy_loss        | 0.618    |
|    std                | 0.135    |
|    value_loss         | 11.7     |
------------------------------------
------------------------------------
| reward                | -1.44    |
| reward_contact        | -0.0123  |
| reward_motion         | 1.32     |
| reward_torque         | -0.235   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 652      |
|    ep_rew_mean        | -942     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 52100    |
|    time_elapsed       | 1894     |
|    total_timesteps    | 416800   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -0.00549 |
|    learning_rate      | 0.00096  |
|    n_updates          | 52099    |
|    policy_loss        | -0.00376 |
|    std                | 0.135    |
|    value_loss         | 5.38     |
------------------------------------
------------------------------------
| reward                | -1.46    |
| reward_contact        | -0.0124  |
| reward_motion         | 1.3      |
| reward_torque         | -0.227   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 659      |
|    ep_rew_mean        | -952     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 52200    |
|    time_elapsed       | 1898     |
|    total_timesteps    | 417600   |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 52199    |
|    policy_loss        | 1.67e-06 |
|    std                | 0.135    |
|    value_loss         | 0.493    |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.28     |
| reward_torque         | -0.222   |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 669      |
|    ep_rew_mean        | -964     |
| time/                 |          |
|    fps                | 220      |
|    iterations         | 52300    |
|    time_elapsed       | 1901     |
|    total_timesteps    | 418400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.127   |
|    learning_rate      | 0.00096  |
|    n_updates          | 52299    |
|    policy_loss        | -0.12    |
|    std                | 0.135    |
|    value_loss         | 1.51     |
------------------------------------
------------------------------------
| reward                | -1.45    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.28     |
| reward_torque         | -0.222   |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 670      |
|    ep_rew_mean        | -965     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 52400    |
|    time_elapsed       | 1905     |
|    total_timesteps    | 419200   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.171    |
|    learning_rate      | 0.00096  |
|    n_updates          | 52399    |
|    policy_loss        | -0.0675  |
|    std                | 0.135    |
|    value_loss         | 3.54     |
------------------------------------
Num timesteps: 420000
Best mean reward: -792.37 - Last mean reward per episode: -964.55
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0128  |
| reward_motion         | 1.27     |
| reward_torque         | -0.23    |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 670      |
|    ep_rew_mean        | -965     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 52500    |
|    time_elapsed       | 1909     |
|    total_timesteps    | 420000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 52499    |
|    policy_loss        | 1.67e-06 |
|    std                | 0.135    |
|    value_loss         | 2.26     |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0131  |
| reward_motion         | 1.26     |
| reward_torque         | -0.227   |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 674      |
|    ep_rew_mean        | -970     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 52600    |
|    time_elapsed       | 1913     |
|    total_timesteps    | 420800   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.354    |
|    learning_rate      | 0.00096  |
|    n_updates          | 52599    |
|    policy_loss        | 0.323    |
|    std                | 0.135    |
|    value_loss         | 0.963    |
------------------------------------
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0129  |
| reward_motion         | 1.3      |
| reward_torque         | -0.242   |
| reward_velocity       | -2.54    |
| rollout/              |          |
|    ep_len_mean        | 657      |
|    ep_rew_mean        | -943     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 52700    |
|    time_elapsed       | 1917     |
|    total_timesteps    | 421600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -7.01    |
|    learning_rate      | 0.00096  |
|    n_updates          | 52699    |
|    policy_loss        | 0.24     |
|    std                | 0.135    |
|    value_loss         | 12.7     |
------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.013   |
| reward_motion         | 1.29     |
| reward_torque         | -0.238   |
| reward_velocity       | -2.51    |
| rollout/              |          |
|    ep_len_mean        | 664      |
|    ep_rew_mean        | -956     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 52800    |
|    time_elapsed       | 1920     |
|    total_timesteps    | 422400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -1.68    |
|    learning_rate      | 0.00096  |
|    n_updates          | 52799    |
|    policy_loss        | -0.0122  |
|    std                | 0.135    |
|    value_loss         | 0.177    |
------------------------------------
-------------------------------------
| reward                | -1.47     |
| reward_contact        | -0.013    |
| reward_motion         | 1.29      |
| reward_torque         | -0.238    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 664       |
|    ep_rew_mean        | -956      |
| time/                 |           |
|    fps                | 219       |
|    iterations         | 52900     |
|    time_elapsed       | 1924      |
|    total_timesteps    | 423200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000343 |
|    learning_rate      | 0.00096   |
|    n_updates          | 52899     |
|    policy_loss        | -0.00546  |
|    std                | 0.135     |
|    value_loss         | 0.078     |
-------------------------------------
------------------------------------
| reward                | -1.49    |
| reward_contact        | -0.0132  |
| reward_motion         | 1.26     |
| reward_torque         | -0.24    |
| reward_velocity       | -2.5     |
| rollout/              |          |
|    ep_len_mean        | 669      |
|    ep_rew_mean        | -964     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53000    |
|    time_elapsed       | 1928     |
|    total_timesteps    | 424000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -1.4     |
|    learning_rate      | 0.00096  |
|    n_updates          | 52999    |
|    policy_loss        | -0.482   |
|    std                | 0.135    |
|    value_loss         | 4.09     |
------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0132   |
| reward_motion         | 1.29      |
| reward_torque         | -0.252    |
| reward_velocity       | -2.52     |
| rollout/              |           |
|    ep_len_mean        | 653       |
|    ep_rew_mean        | -933      |
| time/                 |           |
|    fps                | 219       |
|    iterations         | 53100     |
|    time_elapsed       | 1931      |
|    total_timesteps    | 424800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000728 |
|    learning_rate      | 0.00096   |
|    n_updates          | 53099     |
|    policy_loss        | -0.00276  |
|    std                | 0.135     |
|    value_loss         | 5.21      |
-------------------------------------
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.26     |
| reward_torque         | -0.246   |
| reward_velocity       | -2.52    |
| rollout/              |          |
|    ep_len_mean        | 663      |
|    ep_rew_mean        | -950     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53200    |
|    time_elapsed       | 1935     |
|    total_timesteps    | 425600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.0883   |
|    learning_rate      | 0.00096  |
|    n_updates          | 53199    |
|    policy_loss        | 0.00225  |
|    std                | 0.135    |
|    value_loss         | 0.099    |
------------------------------------
Num timesteps: 426000
Best mean reward: -792.37 - Last mean reward per episode: -950.15
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0134  |
| reward_motion         | 1.26     |
| reward_torque         | -0.246   |
| reward_velocity       | -2.52    |
| rollout/              |          |
|    ep_len_mean        | 671      |
|    ep_rew_mean        | -961     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53300    |
|    time_elapsed       | 1939     |
|    total_timesteps    | 426400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -2.25    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53299    |
|    policy_loss        | -0.231   |
|    std                | 0.135    |
|    value_loss         | 4.06     |
------------------------------------
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0138  |
| reward_motion         | 1.23     |
| reward_torque         | -0.243   |
| reward_velocity       | -2.49    |
| rollout/              |          |
|    ep_len_mean        | 671      |
|    ep_rew_mean        | -961     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53400    |
|    time_elapsed       | 1943     |
|    total_timesteps    | 427200   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.332    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53399    |
|    policy_loss        | -0.0152  |
|    std                | 0.135    |
|    value_loss         | 0.101    |
------------------------------------
------------------------------------
| reward                | -1.53    |
| reward_contact        | -0.0135  |
| reward_motion         | 1.21     |
| reward_torque         | -0.245   |
| reward_velocity       | -2.48    |
| rollout/              |          |
|    ep_len_mean        | 673      |
|    ep_rew_mean        | -968     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53500    |
|    time_elapsed       | 1947     |
|    total_timesteps    | 428000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.567    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53499    |
|    policy_loss        | -0.146   |
|    std                | 0.134    |
|    value_loss         | 0.599    |
------------------------------------
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.0136  |
| reward_motion         | 1.19     |
| reward_torque         | -0.242   |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 683      |
|    ep_rew_mean        | -987     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53600    |
|    time_elapsed       | 1950     |
|    total_timesteps    | 428800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -8.29    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53599    |
|    policy_loss        | -0.212   |
|    std                | 0.135    |
|    value_loss         | 14.4     |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0133  |
| reward_motion         | 1.25     |
| reward_torque         | -0.263   |
| reward_velocity       | -2.48    |
| rollout/              |          |
|    ep_len_mean        | 669      |
|    ep_rew_mean        | -965     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53700    |
|    time_elapsed       | 1954     |
|    total_timesteps    | 429600   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.532   |
|    learning_rate      | 0.00096  |
|    n_updates          | 53699    |
|    policy_loss        | 0.732    |
|    std                | 0.135    |
|    value_loss         | 1.13     |
------------------------------------
------------------------------------
| reward                | -1.47    |
| reward_contact        | -0.0133  |
| reward_motion         | 1.27     |
| reward_torque         | -0.266   |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 669      |
|    ep_rew_mean        | -966     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53800    |
|    time_elapsed       | 1958     |
|    total_timesteps    | 430400   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | 0.818    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53799    |
|    policy_loss        | 0.00303  |
|    std                | 0.135    |
|    value_loss         | 1.1      |
------------------------------------
------------------------------------
| reward                | -1.51    |
| reward_contact        | -0.0133  |
| reward_motion         | 1.25     |
| reward_torque         | -0.263   |
| reward_velocity       | -2.48    |
| rollout/              |          |
|    ep_len_mean        | 673      |
|    ep_rew_mean        | -974     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 53900    |
|    time_elapsed       | 1961     |
|    total_timesteps    | 431200   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.128    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53899    |
|    policy_loss        | 0.127    |
|    std                | 0.135    |
|    value_loss         | 1.67     |
------------------------------------
Num timesteps: 432000
Best mean reward: -792.37 - Last mean reward per episode: -973.16
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.0134  |
| reward_motion         | 1.23     |
| reward_torque         | -0.256   |
| reward_velocity       | -2.45    |
| rollout/              |          |
|    ep_len_mean        | 673      |
|    ep_rew_mean        | -973     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 54000    |
|    time_elapsed       | 1965     |
|    total_timesteps    | 432000   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.17    |
|    learning_rate      | 0.00096  |
|    n_updates          | 53999    |
|    policy_loss        | -0.0384  |
|    std                | 0.134    |
|    value_loss         | 0.819    |
------------------------------------
------------------------------------
| reward                | -1.48    |
| reward_contact        | -0.0137  |
| reward_motion         | 1.21     |
| reward_torque         | -0.254   |
| reward_velocity       | -2.43    |
| rollout/              |          |
|    ep_len_mean        | 682      |
|    ep_rew_mean        | -985     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 54100    |
|    time_elapsed       | 1969     |
|    total_timesteps    | 432800   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | -0.0863  |
|    learning_rate      | 0.00096  |
|    n_updates          | 54099    |
|    policy_loss        | -0.00225 |
|    std                | 0.134    |
|    value_loss         | 6.73     |
------------------------------------
------------------------------------
| reward                | -1.5     |
| reward_contact        | -0.0137  |
| reward_motion         | 1.23     |
| reward_torque         | -0.256   |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 677      |
|    ep_rew_mean        | -973     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 54200    |
|    time_elapsed       | 1973     |
|    total_timesteps    | 433600   |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.0612  |
|    learning_rate      | 0.00096  |
|    n_updates          | 54199    |
|    policy_loss        | -0.0288  |
|    std                | 0.134    |
|    value_loss         | 3.64     |
------------------------------------
-------------------------------------
| reward                | -1.52     |
| reward_contact        | -0.014    |
| reward_motion         | 1.21      |
| reward_torque         | -0.251    |
| reward_velocity       | -2.46     |
| rollout/              |           |
|    ep_len_mean        | 681       |
|    ep_rew_mean        | -982      |
| time/                 |           |
|    fps                | 219       |
|    iterations         | 54300     |
|    time_elapsed       | 1976      |
|    total_timesteps    | 434400    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000248 |
|    learning_rate      | 0.00096   |
|    n_updates          | 54299     |
|    policy_loss        | -0.000662 |
|    std                | 0.134     |
|    value_loss         | 3.67      |
-------------------------------------
------------------------------------
| reward                | -1.52    |
| reward_contact        | -0.014   |
| reward_motion         | 1.21     |
| reward_torque         | -0.251   |
| reward_velocity       | -2.46    |
| rollout/              |          |
|    ep_len_mean        | 681      |
|    ep_rew_mean        | -982     |
| time/                 |          |
|    fps                | 219      |
|    iterations         | 54400    |
|    time_elapsed       | 1980     |
|    total_timesteps    | 435200   |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.0734   |
|    learning_rate      | 0.00096  |
|    n_updates          | 54399    |
|    policy_loss        | -0.0937  |
|    std                | 0.134    |
|    value_loss         | 0.961    |
------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp73/A2C_2
-------------------------------------
| time/                 |           |
|    fps                | 305       |
|    iterations         | 100       |
|    time_elapsed       | 2         |
|    total_timesteps    | 800       |
| train/                |           |
|    entropy_loss       | -16.6     |
|    explained_variance | -1.67e-06 |
|    learning_rate      | 0.00096   |
|    n_updates          | 99        |
|    policy_loss        | 1.16      |
|    std                | 0.135     |
|    value_loss         | 33.4      |
-------------------------------------
-------------------------------------
| reward                | -2.68     |
| reward_contact        | -0.00228  |
| reward_motion         | 1.08      |
| reward_torque         | -0.124    |
| reward_velocity       | -3.64     |
| rollout/              |           |
|    ep_len_mean        | 547       |
|    ep_rew_mean        | -1.09e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 200       |
|    time_elapsed       | 5         |
|    total_timesteps    | 1600      |
| train/                |           |
|    entropy_loss       | -19.3     |
|    explained_variance | 0.000327  |
|    learning_rate      | 0.00096   |
|    n_updates          | 199       |
|    policy_loss        | -0.0316   |
|    std                | 0.135     |
|    value_loss         | 4.71      |
-------------------------------------
-------------------------------------
| reward                | -2.13     |
| reward_contact        | -0.0054   |
| reward_motion         | 0.723     |
| reward_torque         | -0.0838   |
| reward_velocity       | -2.76     |
| rollout/              |           |
|    ep_len_mean        | 706       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 300       |
|    time_elapsed       | 7         |
|    total_timesteps    | 2400      |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.000102  |
|    learning_rate      | 0.00096   |
|    n_updates          | 299       |
|    policy_loss        | 0.00102   |
|    std                | 0.135     |
|    value_loss         | 1.69      |
-------------------------------------
-------------------------------------
| reward                | -2.13     |
| reward_contact        | -0.0054   |
| reward_motion         | 0.723     |
| reward_torque         | -0.0838   |
| reward_velocity       | -2.76     |
| rollout/              |           |
|    ep_len_mean        | 786       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 400       |
|    time_elapsed       | 10        |
|    total_timesteps    | 3200      |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -3.76e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 399       |
|    policy_loss        | 0.0211    |
|    std                | 0.135     |
|    value_loss         | 12.9      |
-------------------------------------
-------------------------------------
| reward                | -1.87     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.59      |
| reward_torque         | -0.0638   |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 786       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 500       |
|    time_elapsed       | 12        |
|    total_timesteps    | 4000      |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -6.32e-06 |
|    learning_rate      | 0.00096   |
|    n_updates          | 499       |
|    policy_loss        | -0.171    |
|    std                | 0.135     |
|    value_loss         | 6.31      |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.0138   |
| reward_motion         | 0.473     |
| reward_torque         | -0.0513   |
| reward_velocity       | -2.13     |
| rollout/              |           |
|    ep_len_mean        | 833       |
|    ep_rew_mean        | -1.46e+03 |
| time/                 |           |
|    fps                | 316       |
|    iterations         | 600       |
|    time_elapsed       | 15        |
|    total_timesteps    | 4800      |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 599       |
|    policy_loss        | -0.00107  |
|    std                | 0.135     |
|    value_loss         | 0.435     |
-------------------------------------
-------------------------------------
| reward                | -1.49     |
| reward_contact        | -0.0128   |
| reward_motion         | 0.399     |
| reward_torque         | -0.0432   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 865       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 700       |
|    time_elapsed       | 17        |
|    total_timesteps    | 5600      |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 4.89e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 699       |
|    policy_loss        | 0.0112    |
|    std                | 0.134     |
|    value_loss         | 3.53      |
-------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -1448.76
Saving new best model to rl/out_dir/models/exp73/best_model.zip
-------------------------------------
| reward                | -1.43     |
| reward_contact        | -0.0167   |
| reward_motion         | 0.343     |
| reward_torque         | -0.0377   |
| reward_velocity       | -1.72     |
| rollout/              |           |
|    ep_len_mean        | 888       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 800       |
|    time_elapsed       | 20        |
|    total_timesteps    | 6400      |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -3.65e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 799       |
|    policy_loss        | 0.0491    |
|    std                | 0.134     |
|    value_loss         | 0.199     |
-------------------------------------
-------------------------------------
| reward                | -1.4      |
| reward_contact        | -0.0147   |
| reward_motion         | 0.463     |
| reward_torque         | -0.0438   |
| reward_velocity       | -1.81     |
| rollout/              |           |
|    ep_len_mean        | 846       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 900       |
|    time_elapsed       | 22        |
|    total_timesteps    | 7200      |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | -3.43e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 899       |
|    policy_loss        | -0.0184   |
|    std                | 0.134     |
|    value_loss         | 1.26      |
-------------------------------------
-------------------------------------
| reward                | -1.44     |
| reward_contact        | -0.0142   |
| reward_motion         | 0.413     |
| reward_torque         | -0.039    |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 866       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 316       |
|    iterations         | 1000      |
|    time_elapsed       | 25        |
|    total_timesteps    | 8000      |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | -0.0022   |
|    learning_rate      | 0.00096   |
|    n_updates          | 999       |
|    policy_loss        | 0.0632    |
|    std                | 0.134     |
|    value_loss         | 1.12      |
-------------------------------------
-------------------------------------
| reward                | -1.44     |
| reward_contact        | -0.0142   |
| reward_motion         | 0.413     |
| reward_torque         | -0.039    |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 866       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1100      |
|    time_elapsed       | 27        |
|    total_timesteps    | 8800      |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.000137 |
|    learning_rate      | 0.00096   |
|    n_updates          | 1099      |
|    policy_loss        | 0.0469    |
|    std                | 0.134     |
|    value_loss         | 0.34      |
-------------------------------------
-------------------------------------
| reward                | -1.48     |
| reward_contact        | -0.0148   |
| reward_motion         | 0.386     |
| reward_torque         | -0.0442   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 882       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1200      |
|    time_elapsed       | 30        |
|    total_timesteps    | 9600      |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | 6.29e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 1199      |
|    policy_loss        | -0.00107  |
|    std                | 0.134     |
|    value_loss         | 0.677     |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0141   |
| reward_motion         | 0.387     |
| reward_torque         | -0.0423   |
| reward_velocity       | -1.88     |
| rollout/              |           |
|    ep_len_mean        | 894       |
|    ep_rew_mean        | -1.49e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1300      |
|    time_elapsed       | 32        |
|    total_timesteps    | 10400     |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | 7.8e-05   |
|    learning_rate      | 0.00096   |
|    n_updates          | 1299      |
|    policy_loss        | -0.0537   |
|    std                | 0.134     |
|    value_loss         | 2.89      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.015    |
| reward_motion         | 0.381     |
| reward_torque         | -0.0388   |
| reward_velocity       | -1.91     |
| rollout/              |           |
|    ep_len_mean        | 905       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 316       |
|    iterations         | 1400      |
|    time_elapsed       | 35        |
|    total_timesteps    | 11200     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 4.83e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 1399      |
|    policy_loss        | 0.445     |
|    std                | 0.134     |
|    value_loss         | 7.7       |
-------------------------------------
Num timesteps: 12000
Best mean reward: -1448.76 - Last mean reward per episode: -1538.59
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0167   |
| reward_motion         | 0.352     |
| reward_torque         | -0.0362   |
| reward_velocity       | -1.92     |
| rollout/              |           |
|    ep_len_mean        | 914       |
|    ep_rew_mean        | -1.54e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1500      |
|    time_elapsed       | 37        |
|    total_timesteps    | 12000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.68     |
|    learning_rate      | 0.00096   |
|    n_updates          | 1499      |
|    policy_loss        | 0.00176   |
|    std                | 0.134     |
|    value_loss         | 7.1       |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0155   |
| reward_motion         | 0.455     |
| reward_torque         | -0.0546   |
| reward_velocity       | -2.03     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.42e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1600      |
|    time_elapsed       | 40        |
|    total_timesteps    | 12800     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | -0.0367   |
|    learning_rate      | 0.00096   |
|    n_updates          | 1599      |
|    policy_loss        | 0.39      |
|    std                | 0.134     |
|    value_loss         | 0.0925    |
-------------------------------------
-------------------------------------
| reward                | -1.64     |
| reward_contact        | -0.0144   |
| reward_motion         | 0.505     |
| reward_torque         | -0.0536   |
| reward_velocity       | -2.07     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.42e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1700      |
|    time_elapsed       | 43        |
|    total_timesteps    | 13600     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.0179   |
|    learning_rate      | 0.00096   |
|    n_updates          | 1699      |
|    policy_loss        | 0.542     |
|    std                | 0.134     |
|    value_loss         | 2.29      |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0142   |
| reward_motion         | 0.49      |
| reward_torque         | -0.054    |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 862       |
|    ep_rew_mean        | -1.42e+03 |
| time/                 |           |
|    fps                | 316       |
|    iterations         | 1800      |
|    time_elapsed       | 45        |
|    total_timesteps    | 14400     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.397    |
|    learning_rate      | 0.00096   |
|    n_updates          | 1799      |
|    policy_loss        | 0.0265    |
|    std                | 0.134     |
|    value_loss         | 2.55      |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0119   |
| reward_motion         | 0.623     |
| reward_torque         | -0.0455   |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 750       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 316       |
|    iterations         | 1900      |
|    time_elapsed       | 48        |
|    total_timesteps    | 15200     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.000404  |
|    learning_rate      | 0.00096   |
|    n_updates          | 1899      |
|    policy_loss        | 0.109     |
|    std                | 0.134     |
|    value_loss         | 0.975     |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0119   |
| reward_motion         | 0.623     |
| reward_torque         | -0.0455   |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 750       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 2000      |
|    time_elapsed       | 50        |
|    total_timesteps    | 16000     |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | 4.64e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 1999      |
|    policy_loss        | -0.00157  |
|    std                | 0.133     |
|    value_loss         | 0.331     |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0128   |
| reward_motion         | 0.594     |
| reward_torque         | -0.0433   |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 763       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 2100      |
|    time_elapsed       | 53        |
|    total_timesteps    | 16800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 2099      |
|    policy_loss        | 0.0144    |
|    std                | 0.133     |
|    value_loss         | 0.0944    |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0138   |
| reward_motion         | 0.567     |
| reward_torque         | -0.0416   |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 775       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 2200      |
|    time_elapsed       | 55        |
|    total_timesteps    | 17600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -7.37e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 2199      |
|    policy_loss        | -0.0136   |
|    std                | 0.134     |
|    value_loss         | 7.63      |
-------------------------------------
Num timesteps: 18000
Best mean reward: -1448.76 - Last mean reward per episode: -1284.51
Saving new best model to rl/out_dir/models/exp73/best_model.zip
------------------------------------
| reward                | -1.66    |
| reward_contact        | -0.0146  |
| reward_motion         | 0.551    |
| reward_torque         | -0.0398  |
| reward_velocity       | -2.16    |
| rollout/              |          |
|    ep_len_mean        | 786      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 315      |
|    iterations         | 2300     |
|    time_elapsed       | 58       |
|    total_timesteps    | 18400    |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 2299     |
|    policy_loss        | -0.0133  |
|    std                | 0.134    |
|    value_loss         | 3.92     |
------------------------------------
-------------------------------------
| reward                | -1.66     |
| reward_contact        | -0.0146   |
| reward_motion         | 0.551     |
| reward_torque         | -0.0398   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 796       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2400      |
|    time_elapsed       | 61        |
|    total_timesteps    | 19200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -2.4e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 2399      |
|    policy_loss        | -0.16     |
|    std                | 0.133     |
|    value_loss         | 3.5       |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0152   |
| reward_motion         | 0.528     |
| reward_torque         | -0.0382   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 796       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2500      |
|    time_elapsed       | 63        |
|    total_timesteps    | 20000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 2499      |
|    policy_loss        | -0.0199   |
|    std                | 0.133     |
|    value_loss         | 3.22      |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0158   |
| reward_motion         | 0.507     |
| reward_torque         | -0.0378   |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 805       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2600      |
|    time_elapsed       | 66        |
|    total_timesteps    | 20800     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 2599      |
|    policy_loss        | 0.0303    |
|    std                | 0.133     |
|    value_loss         | 3.27      |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0157   |
| reward_motion         | 0.491     |
| reward_torque         | -0.0364   |
| reward_velocity       | -2.14     |
| rollout/              |           |
|    ep_len_mean        | 814       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2700      |
|    time_elapsed       | 68        |
|    total_timesteps    | 21600     |
| train/                |           |
|    entropy_loss       | -19.5     |
|    explained_variance | 5.96e-08  |
|    learning_rate      | 0.00096   |
|    n_updates          | 2699      |
|    policy_loss        | 0.192     |
|    std                | 0.133     |
|    value_loss         | 1.88      |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0154   |
| reward_motion         | 0.475     |
| reward_torque         | -0.0351   |
| reward_velocity       | -2.11     |
| rollout/              |           |
|    ep_len_mean        | 821       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2800      |
|    time_elapsed       | 71        |
|    total_timesteps    | 22400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 2799      |
|    policy_loss        | -0.0178   |
|    std                | 0.133     |
|    value_loss         | 0.509     |
-------------------------------------
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0154   |
| reward_motion         | 0.475     |
| reward_torque         | -0.0351   |
| reward_velocity       | -2.11     |
| rollout/              |           |
|    ep_len_mean        | 829       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 2900      |
|    time_elapsed       | 73        |
|    total_timesteps    | 23200     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 2899      |
|    policy_loss        | -0.175    |
|    std                | 0.133     |
|    value_loss         | 0.0307    |
-------------------------------------
Num timesteps: 24000
Best mean reward: -1284.51 - Last mean reward per episode: -1384.62
-------------------------------------
| reward                | -1.68     |
| reward_contact        | -0.0162   |
| reward_motion         | 0.461     |
| reward_torque         | -0.0342   |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 829       |
|    ep_rew_mean        | -1.38e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 3000      |
|    time_elapsed       | 76        |
|    total_timesteps    | 24000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 2999      |
|    policy_loss        | -0.0362   |
|    std                | 0.133     |
|    value_loss         | 1.1       |
-------------------------------------
-------------------------------------
| reward                | -1.67     |
| reward_contact        | -0.0168   |
| reward_motion         | 0.445     |
| reward_torque         | -0.0332   |
| reward_velocity       | -2.07     |
| rollout/              |           |
|    ep_len_mean        | 835       |
|    ep_rew_mean        | -1.39e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 3100      |
|    time_elapsed       | 78        |
|    total_timesteps    | 24800     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.000268 |
|    learning_rate      | 0.00096   |
|    n_updates          | 3099      |
|    policy_loss        | -0.146    |
|    std                | 0.133     |
|    value_loss         | 2.18      |
-------------------------------------
-------------------------------------
| reward                | -1.69     |
| reward_contact        | -0.0175   |
| reward_motion         | 0.431     |
| reward_torque         | -0.0328   |
| reward_velocity       | -2.07     |
| rollout/              |           |
|    ep_len_mean        | 842       |
|    ep_rew_mean        | -1.4e+03  |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 3200      |
|    time_elapsed       | 81        |
|    total_timesteps    | 25600     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -5.84e-06 |
|    learning_rate      | 0.00096   |
|    n_updates          | 3199      |
|    policy_loss        | 0.101     |
|    std                | 0.133     |
|    value_loss         | 2.23      |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.0164   |
| reward_motion         | 0.507     |
| reward_torque         | -0.0482   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 817       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 3300      |
|    time_elapsed       | 83        |
|    total_timesteps    | 26400     |
| train/                |           |
|    entropy_loss       | -19.8     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3299      |
|    policy_loss        | 0.686     |
|    std                | 0.133     |
|    value_loss         | 1.85      |
-------------------------------------
-------------------------------------
| reward                | -1.72     |
| reward_contact        | -0.0164   |
| reward_motion         | 0.507     |
| reward_torque         | -0.0482   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 799       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3400      |
|    time_elapsed       | 86        |
|    total_timesteps    | 27200     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3399      |
|    policy_loss        | -0.287    |
|    std                | 0.134     |
|    value_loss         | 2.48      |
-------------------------------------
-------------------------------------
| reward                | -1.87     |
| reward_contact        | -0.0158   |
| reward_motion         | 0.541     |
| reward_torque         | -0.0569   |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 799       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3500      |
|    time_elapsed       | 88        |
|    total_timesteps    | 28000     |
| train/                |           |
|    entropy_loss       | -19.6     |
|    explained_variance | 1.49e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 3499      |
|    policy_loss        | 0.0779    |
|    std                | 0.134     |
|    value_loss         | 17.8      |
-------------------------------------
-------------------------------------
| reward                | -1.87     |
| reward_contact        | -0.0155   |
| reward_motion         | 0.535     |
| reward_torque         | -0.0574   |
| reward_velocity       | -2.33     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3600      |
|    time_elapsed       | 91        |
|    total_timesteps    | 28800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 5.96e-08  |
|    learning_rate      | 0.00096   |
|    n_updates          | 3599      |
|    policy_loss        | -0.00183  |
|    std                | 0.133     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0163   |
| reward_motion         | 0.567     |
| reward_torque         | -0.064    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 794       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3700      |
|    time_elapsed       | 93        |
|    total_timesteps    | 29600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3699      |
|    policy_loss        | 0.00178   |
|    std                | 0.133     |
|    value_loss         | 1.96      |
-------------------------------------
Num timesteps: 30000
Best mean reward: -1284.51 - Last mean reward per episode: -1328.12
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0163   |
| reward_motion         | 0.567     |
| reward_torque         | -0.064    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 794       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3800      |
|    time_elapsed       | 96        |
|    total_timesteps    | 30400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3799      |
|    policy_loss        | 0.00255   |
|    std                | 0.133     |
|    value_loss         | 0.000925  |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.553     |
| reward_torque         | -0.0624   |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 801       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 3900      |
|    time_elapsed       | 98        |
|    total_timesteps    | 31200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3899      |
|    policy_loss        | 0.00756   |
|    std                | 0.133     |
|    value_loss         | 5.89      |
-------------------------------------
-------------------------------------
| reward                | -1.89     |
| reward_contact        | -0.0174   |
| reward_motion         | 0.538     |
| reward_torque         | -0.0609   |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4000      |
|    time_elapsed       | 101       |
|    total_timesteps    | 32000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 3999      |
|    policy_loss        | -0.00781  |
|    std                | 0.133     |
|    value_loss         | 1.59      |
-------------------------------------
-------------------------------------
| reward                | -2.13     |
| reward_contact        | -0.0164   |
| reward_motion         | 0.615     |
| reward_torque         | -0.0919   |
| reward_velocity       | -2.63     |
| rollout/              |           |
|    ep_len_mean        | 759       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4100      |
|    time_elapsed       | 104       |
|    total_timesteps    | 32800     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | -2.67e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 4099      |
|    policy_loss        | 1.23      |
|    std                | 0.133     |
|    value_loss         | 36.5      |
-------------------------------------
-------------------------------------
| reward                | -2.13     |
| reward_contact        | -0.0164   |
| reward_motion         | 0.615     |
| reward_torque         | -0.0919   |
| reward_velocity       | -2.63     |
| rollout/              |           |
|    ep_len_mean        | 759       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4200      |
|    time_elapsed       | 106       |
|    total_timesteps    | 33600     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | 7.56e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 4199      |
|    policy_loss        | 0.0368    |
|    std                | 0.133     |
|    value_loss         | 80.7      |
-------------------------------------
-------------------------------------
| reward                | -2.19     |
| reward_contact        | -0.0154   |
| reward_motion         | 0.698     |
| reward_torque         | -0.102    |
| reward_velocity       | -2.77     |
| rollout/              |           |
|    ep_len_mean        | 731       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4300      |
|    time_elapsed       | 109       |
|    total_timesteps    | 34400     |
| train/                |           |
|    entropy_loss       | -19.7     |
|    explained_variance | 4.71e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 4299      |
|    policy_loss        | 0.915     |
|    std                | 0.133     |
|    value_loss         | 31.8      |
-------------------------------------
-------------------------------------
| reward                | -2.15     |
| reward_contact        | -0.015    |
| reward_motion         | 0.718     |
| reward_torque         | -0.118    |
| reward_velocity       | -2.74     |
| rollout/              |           |
|    ep_len_mean        | 731       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4400      |
|    time_elapsed       | 111       |
|    total_timesteps    | 35200     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 4399      |
|    policy_loss        | -7.92e-05 |
|    std                | 0.133     |
|    value_loss         | 1.89      |
-------------------------------------
Num timesteps: 36000
Best mean reward: -1284.51 - Last mean reward per episode: -1238.74
Saving new best model to rl/out_dir/models/exp73/best_model.zip
-------------------------------------
| reward                | -2.1      |
| reward_contact        | -0.015    |
| reward_motion         | 0.729     |
| reward_torque         | -0.115    |
| reward_velocity       | -2.7      |
| rollout/              |           |
|    ep_len_mean        | 723       |
|    ep_rew_mean        | -1.24e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4500      |
|    time_elapsed       | 114       |
|    total_timesteps    | 36000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -4.54e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 4499      |
|    policy_loss        | -0.0282   |
|    std                | 0.133     |
|    value_loss         | 1.53      |
-------------------------------------
-------------------------------------
| reward                | -2.08     |
| reward_contact        | -0.015    |
| reward_motion         | 0.715     |
| reward_torque         | -0.113    |
| reward_velocity       | -2.67     |
| rollout/              |           |
|    ep_len_mean        | 729       |
|    ep_rew_mean        | -1.25e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4600      |
|    time_elapsed       | 116       |
|    total_timesteps    | 36800     |
| train/                |           |
|    entropy_loss       | -19.4     |
|    explained_variance | -0.000608 |
|    learning_rate      | 0.00096   |
|    n_updates          | 4599      |
|    policy_loss        | -0.25     |
|    std                | 0.133     |
|    value_loss         | 154       |
-------------------------------------
-------------------------------------
| reward                | -2.08     |
| reward_contact        | -0.015    |
| reward_motion         | 0.702     |
| reward_torque         | -0.111    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 735       |
|    ep_rew_mean        | -1.26e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4700      |
|    time_elapsed       | 119       |
|    total_timesteps    | 37600     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 4699      |
|    policy_loss        | 0.0619    |
|    std                | 0.133     |
|    value_loss         | 0.815     |
-------------------------------------
-------------------------------------
| reward                | -2.08     |
| reward_contact        | -0.015    |
| reward_motion         | 0.702     |
| reward_torque         | -0.111    |
| reward_velocity       | -2.65     |
| rollout/              |           |
|    ep_len_mean        | 735       |
|    ep_rew_mean        | -1.26e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4800      |
|    time_elapsed       | 121       |
|    total_timesteps    | 38400     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | 1.13e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 4799      |
|    policy_loss        | -0.0644   |
|    std                | 0.133     |
|    value_loss         | 1.99      |
-------------------------------------
-------------------------------------
| reward                | -2.08     |
| reward_contact        | -0.0149   |
| reward_motion         | 0.688     |
| reward_torque         | -0.109    |
| reward_velocity       | -2.64     |
| rollout/              |           |
|    ep_len_mean        | 741       |
|    ep_rew_mean        | -1.27e+03 |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 4900      |
|    time_elapsed       | 124       |
|    total_timesteps    | 39200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.00257  |
|    learning_rate      | 0.00096   |
|    n_updates          | 4899      |
|    policy_loss        | -0.0102   |
|    std                | 0.132     |
|    value_loss         | 2.16      |
-------------------------------------
-------------------------------------
| reward                | -2.07     |
| reward_contact        | -0.0153   |
| reward_motion         | 0.675     |
| reward_torque         | -0.107    |
| reward_velocity       | -2.62     |
| rollout/              |           |
|    ep_len_mean        | 746       |
|    ep_rew_mean        | -1.28e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 5000      |
|    time_elapsed       | 127       |
|    total_timesteps    | 40000     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | -0.000709 |
|    learning_rate      | 0.00096   |
|    n_updates          | 4999      |
|    policy_loss        | -0.0151   |
|    std                | 0.132     |
|    value_loss         | 3.17      |
-------------------------------------
-------------------------------------
| reward                | -2.04     |
| reward_contact        | -0.0156   |
| reward_motion         | 0.663     |
| reward_torque         | -0.105    |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 751       |
|    ep_rew_mean        | -1.29e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 5100      |
|    time_elapsed       | 129       |
|    total_timesteps    | 40800     |
| train/                |           |
|    entropy_loss       | -19.8     |
|    explained_variance | -0.000288 |
|    learning_rate      | 0.00096   |
|    n_updates          | 5099      |
|    policy_loss        | -0.604    |
|    std                | 0.132     |
|    value_loss         | 1.27      |
-------------------------------------
------------------------------------
| reward                | -2.04    |
| reward_contact        | -0.0156  |
| reward_motion         | 0.663    |
| reward_torque         | -0.105   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 756      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 5200     |
|    time_elapsed       | 132      |
|    total_timesteps    | 41600    |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | -1.25    |
|    learning_rate      | 0.00096  |
|    n_updates          | 5199     |
|    policy_loss        | -0.186   |
|    std                | 0.133    |
|    value_loss         | 11.1     |
------------------------------------
Num timesteps: 42000
Best mean reward: -1238.74 - Last mean reward per episode: -1296.11
------------------------------------
| reward                | -2.05    |
| reward_contact        | -0.0159  |
| reward_motion         | 0.651    |
| reward_torque         | -0.103   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 756      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 5300     |
|    time_elapsed       | 134      |
|    total_timesteps    | 42400    |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.00725  |
|    learning_rate      | 0.00096  |
|    n_updates          | 5299     |
|    policy_loss        | -0.0505  |
|    std                | 0.133    |
|    value_loss         | 1.27     |
------------------------------------
------------------------------------
| reward                | -2.02    |
| reward_contact        | -0.0162  |
| reward_motion         | 0.648    |
| reward_torque         | -0.102   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 761      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 5400     |
|    time_elapsed       | 137      |
|    total_timesteps    | 43200    |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | -0.0185  |
|    learning_rate      | 0.00096  |
|    n_updates          | 5399     |
|    policy_loss        | 0.226    |
|    std                | 0.133    |
|    value_loss         | 0.373    |
------------------------------------
------------------------------------
| reward                | -2.01    |
| reward_contact        | -0.0164  |
| reward_motion         | 0.637    |
| reward_torque         | -0.0999  |
| reward_velocity       | -2.53    |
| rollout/              |          |
|    ep_len_mean        | 765      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 5500     |
|    time_elapsed       | 140      |
|    total_timesteps    | 44000    |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | -0.0167  |
|    learning_rate      | 0.00096  |
|    n_updates          | 5499     |
|    policy_loss        | -0.381   |
|    std                | 0.133    |
|    value_loss         | 0.97     |
------------------------------------
-------------------------------------
| reward                | -2.01     |
| reward_contact        | -0.0167   |
| reward_motion         | 0.626     |
| reward_torque         | -0.0983   |
| reward_velocity       | -2.52     |
| rollout/              |           |
|    ep_len_mean        | 770       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 5600      |
|    time_elapsed       | 142       |
|    total_timesteps    | 44800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.685     |
|    learning_rate      | 0.00096   |
|    n_updates          | 5599      |
|    policy_loss        | 0.0167    |
|    std                | 0.133     |
|    value_loss         | 1.1       |
-------------------------------------
-------------------------------------
| reward                | -2.01     |
| reward_contact        | -0.0167   |
| reward_motion         | 0.626     |
| reward_torque         | -0.0983   |
| reward_velocity       | -2.52     |
| rollout/              |           |
|    ep_len_mean        | 770       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 5700      |
|    time_elapsed       | 145       |
|    total_timesteps    | 45600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0211   |
|    learning_rate      | 0.00096   |
|    n_updates          | 5699      |
|    policy_loss        | -0.0537   |
|    std                | 0.133     |
|    value_loss         | 1.4       |
-------------------------------------
-------------------------------------
| reward                | -2.01     |
| reward_contact        | -0.017    |
| reward_motion         | 0.615     |
| reward_torque         | -0.0967   |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 774       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 5800      |
|    time_elapsed       | 147       |
|    total_timesteps    | 46400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.107    |
|    learning_rate      | 0.00096   |
|    n_updates          | 5799      |
|    policy_loss        | -0.000462 |
|    std                | 0.133     |
|    value_loss         | 0.02      |
-------------------------------------
-------------------------------------
| reward                | -2        |
| reward_contact        | -0.0174   |
| reward_motion         | 0.605     |
| reward_torque         | -0.0951   |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 778       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 5900      |
|    time_elapsed       | 150       |
|    total_timesteps    | 47200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.178     |
|    learning_rate      | 0.00096   |
|    n_updates          | 5899      |
|    policy_loss        | 0.0206    |
|    std                | 0.133     |
|    value_loss         | 0.0782    |
-------------------------------------
Num timesteps: 48000
Best mean reward: -1238.74 - Last mean reward per episode: -1329.84
-------------------------------------
| reward                | -2        |
| reward_contact        | -0.0178   |
| reward_motion         | 0.595     |
| reward_torque         | -0.0937   |
| reward_velocity       | -2.48     |
| rollout/              |           |
|    ep_len_mean        | 782       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 6000      |
|    time_elapsed       | 152       |
|    total_timesteps    | 48000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0074   |
|    learning_rate      | 0.00096   |
|    n_updates          | 5999      |
|    policy_loss        | -0.0203   |
|    std                | 0.133     |
|    value_loss         | 1.87      |
-------------------------------------
-------------------------------------
| reward                | -2        |
| reward_contact        | -0.0178   |
| reward_motion         | 0.595     |
| reward_torque         | -0.0937   |
| reward_velocity       | -2.48     |
| rollout/              |           |
|    ep_len_mean        | 786       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 6100      |
|    time_elapsed       | 155       |
|    total_timesteps    | 48800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0838    |
|    learning_rate      | 0.00096   |
|    n_updates          | 6099      |
|    policy_loss        | -0.101    |
|    std                | 0.133     |
|    value_loss         | 7.55      |
-------------------------------------
------------------------------------
| reward                | -2.05    |
| reward_contact        | -0.0175  |
| reward_motion         | 0.628    |
| reward_torque         | -0.106   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 762      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 6200     |
|    time_elapsed       | 157      |
|    total_timesteps    | 49600    |
| train/                |          |
|    entropy_loss       | -19.5    |
|    explained_variance | 0.000313 |
|    learning_rate      | 0.00096  |
|    n_updates          | 6199     |
|    policy_loss        | 0.684    |
|    std                | 0.134    |
|    value_loss         | 48.5     |
------------------------------------
------------------------------------
| reward                | -2.05    |
| reward_contact        | -0.0173  |
| reward_motion         | 0.649    |
| reward_torque         | -0.108   |
| reward_velocity       | -2.58    |
| rollout/              |          |
|    ep_len_mean        | 762      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 6300     |
|    time_elapsed       | 160      |
|    total_timesteps    | 50400    |
| train/                |          |
|    entropy_loss       | -19.7    |
|    explained_variance | -0.00486 |
|    learning_rate      | 0.00096  |
|    n_updates          | 6299     |
|    policy_loss        | 0.0641   |
|    std                | 0.134    |
|    value_loss         | 4.63     |
------------------------------------
-------------------------------------
| reward                | -2.05     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.65      |
| reward_torque         | -0.106    |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 760       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 6400      |
|    time_elapsed       | 162       |
|    total_timesteps    | 51200     |
| train/                |           |
|    entropy_loss       | -20       |
|    explained_variance | 0.0515    |
|    learning_rate      | 0.00096   |
|    n_updates          | 6399      |
|    policy_loss        | 0.223     |
|    std                | 0.134     |
|    value_loss         | 2.66      |
-------------------------------------
-------------------------------------
| reward                | -2.05     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.65      |
| reward_torque         | -0.106    |
| reward_velocity       | -2.58     |
| rollout/              |           |
|    ep_len_mean        | 763       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 6500      |
|    time_elapsed       | 165       |
|    total_timesteps    | 52000     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.45     |
|    learning_rate      | 0.00096   |
|    n_updates          | 6499      |
|    policy_loss        | -0.157    |
|    std                | 0.134     |
|    value_loss         | 3.49      |
-------------------------------------
------------------------------------
| reward                | -1.99    |
| reward_contact        | -0.017   |
| reward_motion         | 0.686    |
| reward_torque         | -0.107   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 757      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 6600     |
|    time_elapsed       | 167      |
|    total_timesteps    | 52800    |
| train/                |          |
|    entropy_loss       | -20.1    |
|    explained_variance | 0.421    |
|    learning_rate      | 0.00096  |
|    n_updates          | 6599     |
|    policy_loss        | 0.131    |
|    std                | 0.134    |
|    value_loss         | 0.0849   |
------------------------------------
-------------------------------------
| reward                | -1.98     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.676     |
| reward_torque         | -0.106    |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 760       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 6700      |
|    time_elapsed       | 170       |
|    total_timesteps    | 53600     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.0205    |
|    learning_rate      | 0.00096   |
|    n_updates          | 6699      |
|    policy_loss        | 0.147     |
|    std                | 0.134     |
|    value_loss         | 2.88      |
-------------------------------------
Num timesteps: 54000
Best mean reward: -1238.74 - Last mean reward per episode: -1300.42
------------------------------------
| reward                | -1.99    |
| reward_contact        | -0.0166  |
| reward_motion         | 0.699    |
| reward_torque         | -0.115   |
| reward_velocity       | -2.56    |
| rollout/              |          |
|    ep_len_mean        | 757      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 6800     |
|    time_elapsed       | 173      |
|    total_timesteps    | 54400    |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | -0.222   |
|    learning_rate      | 0.00096  |
|    n_updates          | 6799     |
|    policy_loss        | 0.349    |
|    std                | 0.135    |
|    value_loss         | 1.81     |
------------------------------------
------------------------------------
| reward                | -1.99    |
| reward_contact        | -0.0166  |
| reward_motion         | 0.694    |
| reward_torque         | -0.113   |
| reward_velocity       | -2.55    |
| rollout/              |          |
|    ep_len_mean        | 760      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 6900     |
|    time_elapsed       | 175      |
|    total_timesteps    | 55200    |
| train/                |          |
|    entropy_loss       | -20.2    |
|    explained_variance | -0.873   |
|    learning_rate      | 0.00096  |
|    n_updates          | 6899     |
|    policy_loss        | -0.0732  |
|    std                | 0.135    |
|    value_loss         | 8.96     |
------------------------------------
-------------------------------------
| reward                | -1.99     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.685     |
| reward_torque         | -0.112    |
| reward_velocity       | -2.54     |
| rollout/              |           |
|    ep_len_mean        | 764       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7000      |
|    time_elapsed       | 178       |
|    total_timesteps    | 56000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0014    |
|    learning_rate      | 0.00096   |
|    n_updates          | 6999      |
|    policy_loss        | 0.000292  |
|    std                | 0.135     |
|    value_loss         | 0.274     |
-------------------------------------
-------------------------------------
| reward                | -1.99     |
| reward_contact        | -0.0169   |
| reward_motion         | 0.685     |
| reward_torque         | -0.112    |
| reward_velocity       | -2.54     |
| rollout/              |           |
|    ep_len_mean        | 767       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7100      |
|    time_elapsed       | 180       |
|    total_timesteps    | 56800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.81e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 7099      |
|    policy_loss        | -4.52e-05 |
|    std                | 0.135     |
|    value_loss         | 0.541     |
-------------------------------------
-------------------------------------
| reward                | -1.99     |
| reward_contact        | -0.0172   |
| reward_motion         | 0.676     |
| reward_torque         | -0.11     |
| reward_velocity       | -2.53     |
| rollout/              |           |
|    ep_len_mean        | 767       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7200      |
|    time_elapsed       | 183       |
|    total_timesteps    | 57600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.686    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7199      |
|    policy_loss        | 4.57e-05  |
|    std                | 0.135     |
|    value_loss         | 0.557     |
-------------------------------------
-------------------------------------
| reward                | -1.97     |
| reward_contact        | -0.0175   |
| reward_motion         | 0.667     |
| reward_torque         | -0.109    |
| reward_velocity       | -2.51     |
| rollout/              |           |
|    ep_len_mean        | 771       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7300      |
|    time_elapsed       | 185       |
|    total_timesteps    | 58400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.795    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7299      |
|    policy_loss        | -0.000522 |
|    std                | 0.135     |
|    value_loss         | 1.3e-06   |
-------------------------------------
-------------------------------------
| reward                | -1.96     |
| reward_contact        | -0.0178   |
| reward_motion         | 0.658     |
| reward_torque         | -0.107    |
| reward_velocity       | -2.49     |
| rollout/              |           |
|    ep_len_mean        | 774       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7400      |
|    time_elapsed       | 188       |
|    total_timesteps    | 59200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 7399      |
|    policy_loss        | -0        |
|    std                | 0.135     |
|    value_loss         | 1.99      |
-------------------------------------
Num timesteps: 60000
Best mean reward: -1238.74 - Last mean reward per episode: -1322.39
-------------------------------------
| reward                | -1.95     |
| reward_contact        | -0.0181   |
| reward_motion         | 0.649     |
| reward_torque         | -0.106    |
| reward_velocity       | -2.47     |
| rollout/              |           |
|    ep_len_mean        | 777       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7500      |
|    time_elapsed       | 190       |
|    total_timesteps    | 60000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.404    |
|    learning_rate      | 0.00096   |
|    n_updates          | 7499      |
|    policy_loss        | 0.000174  |
|    std                | 0.135     |
|    value_loss         | 0.0156    |
-------------------------------------
-------------------------------------
| reward                | -1.95     |
| reward_contact        | -0.0181   |
| reward_motion         | 0.649     |
| reward_torque         | -0.106    |
| reward_velocity       | -2.47     |
| rollout/              |           |
|    ep_len_mean        | 777       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7600      |
|    time_elapsed       | 193       |
|    total_timesteps    | 60800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.744     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7599      |
|    policy_loss        | -0.00107  |
|    std                | 0.135     |
|    value_loss         | 0.0642    |
-------------------------------------
-------------------------------------
| reward                | -1.95     |
| reward_contact        | -0.0183   |
| reward_motion         | 0.641     |
| reward_torque         | -0.105    |
| reward_velocity       | -2.47     |
| rollout/              |           |
|    ep_len_mean        | 781       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7700      |
|    time_elapsed       | 195       |
|    total_timesteps    | 61600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -14.7     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7699      |
|    policy_loss        | 0.0414    |
|    std                | 0.135     |
|    value_loss         | 0.996     |
-------------------------------------
-------------------------------------
| reward                | -1.94     |
| reward_contact        | -0.0186   |
| reward_motion         | 0.633     |
| reward_torque         | -0.103    |
| reward_velocity       | -2.45     |
| rollout/              |           |
|    ep_len_mean        | 784       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7800      |
|    time_elapsed       | 198       |
|    total_timesteps    | 62400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.223     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7799      |
|    policy_loss        | -0.00136  |
|    std                | 0.135     |
|    value_loss         | 6.39e-05  |
-------------------------------------
-------------------------------------
| reward                | -1.94     |
| reward_contact        | -0.0188   |
| reward_motion         | 0.625     |
| reward_torque         | -0.102    |
| reward_velocity       | -2.44     |
| rollout/              |           |
|    ep_len_mean        | 787       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 7900      |
|    time_elapsed       | 201       |
|    total_timesteps    | 63200     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 7899      |
|    policy_loss        | 1.91e-06  |
|    std                | 0.135     |
|    value_loss         | 1.36      |
-------------------------------------
-------------------------------------
| reward                | -1.94     |
| reward_contact        | -0.0188   |
| reward_motion         | 0.625     |
| reward_torque         | -0.102    |
| reward_velocity       | -2.44     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8000      |
|    time_elapsed       | 203       |
|    total_timesteps    | 64000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -20.2     |
|    learning_rate      | 0.00096   |
|    n_updates          | 7999      |
|    policy_loss        | 0.0104    |
|    std                | 0.135     |
|    value_loss         | 135       |
-------------------------------------
-------------------------------------
| reward                | -1.91     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.617     |
| reward_torque         | -0.101    |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 790       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8100      |
|    time_elapsed       | 206       |
|    total_timesteps    | 64800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 8099      |
|    policy_loss        | 2.38e-07  |
|    std                | 0.135     |
|    value_loss         | 1.4e-09   |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0195   |
| reward_motion         | 0.61      |
| reward_torque         | -0.0995   |
| reward_velocity       | -2.41     |
| rollout/              |           |
|    ep_len_mean        | 792       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8200      |
|    time_elapsed       | 208       |
|    total_timesteps    | 65600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0392   |
|    learning_rate      | 0.00096   |
|    n_updates          | 8199      |
|    policy_loss        | 0.0196    |
|    std                | 0.135     |
|    value_loss         | 0.583     |
-------------------------------------
Num timesteps: 66000
Best mean reward: -1238.74 - Last mean reward per episode: -1334.00
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0194   |
| reward_motion         | 0.602     |
| reward_torque         | -0.101    |
| reward_velocity       | -2.4      |
| rollout/              |           |
|    ep_len_mean        | 795       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8300      |
|    time_elapsed       | 211       |
|    total_timesteps    | 66400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.000248 |
|    learning_rate      | 0.00096   |
|    n_updates          | 8299      |
|    policy_loss        | 4.05e-06  |
|    std                | 0.135     |
|    value_loss         | 0.00168   |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0197   |
| reward_motion         | 0.597     |
| reward_torque         | -0.1      |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 798       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8400      |
|    time_elapsed       | 213       |
|    total_timesteps    | 67200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -6.87     |
|    learning_rate      | 0.00096   |
|    n_updates          | 8399      |
|    policy_loss        | 0.00451   |
|    std                | 0.135     |
|    value_loss         | 2.44      |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0197   |
| reward_motion         | 0.597     |
| reward_torque         | -0.1      |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 798       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8500      |
|    time_elapsed       | 216       |
|    total_timesteps    | 68000     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.138    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8499      |
|    policy_loss        | -0.0741   |
|    std                | 0.135     |
|    value_loss         | 4.37      |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0195   |
| reward_motion         | 0.59      |
| reward_torque         | -0.0991   |
| reward_velocity       | -2.39     |
| rollout/              |           |
|    ep_len_mean        | 801       |
|    ep_rew_mean        | -1.34e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8600      |
|    time_elapsed       | 218       |
|    total_timesteps    | 68800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.173     |
|    learning_rate      | 0.00096   |
|    n_updates          | 8599      |
|    policy_loss        | -0.0228   |
|    std                | 0.135     |
|    value_loss         | 0.113     |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0194   |
| reward_motion         | 0.583     |
| reward_torque         | -0.0979   |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 803       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8700      |
|    time_elapsed       | 221       |
|    total_timesteps    | 69600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.99     |
|    learning_rate      | 0.00096   |
|    n_updates          | 8699      |
|    policy_loss        | 0.0692    |
|    std                | 0.135     |
|    value_loss         | 0.23      |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0193   |
| reward_motion         | 0.576     |
| reward_torque         | -0.0968   |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8800      |
|    time_elapsed       | 223       |
|    total_timesteps    | 70400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0652    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8799      |
|    policy_loss        | 0.233     |
|    std                | 0.135     |
|    value_loss         | 1.61      |
-------------------------------------
-------------------------------------
| reward                | -1.92     |
| reward_contact        | -0.0193   |
| reward_motion         | 0.576     |
| reward_torque         | -0.0968   |
| reward_velocity       | -2.38     |
| rollout/              |           |
|    ep_len_mean        | 808       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 8900      |
|    time_elapsed       | 226       |
|    total_timesteps    | 71200     |
| train/                |           |
|    entropy_loss       | -19.9     |
|    explained_variance | -0.222    |
|    learning_rate      | 0.00096   |
|    n_updates          | 8899      |
|    policy_loss        | 0.447     |
|    std                | 0.135     |
|    value_loss         | 13        |
-------------------------------------
Num timesteps: 72000
Best mean reward: -1238.74 - Last mean reward per episode: -1304.30
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.0185  |
| reward_motion         | 0.661    |
| reward_torque         | -0.105   |
| reward_velocity       | -2.37    |
| rollout/              |          |
|    ep_len_mean        | 776      |
|    ep_rew_mean        | -1.3e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 9000     |
|    time_elapsed       | 228      |
|    total_timesteps    | 72000    |
| train/                |          |
|    entropy_loss       | -20      |
|    explained_variance | -0.0018  |
|    learning_rate      | 0.00096  |
|    n_updates          | 8999     |
|    policy_loss        | -0.117   |
|    std                | 0.135    |
|    value_loss         | 22.2     |
------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0184   |
| reward_motion         | 0.656     |
| reward_torque         | -0.105    |
| reward_velocity       | -2.37     |
| rollout/              |           |
|    ep_len_mean        | 779       |
|    ep_rew_mean        | -1.31e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9100      |
|    time_elapsed       | 231       |
|    total_timesteps    | 72800     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0.00901   |
|    learning_rate      | 0.00096   |
|    n_updates          | 9099      |
|    policy_loss        | -6.66e-05 |
|    std                | 0.135     |
|    value_loss         | 0.00531   |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0185   |
| reward_motion         | 0.649     |
| reward_torque         | -0.104    |
| reward_velocity       | -2.36     |
| rollout/              |           |
|    ep_len_mean        | 781       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9200      |
|    time_elapsed       | 233       |
|    total_timesteps    | 73600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -3.49e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 9199      |
|    policy_loss        | -0.00167  |
|    std                | 0.135     |
|    value_loss         | 1.76      |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0185   |
| reward_motion         | 0.649     |
| reward_torque         | -0.104    |
| reward_velocity       | -2.36     |
| rollout/              |           |
|    ep_len_mean        | 781       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9300      |
|    time_elapsed       | 236       |
|    total_timesteps    | 74400     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 9299      |
|    policy_loss        | -0        |
|    std                | 0.135     |
|    value_loss         | 0.0334    |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0186   |
| reward_motion         | 0.643     |
| reward_torque         | -0.103    |
| reward_velocity       | -2.35     |
| rollout/              |           |
|    ep_len_mean        | 784       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9400      |
|    time_elapsed       | 239       |
|    total_timesteps    | 75200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0557   |
|    learning_rate      | 0.00096   |
|    n_updates          | 9399      |
|    policy_loss        | 0.0107    |
|    std                | 0.135     |
|    value_loss         | 0.00122   |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.636     |
| reward_torque         | -0.102    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 786       |
|    ep_rew_mean        | -1.32e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9500      |
|    time_elapsed       | 241       |
|    total_timesteps    | 76000     |
| train/                |           |
|    entropy_loss       | -20.1     |
|    explained_variance | -0.0618   |
|    learning_rate      | 0.00096   |
|    n_updates          | 9499      |
|    policy_loss        | -0.0198   |
|    std                | 0.135     |
|    value_loss         | 7.87      |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0188   |
| reward_motion         | 0.63      |
| reward_torque         | -0.101    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 789       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9600      |
|    time_elapsed       | 244       |
|    total_timesteps    | 76800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.265     |
|    learning_rate      | 0.00096   |
|    n_updates          | 9599      |
|    policy_loss        | -0.00822  |
|    std                | 0.135     |
|    value_loss         | 0.256     |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0188   |
| reward_motion         | 0.63      |
| reward_torque         | -0.101    |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 791       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9700      |
|    time_elapsed       | 246       |
|    total_timesteps    | 77600     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0241    |
|    learning_rate      | 0.00096   |
|    n_updates          | 9699      |
|    policy_loss        | 0.000416  |
|    std                | 0.135     |
|    value_loss         | 0.172     |
-------------------------------------
Num timesteps: 78000
Best mean reward: -1238.74 - Last mean reward per episode: -1333.41
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0187   |
| reward_motion         | 0.624     |
| reward_torque         | -0.0998   |
| reward_velocity       | -2.34     |
| rollout/              |           |
|    ep_len_mean        | 791       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9800      |
|    time_elapsed       | 249       |
|    total_timesteps    | 78400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 9799      |
|    policy_loss        | 4.77e-07  |
|    std                | 0.135     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0188   |
| reward_motion         | 0.617     |
| reward_torque         | -0.0988   |
| reward_velocity       | -2.32     |
| rollout/              |           |
|    ep_len_mean        | 794       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 9900      |
|    time_elapsed       | 251       |
|    total_timesteps    | 79200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 9899      |
|    policy_loss        | 7.15e-06  |
|    std                | 0.135     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.019    |
| reward_motion         | 0.611     |
| reward_torque         | -0.0979   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 796       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10000     |
|    time_elapsed       | 254       |
|    total_timesteps    | 80000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.00326   |
|    learning_rate      | 0.00096   |
|    n_updates          | 9999      |
|    policy_loss        | -0        |
|    std                | 0.135     |
|    value_loss         | 0.354     |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.609     |
| reward_torque         | -0.0977   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 796       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10100     |
|    time_elapsed       | 257       |
|    total_timesteps    | 80800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0213   |
|    learning_rate      | 0.00096   |
|    n_updates          | 10099     |
|    policy_loss        | -0.000104 |
|    std                | 0.135     |
|    value_loss         | 9.89e-05  |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0192   |
| reward_motion         | 0.609     |
| reward_torque         | -0.0977   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 796       |
|    ep_rew_mean        | -1.33e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10200     |
|    time_elapsed       | 259       |
|    total_timesteps    | 81600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 10199     |
|    policy_loss        | 8.82e-06  |
|    std                | 0.135     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0195   |
| reward_motion         | 0.59      |
| reward_torque         | -0.0955   |
| reward_velocity       | -2.28     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10300     |
|    time_elapsed       | 262       |
|    total_timesteps    | 82400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 10299     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.135     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0197   |
| reward_motion         | 0.589     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.35e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10400     |
|    time_elapsed       | 264       |
|    total_timesteps    | 83200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -6.23     |
|    learning_rate      | 0.00096   |
|    n_updates          | 10399     |
|    policy_loss        | 0.339     |
|    std                | 0.135     |
|    value_loss         | 1.59      |
-------------------------------------
Num timesteps: 84000
Best mean reward: -1238.74 - Last mean reward per episode: -1355.83
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0196   |
| reward_motion         | 0.588     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10500     |
|    time_elapsed       | 267       |
|    total_timesteps    | 84000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0233    |
|    learning_rate      | 0.00096   |
|    n_updates          | 10499     |
|    policy_loss        | 3.55e-05  |
|    std                | 0.135     |
|    value_loss         | 0.0828    |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0196   |
| reward_motion         | 0.588     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10600     |
|    time_elapsed       | 269       |
|    total_timesteps    | 84800     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.000136  |
|    learning_rate      | 0.00096   |
|    n_updates          | 10599     |
|    policy_loss        | -0.00381  |
|    std                | 0.135     |
|    value_loss         | 2.65      |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0197   |
| reward_motion         | 0.588     |
| reward_torque         | -0.0957   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10700     |
|    time_elapsed       | 272       |
|    total_timesteps    | 85600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -6.85e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 10699     |
|    policy_loss        | -0.0028   |
|    std                | 0.136     |
|    value_loss         | 0.0497    |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.02     |
| reward_motion         | 0.587     |
| reward_torque         | -0.0957   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10800     |
|    time_elapsed       | 274       |
|    total_timesteps    | 86400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0583    |
|    learning_rate      | 0.00096   |
|    n_updates          | 10799     |
|    policy_loss        | 0.0934    |
|    std                | 0.136     |
|    value_loss         | 1.07      |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.587     |
| reward_torque         | -0.0964   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 806       |
|    ep_rew_mean        | -1.36e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 10900     |
|    time_elapsed       | 277       |
|    total_timesteps    | 87200     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 9.64e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 10899     |
|    policy_loss        | 0.000275  |
|    std                | 0.136     |
|    value_loss         | 0.49      |
-------------------------------------
-------------------------------------
| reward                | -1.85     |
| reward_contact        | -0.0202   |
| reward_motion         | 0.574     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11000     |
|    time_elapsed       | 279       |
|    total_timesteps    | 88000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -5.44e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 10999     |
|    policy_loss        | -0.000175 |
|    std                | 0.136     |
|    value_loss         | 1.95      |
-------------------------------------
-------------------------------------
| reward                | -1.85     |
| reward_contact        | -0.0202   |
| reward_motion         | 0.574     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.31     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11100     |
|    time_elapsed       | 282       |
|    total_timesteps    | 88800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -7.4e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11099     |
|    policy_loss        | 6.68e-06  |
|    std                | 0.136     |
|    value_loss         | 1.01      |
-------------------------------------
-------------------------------------
| reward                | -1.85     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.574     |
| reward_torque         | -0.0956   |
| reward_velocity       | -2.3      |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11200     |
|    time_elapsed       | 284       |
|    total_timesteps    | 89600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.244    |
|    learning_rate      | 0.00096   |
|    n_updates          | 11199     |
|    policy_loss        | 6.44e-06  |
|    std                | 0.136     |
|    value_loss         | 2.72e-07  |
-------------------------------------
Num timesteps: 90000
Best mean reward: -1238.74 - Last mean reward per episode: -1373.95
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.573     |
| reward_torque         | -0.0948   |
| reward_velocity       | -2.3      |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11300     |
|    time_elapsed       | 287       |
|    total_timesteps    | 90400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 11299     |
|    policy_loss        | -9.54e-07 |
|    std                | 0.136     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0207   |
| reward_motion         | 0.569     |
| reward_torque         | -0.0946   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11400     |
|    time_elapsed       | 290       |
|    total_timesteps    | 91200     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 7.45e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11399     |
|    policy_loss        | -0        |
|    std                | 0.136     |
|    value_loss         | 3.25e-06  |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0208   |
| reward_motion         | 0.566     |
| reward_torque         | -0.0947   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11500     |
|    time_elapsed       | 292       |
|    total_timesteps    | 92000     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.0728   |
|    learning_rate      | 0.00096   |
|    n_updates          | 11499     |
|    policy_loss        | -2.38e-07 |
|    std                | 0.136     |
|    value_loss         | 0.00181   |
-------------------------------------
-------------------------------------
| reward                | -1.84     |
| reward_contact        | -0.0208   |
| reward_motion         | 0.566     |
| reward_torque         | -0.0947   |
| reward_velocity       | -2.29     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11600     |
|    time_elapsed       | 295       |
|    total_timesteps    | 92800     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 11599     |
|    policy_loss        | -2.15e-06 |
|    std                | 0.136     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0207   |
| reward_motion         | 0.565     |
| reward_torque         | -0.0947   |
| reward_velocity       | -2.28     |
| rollout/              |           |
|    ep_len_mean        | 810       |
|    ep_rew_mean        | -1.37e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11700     |
|    time_elapsed       | 297       |
|    total_timesteps    | 93600     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00447  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11699     |
|    policy_loss        | 0.000948  |
|    std                | 0.136     |
|    value_loss         | 0.194     |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.547     |
| reward_torque         | -0.0918   |
| reward_velocity       | -2.27     |
| rollout/              |           |
|    ep_len_mean        | 816       |
|    ep_rew_mean        | -1.39e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 11800     |
|    time_elapsed       | 300       |
|    total_timesteps    | 94400     |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00187  |
|    learning_rate      | 0.00096   |
|    n_updates          | 11799     |
|    policy_loss        | 0.00084   |
|    std                | 0.135     |
|    value_loss         | 0.19      |
-------------------------------------
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.021   |
| reward_motion         | 0.535    |
| reward_torque         | -0.0914  |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 822      |
|    ep_rew_mean        | -1.4e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 11900    |
|    time_elapsed       | 302      |
|    total_timesteps    | 95200    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | 0.0295   |
|    learning_rate      | 0.00096  |
|    n_updates          | 11899    |
|    policy_loss        | 0.00368  |
|    std                | 0.135    |
|    value_loss         | 3.29     |
------------------------------------
Num timesteps: 96000
Best mean reward: -1238.74 - Last mean reward per episode: -1397.97
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.021   |
| reward_motion         | 0.535    |
| reward_torque         | -0.0914  |
| reward_velocity       | -2.26    |
| rollout/              |          |
|    ep_len_mean        | 822      |
|    ep_rew_mean        | -1.4e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 12000    |
|    time_elapsed       | 305      |
|    total_timesteps    | 96000    |
| train/                |          |
|    entropy_loss       | -19.9    |
|    explained_variance | -77.6    |
|    learning_rate      | 0.00096  |
|    n_updates          | 11999    |
|    policy_loss        | 0.963    |
|    std                | 0.135    |
|    value_loss         | 175      |
------------------------------------
------------------------------------
| reward                | -1.82    |
| reward_contact        | -0.021   |
| reward_motion         | 0.533    |
| reward_torque         | -0.0908  |
| reward_velocity       | -2.24    |
| rollout/              |          |
|    ep_len_mean        | 822      |
|    ep_rew_mean        | -1.4e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 12100    |
|    time_elapsed       | 307      |
|    total_timesteps    | 96800    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.18    |
|    learning_rate      | 0.00096  |
|    n_updates          | 12099    |
|    policy_loss        | -0.00588 |
|    std                | 0.136    |
|    value_loss         | 0.54     |
------------------------------------
------------------------------------
| reward                | -1.83    |
| reward_contact        | -0.021   |
| reward_motion         | 0.533    |
| reward_torque         | -0.0908  |
| reward_velocity       | -2.25    |
| rollout/              |          |
|    ep_len_mean        | 822      |
|    ep_rew_mean        | -1.4e+03 |
| time/                 |          |
|    fps                | 314      |
|    iterations         | 12200    |
|    time_elapsed       | 310      |
|    total_timesteps    | 97600    |
| train/                |          |
|    entropy_loss       | -20.3    |
|    explained_variance | -1.03    |
|    learning_rate      | 0.00096  |
|    n_updates          | 12199    |
|    policy_loss        | -0.136   |
|    std                | 0.136    |
|    value_loss         | 0.193    |
------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.523     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.23     |
| rollout/              |           |
|    ep_len_mean        | 832       |
|    ep_rew_mean        | -1.41e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 12300     |
|    time_elapsed       | 313       |
|    total_timesteps    | 98400     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0158   |
|    learning_rate      | 0.00096   |
|    n_updates          | 12299     |
|    policy_loss        | -0.177    |
|    std                | 0.136     |
|    value_loss         | 1.35      |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.506     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 841       |
|    ep_rew_mean        | -1.43e+03 |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 12400     |
|    time_elapsed       | 315       |
|    total_timesteps    | 99200     |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0146    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12399     |
|    policy_loss        | 0.0552    |
|    std                | 0.136     |
|    value_loss         | 0.0512    |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.506     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.22     |
| rollout/              |           |
|    ep_len_mean        | 841       |
|    ep_rew_mean        | -1.43e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 12500     |
|    time_elapsed       | 318       |
|    total_timesteps    | 100000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.148    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12499     |
|    policy_loss        | 5.1e-05   |
|    std                | 0.136     |
|    value_loss         | 7.84e-08  |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0908   |
| reward_velocity       | -2.2      |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 12600     |
|    time_elapsed       | 321       |
|    total_timesteps    | 100800    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 12599     |
|    policy_loss        | -0        |
|    std                | 0.136     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0909   |
| reward_velocity       | -2.2      |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 12700     |
|    time_elapsed       | 323       |
|    total_timesteps    | 101600    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | 0.0383    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12699     |
|    policy_loss        | -0.00117  |
|    std                | 0.137     |
|    value_loss         | 0.264     |
-------------------------------------
Num timesteps: 102000
Best mean reward: -1238.74 - Last mean reward per episode: -1442.25
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0909   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 12800     |
|    time_elapsed       | 326       |
|    total_timesteps    | 102400    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.382     |
|    learning_rate      | 0.00096   |
|    n_updates          | 12799     |
|    policy_loss        | -0.0379   |
|    std                | 0.137     |
|    value_loss         | 1.31      |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0909   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 12900     |
|    time_elapsed       | 328       |
|    total_timesteps    | 103200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0135    |
|    learning_rate      | 0.00096   |
|    n_updates          | 12899     |
|    policy_loss        | 0.0829    |
|    std                | 0.136     |
|    value_loss         | 0.556     |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.491     |
| reward_torque         | -0.0909   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13000     |
|    time_elapsed       | 331       |
|    total_timesteps    | 104000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000205 |
|    learning_rate      | 0.00096   |
|    n_updates          | 12999     |
|    policy_loss        | 0.00272   |
|    std                | 0.137     |
|    value_loss         | 0.0589    |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0913   |
| reward_velocity       | -2.19     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13100     |
|    time_elapsed       | 333       |
|    total_timesteps    | 104800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00264  |
|    learning_rate      | 0.00096   |
|    n_updates          | 13099     |
|    policy_loss        | 0.00103   |
|    std                | 0.137     |
|    value_loss         | 5.17      |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.493     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.2      |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13200     |
|    time_elapsed       | 336       |
|    total_timesteps    | 105600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 13199     |
|    policy_loss        | 1.91e-06  |
|    std                | 0.137     |
|    value_loss         | 3.72      |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0221   |
| reward_motion         | 0.492     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.19     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13300     |
|    time_elapsed       | 338       |
|    total_timesteps    | 106400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00999   |
|    learning_rate      | 0.00096   |
|    n_updates          | 13299     |
|    policy_loss        | 4.33e-05  |
|    std                | 0.136     |
|    value_loss         | 2.28      |
-------------------------------------
-------------------------------------
| reward                | -1.81     |
| reward_contact        | -0.0221   |
| reward_motion         | 0.492     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.19     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13400     |
|    time_elapsed       | 341       |
|    total_timesteps    | 107200    |
| train/                |           |
|    entropy_loss       | -20.2     |
|    explained_variance | -0.0564   |
|    learning_rate      | 0.00096   |
|    n_updates          | 13399     |
|    policy_loss        | 0.000422  |
|    std                | 0.137     |
|    value_loss         | 7.18e-06  |
-------------------------------------
Num timesteps: 108000
Best mean reward: -1238.74 - Last mean reward per episode: -1440.70
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0221   |
| reward_motion         | 0.498     |
| reward_torque         | -0.0912   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13500     |
|    time_elapsed       | 344       |
|    total_timesteps    | 108000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0333    |
|    learning_rate      | 0.00096   |
|    n_updates          | 13499     |
|    policy_loss        | 0.008     |
|    std                | 0.137     |
|    value_loss         | 0.369     |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.497     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13600     |
|    time_elapsed       | 346       |
|    total_timesteps    | 108800    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.00266  |
|    learning_rate      | 0.00096   |
|    n_updates          | 13599     |
|    policy_loss        | -0.000932 |
|    std                | 0.137     |
|    value_loss         | 0.311     |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.497     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.44e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13700     |
|    time_elapsed       | 349       |
|    total_timesteps    | 109600    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -1.84     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13699     |
|    policy_loss        | -0.312    |
|    std                | 0.137     |
|    value_loss         | 76        |
-------------------------------------
-------------------------------------
| reward                | -1.83     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.497     |
| reward_torque         | -0.0911   |
| reward_velocity       | -2.21     |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13800     |
|    time_elapsed       | 351       |
|    total_timesteps    | 110400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.011     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13799     |
|    policy_loss        | -0.0152   |
|    std                | 0.137     |
|    value_loss         | 4.42      |
-------------------------------------
-------------------------------------
| reward                | -1.82     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.497     |
| reward_torque         | -0.0912   |
| reward_velocity       | -2.2      |
| rollout/              |           |
|    ep_len_mean        | 851       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 13900     |
|    time_elapsed       | 354       |
|    total_timesteps    | 111200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.372     |
|    learning_rate      | 0.00096   |
|    n_updates          | 13899     |
|    policy_loss        | -0.00819  |
|    std                | 0.137     |
|    value_loss         | 0.687     |
-------------------------------------
-------------------------------------
| reward                | -1.8      |
| reward_contact        | -0.0216   |
| reward_motion         | 0.485     |
| reward_torque         | -0.086    |
| reward_velocity       | -2.18     |
| rollout/              |           |
|    ep_len_mean        | 856       |
|    ep_rew_mean        | -1.45e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14000     |
|    time_elapsed       | 357       |
|    total_timesteps    | 112000    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | 0.0725    |
|    learning_rate      | 0.00096   |
|    n_updates          | 13999     |
|    policy_loss        | -0.0299   |
|    std                | 0.136     |
|    value_loss         | 0.104     |
-------------------------------------
-------------------------------------
| reward                | -1.8      |
| reward_contact        | -0.0217   |
| reward_motion         | 0.467     |
| reward_torque         | -0.0856   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 862       |
|    ep_rew_mean        | -1.46e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14100     |
|    time_elapsed       | 359       |
|    total_timesteps    | 112800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.026     |
|    learning_rate      | 0.00096   |
|    n_updates          | 14099     |
|    policy_loss        | -0.000327 |
|    std                | 0.137     |
|    value_loss         | 1.07      |
-------------------------------------
-------------------------------------
| reward                | -1.8      |
| reward_contact        | -0.0216   |
| reward_motion         | 0.467     |
| reward_torque         | -0.0858   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 862       |
|    ep_rew_mean        | -1.46e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14200     |
|    time_elapsed       | 362       |
|    total_timesteps    | 113600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0123   |
|    learning_rate      | 0.00096   |
|    n_updates          | 14199     |
|    policy_loss        | 0.000814  |
|    std                | 0.137     |
|    value_loss         | 1.81      |
-------------------------------------
Num timesteps: 114000
Best mean reward: -1238.74 - Last mean reward per episode: -1460.33
-------------------------------------
| reward                | -1.8      |
| reward_contact        | -0.0216   |
| reward_motion         | 0.467     |
| reward_torque         | -0.0858   |
| reward_velocity       | -2.16     |
| rollout/              |           |
|    ep_len_mean        | 862       |
|    ep_rew_mean        | -1.46e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14300     |
|    time_elapsed       | 364       |
|    total_timesteps    | 114400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0194    |
|    learning_rate      | 0.00096   |
|    n_updates          | 14299     |
|    policy_loss        | -0.000298 |
|    std                | 0.137     |
|    value_loss         | 1.21      |
-------------------------------------
-------------------------------------
| reward                | -1.75     |
| reward_contact        | -0.0217   |
| reward_motion         | 0.446     |
| reward_torque         | -0.0819   |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.48e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14400     |
|    time_elapsed       | 367       |
|    total_timesteps    | 115200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0963    |
|    learning_rate      | 0.00096   |
|    n_updates          | 14399     |
|    policy_loss        | 3.92e-05  |
|    std                | 0.137     |
|    value_loss         | 1.35      |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.443     |
| reward_torque         | -0.0811   |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14500     |
|    time_elapsed       | 369       |
|    total_timesteps    | 116000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0985   |
|    learning_rate      | 0.00096   |
|    n_updates          | 14499     |
|    policy_loss        | 0.0628    |
|    std                | 0.137     |
|    value_loss         | 3.33      |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.443     |
| reward_torque         | -0.0811   |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 872       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14600     |
|    time_elapsed       | 372       |
|    total_timesteps    | 116800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0172    |
|    learning_rate      | 0.00096   |
|    n_updates          | 14599     |
|    policy_loss        | -0.000516 |
|    std                | 0.137     |
|    value_loss         | 0.425     |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.021    |
| reward_motion         | 0.441     |
| reward_torque         | -0.0782   |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 871       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14700     |
|    time_elapsed       | 375       |
|    total_timesteps    | 117600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0553   |
|    learning_rate      | 0.00096   |
|    n_updates          | 14699     |
|    policy_loss        | 0.000293  |
|    std                | 0.137     |
|    value_loss         | 10        |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.021    |
| reward_motion         | 0.441     |
| reward_torque         | -0.0782   |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 871       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14800     |
|    time_elapsed       | 377       |
|    total_timesteps    | 118400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00149  |
|    learning_rate      | 0.00096   |
|    n_updates          | 14799     |
|    policy_loss        | -8.89e-05 |
|    std                | 0.137     |
|    value_loss         | 2.63      |
-------------------------------------
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.0208   |
| reward_motion         | 0.441     |
| reward_torque         | -0.0782   |
| reward_velocity       | -2.09     |
| rollout/              |           |
|    ep_len_mean        | 871       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 14900     |
|    time_elapsed       | 380       |
|    total_timesteps    | 119200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.418     |
|    learning_rate      | 0.00096   |
|    n_updates          | 14899     |
|    policy_loss        | -0.0296   |
|    std                | 0.137     |
|    value_loss         | 0.956     |
-------------------------------------
Num timesteps: 120000
Best mean reward: -1238.74 - Last mean reward per episode: -1467.85
-------------------------------------
| reward                | -1.74     |
| reward_contact        | -0.0208   |
| reward_motion         | 0.438     |
| reward_torque         | -0.0772   |
| reward_velocity       | -2.08     |
| rollout/              |           |
|    ep_len_mean        | 871       |
|    ep_rew_mean        | -1.47e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15000     |
|    time_elapsed       | 382       |
|    total_timesteps    | 120000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00836  |
|    learning_rate      | 0.00096   |
|    n_updates          | 14999     |
|    policy_loss        | 0.00771   |
|    std                | 0.137     |
|    value_loss         | 0.751     |
-------------------------------------
-------------------------------------
| reward                | -1.7      |
| reward_contact        | -0.0207   |
| reward_motion         | 0.416     |
| reward_torque         | -0.0721   |
| reward_velocity       | -2.02     |
| rollout/              |           |
|    ep_len_mean        | 880       |
|    ep_rew_mean        | -1.48e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15100     |
|    time_elapsed       | 385       |
|    total_timesteps    | 120800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.469     |
|    learning_rate      | 0.00096   |
|    n_updates          | 15099     |
|    policy_loss        | -0.000516 |
|    std                | 0.137     |
|    value_loss         | 1.99      |
-------------------------------------
------------------------------------
| reward                | -1.7     |
| reward_contact        | -0.0207  |
| reward_motion         | 0.416    |
| reward_torque         | -0.0721  |
| reward_velocity       | -2.02    |
| rollout/              |          |
|    ep_len_mean        | 890      |
|    ep_rew_mean        | -1.5e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 15200    |
|    time_elapsed       | 387      |
|    total_timesteps    | 121600   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -1.66    |
|    learning_rate      | 0.00096  |
|    n_updates          | 15199    |
|    policy_loss        | 0.000168 |
|    std                | 0.137    |
|    value_loss         | 2.74     |
------------------------------------
------------------------------------
| reward                | -1.66    |
| reward_contact        | -0.0211  |
| reward_motion         | 0.407    |
| reward_torque         | -0.0683  |
| reward_velocity       | -1.97    |
| rollout/              |          |
|    ep_len_mean        | 890      |
|    ep_rew_mean        | -1.5e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 15300    |
|    time_elapsed       | 390      |
|    total_timesteps    | 122400   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.343   |
|    learning_rate      | 0.00096  |
|    n_updates          | 15299    |
|    policy_loss        | -0.0518  |
|    std                | 0.137    |
|    value_loss         | 1.94     |
------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.389     |
| reward_torque         | -0.063    |
| reward_velocity       | -1.93     |
| rollout/              |           |
|    ep_len_mean        | 900       |
|    ep_rew_mean        | -1.52e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15400     |
|    time_elapsed       | 393       |
|    total_timesteps    | 123200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.809     |
|    learning_rate      | 0.00096   |
|    n_updates          | 15399     |
|    policy_loss        | 0.00571   |
|    std                | 0.137     |
|    value_loss         | 0.211     |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0213  |
| reward_motion         | 0.377    |
| reward_torque         | -0.0599  |
| reward_velocity       | -1.89    |
| rollout/              |          |
|    ep_len_mean        | 900      |
|    ep_rew_mean        | -1.5e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 15500    |
|    time_elapsed       | 395      |
|    total_timesteps    | 124000   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -1.58    |
|    learning_rate      | 0.00096  |
|    n_updates          | 15499    |
|    policy_loss        | 0.000744 |
|    std                | 0.137    |
|    value_loss         | 27.8     |
------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.349     |
| reward_torque         | -0.0569   |
| reward_velocity       | -1.85     |
| rollout/              |           |
|    ep_len_mean        | 910       |
|    ep_rew_mean        | -1.52e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15600     |
|    time_elapsed       | 398       |
|    total_timesteps    | 124800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0266    |
|    learning_rate      | 0.00096   |
|    n_updates          | 15599     |
|    policy_loss        | 0.00068   |
|    std                | 0.137     |
|    value_loss         | 4.37      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.349     |
| reward_torque         | -0.0569   |
| reward_velocity       | -1.85     |
| rollout/              |           |
|    ep_len_mean        | 910       |
|    ep_rew_mean        | -1.52e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15700     |
|    time_elapsed       | 400       |
|    total_timesteps    | 125600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.15     |
|    learning_rate      | 0.00096   |
|    n_updates          | 15699     |
|    policy_loss        | 0.0178    |
|    std                | 0.137     |
|    value_loss         | 1.18      |
-------------------------------------
Num timesteps: 126000
Best mean reward: -1238.74 - Last mean reward per episode: -1528.42
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.333     |
| reward_torque         | -0.0557   |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 920       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15800     |
|    time_elapsed       | 403       |
|    total_timesteps    | 126400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.007    |
|    learning_rate      | 0.00096   |
|    n_updates          | 15799     |
|    policy_loss        | 0.00906   |
|    std                | 0.137     |
|    value_loss         | 5.77      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0217   |
| reward_motion         | 0.317     |
| reward_torque         | -0.047    |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 924       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 15900     |
|    time_elapsed       | 405       |
|    total_timesteps    | 127200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0668   |
|    learning_rate      | 0.00096   |
|    n_updates          | 15899     |
|    policy_loss        | 0.00365   |
|    std                | 0.137     |
|    value_loss         | 5         |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.317     |
| reward_torque         | -0.047    |
| reward_velocity       | -1.82     |
| rollout/              |           |
|    ep_len_mean        | 924       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16000     |
|    time_elapsed       | 408       |
|    total_timesteps    | 128000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00227  |
|    learning_rate      | 0.00096   |
|    n_updates          | 15999     |
|    policy_loss        | 1.12e-05  |
|    std                | 0.137     |
|    value_loss         | 0.0302    |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.317     |
| reward_torque         | -0.047    |
| reward_velocity       | -1.82     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.54e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16100     |
|    time_elapsed       | 410       |
|    total_timesteps    | 128800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.335    |
|    learning_rate      | 0.00096   |
|    n_updates          | 16099     |
|    policy_loss        | 0.000803  |
|    std                | 0.137     |
|    value_loss         | 1.73      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.299     |
| reward_torque         | -0.0471   |
| reward_velocity       | -1.82     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.54e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16200     |
|    time_elapsed       | 413       |
|    total_timesteps    | 129600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16199     |
|    policy_loss        | -3.34e-06 |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0218   |
| reward_motion         | 0.299     |
| reward_torque         | -0.0471   |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16300     |
|    time_elapsed       | 416       |
|    total_timesteps    | 130400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 1.19e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 16299     |
|    policy_loss        | -1.91e-06 |
|    std                | 0.137     |
|    value_loss         | 0.0314    |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.301     |
| reward_torque         | -0.0492   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16400     |
|    time_elapsed       | 418       |
|    total_timesteps    | 131200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 1.19e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 16399     |
|    policy_loss        | -2.03e-06 |
|    std                | 0.137     |
|    value_loss         | 0.438     |
-------------------------------------
Num timesteps: 132000
Best mean reward: -1238.74 - Last mean reward per episode: -1528.56
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.306     |
| reward_torque         | -0.0497   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16500     |
|    time_elapsed       | 421       |
|    total_timesteps    | 132000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.357     |
|    learning_rate      | 0.00096   |
|    n_updates          | 16499     |
|    policy_loss        | -0.0016   |
|    std                | 0.137     |
|    value_loss         | 2.89      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.306     |
| reward_torque         | -0.0497   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16600     |
|    time_elapsed       | 423       |
|    total_timesteps    | 132800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16599     |
|    policy_loss        | 1.19e-06  |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.306     |
| reward_torque         | -0.0497   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.52e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16700     |
|    time_elapsed       | 426       |
|    total_timesteps    | 133600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 16699     |
|    policy_loss        | 4.77e-07  |
|    std                | 0.137     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0219   |
| reward_motion         | 0.306     |
| reward_torque         | -0.0496   |
| reward_velocity       | -1.85     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16800     |
|    time_elapsed       | 428       |
|    total_timesteps    | 134400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.56      |
|    learning_rate      | 0.00096   |
|    n_updates          | 16799     |
|    policy_loss        | 7.72e-05  |
|    std                | 0.137     |
|    value_loss         | 0.034     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.022    |
| reward_motion         | 0.306     |
| reward_torque         | -0.0496   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 16900     |
|    time_elapsed       | 431       |
|    total_timesteps    | 135200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -2.33     |
|    learning_rate      | 0.00096   |
|    n_updates          | 16899     |
|    policy_loss        | -0.0105   |
|    std                | 0.137     |
|    value_loss         | 9.19      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.022    |
| reward_motion         | 0.306     |
| reward_torque         | -0.0496   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17000     |
|    time_elapsed       | 434       |
|    total_timesteps    | 136000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.000611  |
|    learning_rate      | 0.00096   |
|    n_updates          | 16999     |
|    policy_loss        | -1.19e-07 |
|    std                | 0.137     |
|    value_loss         | 0.591     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.301     |
| reward_torque         | -0.0495   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17100     |
|    time_elapsed       | 436       |
|    total_timesteps    | 136800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.33      |
|    learning_rate      | 0.00096   |
|    n_updates          | 17099     |
|    policy_loss        | 6.25e-05  |
|    std                | 0.137     |
|    value_loss         | 2.25      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.301     |
| reward_torque         | -0.0497   |
| reward_velocity       | -1.85     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17200     |
|    time_elapsed       | 439       |
|    total_timesteps    | 137600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.522     |
|    learning_rate      | 0.00096   |
|    n_updates          | 17199     |
|    policy_loss        | 0.00954   |
|    std                | 0.137     |
|    value_loss         | 1.6       |
-------------------------------------
Num timesteps: 138000
Best mean reward: -1238.74 - Last mean reward per episode: -1528.56
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.302     |
| reward_torque         | -0.0517   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17300     |
|    time_elapsed       | 441       |
|    total_timesteps    | 138400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -1.73     |
|    learning_rate      | 0.00096   |
|    n_updates          | 17299     |
|    policy_loss        | -0.00574  |
|    std                | 0.137     |
|    value_loss         | 1.01      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.302     |
| reward_torque         | -0.0537   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17400     |
|    time_elapsed       | 444       |
|    total_timesteps    | 139200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.399     |
|    learning_rate      | 0.00096   |
|    n_updates          | 17399     |
|    policy_loss        | -0.089    |
|    std                | 0.137     |
|    value_loss         | 0.878     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.302     |
| reward_torque         | -0.0537   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17500     |
|    time_elapsed       | 446       |
|    total_timesteps    | 140000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.328    |
|    learning_rate      | 0.00096   |
|    n_updates          | 17499     |
|    policy_loss        | 0.000574  |
|    std                | 0.137     |
|    value_loss         | 4.37      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.304     |
| reward_torque         | -0.0537   |
| reward_velocity       | -1.85     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17600     |
|    time_elapsed       | 449       |
|    total_timesteps    | 140800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0174    |
|    learning_rate      | 0.00096   |
|    n_updates          | 17599     |
|    policy_loss        | -0.0295   |
|    std                | 0.138     |
|    value_loss         | 1.08      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.305     |
| reward_torque         | -0.0536   |
| reward_velocity       | -1.84     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17700     |
|    time_elapsed       | 451       |
|    total_timesteps    | 141600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0156    |
|    learning_rate      | 0.00096   |
|    n_updates          | 17699     |
|    policy_loss        | -2.24e-05 |
|    std                | 0.137     |
|    value_loss         | 0.571     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.021    |
| reward_motion         | 0.307     |
| reward_torque         | -0.0561   |
| reward_velocity       | -1.83     |
| rollout/              |           |
|    ep_len_mean        | 933       |
|    ep_rew_mean        | -1.52e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17800     |
|    time_elapsed       | 454       |
|    total_timesteps    | 142400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00376   |
|    learning_rate      | 0.00096   |
|    n_updates          | 17799     |
|    policy_loss        | 0.00731   |
|    std                | 0.138     |
|    value_loss         | 2.34      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.296     |
| reward_torque         | -0.0508   |
| reward_velocity       | -1.81     |
| rollout/              |           |
|    ep_len_mean        | 938       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 17900     |
|    time_elapsed       | 456       |
|    total_timesteps    | 143200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.425     |
|    learning_rate      | 0.00096   |
|    n_updates          | 17899     |
|    policy_loss        | 0.000512  |
|    std                | 0.138     |
|    value_loss         | 0.6       |
-------------------------------------
Num timesteps: 144000
Best mean reward: -1238.74 - Last mean reward per episode: -1534.02
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.296     |
| reward_torque         | -0.0508   |
| reward_velocity       | -1.81     |
| rollout/              |           |
|    ep_len_mean        | 938       |
|    ep_rew_mean        | -1.53e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18000     |
|    time_elapsed       | 459       |
|    total_timesteps    | 144000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0347    |
|    learning_rate      | 0.00096   |
|    n_updates          | 17999     |
|    policy_loss        | 0.0273    |
|    std                | 0.138     |
|    value_loss         | 0.545     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.27      |
| reward_torque         | -0.0466   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 946       |
|    ep_rew_mean        | -1.55e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18100     |
|    time_elapsed       | 462       |
|    total_timesteps    | 144800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.114     |
|    learning_rate      | 0.00096   |
|    n_updates          | 18099     |
|    policy_loss        | -1.1e-05  |
|    std                | 0.138     |
|    value_loss         | 0.547     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.252     |
| reward_torque         | -0.0448   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 956       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18200     |
|    time_elapsed       | 464       |
|    total_timesteps    | 145600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00789  |
|    learning_rate      | 0.00096   |
|    n_updates          | 18199     |
|    policy_loss        | 4.34e-05  |
|    std                | 0.138     |
|    value_loss         | 4.63      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.252     |
| reward_torque         | -0.0448   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 956       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18300     |
|    time_elapsed       | 467       |
|    total_timesteps    | 146400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0837    |
|    learning_rate      | 0.00096   |
|    n_updates          | 18299     |
|    policy_loss        | -3.81e-06 |
|    std                | 0.138     |
|    value_loss         | 19.2      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.252     |
| reward_torque         | -0.0448   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 963       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18400     |
|    time_elapsed       | 469       |
|    total_timesteps    | 147200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00266  |
|    learning_rate      | 0.00096   |
|    n_updates          | 18399     |
|    policy_loss        | -3.53e-05 |
|    std                | 0.138     |
|    value_loss         | 0.265     |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.238     |
| reward_torque         | -0.0444   |
| reward_velocity       | -1.72     |
| rollout/              |           |
|    ep_len_mean        | 963       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18500     |
|    time_elapsed       | 472       |
|    total_timesteps    | 148000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0498   |
|    learning_rate      | 0.00096   |
|    n_updates          | 18499     |
|    policy_loss        | -2.81e-05 |
|    std                | 0.138     |
|    value_loss         | 0.947     |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.238     |
| reward_torque         | -0.0445   |
| reward_velocity       | -1.72     |
| rollout/              |           |
|    ep_len_mean        | 963       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18600     |
|    time_elapsed       | 474       |
|    total_timesteps    | 148800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.000323 |
|    learning_rate      | 0.00096   |
|    n_updates          | 18599     |
|    policy_loss        | 1.26e-05  |
|    std                | 0.138     |
|    value_loss         | 0.114     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.201     |
| reward_torque         | -0.0413   |
| reward_velocity       | -1.72     |
| rollout/              |           |
|    ep_len_mean        | 970       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18700     |
|    time_elapsed       | 477       |
|    total_timesteps    | 149600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00903   |
|    learning_rate      | 0.00096   |
|    n_updates          | 18699     |
|    policy_loss        | 4.32e-05  |
|    std                | 0.138     |
|    value_loss         | 0.876     |
-------------------------------------
Num timesteps: 150000
Best mean reward: -1238.74 - Last mean reward per episode: -1580.60
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.201     |
| reward_torque         | -0.0414   |
| reward_velocity       | -1.73     |
| rollout/              |           |
|    ep_len_mean        | 970       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18800     |
|    time_elapsed       | 480       |
|    total_timesteps    | 150400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -7.27     |
|    learning_rate      | 0.00096   |
|    n_updates          | 18799     |
|    policy_loss        | -0.0004   |
|    std                | 0.138     |
|    value_loss         | 2.59      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.201     |
| reward_torque         | -0.0414   |
| reward_velocity       | -1.73     |
| rollout/              |           |
|    ep_len_mean        | 970       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 18900     |
|    time_elapsed       | 482       |
|    total_timesteps    | 151200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0782   |
|    learning_rate      | 0.00096   |
|    n_updates          | 18899     |
|    policy_loss        | 1.24e-05  |
|    std                | 0.138     |
|    value_loss         | 2.33      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.179     |
| reward_torque         | -0.0343   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19000     |
|    time_elapsed       | 485       |
|    total_timesteps    | 152000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.0247   |
|    learning_rate      | 0.00096   |
|    n_updates          | 18999     |
|    policy_loss        | -1.76e-05 |
|    std                | 0.138     |
|    value_loss         | 0.223     |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.176     |
| reward_torque         | -0.0342   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19100     |
|    time_elapsed       | 487       |
|    total_timesteps    | 152800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.349     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19099     |
|    policy_loss        | 0.000121  |
|    std                | 0.138     |
|    value_loss         | 1.15      |
-------------------------------------
------------------------------------
| reward                | -1.57    |
| reward_contact        | -0.0213  |
| reward_motion         | 0.183    |
| reward_torque         | -0.0363  |
| reward_velocity       | -1.7     |
| rollout/              |          |
|    ep_len_mean        | 976      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 19200    |
|    time_elapsed       | 490      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.029   |
|    learning_rate      | 0.00096  |
|    n_updates          | 19199    |
|    policy_loss        | -0.00412 |
|    std                | 0.138    |
|    value_loss         | 0.778    |
------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0213   |
| reward_motion         | 0.183     |
| reward_torque         | -0.0363   |
| reward_velocity       | -1.7      |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19300     |
|    time_elapsed       | 493       |
|    total_timesteps    | 154400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.176    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19299     |
|    policy_loss        | 0.0019    |
|    std                | 0.138     |
|    value_loss         | 2.4       |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.183     |
| reward_torque         | -0.0364   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19400     |
|    time_elapsed       | 495       |
|    total_timesteps    | 155200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -3.55     |
|    learning_rate      | 0.00096   |
|    n_updates          | 19399     |
|    policy_loss        | 5.02e-05  |
|    std                | 0.138     |
|    value_loss         | 0.00168   |
-------------------------------------
Num timesteps: 156000
Best mean reward: -1238.74 - Last mean reward per episode: -1589.44
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.021    |
| reward_motion         | 0.183     |
| reward_torque         | -0.0364   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19500     |
|    time_elapsed       | 498       |
|    total_timesteps    | 156000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 19499     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.138     |
|    value_loss         | 0.63      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.183     |
| reward_torque         | -0.0365   |
| reward_velocity       | -1.7      |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19600     |
|    time_elapsed       | 500       |
|    total_timesteps    | 156800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 19599     |
|    policy_loss        | -0        |
|    std                | 0.138     |
|    value_loss         | 0.267     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.186     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 19700     |
|    time_elapsed       | 503       |
|    total_timesteps    | 157600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0105    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19699     |
|    policy_loss        | 0.000158  |
|    std                | 0.138     |
|    value_loss         | 0.253     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.186     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 19800     |
|    time_elapsed       | 506       |
|    total_timesteps    | 158400    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.000816  |
|    learning_rate      | 0.00096   |
|    n_updates          | 19799     |
|    policy_loss        | -0.266    |
|    std                | 0.138     |
|    value_loss         | 4.36      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0206   |
| reward_motion         | 0.187     |
| reward_torque         | -0.0369   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 19900     |
|    time_elapsed       | 508       |
|    total_timesteps    | 159200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.194    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19899     |
|    policy_loss        | -0.0424   |
|    std                | 0.138     |
|    value_loss         | 0.0961    |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.187     |
| reward_torque         | -0.0369   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 20000     |
|    time_elapsed       | 511       |
|    total_timesteps    | 160000    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0308    |
|    learning_rate      | 0.00096   |
|    n_updates          | 19999     |
|    policy_loss        | -0.00801  |
|    std                | 0.138     |
|    value_loss         | 0.0696    |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.189     |
| reward_torque         | -0.0373   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 20100     |
|    time_elapsed       | 513       |
|    total_timesteps    | 160800    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -0.00409  |
|    learning_rate      | 0.00096   |
|    n_updates          | 20099     |
|    policy_loss        | 0.00288   |
|    std                | 0.138     |
|    value_loss         | 0.754     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.189     |
| reward_torque         | -0.0373   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.6e+03  |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 20200     |
|    time_elapsed       | 516       |
|    total_timesteps    | 161600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.0244    |
|    learning_rate      | 0.00096   |
|    n_updates          | 20199     |
|    policy_loss        | -0.000763 |
|    std                | 0.138     |
|    value_loss         | 0.693     |
-------------------------------------
Num timesteps: 162000
Best mean reward: -1238.74 - Last mean reward per episode: -1600.20
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0197  |
| reward_motion         | 0.19     |
| reward_torque         | -0.0373  |
| reward_velocity       | -1.72    |
| rollout/              |          |
|    ep_len_mean        | 976      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 20300    |
|    time_elapsed       | 518      |
|    total_timesteps    | 162400   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.0264  |
|    learning_rate      | 0.00096  |
|    n_updates          | 20299    |
|    policy_loss        | 2.12e-05 |
|    std                | 0.138    |
|    value_loss         | 0.0046   |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0196  |
| reward_motion         | 0.19     |
| reward_torque         | -0.0382  |
| reward_velocity       | -1.71    |
| rollout/              |          |
|    ep_len_mean        | 976      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 20400    |
|    time_elapsed       | 521      |
|    total_timesteps    | 163200   |
| train/                |          |
|    entropy_loss       | -20.5    |
|    explained_variance | 0.0215   |
|    learning_rate      | 0.00096  |
|    n_updates          | 20399    |
|    policy_loss        | -0.0124  |
|    std                | 0.138    |
|    value_loss         | 0.0541   |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0198  |
| reward_motion         | 0.19     |
| reward_torque         | -0.0369  |
| reward_velocity       | -1.71    |
| rollout/              |          |
|    ep_len_mean        | 976      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 312      |
|    iterations         | 20500    |
|    time_elapsed       | 523      |
|    total_timesteps    | 164000   |
| train/                |          |
|    entropy_loss       | -20.5    |
|    explained_variance | 0.00104  |
|    learning_rate      | 0.00096  |
|    n_updates          | 20499    |
|    policy_loss        | 2.17e-05 |
|    std                | 0.138    |
|    value_loss         | 0.458    |
------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0196   |
| reward_motion         | 0.188     |
| reward_torque         | -0.0365   |
| reward_velocity       | -1.71     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.6e+03  |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 20600     |
|    time_elapsed       | 526       |
|    total_timesteps    | 164800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.00096   |
|    n_updates          | 20599     |
|    policy_loss        | 1.07e-06  |
|    std                | 0.139     |
|    value_loss         | 0.535     |
-------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0196  |
| reward_motion         | 0.188    |
| reward_torque         | -0.0365  |
| reward_velocity       | -1.71    |
| rollout/              |          |
|    ep_len_mean        | 976      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 20700    |
|    time_elapsed       | 529      |
|    total_timesteps    | 165600   |
| train/                |          |
|    entropy_loss       | -20.5    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 20699    |
|    policy_loss        | 1.19e-07 |
|    std                | 0.139    |
|    value_loss         | 0        |
------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.188     |
| reward_torque         | -0.0365   |
| reward_velocity       | -1.7      |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 20800     |
|    time_elapsed       | 531       |
|    total_timesteps    | 166400    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 20799     |
|    policy_loss        | -3.1e-06  |
|    std                | 0.139     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.188     |
| reward_torque         | -0.0365   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 20900     |
|    time_elapsed       | 534       |
|    total_timesteps    | 167200    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 20899     |
|    policy_loss        | -1.43e-06 |
|    std                | 0.139     |
|    value_loss         | 0         |
-------------------------------------
Num timesteps: 168000
Best mean reward: -1238.74 - Last mean reward per episode: -1579.42
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0203   |
| reward_motion         | 0.188     |
| reward_torque         | -0.0365   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21000     |
|    time_elapsed       | 536       |
|    total_timesteps    | 168000    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 1.43e-06  |
|    learning_rate      | 0.00096   |
|    n_updates          | 20999     |
|    policy_loss        | 2.38e-06  |
|    std                | 0.139     |
|    value_loss         | 0.000597  |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.187     |
| reward_torque         | -0.036    |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21100     |
|    time_elapsed       | 539       |
|    total_timesteps    | 168800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -3.85     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21099     |
|    policy_loss        | -0.0219   |
|    std                | 0.139     |
|    value_loss         | 1.21      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.187     |
| reward_torque         | -0.036    |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 976       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21200     |
|    time_elapsed       | 541       |
|    total_timesteps    | 169600    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.0227   |
|    learning_rate      | 0.00096   |
|    n_updates          | 21199     |
|    policy_loss        | -0.00712  |
|    std                | 0.139     |
|    value_loss         | 0.497     |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0207   |
| reward_motion         | 0.156     |
| reward_torque         | -0.0317   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 984       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21300     |
|    time_elapsed       | 544       |
|    total_timesteps    | 170400    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.00105   |
|    learning_rate      | 0.00096   |
|    n_updates          | 21299     |
|    policy_loss        | 0.0711    |
|    std                | 0.139     |
|    value_loss         | 1.57      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0207   |
| reward_motion         | 0.143     |
| reward_torque         | -0.0316   |
| reward_velocity       | -1.66     |
| rollout/              |           |
|    ep_len_mean        | 994       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21400     |
|    time_elapsed       | 546       |
|    total_timesteps    | 171200    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.000559 |
|    learning_rate      | 0.00096   |
|    n_updates          | 21399     |
|    policy_loss        | 1.24e-05  |
|    std                | 0.139     |
|    value_loss         | 2.1       |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.123     |
| reward_torque         | -0.0304   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1e+03     |
|    ep_rew_mean        | -1.63e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21500     |
|    time_elapsed       | 549       |
|    total_timesteps    | 172000    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.019     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21499     |
|    policy_loss        | -0.000186 |
|    std                | 0.139     |
|    value_loss         | 0.0179    |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.123     |
| reward_torque         | -0.0304   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.64e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21600     |
|    time_elapsed       | 551       |
|    total_timesteps    | 172800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.203     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21599     |
|    policy_loss        | 0.0314    |
|    std                | 0.139     |
|    value_loss         | 12        |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.0931    |
| reward_torque         | -0.0272   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.64e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21700     |
|    time_elapsed       | 554       |
|    total_timesteps    | 173600    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -5.28     |
|    learning_rate      | 0.00096   |
|    n_updates          | 21699     |
|    policy_loss        | -0.0109   |
|    std                | 0.139     |
|    value_loss         | 0.552     |
-------------------------------------
Num timesteps: 174000
Best mean reward: -1238.74 - Last mean reward per episode: -1646.23
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.0926    |
| reward_torque         | -0.0264   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.65e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21800     |
|    time_elapsed       | 556       |
|    total_timesteps    | 174400    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.0848    |
|    learning_rate      | 0.00096   |
|    n_updates          | 21799     |
|    policy_loss        | 0.0222    |
|    std                | 0.139     |
|    value_loss         | 1.4       |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0213   |
| reward_motion         | 0.0926    |
| reward_torque         | -0.0264   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.64e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 21900     |
|    time_elapsed       | 559       |
|    total_timesteps    | 175200    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.00102   |
|    learning_rate      | 0.00096   |
|    n_updates          | 21899     |
|    policy_loss        | -0.000644 |
|    std                | 0.139     |
|    value_loss         | 0.0235    |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0213   |
| reward_motion         | 0.0926    |
| reward_torque         | -0.0268   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.64e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22000     |
|    time_elapsed       | 562       |
|    total_timesteps    | 176000    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.000851 |
|    learning_rate      | 0.00096   |
|    n_updates          | 21999     |
|    policy_loss        | -0.000154 |
|    std                | 0.139     |
|    value_loss         | 0.02      |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.021    |
| reward_motion         | 0.111     |
| reward_torque         | -0.0268   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 1.01e+03  |
|    ep_rew_mean        | -1.63e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22100     |
|    time_elapsed       | 564       |
|    total_timesteps    | 176800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 3.39e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 22099     |
|    policy_loss        | 0.00135   |
|    std                | 0.139     |
|    value_loss         | 0.235     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.123     |
| reward_torque         | -0.027    |
| reward_velocity       | -1.66     |
| rollout/              |           |
|    ep_len_mean        | 999       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22200     |
|    time_elapsed       | 567       |
|    total_timesteps    | 177600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | -4.27e-05 |
|    learning_rate      | 0.00096   |
|    n_updates          | 22199     |
|    policy_loss        | -0.000666 |
|    std                | 0.139     |
|    value_loss         | 1.87      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0206   |
| reward_motion         | 0.143     |
| reward_torque         | -0.0271   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22300     |
|    time_elapsed       | 569       |
|    total_timesteps    | 178400    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.000366 |
|    learning_rate      | 0.00096   |
|    n_updates          | 22299     |
|    policy_loss        | -2.91e-05 |
|    std                | 0.139     |
|    value_loss         | 0.328     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0208   |
| reward_motion         | 0.143     |
| reward_torque         | -0.0271   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22400     |
|    time_elapsed       | 572       |
|    total_timesteps    | 179200    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.0296   |
|    learning_rate      | 0.00096   |
|    n_updates          | 22399     |
|    policy_loss        | 0.00825   |
|    std                | 0.14      |
|    value_loss         | 1.91      |
-------------------------------------
Num timesteps: 180000
Best mean reward: -1238.74 - Last mean reward per episode: -1613.17
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.143     |
| reward_torque         | -0.0271   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22500     |
|    time_elapsed       | 574       |
|    total_timesteps    | 180000    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.0435    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22499     |
|    policy_loss        | -0.00105  |
|    std                | 0.14      |
|    value_loss         | 1.34      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.143     |
| reward_torque         | -0.0271   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22600     |
|    time_elapsed       | 577       |
|    total_timesteps    | 180800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.382    |
|    learning_rate      | 0.00096   |
|    n_updates          | 22599     |
|    policy_loss        | 0.00125   |
|    std                | 0.14      |
|    value_loss         | 0.011     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.146     |
| reward_torque         | -0.0272   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.61e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 22700     |
|    time_elapsed       | 579       |
|    total_timesteps    | 181600    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -13.4     |
|    learning_rate      | 0.00096   |
|    n_updates          | 22699     |
|    policy_loss        | -0.0331   |
|    std                | 0.14      |
|    value_loss         | 1.98      |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0208  |
| reward_motion         | 0.146    |
| reward_torque         | -0.0272  |
| reward_velocity       | -1.69    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 22800    |
|    time_elapsed       | 582      |
|    total_timesteps    | 182400   |
| train/                |          |
|    entropy_loss       | -20.5    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 22799    |
|    policy_loss        | 3.58e-07 |
|    std                | 0.14     |
|    value_loss         | 0        |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0209  |
| reward_motion         | 0.146    |
| reward_torque         | -0.0272  |
| reward_velocity       | -1.68    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 22900    |
|    time_elapsed       | 585      |
|    total_timesteps    | 183200   |
| train/                |          |
|    entropy_loss       | -20.5    |
|    explained_variance | 0.0045   |
|    learning_rate      | 0.00096  |
|    n_updates          | 22899    |
|    policy_loss        | 0.0144   |
|    std                | 0.14     |
|    value_loss         | 0.319    |
------------------------------------
------------------------------------
| reward                | -1.58    |
| reward_contact        | -0.0209  |
| reward_motion         | 0.146    |
| reward_torque         | -0.0272  |
| reward_velocity       | -1.68    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 23000    |
|    time_elapsed       | 587      |
|    total_timesteps    | 184000   |
| train/                |          |
|    entropy_loss       | -20.6    |
|    explained_variance | -0.0439  |
|    learning_rate      | 0.00096  |
|    n_updates          | 22999    |
|    policy_loss        | 0.000234 |
|    std                | 0.14     |
|    value_loss         | 0.00541  |
------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0209  |
| reward_motion         | 0.146    |
| reward_torque         | -0.0271  |
| reward_velocity       | -1.69    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 23100    |
|    time_elapsed       | 590      |
|    total_timesteps    | 184800   |
| train/                |          |
|    entropy_loss       | -20.6    |
|    explained_variance | -87      |
|    learning_rate      | 0.00096  |
|    n_updates          | 23099    |
|    policy_loss        | -0.158   |
|    std                | 0.14     |
|    value_loss         | 14       |
------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0209   |
| reward_motion         | 0.146     |
| reward_torque         | -0.0271   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23200     |
|    time_elapsed       | 592       |
|    total_timesteps    | 185600    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 23199     |
|    policy_loss        | 1.07e-06  |
|    std                | 0.14      |
|    value_loss         | 0.00205   |
-------------------------------------
Num timesteps: 186000
Best mean reward: -1238.74 - Last mean reward per episode: -1593.54
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.021    |
| reward_motion         | 0.147     |
| reward_torque         | -0.0265   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23300     |
|    time_elapsed       | 595       |
|    total_timesteps    | 186400    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.0583    |
|    learning_rate      | 0.00096   |
|    n_updates          | 23299     |
|    policy_loss        | 0.0194    |
|    std                | 0.14      |
|    value_loss         | 0.567     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.147     |
| reward_torque         | -0.0272   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23400     |
|    time_elapsed       | 597       |
|    total_timesteps    | 187200    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0.0125    |
|    learning_rate      | 0.00096   |
|    n_updates          | 23399     |
|    policy_loss        | 0.112     |
|    std                | 0.14      |
|    value_loss         | 1.22      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0211   |
| reward_motion         | 0.147     |
| reward_torque         | -0.0272   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23500     |
|    time_elapsed       | 600       |
|    total_timesteps    | 188000    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.135    |
|    learning_rate      | 0.00096   |
|    n_updates          | 23499     |
|    policy_loss        | 0.0626    |
|    std                | 0.141     |
|    value_loss         | 0.816     |
-------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0211  |
| reward_motion         | 0.147    |
| reward_torque         | -0.028   |
| reward_velocity       | -1.69    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 23600    |
|    time_elapsed       | 603      |
|    total_timesteps    | 188800   |
| train/                |          |
|    entropy_loss       | -20.6    |
|    explained_variance | 0        |
|    learning_rate      | 0.00096  |
|    n_updates          | 23599    |
|    policy_loss        | -0       |
|    std                | 0.141    |
|    value_loss         | 1.93     |
------------------------------------
------------------------------------
| reward                | -1.59    |
| reward_contact        | -0.0213  |
| reward_motion         | 0.147    |
| reward_torque         | -0.028   |
| reward_velocity       | -1.69    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 23700    |
|    time_elapsed       | 605      |
|    total_timesteps    | 189600   |
| train/                |          |
|    entropy_loss       | -20.6    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 23699    |
|    policy_loss        | 1.91e-06 |
|    std                | 0.141    |
|    value_loss         | 0        |
------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0213   |
| reward_motion         | 0.147     |
| reward_torque         | -0.028    |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.6e+03  |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23800     |
|    time_elapsed       | 608       |
|    total_timesteps    | 190400    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0.149     |
|    learning_rate      | 0.00096   |
|    n_updates          | 23799     |
|    policy_loss        | -6.34e-05 |
|    std                | 0.141     |
|    value_loss         | 0.000142  |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.156     |
| reward_torque         | -0.0283   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 23900     |
|    time_elapsed       | 610       |
|    total_timesteps    | 191200    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.0964   |
|    learning_rate      | 0.00096   |
|    n_updates          | 23899     |
|    policy_loss        | 0.000869  |
|    std                | 0.141     |
|    value_loss         | 2.68      |
-------------------------------------
Num timesteps: 192000
Best mean reward: -1238.74 - Last mean reward per episode: -1589.85
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.156     |
| reward_torque         | -0.0283   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24000     |
|    time_elapsed       | 613       |
|    total_timesteps    | 192000    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.0168   |
|    learning_rate      | 0.00096   |
|    n_updates          | 23999     |
|    policy_loss        | 1.93e-05  |
|    std                | 0.141     |
|    value_loss         | 0.00229   |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.157     |
| reward_torque         | -0.0285   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24100     |
|    time_elapsed       | 615       |
|    total_timesteps    | 192800    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0.0147    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24099     |
|    policy_loss        | -0.00846  |
|    std                | 0.141     |
|    value_loss         | 0.205     |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.167     |
| reward_torque         | -0.0286   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24200     |
|    time_elapsed       | 618       |
|    total_timesteps    | 193600    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0.00532   |
|    learning_rate      | 0.00096   |
|    n_updates          | 24199     |
|    policy_loss        | 0.00888   |
|    std                | 0.141     |
|    value_loss         | 0.984     |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.168     |
| reward_torque         | -0.0288   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24300     |
|    time_elapsed       | 620       |
|    total_timesteps    | 194400    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 0.396     |
|    learning_rate      | 0.00096   |
|    n_updates          | 24299     |
|    policy_loss        | -0.00673  |
|    std                | 0.141     |
|    value_loss         | 2.98      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.168     |
| reward_torque         | -0.0288   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24400     |
|    time_elapsed       | 623       |
|    total_timesteps    | 195200    |
| train/                |           |
|    entropy_loss       | -20.3     |
|    explained_variance | -0.316    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24399     |
|    policy_loss        | 0.155     |
|    std                | 0.141     |
|    value_loss         | 1.4e+04   |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0215   |
| reward_motion         | 0.168     |
| reward_torque         | -0.0288   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24500     |
|    time_elapsed       | 626       |
|    total_timesteps    | 196000    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.00571  |
|    learning_rate      | 0.00096   |
|    n_updates          | 24499     |
|    policy_loss        | 0.0295    |
|    std                | 0.141     |
|    value_loss         | 11.7      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.168     |
| reward_torque         | -0.0288   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24600     |
|    time_elapsed       | 628       |
|    total_timesteps    | 196800    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.125    |
|    learning_rate      | 0.00096   |
|    n_updates          | 24599     |
|    policy_loss        | 0.0163    |
|    std                | 0.141     |
|    value_loss         | 3.54      |
-------------------------------------
-------------------------------------
| reward                | -1.57     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.161     |
| reward_torque         | -0.0285   |
| reward_velocity       | -1.68     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24700     |
|    time_elapsed       | 631       |
|    total_timesteps    | 197600    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | 1.19e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 24699     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.141     |
|    value_loss         | 0.121     |
-------------------------------------
Num timesteps: 198000
Best mean reward: -1238.74 - Last mean reward per episode: -1591.57
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.161     |
| reward_torque         | -0.0286   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 24800     |
|    time_elapsed       | 633       |
|    total_timesteps    | 198400    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | -0.00663  |
|    learning_rate      | 0.00096   |
|    n_updates          | 24799     |
|    policy_loss        | 0.00239   |
|    std                | 0.142     |
|    value_loss         | 2.08      |
-------------------------------------
-------------------------------------
| reward                | -1.56     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.161     |
| reward_torque         | -0.0286   |
| reward_velocity       | -1.67     |
| rollout/              |           |
|    ep_len_mean        | 992       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 24900     |
|    time_elapsed       | 636       |
|    total_timesteps    | 199200    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 24899     |
|    policy_loss        | 4.29e-06  |
|    std                | 0.142     |
|    value_loss         | 0         |
-------------------------------------
------------------------------------
| reward                | -1.57    |
| reward_contact        | -0.0213  |
| reward_motion         | 0.161    |
| reward_torque         | -0.0286  |
| reward_velocity       | -1.68    |
| rollout/              |          |
|    ep_len_mean        | 992      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 25000    |
|    time_elapsed       | 638      |
|    total_timesteps    | 200000   |
| train/                |          |
|    entropy_loss       | -20.4    |
|    explained_variance | -0.962   |
|    learning_rate      | 0.00096  |
|    n_updates          | 24999    |
|    policy_loss        | 0.0305   |
|    std                | 0.142    |
|    value_loss         | 0.901    |
------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.193     |
| reward_torque         | -0.0289   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 973       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25100     |
|    time_elapsed       | 641       |
|    total_timesteps    | 200800    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | 0.306     |
|    learning_rate      | 0.00096   |
|    n_updates          | 25099     |
|    policy_loss        | 0.0221    |
|    std                | 0.141     |
|    value_loss         | 13.4      |
-------------------------------------
-------------------------------------
| reward                | -1.55     |
| reward_contact        | -0.0204   |
| reward_motion         | 0.193     |
| reward_torque         | -0.0289   |
| reward_velocity       | -1.69     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.55e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25200     |
|    time_elapsed       | 644       |
|    total_timesteps    | 201600    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.403     |
|    learning_rate      | 0.00096   |
|    n_updates          | 25199     |
|    policy_loss        | 0.179     |
|    std                | 0.142     |
|    value_loss         | 2.96      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0202   |
| reward_motion         | 0.227     |
| reward_torque         | -0.0297   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.55e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25300     |
|    time_elapsed       | 646       |
|    total_timesteps    | 202400    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.839    |
|    learning_rate      | 0.00096   |
|    n_updates          | 25299     |
|    policy_loss        | -0.00122  |
|    std                | 0.142     |
|    value_loss         | 1.07      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.228     |
| reward_torque         | -0.0297   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25400     |
|    time_elapsed       | 649       |
|    total_timesteps    | 203200    |
| train/                |           |
|    entropy_loss       | -20.4     |
|    explained_variance | 0.00525   |
|    learning_rate      | 0.00096   |
|    n_updates          | 25399     |
|    policy_loss        | 1.98      |
|    std                | 0.142     |
|    value_loss         | 3.75      |
-------------------------------------
Num timesteps: 204000
Best mean reward: -1238.74 - Last mean reward per episode: -1558.94
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.227     |
| reward_torque         | -0.0324   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25500     |
|    time_elapsed       | 651       |
|    total_timesteps    | 204000    |
| train/                |           |
|    entropy_loss       | -20.5     |
|    explained_variance | -0.184    |
|    learning_rate      | 0.00096   |
|    n_updates          | 25499     |
|    policy_loss        | -0.0431   |
|    std                | 0.142     |
|    value_loss         | 12.2      |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.228     |
| reward_torque         | -0.0324   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25600     |
|    time_elapsed       | 654       |
|    total_timesteps    | 204800    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | 0.5       |
|    learning_rate      | 0.00096   |
|    n_updates          | 25599     |
|    policy_loss        | 0.0115    |
|    std                | 0.142     |
|    value_loss         | 0.404     |
-------------------------------------
-------------------------------------
| reward                | -1.58     |
| reward_contact        | -0.0201   |
| reward_motion         | 0.228     |
| reward_torque         | -0.0324   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25700     |
|    time_elapsed       | 656       |
|    total_timesteps    | 205600    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | -0.177    |
|    learning_rate      | 0.00096   |
|    n_updates          | 25699     |
|    policy_loss        | -0.0317   |
|    std                | 0.142     |
|    value_loss         | 1.74      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.228     |
| reward_torque         | -0.0324   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25800     |
|    time_elapsed       | 659       |
|    total_timesteps    | 206400    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | -1.61     |
|    learning_rate      | 0.00096   |
|    n_updates          | 25799     |
|    policy_loss        | -0.123    |
|    std                | 0.143     |
|    value_loss         | 0.591     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.227     |
| reward_torque         | -0.0321   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 25900     |
|    time_elapsed       | 661       |
|    total_timesteps    | 207200    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | -0.0208   |
|    learning_rate      | 0.00096   |
|    n_updates          | 25899     |
|    policy_loss        | 0.0108    |
|    std                | 0.143     |
|    value_loss         | 5.4       |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0199   |
| reward_motion         | 0.225     |
| reward_torque         | -0.0321   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 26000     |
|    time_elapsed       | 664       |
|    total_timesteps    | 208000    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | 0.00597   |
|    learning_rate      | 0.00096   |
|    n_updates          | 25999     |
|    policy_loss        | 0.00882   |
|    std                | 0.143     |
|    value_loss         | 10        |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0199   |
| reward_motion         | 0.225     |
| reward_torque         | -0.0321   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26100     |
|    time_elapsed       | 667       |
|    total_timesteps    | 208800    |
| train/                |           |
|    entropy_loss       | -20.7     |
|    explained_variance | 0.487     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26099     |
|    policy_loss        | 0.147     |
|    std                | 0.143     |
|    value_loss         | 0.304     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.228     |
| reward_torque         | -0.0338   |
| reward_velocity       | -1.78     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26200     |
|    time_elapsed       | 669       |
|    total_timesteps    | 209600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.0617    |
|    learning_rate      | 0.00096   |
|    n_updates          | 26199     |
|    policy_loss        | -0.0293   |
|    std                | 0.144     |
|    value_loss         | 1.07      |
-------------------------------------
Num timesteps: 210000
Best mean reward: -1238.74 - Last mean reward per episode: -1564.49
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0199   |
| reward_motion         | 0.229     |
| reward_torque         | -0.034    |
| reward_velocity       | -1.79     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.56e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26300     |
|    time_elapsed       | 672       |
|    total_timesteps    | 210400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.00813   |
|    learning_rate      | 0.00096   |
|    n_updates          | 26299     |
|    policy_loss        | 0.0951    |
|    std                | 0.143     |
|    value_loss         | 1.97      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0198   |
| reward_motion         | 0.237     |
| reward_torque         | -0.0356   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26400     |
|    time_elapsed       | 674       |
|    total_timesteps    | 211200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | -0.468    |
|    learning_rate      | 0.00096   |
|    n_updates          | 26399     |
|    policy_loss        | 0.00686   |
|    std                | 0.144     |
|    value_loss         | 0.372     |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0198   |
| reward_motion         | 0.237     |
| reward_torque         | -0.0356   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26500     |
|    time_elapsed       | 677       |
|    total_timesteps    | 212000    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.377     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26499     |
|    policy_loss        | -0.0654   |
|    std                | 0.144     |
|    value_loss         | 4.55      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0198   |
| reward_motion         | 0.237     |
| reward_torque         | -0.0356   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 26600     |
|    time_elapsed       | 679       |
|    total_timesteps    | 212800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.712     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26599     |
|    policy_loss        | 0.002     |
|    std                | 0.144     |
|    value_loss         | 0.149     |
-------------------------------------
-------------------------------------
| reward                | -1.63     |
| reward_contact        | -0.0198   |
| reward_motion         | 0.238     |
| reward_torque         | -0.0378   |
| reward_velocity       | -1.81     |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 26700     |
|    time_elapsed       | 682       |
|    total_timesteps    | 213600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | -0.83     |
|    learning_rate      | 0.00096   |
|    n_updates          | 26699     |
|    policy_loss        | 0.00133   |
|    std                | 0.144     |
|    value_loss         | 4.44      |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0198   |
| reward_motion         | 0.238     |
| reward_torque         | -0.0378   |
| reward_velocity       | -1.8      |
| rollout/              |           |
|    ep_len_mean        | 964       |
|    ep_rew_mean        | -1.57e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 26800     |
|    time_elapsed       | 685       |
|    total_timesteps    | 214400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 26799     |
|    policy_loss        | 1.43e-06  |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0201   |
| reward_motion         | 0.216     |
| reward_torque         | -0.0371   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 26900     |
|    time_elapsed       | 687       |
|    total_timesteps    | 215200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 26899     |
|    policy_loss        | -0        |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
Num timesteps: 216000
Best mean reward: -1238.74 - Last mean reward per episode: -1590.35
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0203   |
| reward_motion         | 0.216     |
| reward_torque         | -0.0371   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27000     |
|    time_elapsed       | 690       |
|    total_timesteps    | 216000    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 26999     |
|    policy_loss        | -2.98e-06 |
|    std                | 0.144     |
|    value_loss         | 0.753     |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0203   |
| reward_motion         | 0.216     |
| reward_torque         | -0.0371   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27100     |
|    time_elapsed       | 692       |
|    total_timesteps    | 216800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 27099     |
|    policy_loss        | -2.86e-06 |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0205   |
| reward_motion         | 0.216     |
| reward_torque         | -0.0369   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27200     |
|    time_elapsed       | 695       |
|    total_timesteps    | 217600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 27199     |
|    policy_loss        | -2.38e-06 |
|    std                | 0.144     |
|    value_loss         | 0.248     |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0207   |
| reward_motion         | 0.216     |
| reward_torque         | -0.0369   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27300     |
|    time_elapsed       | 697       |
|    total_timesteps    | 218400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 27299     |
|    policy_loss        | -0        |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0206   |
| reward_motion         | 0.215     |
| reward_torque         | -0.0367   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 27400     |
|    time_elapsed       | 700       |
|    total_timesteps    | 219200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.0979    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27399     |
|    policy_loss        | -0.000534 |
|    std                | 0.144     |
|    value_loss         | 0.54      |
-------------------------------------
-------------------------------------
| reward                | -1.59     |
| reward_contact        | -0.0206   |
| reward_motion         | 0.215     |
| reward_torque         | -0.0367   |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27500     |
|    time_elapsed       | 702       |
|    total_timesteps    | 220000    |
| train/                |           |
|    entropy_loss       | -20.6     |
|    explained_variance | -0.123    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27499     |
|    policy_loss        | 0.276     |
|    std                | 0.144     |
|    value_loss         | 8.59e+03  |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0208   |
| reward_motion         | 0.215     |
| reward_torque         | -0.0367   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27600     |
|    time_elapsed       | 705       |
|    total_timesteps    | 220800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.0351    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27599     |
|    policy_loss        | 0.000216  |
|    std                | 0.144     |
|    value_loss         | 0.435     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.021    |
| reward_motion         | 0.215     |
| reward_torque         | -0.0367   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27700     |
|    time_elapsed       | 708       |
|    total_timesteps    | 221600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 1.24e-05  |
|    learning_rate      | 0.00096   |
|    n_updates          | 27699     |
|    policy_loss        | -1.43e-06 |
|    std                | 0.144     |
|    value_loss         | 1.97e-06  |
-------------------------------------
Num timesteps: 222000
Best mean reward: -1238.74 - Last mean reward per episode: -1585.52
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0212   |
| reward_motion         | 0.215     |
| reward_torque         | -0.0367   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27800     |
|    time_elapsed       | 710       |
|    total_timesteps    | 222400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 27799     |
|    policy_loss        | -2.86e-06 |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.213     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 27900     |
|    time_elapsed       | 713       |
|    total_timesteps    | 223200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.000822  |
|    learning_rate      | 0.00096   |
|    n_updates          | 27899     |
|    policy_loss        | 0.000685  |
|    std                | 0.144     |
|    value_loss         | 0.00171   |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0214   |
| reward_motion         | 0.213     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28000     |
|    time_elapsed       | 715       |
|    total_timesteps    | 224000    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.0244    |
|    learning_rate      | 0.00096   |
|    n_updates          | 27999     |
|    policy_loss        | 5.91e-05  |
|    std                | 0.145     |
|    value_loss         | 0.0392    |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0216   |
| reward_motion         | 0.213     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.58e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28100     |
|    time_elapsed       | 718       |
|    total_timesteps    | 224800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 28099     |
|    policy_loss        | -3.81e-06 |
|    std                | 0.144     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.0218   |
| reward_motion         | 0.213     |
| reward_torque         | -0.0366   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28200     |
|    time_elapsed       | 720       |
|    total_timesteps    | 225600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0.57      |
|    learning_rate      | 0.00096   |
|    n_updates          | 28199     |
|    policy_loss        | -7.1e-05  |
|    std                | 0.145     |
|    value_loss         | 0.0177    |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.022    |
| reward_motion         | 0.211     |
| reward_torque         | -0.0356   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28300     |
|    time_elapsed       | 723       |
|    total_timesteps    | 226400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 28299     |
|    policy_loss        | -2.03e-06 |
|    std                | 0.145     |
|    value_loss         | 0.772     |
-------------------------------------
-------------------------------------
| reward                | -1.62     |
| reward_contact        | -0.022    |
| reward_motion         | 0.211     |
| reward_torque         | -0.0356   |
| reward_velocity       | -1.77     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.6e+03  |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28400     |
|    time_elapsed       | 725       |
|    total_timesteps    | 227200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | -0.000724 |
|    learning_rate      | 0.00096   |
|    n_updates          | 28399     |
|    policy_loss        | -5.6e-06  |
|    std                | 0.145     |
|    value_loss         | 2.08      |
-------------------------------------
Num timesteps: 228000
Best mean reward: -1238.74 - Last mean reward per episode: -1598.40
------------------------------------
| reward                | -1.62    |
| reward_contact        | -0.022   |
| reward_motion         | 0.211    |
| reward_torque         | -0.0356  |
| reward_velocity       | -1.77    |
| rollout/              |          |
|    ep_len_mean        | 974      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 28500    |
|    time_elapsed       | 728      |
|    total_timesteps    | 228000   |
| train/                |          |
|    entropy_loss       | -20.8    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 28499    |
|    policy_loss        | 3.34e-06 |
|    std                | 0.145    |
|    value_loss         | 0        |
------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0221   |
| reward_motion         | 0.209     |
| reward_torque         | -0.0335   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 28600     |
|    time_elapsed       | 730       |
|    total_timesteps    | 228800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 28599     |
|    policy_loss        | 2.5e-06   |
|    std                | 0.145     |
|    value_loss         | 0         |
-------------------------------------
------------------------------------
| reward                | -1.6     |
| reward_contact        | -0.0222  |
| reward_motion         | 0.204    |
| reward_torque         | -0.033   |
| reward_velocity       | -1.75    |
| rollout/              |          |
|    ep_len_mean        | 974      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 312      |
|    iterations         | 28700    |
|    time_elapsed       | 733      |
|    total_timesteps    | 229600   |
| train/                |          |
|    entropy_loss       | -20.8    |
|    explained_variance | nan      |
|    learning_rate      | 0.00096  |
|    n_updates          | 28699    |
|    policy_loss        | 3.58e-06 |
|    std                | 0.145    |
|    value_loss         | 0        |
------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0222   |
| reward_motion         | 0.204     |
| reward_torque         | -0.033    |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 28800     |
|    time_elapsed       | 736       |
|    total_timesteps    | 230400    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | -0.0244   |
|    learning_rate      | 0.00096   |
|    n_updates          | 28799     |
|    policy_loss        | 2.86e-05  |
|    std                | 0.145     |
|    value_loss         | 2.64      |
-------------------------------------
-------------------------------------
| reward                | -1.6      |
| reward_contact        | -0.0222   |
| reward_motion         | 0.204     |
| reward_torque         | -0.033    |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 28900     |
|    time_elapsed       | 738       |
|    total_timesteps    | 231200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | -0.00228  |
|    learning_rate      | 0.00096   |
|    n_updates          | 28899     |
|    policy_loss        | -2.67e-05 |
|    std                | 0.145     |
|    value_loss         | 0.0866    |
-------------------------------------
------------------------------------
| reward                | -1.6     |
| reward_contact        | -0.0222  |
| reward_motion         | 0.204    |
| reward_torque         | -0.033   |
| reward_velocity       | -1.75    |
| rollout/              |          |
|    ep_len_mean        | 974      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 29000    |
|    time_elapsed       | 741      |
|    total_timesteps    | 232000   |
| train/                |          |
|    entropy_loss       | -20.8    |
|    explained_variance | 0.248    |
|    learning_rate      | 0.00096  |
|    n_updates          | 28999    |
|    policy_loss        | 5.6e-06  |
|    std                | 0.145    |
|    value_loss         | 0.229    |
------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0221   |
| reward_motion         | 0.204     |
| reward_torque         | -0.033    |
| reward_velocity       | -1.75     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 29100     |
|    time_elapsed       | 743       |
|    total_timesteps    | 232800    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 0         |
|    learning_rate      | 0.00096   |
|    n_updates          | 29099     |
|    policy_loss        | 4.77e-07  |
|    std                | 0.145     |
|    value_loss         | 0.505     |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0223   |
| reward_motion         | 0.204     |
| reward_torque         | -0.033    |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 312       |
|    iterations         | 29200     |
|    time_elapsed       | 746       |
|    total_timesteps    | 233600    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | 1.79e-07  |
|    learning_rate      | 0.00096   |
|    n_updates          | 29199     |
|    policy_loss        | 2.03e-06  |
|    std                | 0.145     |
|    value_loss         | 0.321     |
-------------------------------------
Num timesteps: 234000
Best mean reward: -1238.74 - Last mean reward per episode: -1594.19
------------------------------------
| reward                | -1.61    |
| reward_contact        | -0.0223  |
| reward_motion         | 0.204    |
| reward_torque         | -0.033   |
| reward_velocity       | -1.76    |
| rollout/              |          |
|    ep_len_mean        | 974      |
|    ep_rew_mean        | -1.6e+03 |
| time/                 |          |
|    fps                | 313      |
|    iterations         | 29300    |
|    time_elapsed       | 748      |
|    total_timesteps    | 234400   |
| train/                |          |
|    entropy_loss       | -20.8    |
|    explained_variance | 0.0741   |
|    learning_rate      | 0.00096  |
|    n_updates          | 29299    |
|    policy_loss        | 0.000177 |
|    std                | 0.145    |
|    value_loss         | 1.05     |
------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0225   |
| reward_motion         | 0.204     |
| reward_torque         | -0.0327   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.6e+03  |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 29400     |
|    time_elapsed       | 751       |
|    total_timesteps    | 235200    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 29399     |
|    policy_loss        | -7.15e-07 |
|    std                | 0.145     |
|    value_loss         | 0         |
-------------------------------------
-------------------------------------
| reward                | -1.61     |
| reward_contact        | -0.0224   |
| reward_motion         | 0.203     |
| reward_torque         | -0.0307   |
| reward_velocity       | -1.76     |
| rollout/              |           |
|    ep_len_mean        | 974       |
|    ep_rew_mean        | -1.59e+03 |
| time/                 |           |
|    fps                | 313       |
|    iterations         | 29500     |
|    time_elapsed       | 753       |
|    total_timesteps    | 236000    |
| train/                |           |
|    entropy_loss       | -20.8     |
|    explained_variance | nan       |
|    learning_rate      | 0.00096   |
|    n_updates          | 29499     |
|    policy_loss        | -0        |
|    std                | 0.145     |
|    value_loss         | 0         |
-------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
[DDPG] MultiInputPolicy
Using cuda device
Logging to rl/out_dir/models/exp73/TD3_1
---------------------------------
| reward             | 0.279    |
| reward_contact     | -0.016   |
| reward_motion      | 1        |
| reward_torque      | -0.346   |
| reward_velocity    | -0.359   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -2.7     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 1183     |
|    time_elapsed    | 3        |
|    total timesteps | 4096     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -72.57
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.32    |
| reward_contact     | -0.0118  |
| reward_motion      | 0.571    |
| reward_torque      | -0.272   |
| reward_velocity    | -0.608   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -79.9    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 33       |
|    total timesteps | 8164     |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.165    |
|    learning_rate   | 0.001    |
|    n_updates       | 3160     |
---------------------------------
Num timesteps: 12000
Best mean reward: -72.57 - Last mean reward per episode: -77.05
---------------------------------
| reward             | -0.258   |
| reward_contact     | -0.0107  |
| reward_motion      | 0.455    |
| reward_torque      | -0.197   |
| reward_velocity    | -0.505   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -84.5    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 171      |
|    time_elapsed    | 71       |
|    total timesteps | 12260    |
| train/             |          |
|    actor_loss      | -6.65    |
|    critic_loss     | 0.236    |
|    learning_rate   | 0.001    |
|    n_updates       | 7255     |
---------------------------------
---------------------------------
| reward             | -0.177   |
| reward_contact     | -0.00972 |
| reward_motion      | 0.533    |
| reward_torque      | -0.206   |
| reward_velocity    | -0.495   |
| rollout/           |          |
|    ep_len_mean     | 965      |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 153      |
|    time_elapsed    | 100      |
|    total timesteps | 15437    |
| train/             |          |
|    actor_loss      | -9.26    |
|    critic_loss     | 0.304    |
|    learning_rate   | 0.001    |
|    n_updates       | 10435    |
---------------------------------
Num timesteps: 18000
Best mean reward: -72.57 - Last mean reward per episode: -173.39
---------------------------------
| reward             | -0.296   |
| reward_contact     | -0.0108  |
| reward_motion      | 0.474    |
| reward_torque      | -0.221   |
| reward_velocity    | -0.538   |
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 141      |
|    time_elapsed    | 132      |
|    total timesteps | 18753    |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.421    |
|    learning_rate   | 0.001    |
|    n_updates       | 13750    |
---------------------------------
---------------------------------
| reward             | -0.334   |
| reward_contact     | -0.0112  |
| reward_motion      | 0.391    |
| reward_torque      | -0.191   |
| reward_velocity    | -0.523   |
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | -194     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 134      |
|    time_elapsed    | 170      |
|    total timesteps | 22849    |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.443    |
|    learning_rate   | 0.001    |
|    n_updates       | 17845    |
---------------------------------
Num timesteps: 24000
Best mean reward: -72.57 - Last mean reward per episode: -188.62
---------------------------------
| reward             | -0.331   |
| reward_contact     | -0.016   |
| reward_motion      | 0.37     |
| reward_torque      | -0.183   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 962      |
|    ep_rew_mean     | -178     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 128      |
|    time_elapsed    | 209      |
|    total timesteps | 26945    |
| train/             |          |
|    actor_loss      | -18.5    |
|    critic_loss     | 0.575    |
|    learning_rate   | 0.001    |
|    n_updates       | 21940    |
---------------------------------
Num timesteps: 30000
Best mean reward: -72.57 - Last mean reward per episode: -201.06
---------------------------------
| reward             | -0.339   |
| reward_contact     | -0.015   |
| reward_motion      | 0.355    |
| reward_torque      | -0.169   |
| reward_velocity    | -0.51    |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 125      |
|    time_elapsed    | 247      |
|    total timesteps | 31041    |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 0.691    |
|    learning_rate   | 0.001    |
|    n_updates       | 26040    |
---------------------------------
---------------------------------
| reward             | -0.294   |
| reward_contact     | -0.0139  |
| reward_motion      | 0.371    |
| reward_torque      | -0.174   |
| reward_velocity    | -0.478   |
| rollout/           |          |
|    ep_len_mean     | 976      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 122      |
|    time_elapsed    | 285      |
|    total timesteps | 35137    |
| train/             |          |
|    actor_loss      | -22      |
|    critic_loss     | 0.767    |
|    learning_rate   | 0.001    |
|    n_updates       | 30135    |
---------------------------------
Num timesteps: 36000
Best mean reward: -72.57 - Last mean reward per episode: -155.19
---------------------------------
| reward             | -0.293   |
| reward_contact     | -0.0146  |
| reward_motion      | 0.385    |
| reward_torque      | -0.177   |
| reward_velocity    | -0.486   |
| rollout/           |          |
|    ep_len_mean     | 981      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 121      |
|    time_elapsed    | 323      |
|    total timesteps | 39233    |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 0.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 34230    |
---------------------------------
Num timesteps: 42000
Best mean reward: -72.57 - Last mean reward per episode: -123.79
---------------------------------
| reward             | -0.21    |
| reward_contact     | -0.0153  |
| reward_motion      | 0.442    |
| reward_torque      | -0.171   |
| reward_velocity    | -0.465   |
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 119      |
|    time_elapsed    | 361      |
|    total timesteps | 43329    |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 0.844    |
|    learning_rate   | 0.001    |
|    n_updates       | 38325    |
---------------------------------
---------------------------------
| reward             | -0.207   |
| reward_contact     | -0.0152  |
| reward_motion      | 0.447    |
| reward_torque      | -0.168   |
| reward_velocity    | -0.471   |
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 118      |
|    time_elapsed    | 399      |
|    total timesteps | 47425    |
| train/             |          |
|    actor_loss      | -25.2    |
|    critic_loss     | 0.835    |
|    learning_rate   | 0.001    |
|    n_updates       | 42420    |
---------------------------------
Num timesteps: 48000
Best mean reward: -72.57 - Last mean reward per episode: -105.65
---------------------------------
| reward             | -0.192   |
| reward_contact     | -0.0156  |
| reward_motion      | 0.451    |
| reward_torque      | -0.17    |
| reward_velocity    | -0.457   |
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 117      |
|    time_elapsed    | 437      |
|    total timesteps | 51521    |
| train/             |          |
|    actor_loss      | -25.4    |
|    critic_loss     | 0.878    |
|    learning_rate   | 0.001    |
|    n_updates       | 46520    |
---------------------------------
Num timesteps: 54000
Best mean reward: -72.57 - Last mean reward per episode: -121.00
---------------------------------
| reward             | -0.206   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.436    |
| reward_torque      | -0.162   |
| reward_velocity    | -0.464   |
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | -121     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 117      |
|    time_elapsed    | 467      |
|    total timesteps | 54727    |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.952    |
|    learning_rate   | 0.001    |
|    n_updates       | 49725    |
---------------------------------
---------------------------------
| reward             | -0.187   |
| reward_contact     | -0.0161  |
| reward_motion      | 0.458    |
| reward_torque      | -0.169   |
| reward_velocity    | -0.459   |
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 116      |
|    time_elapsed    | 495      |
|    total timesteps | 57862    |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 1.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 52860    |
---------------------------------
Num timesteps: 60000
Best mean reward: -72.57 - Last mean reward per episode: -121.37
---------------------------------
| reward             | -0.183   |
| reward_contact     | -0.0156  |
| reward_motion      | 0.492    |
| reward_torque      | -0.175   |
| reward_velocity    | -0.485   |
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 116      |
|    time_elapsed    | 533      |
|    total timesteps | 61958    |
| train/             |          |
|    actor_loss      | -25.2    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 56955    |
---------------------------------
Num timesteps: 66000
Best mean reward: -72.57 - Last mean reward per episode: -113.26
---------------------------------
| reward             | -0.165   |
| reward_contact     | -0.0156  |
| reward_motion      | 0.522    |
| reward_torque      | -0.18    |
| reward_velocity    | -0.492   |
| rollout/           |          |
|    ep_len_mean     | 971      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 115      |
|    time_elapsed    | 571      |
|    total timesteps | 66054    |
| train/             |          |
|    actor_loss      | -24.6    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 61050    |
---------------------------------
---------------------------------
| reward             | -0.157   |
| reward_contact     | -0.0155  |
| reward_motion      | 0.535    |
| reward_torque      | -0.182   |
| reward_velocity    | -0.495   |
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 115      |
|    time_elapsed    | 609      |
|    total timesteps | 70150    |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 1.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 65145    |
---------------------------------
Num timesteps: 72000
Best mean reward: -72.57 - Last mean reward per episode: -98.38
---------------------------------
| reward             | -0.168   |
| reward_contact     | -0.0156  |
| reward_motion      | 0.527    |
| reward_torque      | -0.184   |
| reward_velocity    | -0.495   |
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 114      |
|    time_elapsed    | 638      |
|    total timesteps | 73284    |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 68280    |
---------------------------------
---------------------------------
| reward             | -0.161   |
| reward_contact     | -0.0155  |
| reward_motion      | 0.532    |
| reward_torque      | -0.183   |
| reward_velocity    | -0.494   |
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 114      |
|    time_elapsed    | 676      |
|    total timesteps | 77380    |
| train/             |          |
|    actor_loss      | -23.5    |
|    critic_loss     | 1.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 72375    |
---------------------------------
Num timesteps: 78000
Best mean reward: -72.57 - Last mean reward per episode: -102.20
---------------------------------
| reward             | -0.148   |
| reward_contact     | -0.0151  |
| reward_motion      | 0.554    |
| reward_torque      | -0.182   |
| reward_velocity    | -0.505   |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 114      |
|    time_elapsed    | 714      |
|    total timesteps | 81476    |
| train/             |          |
|    actor_loss      | -22      |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 76475    |
---------------------------------
Num timesteps: 84000
Best mean reward: -72.57 - Last mean reward per episode: -94.43
---------------------------------
| reward             | -0.141   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.552    |
| reward_torque      | -0.183   |
| reward_velocity    | -0.495   |
| rollout/           |          |
|    ep_len_mean     | 972      |
|    ep_rew_mean     | -93.3    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 113      |
|    time_elapsed    | 751      |
|    total timesteps | 85572    |
| train/             |          |
|    actor_loss      | -22.6    |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 80570    |
---------------------------------
---------------------------------
| reward             | -0.132   |
| reward_contact     | -0.0144  |
| reward_motion      | 0.56     |
| reward_torque      | -0.183   |
| reward_velocity    | -0.495   |
| rollout/           |          |
|    ep_len_mean     | 975      |
|    ep_rew_mean     | -96.7    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 113      |
|    time_elapsed    | 789      |
|    total timesteps | 89668    |
| train/             |          |
|    actor_loss      | -19.9    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.001    |
|    n_updates       | 84665    |
---------------------------------
Num timesteps: 90000
Best mean reward: -72.57 - Last mean reward per episode: -96.67
---------------------------------
| reward             | -0.126   |
| reward_contact     | -0.0145  |
| reward_motion      | 0.579    |
| reward_torque      | -0.191   |
| reward_velocity    | -0.499   |
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | -93.4    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 113      |
|    time_elapsed    | 827      |
|    total timesteps | 93764    |
| train/             |          |
|    actor_loss      | -19.3    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 88760    |
---------------------------------
Num timesteps: 96000
Best mean reward: -72.57 - Last mean reward per episode: -86.12
---------------------------------
| reward             | -0.113   |
| reward_contact     | -0.0144  |
| reward_motion      | 0.586    |
| reward_torque      | -0.192   |
| reward_velocity    | -0.493   |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -84.9    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 113      |
|    time_elapsed    | 857      |
|    total timesteps | 97005    |
| train/             |          |
|    actor_loss      | -19.1    |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 92000    |
---------------------------------
---------------------------------
| reward             | -0.118   |
| reward_contact     | -0.0143  |
| reward_motion      | 0.58     |
| reward_torque      | -0.181   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -92.4    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 112      |
|    time_elapsed    | 895      |
|    total timesteps | 101101   |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.001    |
|    n_updates       | 96100    |
---------------------------------
Num timesteps: 102000
Best mean reward: -72.57 - Last mean reward per episode: -92.36
---------------------------------
| reward             | -0.0971  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.59     |
| reward_torque      | -0.18    |
| reward_velocity    | -0.492   |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -93.3    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 112      |
|    time_elapsed    | 934      |
|    total timesteps | 105197   |
| train/             |          |
|    actor_loss      | -19.1    |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 100195   |
---------------------------------
Num timesteps: 108000
Best mean reward: -72.57 - Last mean reward per episode: -94.51
---------------------------------
| reward             | -0.105   |
| reward_contact     | -0.0149  |
| reward_motion      | 0.6      |
| reward_torque      | -0.187   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | -93.1    |
| time/              |          |
|    episodes        | 112      |
|    fps             | 112      |
|    time_elapsed    | 971      |
|    total timesteps | 109293   |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 1.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 104290   |
---------------------------------
---------------------------------
| reward             | -0.106   |
| reward_contact     | -0.0153  |
| reward_motion      | 0.59     |
| reward_torque      | -0.183   |
| reward_velocity    | -0.497   |
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | -81.1    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 112      |
|    time_elapsed    | 1010     |
|    total timesteps | 113389   |
| train/             |          |
|    actor_loss      | -18.4    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 108385   |
---------------------------------
Num timesteps: 114000
Best mean reward: -72.57 - Last mean reward per episode: -81.11
---------------------------------
| reward             | -0.0817  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.6      |
| reward_torque      | -0.18    |
| reward_velocity    | -0.487   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -73.7    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 112      |
|    time_elapsed    | 1048     |
|    total timesteps | 117485   |
| train/             |          |
|    actor_loss      | -19.5    |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 112480   |
---------------------------------
Num timesteps: 120000
Best mean reward: -72.57 - Last mean reward per episode: -65.54
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0662  |
| reward_contact     | -0.0151  |
| reward_motion      | 0.63     |
| reward_torque      | -0.183   |
| reward_velocity    | -0.498   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -74.4    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 111      |
|    time_elapsed    | 1085     |
|    total timesteps | 121581   |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 2.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 116580   |
---------------------------------
---------------------------------
| reward             | -0.0576  |
| reward_contact     | -0.0138  |
| reward_motion      | 0.66     |
| reward_torque      | -0.19    |
| reward_velocity    | -0.514   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -65.3    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 111      |
|    time_elapsed    | 1124     |
|    total timesteps | 125677   |
| train/             |          |
|    actor_loss      | -15.7    |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.001    |
|    n_updates       | 120675   |
---------------------------------
Num timesteps: 126000
Best mean reward: -65.54 - Last mean reward per episode: -65.27
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0226  |
| reward_contact     | -0.0141  |
| reward_motion      | 0.69     |
| reward_torque      | -0.195   |
| reward_velocity    | -0.504   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -71.2    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 111      |
|    time_elapsed    | 1162     |
|    total timesteps | 129773   |
| train/             |          |
|    actor_loss      | -16.3    |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 124770   |
---------------------------------
Num timesteps: 132000
Best mean reward: -65.27 - Last mean reward per episode: -68.29
---------------------------------
| reward             | -0.0159  |
| reward_contact     | -0.0148  |
| reward_motion      | 0.7      |
| reward_torque      | -0.19    |
| reward_velocity    | -0.511   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -69.9    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 111      |
|    time_elapsed    | 1201     |
|    total timesteps | 133869   |
| train/             |          |
|    actor_loss      | -16.2    |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 128865   |
---------------------------------
---------------------------------
| reward             | -0.0221  |
| reward_contact     | -0.0145  |
| reward_motion      | 0.69     |
| reward_torque      | -0.189   |
| reward_velocity    | -0.509   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -79.2    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 111      |
|    time_elapsed    | 1239     |
|    total timesteps | 137965   |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 132960   |
---------------------------------
Num timesteps: 138000
Best mean reward: -65.27 - Last mean reward per episode: -79.23
---------------------------------
| reward             | -0.0486  |
| reward_contact     | -0.0143  |
| reward_motion      | 0.67     |
| reward_torque      | -0.188   |
| reward_velocity    | -0.516   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -92.9    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 111      |
|    time_elapsed    | 1277     |
|    total timesteps | 142061   |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.001    |
|    n_updates       | 137060   |
---------------------------------
Num timesteps: 144000
Best mean reward: -65.27 - Last mean reward per episode: -103.85
---------------------------------
| reward             | -0.0552  |
| reward_contact     | -0.014   |
| reward_motion      | 0.67     |
| reward_torque      | -0.19    |
| reward_velocity    | -0.521   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 111      |
|    time_elapsed    | 1315     |
|    total timesteps | 146157   |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 2.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 141155   |
---------------------------------
Num timesteps: 150000
Best mean reward: -65.27 - Last mean reward per episode: -118.92
---------------------------------
| reward             | -0.0609  |
| reward_contact     | -0.014   |
| reward_motion      | 0.67     |
| reward_torque      | -0.188   |
| reward_velocity    | -0.529   |
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 110      |
|    time_elapsed    | 1354     |
|    total timesteps | 150253   |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 145250   |
---------------------------------
---------------------------------
| reward             | -0.0786  |
| reward_contact     | -0.0135  |
| reward_motion      | 0.66     |
| reward_torque      | -0.193   |
| reward_velocity    | -0.532   |
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | -120     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 110      |
|    time_elapsed    | 1391     |
|    total timesteps | 154349   |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.001    |
|    n_updates       | 149345   |
---------------------------------
Num timesteps: 156000
Best mean reward: -65.27 - Last mean reward per episode: -122.42
---------------------------------
| reward             | -0.0893  |
| reward_contact     | -0.0138  |
| reward_motion      | 0.65     |
| reward_torque      | -0.187   |
| reward_velocity    | -0.538   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -123     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 110      |
|    time_elapsed    | 1430     |
|    total timesteps | 158445   |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 1.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 153440   |
---------------------------------
Num timesteps: 162000
Best mean reward: -65.27 - Last mean reward per episode: -130.94
---------------------------------
| reward             | -0.0855  |
| reward_contact     | -0.0142  |
| reward_motion      | 0.64     |
| reward_torque      | -0.183   |
| reward_velocity    | -0.529   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -132     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 110      |
|    time_elapsed    | 1468     |
|    total timesteps | 162541   |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 157540   |
---------------------------------
---------------------------------
| reward             | -0.103   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.61     |
| reward_torque      | -0.174   |
| reward_velocity    | -0.525   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 110      |
|    time_elapsed    | 1507     |
|    total timesteps | 166637   |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 161635   |
---------------------------------
Num timesteps: 168000
Best mean reward: -65.27 - Last mean reward per episode: -144.92
---------------------------------
| reward             | -0.114   |
| reward_contact     | -0.0149  |
| reward_motion      | 0.6      |
| reward_torque      | -0.168   |
| reward_velocity    | -0.53    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -149     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 110      |
|    time_elapsed    | 1545     |
|    total timesteps | 170733   |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.001    |
|    n_updates       | 165730   |
---------------------------------
Num timesteps: 174000
Best mean reward: -65.27 - Last mean reward per episode: -152.42
---------------------------------
| reward             | -0.104   |
| reward_contact     | -0.0149  |
| reward_motion      | 0.6      |
| reward_torque      | -0.166   |
| reward_velocity    | -0.523   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -156     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 110      |
|    time_elapsed    | 1583     |
|    total timesteps | 174829   |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 169825   |
---------------------------------
---------------------------------
| reward             | -0.0953  |
| reward_contact     | -0.0145  |
| reward_motion      | 0.61     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.528   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -153     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 110      |
|    time_elapsed    | 1621     |
|    total timesteps | 178925   |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.001    |
|    n_updates       | 173920   |
---------------------------------
Num timesteps: 180000
Best mean reward: -65.27 - Last mean reward per episode: -149.40
---------------------------------
| reward             | -0.0988  |
| reward_contact     | -0.0148  |
| reward_motion      | 0.59     |
| reward_torque      | -0.16    |
| reward_velocity    | -0.514   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -147     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 110      |
|    time_elapsed    | 1658     |
|    total timesteps | 183021   |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 178020   |
---------------------------------
Num timesteps: 186000
Best mean reward: -65.27 - Last mean reward per episode: -156.90
---------------------------------
| reward             | -0.124   |
| reward_contact     | -0.015   |
| reward_motion      | 0.57     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.523   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 110      |
|    time_elapsed    | 1697     |
|    total timesteps | 187117   |
| train/             |          |
|    actor_loss      | -6.86    |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 182115   |
---------------------------------
---------------------------------
| reward             | -0.132   |
| reward_contact     | -0.0151  |
| reward_motion      | 0.56     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.521   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 110      |
|    time_elapsed    | 1735     |
|    total timesteps | 191213   |
| train/             |          |
|    actor_loss      | -7.2     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 186210   |
---------------------------------
Num timesteps: 192000
Best mean reward: -65.27 - Last mean reward per episode: -154.19
---------------------------------
| reward             | -0.141   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.55     |
| reward_torque      | -0.148   |
| reward_velocity    | -0.528   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -153     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 110      |
|    time_elapsed    | 1764     |
|    total timesteps | 194352   |
| train/             |          |
|    actor_loss      | -6.69    |
|    critic_loss     | 1.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 189350   |
---------------------------------
Num timesteps: 198000
Best mean reward: -65.27 - Last mean reward per episode: -163.68
---------------------------------
| reward             | -0.157   |
| reward_contact     | -0.0146  |
| reward_motion      | 0.54     |
| reward_torque      | -0.144   |
| reward_velocity    | -0.539   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -161     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 110      |
|    time_elapsed    | 1803     |
|    total timesteps | 198448   |
| train/             |          |
|    actor_loss      | -4.83    |
|    critic_loss     | 1.78     |
|    learning_rate   | 0.001    |
|    n_updates       | 193445   |
---------------------------------
---------------------------------
| reward             | -0.155   |
| reward_contact     | -0.015   |
| reward_motion      | 0.53     |
| reward_torque      | -0.141   |
| reward_velocity    | -0.528   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -165     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 109      |
|    time_elapsed    | 1841     |
|    total timesteps | 202544   |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.001    |
|    n_updates       | 197540   |
---------------------------------
Num timesteps: 204000
Best mean reward: -65.27 - Last mean reward per episode: -163.33
---------------------------------
| reward             | -0.173   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.52     |
| reward_torque      | -0.141   |
| reward_velocity    | -0.537   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -168     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 109      |
|    time_elapsed    | 1879     |
|    total timesteps | 206640   |
| train/             |          |
|    actor_loss      | -5.81    |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 201635   |
---------------------------------
Num timesteps: 210000
Best mean reward: -65.27 - Last mean reward per episode: -170.98
---------------------------------
| reward             | -0.182   |
| reward_contact     | -0.0146  |
| reward_motion      | 0.51     |
| reward_torque      | -0.137   |
| reward_velocity    | -0.54    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -178     |
| time/              |          |
|    episodes        | 212      |
|    fps             | 109      |
|    time_elapsed    | 1918     |
|    total timesteps | 210736   |
| train/             |          |
|    actor_loss      | -2.75    |
|    critic_loss     | 1.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 205735   |
---------------------------------
---------------------------------
| reward             | -0.217   |
| reward_contact     | -0.0143  |
| reward_motion      | 0.49     |
| reward_torque      | -0.141   |
| reward_velocity    | -0.552   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -177     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 109      |
|    time_elapsed    | 1956     |
|    total timesteps | 214832   |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 209830   |
---------------------------------
Num timesteps: 216000
Best mean reward: -65.27 - Last mean reward per episode: -182.27
---------------------------------
| reward             | -0.231   |
| reward_contact     | -0.0153  |
| reward_motion      | 0.49     |
| reward_torque      | -0.14    |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 220      |
|    fps             | 109      |
|    time_elapsed    | 1994     |
|    total timesteps | 218928   |
| train/             |          |
|    actor_loss      | -3.19    |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 213925   |
---------------------------------
Num timesteps: 222000
Best mean reward: -65.27 - Last mean reward per episode: -182.68
---------------------------------
| reward             | -0.239   |
| reward_contact     | -0.0152  |
| reward_motion      | 0.48     |
| reward_torque      | -0.143   |
| reward_velocity    | -0.561   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -184     |
| time/              |          |
|    episodes        | 224      |
|    fps             | 109      |
|    time_elapsed    | 2032     |
|    total timesteps | 223024   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 218020   |
---------------------------------
---------------------------------
| reward             | -0.257   |
| reward_contact     | -0.0155  |
| reward_motion      | 0.45     |
| reward_torque      | -0.139   |
| reward_velocity    | -0.552   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -193     |
| time/              |          |
|    episodes        | 228      |
|    fps             | 109      |
|    time_elapsed    | 2071     |
|    total timesteps | 227120   |
| train/             |          |
|    actor_loss      | -2.63    |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.001    |
|    n_updates       | 222115   |
---------------------------------
Num timesteps: 228000
Best mean reward: -65.27 - Last mean reward per episode: -193.08
---------------------------------
| reward             | -0.278   |
| reward_contact     | -0.0159  |
| reward_motion      | 0.43     |
| reward_torque      | -0.136   |
| reward_velocity    | -0.556   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -180     |
| time/              |          |
|    episodes        | 232      |
|    fps             | 109      |
|    time_elapsed    | 2110     |
|    total timesteps | 231216   |
| train/             |          |
|    actor_loss      | -2.01    |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 226215   |
---------------------------------
Num timesteps: 234000
Best mean reward: -65.27 - Last mean reward per episode: -186.81
---------------------------------
| reward             | -0.308   |
| reward_contact     | -0.0157  |
| reward_motion      | 0.43     |
| reward_torque      | -0.145   |
| reward_velocity    | -0.577   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 236      |
|    fps             | 109      |
|    time_elapsed    | 2147     |
|    total timesteps | 235312   |
| train/             |          |
|    actor_loss      | -0.139   |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 230310   |
---------------------------------
---------------------------------
| reward             | -0.306   |
| reward_contact     | -0.0165  |
| reward_motion      | 0.43     |
| reward_torque      | -0.147   |
| reward_velocity    | -0.572   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -173     |
| time/              |          |
|    episodes        | 240      |
|    fps             | 109      |
|    time_elapsed    | 2176     |
|    total timesteps | 238415   |
| train/             |          |
|    actor_loss      | 0.151    |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 233410   |
---------------------------------
Num timesteps: 240000
Best mean reward: -65.27 - Last mean reward per episode: -168.57
---------------------------------
| reward             | -0.294   |
| reward_contact     | -0.0174  |
| reward_motion      | 0.44     |
| reward_torque      | -0.147   |
| reward_velocity    | -0.57    |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -172     |
| time/              |          |
|    episodes        | 244      |
|    fps             | 109      |
|    time_elapsed    | 2214     |
|    total timesteps | 242511   |
| train/             |          |
|    actor_loss      | 0.517    |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.001    |
|    n_updates       | 237510   |
---------------------------------
Num timesteps: 246000
Best mean reward: -65.27 - Last mean reward per episode: -148.91
---------------------------------
| reward             | -0.26    |
| reward_contact     | -0.0176  |
| reward_motion      | 0.46     |
| reward_torque      | -0.147   |
| reward_velocity    | -0.556   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 248      |
|    fps             | 109      |
|    time_elapsed    | 2252     |
|    total timesteps | 246607   |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 241605   |
---------------------------------
---------------------------------
| reward             | -0.258   |
| reward_contact     | -0.0175  |
| reward_motion      | 0.46     |
| reward_torque      | -0.152   |
| reward_velocity    | -0.549   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -137     |
| time/              |          |
|    episodes        | 252      |
|    fps             | 109      |
|    time_elapsed    | 2290     |
|    total timesteps | 250703   |
| train/             |          |
|    actor_loss      | 0.736    |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 245700   |
---------------------------------
Num timesteps: 252000
Best mean reward: -65.27 - Last mean reward per episode: -126.18
---------------------------------
| reward             | -0.229   |
| reward_contact     | -0.017   |
| reward_motion      | 0.48     |
| reward_torque      | -0.154   |
| reward_velocity    | -0.538   |
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    episodes        | 256      |
|    fps             | 109      |
|    time_elapsed    | 2319     |
|    total timesteps | 253820   |
| train/             |          |
|    actor_loss      | 0.794    |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 248815   |
---------------------------------
---------------------------------
| reward             | -0.245   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.47     |
| reward_torque      | -0.149   |
| reward_velocity    | -0.549   |
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | -142     |
| time/              |          |
|    episodes        | 260      |
|    fps             | 109      |
|    time_elapsed    | 2358     |
|    total timesteps | 257916   |
| train/             |          |
|    actor_loss      | 1.53     |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 252915   |
---------------------------------
Num timesteps: 258000
Best mean reward: -65.27 - Last mean reward per episode: -141.69
---------------------------------
| reward             | -0.254   |
| reward_contact     | -0.0169  |
| reward_motion      | 0.45     |
| reward_torque      | -0.147   |
| reward_velocity    | -0.541   |
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 264      |
|    fps             | 109      |
|    time_elapsed    | 2396     |
|    total timesteps | 262012   |
| train/             |          |
|    actor_loss      | 1.83     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 257010   |
---------------------------------
Num timesteps: 264000
Best mean reward: -65.27 - Last mean reward per episode: -135.97
---------------------------------
| reward             | -0.273   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.44     |
| reward_torque      | -0.148   |
| reward_velocity    | -0.548   |
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    episodes        | 268      |
|    fps             | 109      |
|    time_elapsed    | 2434     |
|    total timesteps | 266108   |
| train/             |          |
|    actor_loss      | 2.36     |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 261105   |
---------------------------------
Num timesteps: 270000
Best mean reward: -65.27 - Last mean reward per episode: -125.10
---------------------------------
| reward             | -0.263   |
| reward_contact     | -0.017   |
| reward_motion      | 0.45     |
| reward_torque      | -0.154   |
| reward_velocity    | -0.542   |
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    episodes        | 272      |
|    fps             | 109      |
|    time_elapsed    | 2472     |
|    total timesteps | 270204   |
| train/             |          |
|    actor_loss      | 2.48     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 265200   |
---------------------------------
---------------------------------
| reward             | -0.277   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.46     |
| reward_torque      | -0.157   |
| reward_velocity    | -0.564   |
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 276      |
|    fps             | 109      |
|    time_elapsed    | 2501     |
|    total timesteps | 273299   |
| train/             |          |
|    actor_loss      | 2.44     |
|    critic_loss     | 1.46     |
|    learning_rate   | 0.001    |
|    n_updates       | 268295   |
---------------------------------
Num timesteps: 276000
Best mean reward: -65.27 - Last mean reward per episode: -138.53
---------------------------------
| reward             | -0.29    |
| reward_contact     | -0.0168  |
| reward_motion      | 0.45     |
| reward_torque      | -0.159   |
| reward_velocity    | -0.565   |
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    episodes        | 280      |
|    fps             | 109      |
|    time_elapsed    | 2539     |
|    total timesteps | 277395   |
| train/             |          |
|    actor_loss      | 0.878    |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 272390   |
---------------------------------
---------------------------------
| reward             | -0.296   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.46     |
| reward_torque      | -0.164   |
| reward_velocity    | -0.576   |
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 284      |
|    fps             | 109      |
|    time_elapsed    | 2577     |
|    total timesteps | 281491   |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.001    |
|    n_updates       | 276490   |
---------------------------------
Num timesteps: 282000
Best mean reward: -65.27 - Last mean reward per episode: -145.07
---------------------------------
| reward             | -0.294   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.48     |
| reward_torque      | -0.168   |
| reward_velocity    | -0.589   |
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    episodes        | 288      |
|    fps             | 109      |
|    time_elapsed    | 2615     |
|    total timesteps | 285587   |
| train/             |          |
|    actor_loss      | 4.14     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 280585   |
---------------------------------
Num timesteps: 288000
Best mean reward: -65.27 - Last mean reward per episode: -154.41
---------------------------------
| reward             | -0.283   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.49     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.587   |
| rollout/           |          |
|    ep_len_mean     | 975      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 292      |
|    fps             | 109      |
|    time_elapsed    | 2643     |
|    total timesteps | 288694   |
| train/             |          |
|    actor_loss      | 1.55     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 283690   |
---------------------------------
---------------------------------
| reward             | -0.261   |
| reward_contact     | -0.0176  |
| reward_motion      | 0.49     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 109      |
|    time_elapsed    | 2682     |
|    total timesteps | 292790   |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.001    |
|    n_updates       | 287785   |
---------------------------------
Num timesteps: 294000
Best mean reward: -65.27 - Last mean reward per episode: -152.34
---------------------------------
| reward             | -0.268   |
| reward_contact     | -0.0181  |
| reward_motion      | 0.49     |
| reward_torque      | -0.17    |
| reward_velocity    | -0.57    |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -161     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 109      |
|    time_elapsed    | 2720     |
|    total timesteps | 296886   |
| train/             |          |
|    actor_loss      | 3.11     |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 291885   |
---------------------------------
Num timesteps: 300000
Best mean reward: -65.27 - Last mean reward per episode: -168.45
---------------------------------
| reward             | -0.269   |
| reward_contact     | -0.0178  |
| reward_motion      | 0.5      |
| reward_torque      | -0.175   |
| reward_velocity    | -0.576   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 109      |
|    time_elapsed    | 2758     |
|    total timesteps | 300982   |
| train/             |          |
|    actor_loss      | 3.4      |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 295980   |
---------------------------------
---------------------------------
| reward             | -0.247   |
| reward_contact     | -0.0175  |
| reward_motion      | 0.52     |
| reward_torque      | -0.181   |
| reward_velocity    | -0.569   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 109      |
|    time_elapsed    | 2797     |
|    total timesteps | 305078   |
| train/             |          |
|    actor_loss      | 1.6      |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 300075   |
---------------------------------
Num timesteps: 306000
Best mean reward: -65.27 - Last mean reward per episode: -156.66
---------------------------------
| reward             | -0.233   |
| reward_contact     | -0.0184  |
| reward_motion      | 0.53     |
| reward_torque      | -0.179   |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -140     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 109      |
|    time_elapsed    | 2835     |
|    total timesteps | 309174   |
| train/             |          |
|    actor_loss      | 3.25     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 304170   |
---------------------------------
Num timesteps: 312000
Best mean reward: -65.27 - Last mean reward per episode: -144.82
---------------------------------
| reward             | -0.194   |
| reward_contact     | -0.0183  |
| reward_motion      | 0.57     |
| reward_torque      | -0.176   |
| reward_velocity    | -0.569   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -146     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 108      |
|    time_elapsed    | 2874     |
|    total timesteps | 313270   |
| train/             |          |
|    actor_loss      | 1.92     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 308265   |
---------------------------------
---------------------------------
| reward             | -0.167   |
| reward_contact     | -0.0179  |
| reward_motion      | 0.58     |
| reward_torque      | -0.178   |
| reward_velocity    | -0.55    |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -125     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 108      |
|    time_elapsed    | 2912     |
|    total timesteps | 317366   |
| train/             |          |
|    actor_loss      | 5.75     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.001    |
|    n_updates       | 312365   |
---------------------------------
Num timesteps: 318000
Best mean reward: -65.27 - Last mean reward per episode: -124.95
---------------------------------
| reward             | -0.152   |
| reward_contact     | -0.0181  |
| reward_motion      | 0.57     |
| reward_torque      | -0.174   |
| reward_velocity    | -0.53    |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 108      |
|    time_elapsed    | 2949     |
|    total timesteps | 321462   |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 316460   |
---------------------------------
Num timesteps: 324000
Best mean reward: -65.27 - Last mean reward per episode: -98.11
---------------------------------
| reward             | -0.127   |
| reward_contact     | -0.0182  |
| reward_motion      | 0.58     |
| reward_torque      | -0.173   |
| reward_velocity    | -0.516   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -95.6    |
| time/              |          |
|    episodes        | 328      |
|    fps             | 108      |
|    time_elapsed    | 2987     |
|    total timesteps | 325558   |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 320555   |
---------------------------------
---------------------------------
| reward             | -0.126   |
| reward_contact     | -0.0175  |
| reward_motion      | 0.58     |
| reward_torque      | -0.172   |
| reward_velocity    | -0.516   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -89.2    |
| time/              |          |
|    episodes        | 332      |
|    fps             | 108      |
|    time_elapsed    | 3026     |
|    total timesteps | 329654   |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.001    |
|    n_updates       | 324650   |
---------------------------------
Num timesteps: 330000
Best mean reward: -65.27 - Last mean reward per episode: -89.20
---------------------------------
| reward             | -0.138   |
| reward_contact     | -0.0176  |
| reward_motion      | 0.55     |
| reward_torque      | -0.171   |
| reward_velocity    | -0.499   |
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    episodes        | 336      |
|    fps             | 108      |
|    time_elapsed    | 3064     |
|    total timesteps | 333750   |
| train/             |          |
|    actor_loss      | 1.02     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 328745   |
---------------------------------
Num timesteps: 336000
Best mean reward: -65.27 - Last mean reward per episode: -91.54
---------------------------------
| reward             | -0.121   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.57     |
| reward_torque      | -0.17    |
| reward_velocity    | -0.503   |
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | -93.4    |
| time/              |          |
|    episodes        | 340      |
|    fps             | 108      |
|    time_elapsed    | 3102     |
|    total timesteps | 337846   |
| train/             |          |
|    actor_loss      | 1.95     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 332845   |
---------------------------------
---------------------------------
| reward             | -0.127   |
| reward_contact     | -0.0164  |
| reward_motion      | 0.57     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.506   |
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | -83.4    |
| time/              |          |
|    episodes        | 344      |
|    fps             | 108      |
|    time_elapsed    | 3138     |
|    total timesteps | 341942   |
| train/             |          |
|    actor_loss      | 3.85     |
|    critic_loss     | 1.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 336940   |
---------------------------------
Num timesteps: 342000
Best mean reward: -65.27 - Last mean reward per episode: -83.35
---------------------------------
| reward             | -0.149   |
| reward_contact     | -0.017   |
| reward_motion      | 0.55     |
| reward_torque      | -0.171   |
| reward_velocity    | -0.511   |
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | -89.3    |
| time/              |          |
|    episodes        | 348      |
|    fps             | 109      |
|    time_elapsed    | 3171     |
|    total timesteps | 346038   |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 341035   |
---------------------------------
Num timesteps: 348000
Best mean reward: -65.27 - Last mean reward per episode: -98.04
---------------------------------
| reward             | -0.152   |
| reward_contact     | -0.0169  |
| reward_motion      | 0.56     |
| reward_torque      | -0.165   |
| reward_velocity    | -0.53    |
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 109      |
|    time_elapsed    | 3204     |
|    total timesteps | 350134   |
| train/             |          |
|    actor_loss      | 5.85     |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.001    |
|    n_updates       | 345130   |
---------------------------------
Num timesteps: 354000
Best mean reward: -65.27 - Last mean reward per episode: -102.45
---------------------------------
| reward             | -0.143   |
| reward_contact     | -0.0169  |
| reward_motion      | 0.57     |
| reward_torque      | -0.164   |
| reward_velocity    | -0.532   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -95.9    |
| time/              |          |
|    episodes        | 356      |
|    fps             | 109      |
|    time_elapsed    | 3237     |
|    total timesteps | 354230   |
| train/             |          |
|    actor_loss      | 1.13     |
|    critic_loss     | 1.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 349225   |
---------------------------------
---------------------------------
| reward             | -0.117   |
| reward_contact     | -0.0177  |
| reward_motion      | 0.6      |
| reward_torque      | -0.172   |
| reward_velocity    | -0.527   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -77.9    |
| time/              |          |
|    episodes        | 360      |
|    fps             | 109      |
|    time_elapsed    | 3269     |
|    total timesteps | 358326   |
| train/             |          |
|    actor_loss      | 6.51     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 353325   |
---------------------------------
Num timesteps: 360000
Best mean reward: -65.27 - Last mean reward per episode: -70.19
---------------------------------
| reward             | -0.108   |
| reward_contact     | -0.0173  |
| reward_motion      | 0.6      |
| reward_torque      | -0.171   |
| reward_velocity    | -0.52    |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -66.9    |
| time/              |          |
|    episodes        | 364      |
|    fps             | 109      |
|    time_elapsed    | 3302     |
|    total timesteps | 362422   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 357420   |
---------------------------------
Num timesteps: 366000
Best mean reward: -65.27 - Last mean reward per episode: -53.30
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0652  |
| reward_contact     | -0.017   |
| reward_motion      | 0.63     |
| reward_torque      | -0.176   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -59.8    |
| time/              |          |
|    episodes        | 368      |
|    fps             | 109      |
|    time_elapsed    | 3335     |
|    total timesteps | 366518   |
| train/             |          |
|    actor_loss      | 3.41     |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 361515   |
---------------------------------
---------------------------------
| reward             | -0.0924  |
| reward_contact     | -0.017   |
| reward_motion      | 0.6      |
| reward_torque      | -0.174   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -45.8    |
| time/              |          |
|    episodes        | 372      |
|    fps             | 110      |
|    time_elapsed    | 3368     |
|    total timesteps | 370614   |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 365610   |
---------------------------------
Num timesteps: 372000
Best mean reward: -53.30 - Last mean reward per episode: -46.69
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0999  |
| reward_contact     | -0.0174  |
| reward_motion      | 0.57     |
| reward_torque      | -0.17    |
| reward_velocity    | -0.482   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    episodes        | 376      |
|    fps             | 110      |
|    time_elapsed    | 3401     |
|    total timesteps | 374710   |
| train/             |          |
|    actor_loss      | 4.22     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 369705   |
---------------------------------
Num timesteps: 378000
Best mean reward: -46.69 - Last mean reward per episode: -34.42
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0909  |
| reward_contact     | -0.0178  |
| reward_motion      | 0.57     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.474   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -33.1    |
| time/              |          |
|    episodes        | 380      |
|    fps             | 110      |
|    time_elapsed    | 3434     |
|    total timesteps | 378806   |
| train/             |          |
|    actor_loss      | 4.19     |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 373805   |
---------------------------------
---------------------------------
| reward             | -0.0941  |
| reward_contact     | -0.0179  |
| reward_motion      | 0.56     |
| reward_torque      | -0.168   |
| reward_velocity    | -0.469   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -28.9    |
| time/              |          |
|    episodes        | 384      |
|    fps             | 110      |
|    time_elapsed    | 3467     |
|    total timesteps | 382902   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 377900   |
---------------------------------
Num timesteps: 384000
Best mean reward: -34.42 - Last mean reward per episode: -22.22
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.065   |
| reward_contact     | -0.0179  |
| reward_motion      | 0.57     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.45    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -6.75    |
| time/              |          |
|    episodes        | 388      |
|    fps             | 110      |
|    time_elapsed    | 3500     |
|    total timesteps | 386998   |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 381995   |
---------------------------------
Num timesteps: 390000
Best mean reward: -22.22 - Last mean reward per episode: -18.94
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0716  |
| reward_contact     | -0.0178  |
| reward_motion      | 0.57     |
| reward_torque      | -0.165   |
| reward_velocity    | -0.459   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -17.7    |
| time/              |          |
|    episodes        | 392      |
|    fps             | 110      |
|    time_elapsed    | 3533     |
|    total timesteps | 391094   |
| train/             |          |
|    actor_loss      | 2.56     |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 386090   |
---------------------------------
---------------------------------
| reward             | -0.0971  |
| reward_contact     | -0.0173  |
| reward_motion      | 0.56     |
| reward_torque      | -0.168   |
| reward_velocity    | -0.472   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    episodes        | 396      |
|    fps             | 110      |
|    time_elapsed    | 3566     |
|    total timesteps | 395190   |
| train/             |          |
|    actor_loss      | 4.07     |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 390185   |
---------------------------------
Num timesteps: 396000
Best mean reward: -18.94 - Last mean reward per episode: -13.45
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0791  |
| reward_contact     | -0.0166  |
| reward_motion      | 0.57     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.463   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -3.55    |
| time/              |          |
|    episodes        | 400      |
|    fps             | 110      |
|    time_elapsed    | 3599     |
|    total timesteps | 399286   |
| train/             |          |
|    actor_loss      | 6.8      |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 394285   |
---------------------------------
Num timesteps: 402000
Best mean reward: -13.45 - Last mean reward per episode: 7.46
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.088   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.57     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.466   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    episodes        | 404      |
|    fps             | 111      |
|    time_elapsed    | 3632     |
|    total timesteps | 403382   |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 398380   |
---------------------------------
---------------------------------
| reward             | -0.0819  |
| reward_contact     | -0.0175  |
| reward_motion      | 0.56     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.455   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 22.2     |
| time/              |          |
|    episodes        | 408      |
|    fps             | 111      |
|    time_elapsed    | 3665     |
|    total timesteps | 407478   |
| train/             |          |
|    actor_loss      | 2.93     |
|    critic_loss     | 1.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 402475   |
---------------------------------
Num timesteps: 408000
Best mean reward: 7.46 - Last mean reward per episode: 22.15
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0945  |
| reward_contact     | -0.0166  |
| reward_motion      | 0.57     |
| reward_torque      | -0.177   |
| reward_velocity    | -0.471   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 4.59     |
| time/              |          |
|    episodes        | 412      |
|    fps             | 111      |
|    time_elapsed    | 3698     |
|    total timesteps | 411574   |
| train/             |          |
|    actor_loss      | 3.81     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 406570   |
---------------------------------
Num timesteps: 414000
Best mean reward: 22.15 - Last mean reward per episode: 12.01
---------------------------------
| reward             | -0.0945  |
| reward_contact     | -0.0167  |
| reward_motion      | 0.56     |
| reward_torque      | -0.174   |
| reward_velocity    | -0.463   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    episodes        | 416      |
|    fps             | 111      |
|    time_elapsed    | 3731     |
|    total timesteps | 415670   |
| train/             |          |
|    actor_loss      | 3.94     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 410665   |
---------------------------------
---------------------------------
| reward             | -0.121   |
| reward_contact     | -0.0165  |
| reward_motion      | 0.55     |
| reward_torque      | -0.174   |
| reward_velocity    | -0.481   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -3.2     |
| time/              |          |
|    episodes        | 420      |
|    fps             | 111      |
|    time_elapsed    | 3764     |
|    total timesteps | 419766   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 414765   |
---------------------------------
Num timesteps: 420000
Best mean reward: 22.15 - Last mean reward per episode: -3.20
---------------------------------
| reward             | -0.127   |
| reward_contact     | -0.016   |
| reward_motion      | 0.56     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.496   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    episodes        | 424      |
|    fps             | 111      |
|    time_elapsed    | 3797     |
|    total timesteps | 423862   |
| train/             |          |
|    actor_loss      | 4.28     |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 418860   |
---------------------------------
Num timesteps: 426000
Best mean reward: 22.15 - Last mean reward per episode: -9.38
---------------------------------
| reward             | -0.134   |
| reward_contact     | -0.0162  |
| reward_motion      | 0.57     |
| reward_torque      | -0.178   |
| reward_velocity    | -0.511   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    episodes        | 428      |
|    fps             | 111      |
|    time_elapsed    | 3829     |
|    total timesteps | 427958   |
| train/             |          |
|    actor_loss      | 3.62     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 422955   |
---------------------------------
Num timesteps: 432000
Best mean reward: 22.15 - Last mean reward per episode: -15.96
---------------------------------
| reward             | -0.119   |
| reward_contact     | -0.0161  |
| reward_motion      | 0.59     |
| reward_torque      | -0.179   |
| reward_velocity    | -0.513   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    episodes        | 432      |
|    fps             | 111      |
|    time_elapsed    | 3862     |
|    total timesteps | 432054   |
| train/             |          |
|    actor_loss      | 1.45     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 427050   |
---------------------------------
---------------------------------
| reward             | -0.0784  |
| reward_contact     | -0.016   |
| reward_motion      | 0.62     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.508   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    episodes        | 436      |
|    fps             | 111      |
|    time_elapsed    | 3895     |
|    total timesteps | 436150   |
| train/             |          |
|    actor_loss      | 3.43     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 431145   |
---------------------------------
Num timesteps: 438000
Best mean reward: 22.15 - Last mean reward per episode: -16.67
---------------------------------
| reward             | -0.106   |
| reward_contact     | -0.0157  |
| reward_motion      | 0.61     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.525   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -25.7    |
| time/              |          |
|    episodes        | 440      |
|    fps             | 112      |
|    time_elapsed    | 3928     |
|    total timesteps | 440246   |
| train/             |          |
|    actor_loss      | 3.26     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.001    |
|    n_updates       | 435245   |
---------------------------------
Num timesteps: 444000
Best mean reward: 22.15 - Last mean reward per episode: -25.50
---------------------------------
| reward             | -0.0974  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.62     |
| reward_torque      | -0.173   |
| reward_velocity    | -0.53    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -26.1    |
| time/              |          |
|    episodes        | 444      |
|    fps             | 112      |
|    time_elapsed    | 3961     |
|    total timesteps | 444342   |
| train/             |          |
|    actor_loss      | 3.54     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 439340   |
---------------------------------
---------------------------------
| reward             | -0.0954  |
| reward_contact     | -0.0145  |
| reward_motion      | 0.62     |
| reward_torque      | -0.172   |
| reward_velocity    | -0.529   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    episodes        | 448      |
|    fps             | 112      |
|    time_elapsed    | 3994     |
|    total timesteps | 448438   |
| train/             |          |
|    actor_loss      | 2.16     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 443435   |
---------------------------------
Num timesteps: 450000
Best mean reward: 22.15 - Last mean reward per episode: -9.64
---------------------------------
| reward             | -0.0868  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.62     |
| reward_torque      | -0.171   |
| reward_velocity    | -0.521   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -2.38    |
| time/              |          |
|    episodes        | 452      |
|    fps             | 112      |
|    time_elapsed    | 4026     |
|    total timesteps | 452534   |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 447530   |
---------------------------------
Num timesteps: 456000
Best mean reward: 22.15 - Last mean reward per episode: -2.27
---------------------------------
| reward             | -0.077   |
| reward_contact     | -0.0162  |
| reward_motion      | 0.62     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.514   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 456      |
|    fps             | 112      |
|    time_elapsed    | 4059     |
|    total timesteps | 456630   |
| train/             |          |
|    actor_loss      | 5.69     |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 451625   |
---------------------------------
---------------------------------
| reward             | -0.083   |
| reward_contact     | -0.0154  |
| reward_motion      | 0.6      |
| reward_torque      | -0.165   |
| reward_velocity    | -0.502   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -7.3     |
| time/              |          |
|    episodes        | 460      |
|    fps             | 112      |
|    time_elapsed    | 4092     |
|    total timesteps | 460726   |
| train/             |          |
|    actor_loss      | 1.84     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 455725   |
---------------------------------
Num timesteps: 462000
Best mean reward: 22.15 - Last mean reward per episode: -13.05
---------------------------------
| reward             | -0.0958  |
| reward_contact     | -0.0155  |
| reward_motion      | 0.61     |
| reward_torque      | -0.168   |
| reward_velocity    | -0.522   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    episodes        | 464      |
|    fps             | 112      |
|    time_elapsed    | 4124     |
|    total timesteps | 464822   |
| train/             |          |
|    actor_loss      | 1.57     |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 459820   |
---------------------------------
Num timesteps: 468000
Best mean reward: 22.15 - Last mean reward per episode: -26.28
---------------------------------
| reward             | -0.11    |
| reward_contact     | -0.0151  |
| reward_motion      | 0.61     |
| reward_torque      | -0.166   |
| reward_velocity    | -0.539   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -22      |
| time/              |          |
|    episodes        | 468      |
|    fps             | 112      |
|    time_elapsed    | 4157     |
|    total timesteps | 468918   |
| train/             |          |
|    actor_loss      | 3.03     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 463915   |
---------------------------------
---------------------------------
| reward             | -0.0796  |
| reward_contact     | -0.015   |
| reward_motion      | 0.64     |
| reward_torque      | -0.164   |
| reward_velocity    | -0.541   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -25.7    |
| time/              |          |
|    episodes        | 472      |
|    fps             | 112      |
|    time_elapsed    | 4190     |
|    total timesteps | 473014   |
| train/             |          |
|    actor_loss      | 4.89     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 468010   |
---------------------------------
Num timesteps: 474000
Best mean reward: 22.15 - Last mean reward per episode: -25.68
---------------------------------
| reward             | -0.0688  |
| reward_contact     | -0.0146  |
| reward_motion      | 0.67     |
| reward_torque      | -0.166   |
| reward_velocity    | -0.558   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -31.8    |
| time/              |          |
|    episodes        | 476      |
|    fps             | 112      |
|    time_elapsed    | 4223     |
|    total timesteps | 477110   |
| train/             |          |
|    actor_loss      | 4.67     |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 472105   |
---------------------------------
Num timesteps: 480000
Best mean reward: 22.15 - Last mean reward per episode: -28.91
---------------------------------
| reward             | -0.0476  |
| reward_contact     | -0.014   |
| reward_motion      | 0.69     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.556   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -24.5    |
| time/              |          |
|    episodes        | 480      |
|    fps             | 113      |
|    time_elapsed    | 4255     |
|    total timesteps | 481206   |
| train/             |          |
|    actor_loss      | 6.62     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 476205   |
---------------------------------
---------------------------------
| reward             | -0.0682  |
| reward_contact     | -0.015   |
| reward_motion      | 0.68     |
| reward_torque      | -0.163   |
| reward_velocity    | -0.57    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -39.3    |
| time/              |          |
|    episodes        | 484      |
|    fps             | 113      |
|    time_elapsed    | 4288     |
|    total timesteps | 485302   |
| train/             |          |
|    actor_loss      | 5.31     |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 480300   |
---------------------------------
Num timesteps: 486000
Best mean reward: 22.15 - Last mean reward per episode: -39.34
---------------------------------
| reward             | -0.0869  |
| reward_contact     | -0.015   |
| reward_motion      | 0.68     |
| reward_torque      | -0.166   |
| reward_velocity    | -0.586   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -48.2    |
| time/              |          |
|    episodes        | 488      |
|    fps             | 113      |
|    time_elapsed    | 4321     |
|    total timesteps | 489398   |
| train/             |          |
|    actor_loss      | 5.85     |
|    critic_loss     | 1.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 484395   |
---------------------------------
Num timesteps: 492000
Best mean reward: 22.15 - Last mean reward per episode: -36.99
---------------------------------
| reward             | -0.0729  |
| reward_contact     | -0.015   |
| reward_motion      | 0.69     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.579   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    episodes        | 492      |
|    fps             | 113      |
|    time_elapsed    | 4354     |
|    total timesteps | 493494   |
| train/             |          |
|    actor_loss      | 5.23     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 488490   |
---------------------------------
---------------------------------
| reward             | -0.0457  |
| reward_contact     | -0.015   |
| reward_motion      | 0.71     |
| reward_torque      | -0.17    |
| reward_velocity    | -0.571   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -31      |
| time/              |          |
|    episodes        | 496      |
|    fps             | 113      |
|    time_elapsed    | 4387     |
|    total timesteps | 497590   |
| train/             |          |
|    actor_loss      | 3.08     |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 492585   |
---------------------------------
Num timesteps: 498000
Best mean reward: 22.15 - Last mean reward per episode: -31.00
---------------------------------
| reward             | -0.035   |
| reward_contact     | -0.0153  |
| reward_motion      | 0.71     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.562   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 500      |
|    fps             | 113      |
|    time_elapsed    | 4419     |
|    total timesteps | 501686   |
| train/             |          |
|    actor_loss      | 2.16     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 496685   |
---------------------------------
Num timesteps: 504000
Best mean reward: 22.15 - Last mean reward per episode: -22.20
---------------------------------
| reward             | -0.0187  |
| reward_contact     | -0.0151  |
| reward_motion      | 0.72     |
| reward_torque      | -0.165   |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -25.4    |
| time/              |          |
|    episodes        | 504      |
|    fps             | 113      |
|    time_elapsed    | 4452     |
|    total timesteps | 505782   |
| train/             |          |
|    actor_loss      | 2.53     |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 500780   |
---------------------------------
---------------------------------
| reward             | -0.0123  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.73     |
| reward_torque      | -0.166   |
| reward_velocity    | -0.562   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    episodes        | 508      |
|    fps             | 113      |
|    time_elapsed    | 4485     |
|    total timesteps | 509878   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 504875   |
---------------------------------
Num timesteps: 510000
Best mean reward: 22.15 - Last mean reward per episode: -14.92
---------------------------------
| reward             | 0.00973  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.74     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.553   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    episodes        | 512      |
|    fps             | 113      |
|    time_elapsed    | 4517     |
|    total timesteps | 513974   |
| train/             |          |
|    actor_loss      | 3.56     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 508970   |
---------------------------------
Num timesteps: 516000
Best mean reward: 22.15 - Last mean reward per episode: 0.46
---------------------------------
| reward             | 0.00732  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.74     |
| reward_torque      | -0.164   |
| reward_velocity    | -0.554   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -1.32    |
| time/              |          |
|    episodes        | 516      |
|    fps             | 113      |
|    time_elapsed    | 4550     |
|    total timesteps | 518070   |
| train/             |          |
|    actor_loss      | 2.48     |
|    critic_loss     | 1.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 513065   |
---------------------------------
Num timesteps: 522000
Best mean reward: 22.15 - Last mean reward per episode: 7.22
---------------------------------
| reward             | 0.0293   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.76     |
| reward_torque      | -0.16    |
| reward_velocity    | -0.556   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 5.11     |
| time/              |          |
|    episodes        | 520      |
|    fps             | 113      |
|    time_elapsed    | 4583     |
|    total timesteps | 522166   |
| train/             |          |
|    actor_loss      | 3.22     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 517165   |
---------------------------------
---------------------------------
| reward             | 0.0476   |
| reward_contact     | -0.0147  |
| reward_motion      | 0.78     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.556   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 524      |
|    fps             | 114      |
|    time_elapsed    | 4615     |
|    total timesteps | 526262   |
| train/             |          |
|    actor_loss      | 1.86     |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.001    |
|    n_updates       | 521260   |
---------------------------------
Num timesteps: 528000
Best mean reward: 22.15 - Last mean reward per episode: 2.50
---------------------------------
| reward             | 0.0448   |
| reward_contact     | -0.0148  |
| reward_motion      | 0.77     |
| reward_torque      | -0.159   |
| reward_velocity    | -0.552   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -0.66    |
| time/              |          |
|    episodes        | 528      |
|    fps             | 114      |
|    time_elapsed    | 4641     |
|    total timesteps | 529448   |
| train/             |          |
|    actor_loss      | 2.99     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 524445   |
---------------------------------
---------------------------------
| reward             | 0.0156   |
| reward_contact     | -0.0148  |
| reward_motion      | 0.74     |
| reward_torque      | -0.157   |
| reward_velocity    | -0.553   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.12     |
| time/              |          |
|    episodes        | 532      |
|    fps             | 114      |
|    time_elapsed    | 4674     |
|    total timesteps | 533544   |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.001    |
|    n_updates       | 528540   |
---------------------------------
Num timesteps: 534000
Best mean reward: 22.15 - Last mean reward per episode: 7.12
---------------------------------
| reward             | 0.000193 |
| reward_contact     | -0.0162  |
| reward_motion      | 0.73     |
| reward_torque      | -0.155   |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | -0.143   |
| time/              |          |
|    episodes        | 536      |
|    fps             | 114      |
|    time_elapsed    | 4706     |
|    total timesteps | 537640   |
| train/             |          |
|    actor_loss      | 2.73     |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 532635   |
---------------------------------
Num timesteps: 540000
Best mean reward: 22.15 - Last mean reward per episode: 12.49
---------------------------------
| reward             | 0.0241   |
| reward_contact     | -0.0165  |
| reward_motion      | 0.73     |
| reward_torque      | -0.151   |
| reward_velocity    | -0.538   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    episodes        | 540      |
|    fps             | 114      |
|    time_elapsed    | 4739     |
|    total timesteps | 541736   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 536735   |
---------------------------------
---------------------------------
| reward             | 0.00249  |
| reward_contact     | -0.017   |
| reward_motion      | 0.71     |
| reward_torque      | -0.148   |
| reward_velocity    | -0.543   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 5.04     |
| time/              |          |
|    episodes        | 544      |
|    fps             | 114      |
|    time_elapsed    | 4772     |
|    total timesteps | 545832   |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 540830   |
---------------------------------
Num timesteps: 546000
Best mean reward: 22.15 - Last mean reward per episode: 5.04
---------------------------------
| reward             | 0.00905  |
| reward_contact     | -0.0165  |
| reward_motion      | 0.73     |
| reward_torque      | -0.15    |
| reward_velocity    | -0.554   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 9.45     |
| time/              |          |
|    episodes        | 548      |
|    fps             | 114      |
|    time_elapsed    | 4805     |
|    total timesteps | 549928   |
| train/             |          |
|    actor_loss      | 2.82     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 544925   |
---------------------------------
Num timesteps: 552000
Best mean reward: 22.15 - Last mean reward per episode: 12.48
---------------------------------
| reward             | 0.0177   |
| reward_contact     | -0.0169  |
| reward_motion      | 0.74     |
| reward_torque      | -0.155   |
| reward_velocity    | -0.55    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    episodes        | 552      |
|    fps             | 114      |
|    time_elapsed    | 4837     |
|    total timesteps | 554024   |
| train/             |          |
|    actor_loss      | 2.06     |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.001    |
|    n_updates       | 549020   |
---------------------------------
Num timesteps: 558000
Best mean reward: 22.15 - Last mean reward per episode: 14.27
---------------------------------
| reward             | 0.0022   |
| reward_contact     | -0.0155  |
| reward_motion      | 0.74     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.56    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    episodes        | 556      |
|    fps             | 114      |
|    time_elapsed    | 4870     |
|    total timesteps | 558120   |
| train/             |          |
|    actor_loss      | 1.25     |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 553115   |
---------------------------------
---------------------------------
| reward             | -0.0162  |
| reward_contact     | -0.0158  |
| reward_motion      | 0.73     |
| reward_torque      | -0.16    |
| reward_velocity    | -0.571   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 9.98     |
| time/              |          |
|    episodes        | 560      |
|    fps             | 114      |
|    time_elapsed    | 4903     |
|    total timesteps | 562216   |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.001    |
|    n_updates       | 557215   |
---------------------------------
Num timesteps: 564000
Best mean reward: 22.15 - Last mean reward per episode: 13.07
---------------------------------
| reward             | 0.00183  |
| reward_contact     | -0.0165  |
| reward_motion      | 0.73     |
| reward_torque      | -0.159   |
| reward_velocity    | -0.553   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    episodes        | 564      |
|    fps             | 114      |
|    time_elapsed    | 4935     |
|    total timesteps | 566312   |
| train/             |          |
|    actor_loss      | 0.167    |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.001    |
|    n_updates       | 561310   |
---------------------------------
Num timesteps: 570000
Best mean reward: 22.15 - Last mean reward per episode: 19.11
---------------------------------
| reward             | 0.00924  |
| reward_contact     | -0.0164  |
| reward_motion      | 0.73     |
| reward_torque      | -0.161   |
| reward_velocity    | -0.543   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    episodes        | 568      |
|    fps             | 114      |
|    time_elapsed    | 4968     |
|    total timesteps | 570408   |
| train/             |          |
|    actor_loss      | 1.15     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 565405   |
---------------------------------
---------------------------------
| reward             | 0.0156   |
| reward_contact     | -0.0162  |
| reward_motion      | 0.74     |
| reward_torque      | -0.169   |
| reward_velocity    | -0.54    |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 30.8     |
| time/              |          |
|    episodes        | 572      |
|    fps             | 114      |
|    time_elapsed    | 5001     |
|    total timesteps | 574504   |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 569500   |
---------------------------------
Num timesteps: 576000
Best mean reward: 22.15 - Last mean reward per episode: 33.45
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | 0.0452   |
| reward_contact     | -0.0167  |
| reward_motion      | 0.75     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.521   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 42.7     |
| time/              |          |
|    episodes        | 576      |
|    fps             | 114      |
|    time_elapsed    | 5034     |
|    total timesteps | 578600   |
| train/             |          |
|    actor_loss      | 1.78     |
|    critic_loss     | 2        |
|    learning_rate   | 0.001    |
|    n_updates       | 573595   |
---------------------------------
Num timesteps: 582000
Best mean reward: 33.45 - Last mean reward per episode: 35.80
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | 0.0328   |
| reward_contact     | -0.0174  |
| reward_motion      | 0.75     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.533   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 37.6     |
| time/              |          |
|    episodes        | 580      |
|    fps             | 114      |
|    time_elapsed    | 5067     |
|    total timesteps | 582696   |
| train/             |          |
|    actor_loss      | 2.62     |
|    critic_loss     | 1.78     |
|    learning_rate   | 0.001    |
|    n_updates       | 577695   |
---------------------------------
---------------------------------
| reward             | 0.0942   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.78     |
| reward_torque      | -0.168   |
| reward_velocity    | -0.501   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 65.5     |
| time/              |          |
|    episodes        | 584      |
|    fps             | 115      |
|    time_elapsed    | 5100     |
|    total timesteps | 586792   |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 581790   |
---------------------------------
Num timesteps: 588000
Best mean reward: 35.80 - Last mean reward per episode: 68.37
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | 0.0988   |
| reward_contact     | -0.0166  |
| reward_motion      | 0.77     |
| reward_torque      | -0.166   |
| reward_velocity    | -0.489   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 57.7     |
| time/              |          |
|    episodes        | 588      |
|    fps             | 115      |
|    time_elapsed    | 5133     |
|    total timesteps | 590888   |
| train/             |          |
|    actor_loss      | 1.78     |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 585885   |
---------------------------------
Num timesteps: 594000
Best mean reward: 68.37 - Last mean reward per episode: 59.30
---------------------------------
| reward             | 0.076    |
| reward_contact     | -0.0172  |
| reward_motion      | 0.75     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.494   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 60.8     |
| time/              |          |
|    episodes        | 592      |
|    fps             | 115      |
|    time_elapsed    | 5166     |
|    total timesteps | 594984   |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 589980   |
---------------------------------
---------------------------------
| reward             | 0.0562   |
| reward_contact     | -0.0171  |
| reward_motion      | 0.74     |
| reward_torque      | -0.16    |
| reward_velocity    | -0.506   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 46.2     |
| time/              |          |
|    episodes        | 596      |
|    fps             | 115      |
|    time_elapsed    | 5198     |
|    total timesteps | 599080   |
| train/             |          |
|    actor_loss      | 1.97     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 594075   |
---------------------------------
Num timesteps: 600000
Best mean reward: 68.37 - Last mean reward per episode: 46.19
---------------------------------
| reward             | 0.032    |
| reward_contact     | -0.0166  |
| reward_motion      | 0.74     |
| reward_torque      | -0.163   |
| reward_velocity    | -0.529   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 29.5     |
| time/              |          |
|    episodes        | 600      |
|    fps             | 115      |
|    time_elapsed    | 5231     |
|    total timesteps | 603176   |
| train/             |          |
|    actor_loss      | 1.78     |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.001    |
|    n_updates       | 598175   |
---------------------------------
Num timesteps: 606000
Best mean reward: 68.37 - Last mean reward per episode: 22.09
---------------------------------
| reward             | 0.0266   |
| reward_contact     | -0.0164  |
| reward_motion      | 0.74     |
| reward_torque      | -0.158   |
| reward_velocity    | -0.539   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 24       |
| time/              |          |
|    episodes        | 604      |
|    fps             | 115      |
|    time_elapsed    | 5264     |
|    total timesteps | 607272   |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 1.46     |
|    learning_rate   | 0.001    |
|    n_updates       | 602270   |
---------------------------------
---------------------------------
| reward             | 0.00546  |
| reward_contact     | -0.0163  |
| reward_motion      | 0.74     |
| reward_torque      | -0.159   |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    episodes        | 608      |
|    fps             | 115      |
|    time_elapsed    | 5296     |
|    total timesteps | 611368   |
| train/             |          |
|    actor_loss      | 1.51     |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 606365   |
---------------------------------
Num timesteps: 612000
Best mean reward: 68.37 - Last mean reward per episode: 12.79
---------------------------------
| reward             | 0.0107   |
| reward_contact     | -0.0165  |
| reward_motion      | 0.72     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.537   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    episodes        | 612      |
|    fps             | 115      |
|    time_elapsed    | 5329     |
|    total timesteps | 615464   |
| train/             |          |
|    actor_loss      | 0.897    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.001    |
|    n_updates       | 610460   |
---------------------------------
Num timesteps: 618000
Best mean reward: 68.37 - Last mean reward per episode: 27.54
---------------------------------
| reward             | 0.0255   |
| reward_contact     | -0.0169  |
| reward_motion      | 0.72     |
| reward_torque      | -0.155   |
| reward_velocity    | -0.523   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 33.8     |
| time/              |          |
|    episodes        | 616      |
|    fps             | 115      |
|    time_elapsed    | 5362     |
|    total timesteps | 619560   |
| train/             |          |
|    actor_loss      | 2.64     |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 614555   |
---------------------------------
---------------------------------
| reward             | 0.0202   |
| reward_contact     | -0.0168  |
| reward_motion      | 0.71     |
| reward_torque      | -0.158   |
| reward_velocity    | -0.515   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 39.1     |
| time/              |          |
|    episodes        | 620      |
|    fps             | 115      |
|    time_elapsed    | 5390     |
|    total timesteps | 623100   |
| train/             |          |
|    actor_loss      | 2.57     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 618095   |
---------------------------------
Num timesteps: 624000
Best mean reward: 68.37 - Last mean reward per episode: 39.13
---------------------------------
| reward             | -0.00695 |
| reward_contact     | -0.017   |
| reward_motion      | 0.68     |
| reward_torque      | -0.155   |
| reward_velocity    | -0.515   |
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 29.2     |
| time/              |          |
|    episodes        | 624      |
|    fps             | 115      |
|    time_elapsed    | 5423     |
|    total timesteps | 627196   |
| train/             |          |
|    actor_loss      | 2.16     |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 622195   |
---------------------------------
Num timesteps: 630000
Best mean reward: 68.37 - Last mean reward per episode: 40.25
---------------------------------
| reward             | 0.00475  |
| reward_contact     | -0.0165  |
| reward_motion      | 0.69     |
| reward_torque      | -0.152   |
| reward_velocity    | -0.516   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 42.3     |
| time/              |          |
|    episodes        | 628      |
|    fps             | 115      |
|    time_elapsed    | 5456     |
|    total timesteps | 631292   |
| train/             |          |
|    actor_loss      | 1.53     |
|    critic_loss     | 1.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 626290   |
---------------------------------
---------------------------------
| reward             | 0.00152  |
| reward_contact     | -0.0167  |
| reward_motion      | 0.7      |
| reward_torque      | -0.155   |
| reward_velocity    | -0.527   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 37.1     |
| time/              |          |
|    episodes        | 632      |
|    fps             | 115      |
|    time_elapsed    | 5488     |
|    total timesteps | 635388   |
| train/             |          |
|    actor_loss      | 0.742    |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 630385   |
---------------------------------
Num timesteps: 636000
Best mean reward: 68.37 - Last mean reward per episode: 37.14
---------------------------------
| reward             | 0.0193   |
| reward_contact     | -0.0149  |
| reward_motion      | 0.72     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.53    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 43.2     |
| time/              |          |
|    episodes        | 636      |
|    fps             | 115      |
|    time_elapsed    | 5521     |
|    total timesteps | 639484   |
| train/             |          |
|    actor_loss      | 0.877    |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 634480   |
---------------------------------
Num timesteps: 642000
Best mean reward: 68.37 - Last mean reward per episode: 27.94
---------------------------------
| reward             | -0.0108  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.71     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.55    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    episodes        | 640      |
|    fps             | 115      |
|    time_elapsed    | 5554     |
|    total timesteps | 643580   |
| train/             |          |
|    actor_loss      | 1.67     |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 638575   |
---------------------------------
---------------------------------
| reward             | -0.00784 |
| reward_contact     | -0.0143  |
| reward_motion      | 0.72     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.558   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    episodes        | 644      |
|    fps             | 115      |
|    time_elapsed    | 5587     |
|    total timesteps | 647676   |
| train/             |          |
|    actor_loss      | 4.62     |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 642675   |
---------------------------------
Num timesteps: 648000
Best mean reward: 68.37 - Last mean reward per episode: 21.72
---------------------------------
| reward             | -0.0141  |
| reward_contact     | -0.0148  |
| reward_motion      | 0.71     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.554   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    episodes        | 648      |
|    fps             | 115      |
|    time_elapsed    | 5619     |
|    total timesteps | 651772   |
| train/             |          |
|    actor_loss      | 0.153    |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.001    |
|    n_updates       | 646770   |
---------------------------------
Num timesteps: 654000
Best mean reward: 68.37 - Last mean reward per episode: 11.00
---------------------------------
| reward             | -0.0163  |
| reward_contact     | -0.0141  |
| reward_motion      | 0.7      |
| reward_torque      | -0.151   |
| reward_velocity    | -0.551   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -1.82    |
| time/              |          |
|    episodes        | 652      |
|    fps             | 116      |
|    time_elapsed    | 5652     |
|    total timesteps | 655868   |
| train/             |          |
|    actor_loss      | 4.76     |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 650865   |
---------------------------------
---------------------------------
| reward             | -0.0129  |
| reward_contact     | -0.0142  |
| reward_motion      | 0.71     |
| reward_torque      | -0.15    |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -6.5     |
| time/              |          |
|    episodes        | 656      |
|    fps             | 116      |
|    time_elapsed    | 5685     |
|    total timesteps | 659964   |
| train/             |          |
|    actor_loss      | 1.95     |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 654960   |
---------------------------------
Num timesteps: 660000
Best mean reward: 68.37 - Last mean reward per episode: -6.50
---------------------------------
| reward             | 0.022    |
| reward_contact     | -0.0139  |
| reward_motion      | 0.73     |
| reward_torque      | -0.151   |
| reward_velocity    | -0.543   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 4.69     |
| time/              |          |
|    episodes        | 660      |
|    fps             | 116      |
|    time_elapsed    | 5718     |
|    total timesteps | 664060   |
| train/             |          |
|    actor_loss      | 1.28     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.001    |
|    n_updates       | 659055   |
---------------------------------
Num timesteps: 666000
Best mean reward: 68.37 - Last mean reward per episode: 8.25
---------------------------------
| reward             | 0.0122   |
| reward_contact     | -0.013   |
| reward_motion      | 0.73     |
| reward_torque      | -0.15    |
| reward_velocity    | -0.555   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    episodes        | 664      |
|    fps             | 116      |
|    time_elapsed    | 5750     |
|    total timesteps | 668156   |
| train/             |          |
|    actor_loss      | 0.244    |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 663155   |
---------------------------------
Num timesteps: 672000
Best mean reward: 68.37 - Last mean reward per episode: 6.80
---------------------------------
| reward             | 0.00618  |
| reward_contact     | -0.0133  |
| reward_motion      | 0.73     |
| reward_torque      | -0.153   |
| reward_velocity    | -0.558   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    episodes        | 668      |
|    fps             | 116      |
|    time_elapsed    | 5783     |
|    total timesteps | 672252   |
| train/             |          |
|    actor_loss      | 1.77     |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 667250   |
---------------------------------
---------------------------------
| reward             | 0.012    |
| reward_contact     | -0.013   |
| reward_motion      | 0.73     |
| reward_torque      | -0.146   |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    episodes        | 672      |
|    fps             | 116      |
|    time_elapsed    | 5816     |
|    total timesteps | 676348   |
| train/             |          |
|    actor_loss      | 1.13     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 671345   |
---------------------------------
Num timesteps: 678000
Best mean reward: 68.37 - Last mean reward per episode: -1.57
---------------------------------
| reward             | -0.0133  |
| reward_contact     | -0.0131  |
| reward_motion      | 0.71     |
| reward_torque      | -0.144   |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -3.26    |
| time/              |          |
|    episodes        | 676      |
|    fps             | 116      |
|    time_elapsed    | 5849     |
|    total timesteps | 680444   |
| train/             |          |
|    actor_loss      | 2.15     |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.001    |
|    n_updates       | 675440   |
---------------------------------
Num timesteps: 684000
Best mean reward: 68.37 - Last mean reward per episode: -7.13
---------------------------------
| reward             | -0.0148  |
| reward_contact     | -0.0132  |
| reward_motion      | 0.71     |
| reward_torque      | -0.147   |
| reward_velocity    | -0.564   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -7.09    |
| time/              |          |
|    episodes        | 680      |
|    fps             | 116      |
|    time_elapsed    | 5881     |
|    total timesteps | 684540   |
| train/             |          |
|    actor_loss      | 0.554    |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 679535   |
---------------------------------
---------------------------------
| reward             | -0.05    |
| reward_contact     | -0.0129  |
| reward_motion      | 0.7      |
| reward_torque      | -0.149   |
| reward_velocity    | -0.588   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -27.8    |
| time/              |          |
|    episodes        | 684      |
|    fps             | 116      |
|    time_elapsed    | 5914     |
|    total timesteps | 688636   |
| train/             |          |
|    actor_loss      | 2.69     |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 683635   |
---------------------------------
Num timesteps: 690000
Best mean reward: 68.37 - Last mean reward per episode: -24.35
---------------------------------
| reward             | -0.0413  |
| reward_contact     | -0.0134  |
| reward_motion      | 0.7      |
| reward_torque      | -0.145   |
| reward_velocity    | -0.583   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    episodes        | 688      |
|    fps             | 116      |
|    time_elapsed    | 5947     |
|    total timesteps | 692732   |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 687730   |
---------------------------------
Num timesteps: 696000
Best mean reward: 68.37 - Last mean reward per episode: -15.63
---------------------------------
| reward             | -0.0332  |
| reward_contact     | -0.0126  |
| reward_motion      | 0.71     |
| reward_torque      | -0.143   |
| reward_velocity    | -0.587   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    episodes        | 692      |
|    fps             | 116      |
|    time_elapsed    | 5979     |
|    total timesteps | 696828   |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 691825   |
---------------------------------
---------------------------------
| reward             | -0.0364  |
| reward_contact     | -0.0128  |
| reward_motion      | 0.7      |
| reward_torque      | -0.144   |
| reward_velocity    | -0.579   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -0.898   |
| time/              |          |
|    episodes        | 696      |
|    fps             | 116      |
|    time_elapsed    | 6012     |
|    total timesteps | 700924   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 695920   |
---------------------------------
Num timesteps: 702000
Best mean reward: 68.37 - Last mean reward per episode: -1.16
---------------------------------
| reward             | -0.039   |
| reward_contact     | -0.0135  |
| reward_motion      | 0.69     |
| reward_torque      | -0.143   |
| reward_velocity    | -0.572   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -3.91    |
| time/              |          |
|    episodes        | 700      |
|    fps             | 116      |
|    time_elapsed    | 6045     |
|    total timesteps | 705020   |
| train/             |          |
|    actor_loss      | 2.79     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 700015   |
---------------------------------
Num timesteps: 708000
Best mean reward: 68.37 - Last mean reward per episode: 2.40
---------------------------------
| reward             | -0.0535  |
| reward_contact     | -0.0139  |
| reward_motion      | 0.67     |
| reward_torque      | -0.144   |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    episodes        | 704      |
|    fps             | 116      |
|    time_elapsed    | 6078     |
|    total timesteps | 709116   |
| train/             |          |
|    actor_loss      | 5.25     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 704115   |
---------------------------------
---------------------------------
| reward             | -0.0475  |
| reward_contact     | -0.0144  |
| reward_motion      | 0.67     |
| reward_torque      | -0.144   |
| reward_velocity    | -0.559   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 4.66     |
| time/              |          |
|    episodes        | 708      |
|    fps             | 116      |
|    time_elapsed    | 6111     |
|    total timesteps | 713212   |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 1.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 708210   |
---------------------------------
Num timesteps: 714000
Best mean reward: 68.37 - Last mean reward per episode: 4.66
---------------------------------
| reward             | -0.0671  |
| reward_contact     | -0.0141  |
| reward_motion      | 0.68     |
| reward_torque      | -0.148   |
| reward_velocity    | -0.585   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -16.2    |
| time/              |          |
|    episodes        | 712      |
|    fps             | 116      |
|    time_elapsed    | 6144     |
|    total timesteps | 717308   |
| train/             |          |
|    actor_loss      | 6.02     |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 712305   |
---------------------------------
Num timesteps: 720000
Best mean reward: 68.37 - Last mean reward per episode: -23.33
---------------------------------
| reward             | -0.0929  |
| reward_contact     | -0.0133  |
| reward_motion      | 0.67     |
| reward_torque      | -0.149   |
| reward_velocity    | -0.6     |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -27.1    |
| time/              |          |
|    episodes        | 716      |
|    fps             | 116      |
|    time_elapsed    | 6177     |
|    total timesteps | 721404   |
| train/             |          |
|    actor_loss      | 4.2      |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 716400   |
---------------------------------
---------------------------------
| reward             | -0.103   |
| reward_contact     | -0.0133  |
| reward_motion      | 0.67     |
| reward_torque      | -0.152   |
| reward_velocity    | -0.607   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -36.2    |
| time/              |          |
|    episodes        | 720      |
|    fps             | 116      |
|    time_elapsed    | 6209     |
|    total timesteps | 725500   |
| train/             |          |
|    actor_loss      | 2.59     |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 720495   |
---------------------------------
Num timesteps: 726000
Best mean reward: 68.37 - Last mean reward per episode: -36.18
---------------------------------
| reward             | -0.09    |
| reward_contact     | -0.0137  |
| reward_motion      | 0.69     |
| reward_torque      | -0.156   |
| reward_velocity    | -0.61    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -29.9    |
| time/              |          |
|    episodes        | 724      |
|    fps             | 116      |
|    time_elapsed    | 6242     |
|    total timesteps | 729596   |
| train/             |          |
|    actor_loss      | 2.11     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 724595   |
---------------------------------
Num timesteps: 732000
Best mean reward: 68.37 - Last mean reward per episode: -38.51
---------------------------------
| reward             | -0.0851  |
| reward_contact     | -0.0139  |
| reward_motion      | 0.7      |
| reward_torque      | -0.159   |
| reward_velocity    | -0.612   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -38.8    |
| time/              |          |
|    episodes        | 728      |
|    fps             | 116      |
|    time_elapsed    | 6275     |
|    total timesteps | 733692   |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 728690   |
---------------------------------
---------------------------------
| reward             | -0.0554  |
| reward_contact     | -0.0141  |
| reward_motion      | 0.72     |
| reward_torque      | -0.162   |
| reward_velocity    | -0.599   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -37      |
| time/              |          |
|    episodes        | 732      |
|    fps             | 116      |
|    time_elapsed    | 6308     |
|    total timesteps | 737788   |
| train/             |          |
|    actor_loss      | 3.29     |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.001    |
|    n_updates       | 732785   |
---------------------------------
Num timesteps: 738000
Best mean reward: 68.37 - Last mean reward per episode: -36.95
---------------------------------
| reward             | -0.0618  |
| reward_contact     | -0.0143  |
| reward_motion      | 0.71     |
| reward_torque      | -0.161   |
| reward_velocity    | -0.596   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -39.5    |
| time/              |          |
|    episodes        | 736      |
|    fps             | 117      |
|    time_elapsed    | 6340     |
|    total timesteps | 741884   |
| train/             |          |
|    actor_loss      | 1.58     |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.001    |
|    n_updates       | 736880   |
---------------------------------
Num timesteps: 744000
Best mean reward: 68.37 - Last mean reward per episode: -32.61
---------------------------------
| reward             | -0.0523  |
| reward_contact     | -0.0142  |
| reward_motion      | 0.72     |
| reward_torque      | -0.167   |
| reward_velocity    | -0.591   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -20.2    |
| time/              |          |
|    episodes        | 740      |
|    fps             | 117      |
|    time_elapsed    | 6373     |
|    total timesteps | 745980   |
| train/             |          |
|    actor_loss      | 2.99     |
|    critic_loss     | 2.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 740975   |
---------------------------------
Num timesteps: 750000
Best mean reward: 68.37 - Last mean reward per episode: -9.53
---------------------------------
| reward             | -0.0494  |
| reward_contact     | -0.0142  |
| reward_motion      | 0.71     |
| reward_torque      | -0.173   |
| reward_velocity    | -0.572   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    episodes        | 744      |
|    fps             | 117      |
|    time_elapsed    | 6406     |
|    total timesteps | 750076   |
| train/             |          |
|    actor_loss      | 3.01     |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 745075   |
---------------------------------
---------------------------------
| reward             | -0.0487  |
| reward_contact     | -0.0146  |
| reward_motion      | 0.71     |
| reward_torque      | -0.178   |
| reward_velocity    | -0.566   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    episodes        | 748      |
|    fps             | 117      |
|    time_elapsed    | 6438     |
|    total timesteps | 754172   |
| train/             |          |
|    actor_loss      | 1.93     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 749170   |
---------------------------------
Num timesteps: 756000
Best mean reward: 68.37 - Last mean reward per episode: -12.63
---------------------------------
| reward             | -0.0456  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.71     |
| reward_torque      | -0.179   |
| reward_velocity    | -0.562   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    episodes        | 752      |
|    fps             | 117      |
|    time_elapsed    | 6471     |
|    total timesteps | 758268   |
| train/             |          |
|    actor_loss      | -0.445   |
|    critic_loss     | 2.29     |
|    learning_rate   | 0.001    |
|    n_updates       | 753265   |
---------------------------------
Num timesteps: 762000
Best mean reward: 68.37 - Last mean reward per episode: 14.33
---------------------------------
| reward             | -0.0491  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.69     |
| reward_torque      | -0.173   |
| reward_velocity    | -0.551   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    episodes        | 756      |
|    fps             | 117      |
|    time_elapsed    | 6504     |
|    total timesteps | 762364   |
| train/             |          |
|    actor_loss      | 3.79     |
|    critic_loss     | 1.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 757360   |
---------------------------------
---------------------------------
| reward             | -0.0614  |
| reward_contact     | -0.0148  |
| reward_motion      | 0.69     |
| reward_torque      | -0.177   |
| reward_velocity    | -0.56    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    episodes        | 760      |
|    fps             | 117      |
|    time_elapsed    | 6536     |
|    total timesteps | 766460   |
| train/             |          |
|    actor_loss      | 2.1      |
|    critic_loss     | 2        |
|    learning_rate   | 0.001    |
|    n_updates       | 761455   |
---------------------------------
Num timesteps: 768000
Best mean reward: 68.37 - Last mean reward per episode: 7.32
---------------------------------
| reward             | -0.0545  |
| reward_contact     | -0.0152  |
| reward_motion      | 0.7      |
| reward_torque      | -0.18    |
| reward_velocity    | -0.56    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 6        |
| time/              |          |
|    episodes        | 764      |
|    fps             | 117      |
|    time_elapsed    | 6569     |
|    total timesteps | 770556   |
| train/             |          |
|    actor_loss      | 2.04     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.001    |
|    n_updates       | 765555   |
---------------------------------
Num timesteps: 774000
Best mean reward: 68.37 - Last mean reward per episode: 5.37
---------------------------------
| reward             | -0.0408  |
| reward_contact     | -0.0152  |
| reward_motion      | 0.71     |
| reward_torque      | -0.175   |
| reward_velocity    | -0.561   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    episodes        | 768      |
|    fps             | 117      |
|    time_elapsed    | 6602     |
|    total timesteps | 774652   |
| train/             |          |
|    actor_loss      | 4.04     |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.001    |
|    n_updates       | 769650   |
---------------------------------
---------------------------------
| reward             | -0.054   |
| reward_contact     | -0.0157  |
| reward_motion      | 0.7      |
| reward_torque      | -0.184   |
| reward_velocity    | -0.555   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    episodes        | 772      |
|    fps             | 117      |
|    time_elapsed    | 6635     |
|    total timesteps | 778748   |
| train/             |          |
|    actor_loss      | 4.85     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 773745   |
---------------------------------
Num timesteps: 780000
Best mean reward: 68.37 - Last mean reward per episode: 8.19
---------------------------------
| reward             | -0.0561  |
| reward_contact     | -0.0151  |
| reward_motion      | 0.7      |
| reward_torque      | -0.19    |
| reward_velocity    | -0.551   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    episodes        | 776      |
|    fps             | 117      |
|    time_elapsed    | 6668     |
|    total timesteps | 782844   |
| train/             |          |
|    actor_loss      | 5.5      |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 777840   |
---------------------------------
Num timesteps: 786000
Best mean reward: 68.37 - Last mean reward per episode: 20.91
---------------------------------
| reward             | -0.0539  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.68     |
| reward_torque      | -0.184   |
| reward_velocity    | -0.535   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 18       |
| time/              |          |
|    episodes        | 780      |
|    fps             | 117      |
|    time_elapsed    | 6700     |
|    total timesteps | 786940   |
| train/             |          |
|    actor_loss      | 3.2      |
|    critic_loss     | 1.45     |
|    learning_rate   | 0.001    |
|    n_updates       | 781935   |
---------------------------------
---------------------------------
| reward             | -0.0547  |
| reward_contact     | -0.0146  |
| reward_motion      | 0.68     |
| reward_torque      | -0.187   |
| reward_velocity    | -0.533   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    episodes        | 784      |
|    fps             | 117      |
|    time_elapsed    | 6733     |
|    total timesteps | 791036   |
| train/             |          |
|    actor_loss      | 5.01     |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 786035   |
---------------------------------
Num timesteps: 792000
Best mean reward: 68.37 - Last mean reward per episode: 23.61
---------------------------------
| reward             | -0.0547  |
| reward_contact     | -0.0146  |
| reward_motion      | 0.69     |
| reward_torque      | -0.19    |
| reward_velocity    | -0.54    |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    episodes        | 788      |
|    fps             | 117      |
|    time_elapsed    | 6765     |
|    total timesteps | 795132   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 790130   |
---------------------------------
Num timesteps: 798000
Best mean reward: 68.37 - Last mean reward per episode: 19.45
---------------------------------
| reward             | -0.0536  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.69     |
| reward_torque      | -0.193   |
| reward_velocity    | -0.536   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    episodes        | 792      |
|    fps             | 117      |
|    time_elapsed    | 6798     |
|    total timesteps | 799228   |
| train/             |          |
|    actor_loss      | 5.88     |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 794225   |
---------------------------------
---------------------------------
| reward             | -0.0743  |
| reward_contact     | -0.0144  |
| reward_motion      | 0.68     |
| reward_torque      | -0.195   |
| reward_velocity    | -0.544   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 8.81     |
| time/              |          |
|    episodes        | 796      |
|    fps             | 117      |
|    time_elapsed    | 6831     |
|    total timesteps | 803324   |
| train/             |          |
|    actor_loss      | 4.15     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 798320   |
---------------------------------
Num timesteps: 804000
Best mean reward: 68.37 - Last mean reward per episode: 8.81
---------------------------------
| reward             | -0.0717  |
| reward_contact     | -0.0144  |
| reward_motion      | 0.67     |
| reward_torque      | -0.192   |
| reward_velocity    | -0.535   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 24.2     |
| time/              |          |
|    episodes        | 800      |
|    fps             | 117      |
|    time_elapsed    | 6864     |
|    total timesteps | 807420   |
| train/             |          |
|    actor_loss      | 5.06     |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.001    |
|    n_updates       | 802415   |
---------------------------------
Num timesteps: 810000
Best mean reward: 68.37 - Last mean reward per episode: 31.93
---------------------------------
| reward             | -0.0466  |
| reward_contact     | -0.0139  |
| reward_motion      | 0.68     |
| reward_torque      | -0.195   |
| reward_velocity    | -0.518   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 31.3     |
| time/              |          |
|    episodes        | 804      |
|    fps             | 117      |
|    time_elapsed    | 6896     |
|    total timesteps | 811516   |
| train/             |          |
|    actor_loss      | 4.81     |
|    critic_loss     | 1.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 806515   |
---------------------------------
---------------------------------
| reward             | -0.0577  |
| reward_contact     | -0.0132  |
| reward_motion      | 0.67     |
| reward_torque      | -0.196   |
| reward_velocity    | -0.518   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 33       |
| time/              |          |
|    episodes        | 808      |
|    fps             | 117      |
|    time_elapsed    | 6924     |
|    total timesteps | 815005   |
| train/             |          |
|    actor_loss      | 3.37     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 810000   |
---------------------------------
Num timesteps: 816000
Best mean reward: 68.37 - Last mean reward per episode: 33.01
---------------------------------
| reward             | -0.022   |
| reward_contact     | -0.0141  |
| reward_motion      | 0.67     |
| reward_torque      | -0.191   |
| reward_velocity    | -0.487   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 48.8     |
| time/              |          |
|    episodes        | 812      |
|    fps             | 117      |
|    time_elapsed    | 6957     |
|    total timesteps | 819101   |
| train/             |          |
|    actor_loss      | 2.88     |
|    critic_loss     | 1.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 814100   |
---------------------------------
Num timesteps: 822000
Best mean reward: 68.37 - Last mean reward per episode: 48.14
---------------------------------
| reward             | -0.0314  |
| reward_contact     | -0.0144  |
| reward_motion      | 0.67     |
| reward_torque      | -0.193   |
| reward_velocity    | -0.494   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 43.6     |
| time/              |          |
|    episodes        | 816      |
|    fps             | 117      |
|    time_elapsed    | 6990     |
|    total timesteps | 823197   |
| train/             |          |
|    actor_loss      | 5.07     |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 818195   |
---------------------------------
---------------------------------
| reward             | -0.011   |
| reward_contact     | -0.0143  |
| reward_motion      | 0.68     |
| reward_torque      | -0.189   |
| reward_velocity    | -0.488   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 57.2     |
| time/              |          |
|    episodes        | 820      |
|    fps             | 117      |
|    time_elapsed    | 7022     |
|    total timesteps | 827293   |
| train/             |          |
|    actor_loss      | 5.35     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 822290   |
---------------------------------
Num timesteps: 828000
Best mean reward: 68.37 - Last mean reward per episode: 57.15
---------------------------------
| reward             | -0.00411 |
| reward_contact     | -0.0139  |
| reward_motion      | 0.68     |
| reward_torque      | -0.187   |
| reward_velocity    | -0.483   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 54.1     |
| time/              |          |
|    episodes        | 824      |
|    fps             | 117      |
|    time_elapsed    | 7055     |
|    total timesteps | 831389   |
| train/             |          |
|    actor_loss      | 3.53     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 826385   |
---------------------------------
Num timesteps: 834000
Best mean reward: 68.37 - Last mean reward per episode: 59.02
---------------------------------
| reward             | -0.0216  |
| reward_contact     | -0.014   |
| reward_motion      | 0.66     |
| reward_torque      | -0.186   |
| reward_velocity    | -0.482   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    episodes        | 828      |
|    fps             | 117      |
|    time_elapsed    | 7088     |
|    total timesteps | 835485   |
| train/             |          |
|    actor_loss      | 3.63     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 830480   |
---------------------------------
---------------------------------
| reward             | -0.0615  |
| reward_contact     | -0.0142  |
| reward_motion      | 0.63     |
| reward_torque      | -0.188   |
| reward_velocity    | -0.489   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 53       |
| time/              |          |
|    episodes        | 832      |
|    fps             | 117      |
|    time_elapsed    | 7121     |
|    total timesteps | 839581   |
| train/             |          |
|    actor_loss      | 4.99     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 834580   |
---------------------------------
Num timesteps: 840000
Best mean reward: 68.37 - Last mean reward per episode: 53.00
---------------------------------
| reward             | -0.0586  |
| reward_contact     | -0.0147  |
| reward_motion      | 0.63     |
| reward_torque      | -0.189   |
| reward_velocity    | -0.485   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 64.5     |
| time/              |          |
|    episodes        | 836      |
|    fps             | 117      |
|    time_elapsed    | 7153     |
|    total timesteps | 843677   |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 1.78     |
|    learning_rate   | 0.001    |
|    n_updates       | 838675   |
---------------------------------
Num timesteps: 846000
Best mean reward: 68.37 - Last mean reward per episode: 66.42
---------------------------------
| reward             | -0.0314  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.65     |
| reward_torque      | -0.187   |
| reward_velocity    | -0.479   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 61.1     |
| time/              |          |
|    episodes        | 840      |
|    fps             | 117      |
|    time_elapsed    | 7186     |
|    total timesteps | 847773   |
| train/             |          |
|    actor_loss      | 4.95     |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 842770   |
---------------------------------
---------------------------------
| reward             | -0.0216  |
| reward_contact     | -0.0155  |
| reward_motion      | 0.66     |
| reward_torque      | -0.184   |
| reward_velocity    | -0.482   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 61.2     |
| time/              |          |
|    episodes        | 844      |
|    fps             | 117      |
|    time_elapsed    | 7219     |
|    total timesteps | 851869   |
| train/             |          |
|    actor_loss      | 1.85     |
|    critic_loss     | 1.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 846865   |
---------------------------------
Num timesteps: 852000
Best mean reward: 68.37 - Last mean reward per episode: 61.24
---------------------------------
| reward             | -0.0189  |
| reward_contact     | -0.0149  |
| reward_motion      | 0.66     |
| reward_torque      | -0.179   |
| reward_velocity    | -0.485   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 66.6     |
| time/              |          |
|    episodes        | 848      |
|    fps             | 118      |
|    time_elapsed    | 7251     |
|    total timesteps | 855965   |
| train/             |          |
|    actor_loss      | 3.93     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 850960   |
---------------------------------
Num timesteps: 858000
Best mean reward: 68.37 - Last mean reward per episode: 69.45
Saving new best model to rl/out_dir/models/exp73/best_model.zip
---------------------------------
| reward             | -0.0213  |
| reward_contact     | -0.0144  |
| reward_motion      | 0.66     |
| reward_torque      | -0.181   |
| reward_velocity    | -0.486   |
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 60.7     |
| time/              |          |
|    episodes        | 852      |
|    fps             | 118      |
|    time_elapsed    | 7284     |
|    total timesteps | 860061   |
| train/             |          |
|    actor_loss      | 4.5      |
|    critic_loss     | 1.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 855060   |
---------------------------------
