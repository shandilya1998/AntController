running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp75/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 128      |
---------------------------------
---------------------------------------
| reward                  | 0.744     |
| reward_contact          | 0.0205    |
| reward_motion           | 0.195     |
| reward_position         | 0.366     |
| reward_torque           | 0.00653   |
| reward_velocity         | 0.155     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 113       |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 2         |
|    time_elapsed         | 7         |
|    total_timesteps      | 256       |
| train/                  |           |
|    approx_kl            | 0.9058078 |
|    clip_fraction        | 0.611     |
|    clip_range           | 0.4       |
|    entropy_loss         | -84.8     |
|    explained_variance   | -0.0363   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.52      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.114    |
|    std                  | 0.367     |
|    value_loss           | 11.8      |
---------------------------------------
----------------------------------------
| reward                  | 0.888      |
| reward_contact          | 0.0118     |
| reward_motion           | 0.221      |
| reward_position         | 0.344      |
| reward_torque           | 0.0094     |
| reward_velocity         | 0.302      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 113        |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 3          |
|    time_elapsed         | 10         |
|    total_timesteps      | 384        |
| train/                  |            |
|    approx_kl            | 0.88566685 |
|    clip_fraction        | 0.659      |
|    clip_range           | 0.4        |
|    entropy_loss         | -89.6      |
|    explained_variance   | -0.0502    |
|    learning_rate        | 0.0005     |
|    loss                 | 5.7        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0847    |
|    std                  | 0.367      |
|    value_loss           | 29.9       |
----------------------------------------
---------------------------------------
| reward                  | 0.873     |
| reward_contact          | 0.016     |
| reward_motion           | 0.222     |
| reward_position         | 0.372     |
| reward_torque           | 0.00845   |
| reward_velocity         | 0.254     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 113       |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 4         |
|    time_elapsed         | 14        |
|    total_timesteps      | 512       |
| train/                  |           |
|    approx_kl            | 0.6368348 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -93       |
|    explained_variance   | -4.44     |
|    learning_rate        | 0.0005    |
|    loss                 | 28.7      |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.139    |
|    std                  | 0.367     |
|    value_loss           | 82.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.904     |
| reward_contact          | 0.0149    |
| reward_motion           | 0.226     |
| reward_position         | 0.363     |
| reward_torque           | 0.00841   |
| reward_velocity         | 0.292     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 116       |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 5         |
|    time_elapsed         | 17        |
|    total_timesteps      | 640       |
| train/                  |           |
|    approx_kl            | 1.2171564 |
|    clip_fraction        | 0.584     |
|    clip_range           | 0.4       |
|    entropy_loss         | -92.6     |
|    explained_variance   | -0.0758   |
|    learning_rate        | 0.0005    |
|    loss                 | 15.7      |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.153    |
|    std                  | 0.367     |
|    value_loss           | 46.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.895     |
| reward_contact          | 0.0187    |
| reward_motion           | 0.212     |
| reward_position         | 0.356     |
| reward_torque           | 0.00853   |
| reward_velocity         | 0.299     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 114       |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 6         |
|    time_elapsed         | 21        |
|    total_timesteps      | 768       |
| train/                  |           |
|    approx_kl            | 1.3211875 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.4       |
|    entropy_loss         | -87.5     |
|    explained_variance   | -0.0447   |
|    learning_rate        | 0.0005    |
|    loss                 | 8.79      |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.367     |
|    value_loss           | 36        |
---------------------------------------
----------------------------------------
| reward                  | 0.889      |
| reward_contact          | 0.0176     |
| reward_motion           | 0.211      |
| reward_position         | 0.356      |
| reward_torque           | 0.00862    |
| reward_velocity         | 0.296      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 112        |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 7          |
|    time_elapsed         | 24         |
|    total_timesteps      | 896        |
| train/                  |            |
|    approx_kl            | 0.34302574 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -96.8      |
|    explained_variance   | -2.05      |
|    learning_rate        | 0.0005     |
|    loss                 | 16.4       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.367      |
|    value_loss           | 81.4       |
----------------------------------------
---------------------------------------
| reward                  | 0.895     |
| reward_contact          | 0.0165    |
| reward_motion           | 0.216     |
| reward_position         | 0.351     |
| reward_torque           | 0.00889   |
| reward_velocity         | 0.301     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 114       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 8         |
|    time_elapsed         | 28        |
|    total_timesteps      | 1024      |
| train/                  |           |
|    approx_kl            | 1.3055801 |
|    clip_fraction        | 0.543     |
|    clip_range           | 0.4       |
|    entropy_loss         | -96.5     |
|    explained_variance   | -2.08     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.4       |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.145    |
|    std                  | 0.367     |
|    value_loss           | 13.5      |
---------------------------------------
----------------------------------------
| reward                  | 0.914      |
| reward_contact          | 0.0152     |
| reward_motion           | 0.212      |
| reward_position         | 0.348      |
| reward_torque           | 0.00909    |
| reward_velocity         | 0.329      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 9          |
|    time_elapsed         | 32         |
|    total_timesteps      | 1152       |
| train/                  |            |
|    approx_kl            | 0.33446097 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.4        |
|    entropy_loss         | -96.3      |
|    explained_variance   | -1.14      |
|    learning_rate        | 0.0005     |
|    loss                 | 13.4       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.367      |
|    value_loss           | 54.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.909      |
| reward_contact          | 0.0156     |
| reward_motion           | 0.21       |
| reward_position         | 0.362      |
| reward_torque           | 0.00899    |
| reward_velocity         | 0.313      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 10         |
|    time_elapsed         | 35         |
|    total_timesteps      | 1280       |
| train/                  |            |
|    approx_kl            | 0.34891474 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.4        |
|    entropy_loss         | -98.2      |
|    explained_variance   | -0.342     |
|    learning_rate        | 0.0005     |
|    loss                 | 1.94       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.367      |
|    value_loss           | 30.6       |
----------------------------------------
---------------------------------------
| reward                  | 0.919     |
| reward_contact          | 0.0146    |
| reward_motion           | 0.212     |
| reward_position         | 0.369     |
| reward_torque           | 0.00916   |
| reward_velocity         | 0.315     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 116       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 11        |
|    time_elapsed         | 39        |
|    total_timesteps      | 1408      |
| train/                  |           |
|    approx_kl            | 1.1445346 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | -0.014    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.77      |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.367     |
|    value_loss           | 19.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.925     |
| reward_contact          | 0.0156    |
| reward_motion           | 0.211     |
| reward_position         | 0.375     |
| reward_torque           | 0.00907   |
| reward_velocity         | 0.314     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 116       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 12        |
|    time_elapsed         | 42        |
|    total_timesteps      | 1536      |
| train/                  |           |
|    approx_kl            | 0.4911908 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | -1.6      |
|    learning_rate        | 0.0005    |
|    loss                 | 22.4      |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0661   |
|    std                  | 0.367     |
|    value_loss           | 85.6      |
---------------------------------------
---------------------------------------
| reward                  | 0.932     |
| reward_contact          | 0.0153    |
| reward_motion           | 0.205     |
| reward_position         | 0.383     |
| reward_torque           | 0.0091    |
| reward_velocity         | 0.321     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 117       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 13        |
|    time_elapsed         | 46        |
|    total_timesteps      | 1664      |
| train/                  |           |
|    approx_kl            | 0.8784758 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -91.9     |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.78      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.198    |
|    std                  | 0.367     |
|    value_loss           | 14.8      |
---------------------------------------
----------------------------------------
| reward                  | 0.947      |
| reward_contact          | 0.015      |
| reward_motion           | 0.206      |
| reward_position         | 0.383      |
| reward_torque           | 0.00935    |
| reward_velocity         | 0.334      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 14         |
|    time_elapsed         | 50         |
|    total_timesteps      | 1792       |
| train/                  |            |
|    approx_kl            | 0.56340194 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.31       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.367      |
|    value_loss           | 16.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.957      |
| reward_contact          | 0.0145     |
| reward_motion           | 0.204      |
| reward_position         | 0.381      |
| reward_torque           | 0.00956    |
| reward_velocity         | 0.347      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 15         |
|    time_elapsed         | 53         |
|    total_timesteps      | 1920       |
| train/                  |            |
|    approx_kl            | 0.26506788 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.42       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.367      |
|    value_loss           | 22.6       |
----------------------------------------
---------------------------------------
| reward                  | 0.952     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.206     |
| reward_position         | 0.381     |
| reward_torque           | 0.00942   |
| reward_velocity         | 0.342     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 118       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 16        |
|    time_elapsed         | 57        |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 0.7898835 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.518     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.19      |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.367     |
|    value_loss           | 21.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.953      |
| reward_contact          | 0.0139     |
| reward_motion           | 0.207      |
| reward_position         | 0.379      |
| reward_torque           | 0.00934    |
| reward_velocity         | 0.344      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 17         |
|    time_elapsed         | 60         |
|    total_timesteps      | 2176       |
| train/                  |            |
|    approx_kl            | 0.56034935 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -95        |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.0005     |
|    loss                 | 10.3       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.367      |
|    value_loss           | 31.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.956      |
| reward_contact          | 0.0134     |
| reward_motion           | 0.208      |
| reward_position         | 0.378      |
| reward_torque           | 0.00925    |
| reward_velocity         | 0.347      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 18         |
|    time_elapsed         | 64         |
|    total_timesteps      | 2304       |
| train/                  |            |
|    approx_kl            | 0.39904028 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0005     |
|    loss                 | 7.62       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.367      |
|    value_loss           | 21         |
----------------------------------------
---------------------------------------
| reward                  | 0.961     |
| reward_contact          | 0.0129    |
| reward_motion           | 0.209     |
| reward_position         | 0.377     |
| reward_torque           | 0.0094    |
| reward_velocity         | 0.353     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 118       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 19        |
|    time_elapsed         | 68        |
|    total_timesteps      | 2432      |
| train/                  |           |
|    approx_kl            | 0.5598656 |
|    clip_fraction        | 0.553     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.314     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.71      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.134    |
|    std                  | 0.366     |
|    value_loss           | 26.8      |
---------------------------------------
---------------------------------------
| reward                  | 0.969     |
| reward_contact          | 0.0126    |
| reward_motion           | 0.211     |
| reward_position         | 0.375     |
| reward_torque           | 0.00932   |
| reward_velocity         | 0.361     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 117       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 20        |
|    time_elapsed         | 71        |
|    total_timesteps      | 2560      |
| train/                  |           |
|    approx_kl            | 0.5915173 |
|    clip_fraction        | 0.611     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.07      |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.366     |
|    value_loss           | 18.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.969     |
| reward_contact          | 0.0122    |
| reward_motion           | 0.213     |
| reward_position         | 0.376     |
| reward_torque           | 0.00939   |
| reward_velocity         | 0.359     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 117       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 21        |
|    time_elapsed         | 75        |
|    total_timesteps      | 2688      |
| train/                  |           |
|    approx_kl            | 0.2758628 |
|    clip_fraction        | 0.361     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.837     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.17      |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.131    |
|    std                  | 0.366     |
|    value_loss           | 12.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.961      |
| reward_contact          | 0.0131     |
| reward_motion           | 0.213      |
| reward_position         | 0.373      |
| reward_torque           | 0.00926    |
| reward_velocity         | 0.353      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 22         |
|    time_elapsed         | 79         |
|    total_timesteps      | 2816       |
| train/                  |            |
|    approx_kl            | 0.33273324 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.95       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.366      |
|    value_loss           | 19.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.961      |
| reward_contact          | 0.0131     |
| reward_motion           | 0.212      |
| reward_position         | 0.375      |
| reward_torque           | 0.0092     |
| reward_velocity         | 0.352      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 118        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 23         |
|    time_elapsed         | 82         |
|    total_timesteps      | 2944       |
| train/                  |            |
|    approx_kl            | 0.33004022 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.31       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.366      |
|    value_loss           | 10.4       |
----------------------------------------
---------------------------------------
| reward                  | 0.968     |
| reward_contact          | 0.0128    |
| reward_motion           | 0.21      |
| reward_position         | 0.378     |
| reward_torque           | 0.00921   |
| reward_velocity         | 0.359     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 118       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 24        |
|    time_elapsed         | 86        |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 1.1252203 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.4       |
|    entropy_loss         | -90.4     |
|    explained_variance   | -0.00453  |
|    learning_rate        | 0.0005    |
|    loss                 | 25        |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.366     |
|    value_loss           | 52.6      |
---------------------------------------
---------------------------------------
| reward                  | 0.963     |
| reward_contact          | 0.013     |
| reward_motion           | 0.21      |
| reward_position         | 0.378     |
| reward_torque           | 0.00915   |
| reward_velocity         | 0.353     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 118       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 25        |
|    time_elapsed         | 89        |
|    total_timesteps      | 3200      |
| train/                  |           |
|    approx_kl            | 0.4373735 |
|    clip_fraction        | 0.536     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.78      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.366     |
|    value_loss           | 32.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.964     |
| reward_contact          | 0.0128    |
| reward_motion           | 0.21      |
| reward_position         | 0.379     |
| reward_torque           | 0.00914   |
| reward_velocity         | 0.353     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 119       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 26        |
|    time_elapsed         | 93        |
|    total_timesteps      | 3328      |
| train/                  |           |
|    approx_kl            | 0.5290619 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.58      |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.366     |
|    value_loss           | 18.5      |
---------------------------------------
----------------------------------------
| reward                  | 0.97       |
| reward_contact          | 0.0133     |
| reward_motion           | 0.208      |
| reward_position         | 0.381      |
| reward_torque           | 0.00916    |
| reward_velocity         | 0.359      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 119        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 27         |
|    time_elapsed         | 97         |
|    total_timesteps      | 3456       |
| train/                  |            |
|    approx_kl            | 0.54546714 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.02       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.366      |
|    value_loss           | 21         |
----------------------------------------
----------------------------------------
| reward                  | 0.976      |
| reward_contact          | 0.0138     |
| reward_motion           | 0.209      |
| reward_position         | 0.379      |
| reward_torque           | 0.00919    |
| reward_velocity         | 0.364      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 119        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 28         |
|    time_elapsed         | 101        |
|    total_timesteps      | 3584       |
| train/                  |            |
|    approx_kl            | 0.35873818 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.134      |
|    learning_rate        | 0.0005     |
|    loss                 | 8.2        |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.366      |
|    value_loss           | 44.5       |
----------------------------------------
---------------------------------------
| reward                  | 0.975     |
| reward_contact          | 0.0135    |
| reward_motion           | 0.209     |
| reward_position         | 0.377     |
| reward_torque           | 0.00924   |
| reward_velocity         | 0.367     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 119       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 29        |
|    time_elapsed         | 104       |
|    total_timesteps      | 3712      |
| train/                  |           |
|    approx_kl            | 0.3491305 |
|    clip_fraction        | 0.622     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.876     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.92      |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.366     |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.977     |
| reward_contact          | 0.0132    |
| reward_motion           | 0.21      |
| reward_position         | 0.378     |
| reward_torque           | 0.00928   |
| reward_velocity         | 0.366     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 119       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 30        |
|    time_elapsed         | 108       |
|    total_timesteps      | 3840      |
| train/                  |           |
|    approx_kl            | 0.5993794 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.81      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.86      |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.366     |
|    value_loss           | 14.5      |
---------------------------------------
----------------------------------------
| reward                  | 0.98       |
| reward_contact          | 0.0133     |
| reward_motion           | 0.209      |
| reward_position         | 0.378      |
| reward_torque           | 0.00932    |
| reward_velocity         | 0.371      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 31         |
|    time_elapsed         | 112        |
|    total_timesteps      | 3968       |
| train/                  |            |
|    approx_kl            | 0.59546226 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0005     |
|    loss                 | 5.1        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.366      |
|    value_loss           | 24         |
----------------------------------------
--------------------------------------
| reward                  | 0.976    |
| reward_contact          | 0.0133   |
| reward_motion           | 0.209    |
| reward_position         | 0.378    |
| reward_torque           | 0.00929  |
| reward_velocity         | 0.367    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 120      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 32       |
|    time_elapsed         | 115      |
|    total_timesteps      | 4096     |
| train/                  |          |
|    approx_kl            | 1.94748  |
|    clip_fraction        | 0.778    |
|    clip_range           | 0.4      |
|    entropy_loss         | -93.7    |
|    explained_variance   | 0.0137   |
|    learning_rate        | 0.0005   |
|    loss                 | 5.15     |
|    n_updates            | 620      |
|    policy_gradient_loss | -0.133   |
|    std                  | 0.366    |
|    value_loss           | 51.6     |
--------------------------------------
---------------------------------------
| reward                  | 0.976     |
| reward_contact          | 0.0135    |
| reward_motion           | 0.208     |
| reward_position         | 0.376     |
| reward_torque           | 0.00924   |
| reward_velocity         | 0.368     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 33        |
|    time_elapsed         | 119       |
|    total_timesteps      | 4224      |
| train/                  |           |
|    approx_kl            | 1.8452873 |
|    clip_fraction        | 0.727     |
|    clip_range           | 0.4       |
|    entropy_loss         | -94.7     |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0005    |
|    loss                 | 15.2      |
|    n_updates            | 640       |
|    policy_gradient_loss | -0.137    |
|    std                  | 0.366     |
|    value_loss           | 71.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.974      |
| reward_contact          | 0.0134     |
| reward_motion           | 0.209      |
| reward_position         | 0.376      |
| reward_torque           | 0.00924    |
| reward_velocity         | 0.366      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 34         |
|    time_elapsed         | 123        |
|    total_timesteps      | 4352       |
| train/                  |            |
|    approx_kl            | 0.16114452 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.37       |
|    learning_rate        | 0.0005     |
|    loss                 | 5.39       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.366      |
|    value_loss           | 60.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.976      |
| reward_contact          | 0.0133     |
| reward_motion           | 0.209      |
| reward_position         | 0.376      |
| reward_torque           | 0.00924    |
| reward_velocity         | 0.368      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 35         |
|    time_elapsed         | 127        |
|    total_timesteps      | 4480       |
| train/                  |            |
|    approx_kl            | 0.57175493 |
|    clip_fraction        | 0.527      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.5        |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.366      |
|    value_loss           | 12         |
----------------------------------------
----------------------------------------
| reward                  | 0.981      |
| reward_contact          | 0.0134     |
| reward_motion           | 0.21       |
| reward_position         | 0.375      |
| reward_torque           | 0.00928    |
| reward_velocity         | 0.373      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 36         |
|    time_elapsed         | 130        |
|    total_timesteps      | 4608       |
| train/                  |            |
|    approx_kl            | 0.98058385 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.02       |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.366      |
|    value_loss           | 12.9       |
----------------------------------------
---------------------------------------
| reward                  | 0.982     |
| reward_contact          | 0.0132    |
| reward_motion           | 0.21      |
| reward_position         | 0.376     |
| reward_torque           | 0.00928   |
| reward_velocity         | 0.374     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 37        |
|    time_elapsed         | 134       |
|    total_timesteps      | 4736      |
| train/                  |           |
|    approx_kl            | 1.5449458 |
|    clip_fraction        | 0.659     |
|    clip_range           | 0.4       |
|    entropy_loss         | -95.4     |
|    explained_variance   | 0.838     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.23      |
|    n_updates            | 720       |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.366     |
|    value_loss           | 19.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.982      |
| reward_contact          | 0.013      |
| reward_motion           | 0.209      |
| reward_position         | 0.375      |
| reward_torque           | 0.00932    |
| reward_velocity         | 0.376      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 38         |
|    time_elapsed         | 138        |
|    total_timesteps      | 4864       |
| train/                  |            |
|    approx_kl            | 0.59032524 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.542      |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.366      |
|    value_loss           | 12.7       |
----------------------------------------
----------------------------------------
| reward                  | 0.996      |
| reward_contact          | 0.0128     |
| reward_motion           | 0.209      |
| reward_position         | 0.378      |
| reward_torque           | 0.00929    |
| reward_velocity         | 0.388      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 39         |
|    time_elapsed         | 141        |
|    total_timesteps      | 4992       |
| train/                  |            |
|    approx_kl            | 0.44036755 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.44       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.366      |
|    value_loss           | 32.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.992     |
| reward_contact          | 0.0129    |
| reward_motion           | 0.209     |
| reward_position         | 0.378     |
| reward_torque           | 0.00925   |
| reward_velocity         | 0.383     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 40        |
|    time_elapsed         | 145       |
|    total_timesteps      | 5120      |
| train/                  |           |
|    approx_kl            | 0.6055172 |
|    clip_fraction        | 0.496     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.32      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.366     |
|    value_loss           | 16.8      |
---------------------------------------
---------------------------------------
| reward                  | 0.994     |
| reward_contact          | 0.0128    |
| reward_motion           | 0.208     |
| reward_position         | 0.378     |
| reward_torque           | 0.00928   |
| reward_velocity         | 0.386     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 41        |
|    time_elapsed         | 149       |
|    total_timesteps      | 5248      |
| train/                  |           |
|    approx_kl            | 0.4436162 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.803     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.52      |
|    n_updates            | 800       |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.366     |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.994     |
| reward_contact          | 0.0129    |
| reward_motion           | 0.208     |
| reward_position         | 0.38      |
| reward_torque           | 0.00928   |
| reward_velocity         | 0.384     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 42        |
|    time_elapsed         | 153       |
|    total_timesteps      | 5376      |
| train/                  |           |
|    approx_kl            | 0.5588426 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.26      |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.366     |
|    value_loss           | 16.5      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0128     |
| reward_motion           | 0.208      |
| reward_position         | 0.381      |
| reward_torque           | 0.00935    |
| reward_velocity         | 0.394      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 43         |
|    time_elapsed         | 156        |
|    total_timesteps      | 5504       |
| train/                  |            |
|    approx_kl            | 0.65881276 |
|    clip_fraction        | 0.627      |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.64       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.366      |
|    value_loss           | 17.8       |
----------------------------------------
---------------------------------------
| reward                  | 1         |
| reward_contact          | 0.0127    |
| reward_motion           | 0.209     |
| reward_position         | 0.381     |
| reward_torque           | 0.00936   |
| reward_velocity         | 0.392     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 44        |
|    time_elapsed         | 160       |
|    total_timesteps      | 5632      |
| train/                  |           |
|    approx_kl            | 0.5504693 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.94      |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.366     |
|    value_loss           | 13.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0128     |
| reward_motion           | 0.21       |
| reward_position         | 0.381      |
| reward_torque           | 0.00932    |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 45         |
|    time_elapsed         | 164        |
|    total_timesteps      | 5760       |
| train/                  |            |
|    approx_kl            | 0.67172784 |
|    clip_fraction        | 0.652      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.24      |
|    std                  | 0.366      |
|    value_loss           | 12.9       |
----------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0129     |
| reward_motion           | 0.21       |
| reward_position         | 0.381      |
| reward_torque           | 0.00932    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 46         |
|    time_elapsed         | 167        |
|    total_timesteps      | 5888       |
| train/                  |            |
|    approx_kl            | 0.38610026 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.06       |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.366      |
|    value_loss           | 18.5       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 121.84
Saving new best model to rl/out_dir/models/exp75/best_model.zip
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0129     |
| reward_motion           | 0.211      |
| reward_position         | 0.38       |
| reward_torque           | 0.00928    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 47         |
|    time_elapsed         | 171        |
|    total_timesteps      | 6016       |
| train/                  |            |
|    approx_kl            | 0.38454822 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.955      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.366      |
|    value_loss           | 11         |
----------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0129     |
| reward_motion           | 0.212      |
| reward_position         | 0.38       |
| reward_torque           | 0.00926    |
| reward_velocity         | 0.392      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 48         |
|    time_elapsed         | 175        |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.40854427 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0005     |
|    loss                 | 1.26       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.366      |
|    value_loss           | 10.8       |
----------------------------------------
--------------------------------------
| reward                  | 1.01     |
| reward_contact          | 0.0128   |
| reward_motion           | 0.213    |
| reward_position         | 0.379    |
| reward_torque           | 0.00926  |
| reward_velocity         | 0.393    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 122      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 49       |
|    time_elapsed         | 179      |
|    total_timesteps      | 6272     |
| train/                  |          |
|    approx_kl            | 0.545705 |
|    clip_fraction        | 0.414    |
|    clip_range           | 0.4      |
|    entropy_loss         | -106     |
|    explained_variance   | 0.933    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.6      |
|    n_updates            | 960      |
|    policy_gradient_loss | -0.158   |
|    std                  | 0.366    |
|    value_loss           | 9.19     |
--------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0128     |
| reward_motion           | 0.212      |
| reward_position         | 0.378      |
| reward_torque           | 0.00924    |
| reward_velocity         | 0.392      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 50         |
|    time_elapsed         | 182        |
|    total_timesteps      | 6400       |
| train/                  |            |
|    approx_kl            | 0.42969418 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.73       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.366      |
|    value_loss           | 17.3       |
----------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0129     |
| reward_motion           | 0.212      |
| reward_position         | 0.378      |
| reward_torque           | 0.00923    |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 51         |
|    time_elapsed         | 186        |
|    total_timesteps      | 6528       |
| train/                  |            |
|    approx_kl            | 0.49119407 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.76       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.366      |
|    value_loss           | 10.1       |
----------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0131     |
| reward_motion           | 0.212      |
| reward_position         | 0.379      |
| reward_torque           | 0.00918    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 52         |
|    time_elapsed         | 189        |
|    total_timesteps      | 6656       |
| train/                  |            |
|    approx_kl            | 0.33202672 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.4        |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.366      |
|    value_loss           | 12         |
----------------------------------------
---------------------------------------
| reward                  | 1         |
| reward_contact          | 0.0133    |
| reward_motion           | 0.212     |
| reward_position         | 0.379     |
| reward_torque           | 0.00916   |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 53        |
|    time_elapsed         | 193       |
|    total_timesteps      | 6784      |
| train/                  |           |
|    approx_kl            | 0.5235497 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.51      |
|    n_updates            | 1040      |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.366     |
|    value_loss           | 10.5      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0132     |
| reward_motion           | 0.213      |
| reward_position         | 0.379      |
| reward_torque           | 0.00918    |
| reward_velocity         | 0.386      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 54         |
|    time_elapsed         | 197        |
|    total_timesteps      | 6912       |
| train/                  |            |
|    approx_kl            | 0.12817666 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.97       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.078     |
|    std                  | 0.366      |
|    value_loss           | 7.7        |
----------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0131    |
| reward_motion           | 0.213     |
| reward_position         | 0.38      |
| reward_torque           | 0.00924   |
| reward_velocity         | 0.396     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 55        |
|    time_elapsed         | 201       |
|    total_timesteps      | 7040      |
| train/                  |           |
|    approx_kl            | 1.5646391 |
|    clip_fraction        | 0.696     |
|    clip_range           | 0.4       |
|    entropy_loss         | -98.7     |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0005    |
|    loss                 | 19        |
|    n_updates            | 1080      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.366     |
|    value_loss           | 48.3      |
---------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0133     |
| reward_motion           | 0.214      |
| reward_position         | 0.379      |
| reward_torque           | 0.00923    |
| reward_velocity         | 0.394      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 56         |
|    time_elapsed         | 204        |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.32551444 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.75       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.366      |
|    value_loss           | 15.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0135     |
| reward_motion           | 0.213      |
| reward_position         | 0.38       |
| reward_torque           | 0.0092     |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 57         |
|    time_elapsed         | 208        |
|    total_timesteps      | 7296       |
| train/                  |            |
|    approx_kl            | 0.79356605 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.36       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.366      |
|    value_loss           | 15.8       |
----------------------------------------
---------------------------------------
| reward                  | 1         |
| reward_contact          | 0.0135    |
| reward_motion           | 0.214     |
| reward_position         | 0.38      |
| reward_torque           | 0.00917   |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 58        |
|    time_elapsed         | 212       |
|    total_timesteps      | 7424      |
| train/                  |           |
|    approx_kl            | 0.4921527 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.932     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.83      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.366     |
|    value_loss           | 10.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0138    |
| reward_motion           | 0.213     |
| reward_position         | 0.38      |
| reward_torque           | 0.00919   |
| reward_velocity         | 0.39      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 59        |
|    time_elapsed         | 215       |
|    total_timesteps      | 7552      |
| train/                  |           |
|    approx_kl            | 0.6370198 |
|    clip_fraction        | 0.583     |
|    clip_range           | 0.4       |
|    entropy_loss         | -100      |
|    explained_variance   | 0.1       |
|    learning_rate        | 0.0005    |
|    loss                 | 17.6      |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.153    |
|    std                  | 0.366     |
|    value_loss           | 55.7      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0139     |
| reward_motion           | 0.214      |
| reward_position         | 0.38       |
| reward_torque           | 0.00915    |
| reward_velocity         | 0.386      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 60         |
|    time_elapsed         | 219        |
|    total_timesteps      | 7680       |
| train/                  |            |
|    approx_kl            | 0.19934171 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.05       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.366      |
|    value_loss           | 12.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.014     |
| reward_motion           | 0.214     |
| reward_position         | 0.381     |
| reward_torque           | 0.00915   |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 61        |
|    time_elapsed         | 223       |
|    total_timesteps      | 7808      |
| train/                  |           |
|    approx_kl            | 0.6639986 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.265     |
|    learning_rate        | 0.0005    |
|    loss                 | 14.3      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.366     |
|    value_loss           | 41.6      |
---------------------------------------
--------------------------------------
| reward                  | 1.01     |
| reward_contact          | 0.0142   |
| reward_motion           | 0.214    |
| reward_position         | 0.381    |
| reward_torque           | 0.00912  |
| reward_velocity         | 0.388    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 122      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 62       |
|    time_elapsed         | 226      |
|    total_timesteps      | 7936     |
| train/                  |          |
|    approx_kl            | 1.003297 |
|    clip_fraction        | 0.659    |
|    clip_range           | 0.4      |
|    entropy_loss         | -98.7    |
|    explained_variance   | 0.294    |
|    learning_rate        | 0.0005   |
|    loss                 | 12.1     |
|    n_updates            | 1220     |
|    policy_gradient_loss | -0.169   |
|    std                  | 0.366    |
|    value_loss           | 43.5     |
--------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0145    |
| reward_motion           | 0.215     |
| reward_position         | 0.38      |
| reward_torque           | 0.00909   |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 63        |
|    time_elapsed         | 230       |
|    total_timesteps      | 8064      |
| train/                  |           |
|    approx_kl            | 1.5706038 |
|    clip_fraction        | 0.722     |
|    clip_range           | 0.4       |
|    entropy_loss         | -100      |
|    explained_variance   | 0.686     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.11      |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.366     |
|    value_loss           | 24        |
---------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0143     |
| reward_motion           | 0.215      |
| reward_position         | 0.379      |
| reward_torque           | 0.0091     |
| reward_velocity         | 0.388      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 64         |
|    time_elapsed         | 234        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.26951572 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.72       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.366      |
|    value_loss           | 10.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0142    |
| reward_motion           | 0.215     |
| reward_position         | 0.379     |
| reward_torque           | 0.00911   |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 65        |
|    time_elapsed         | 237       |
|    total_timesteps      | 8320      |
| train/                  |           |
|    approx_kl            | 0.4656298 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.511     |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.366     |
|    value_loss           | 17.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0142     |
| reward_motion           | 0.216      |
| reward_position         | 0.379      |
| reward_torque           | 0.00912    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 66         |
|    time_elapsed         | 241        |
|    total_timesteps      | 8448       |
| train/                  |            |
|    approx_kl            | 0.44841817 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.03       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.366      |
|    value_loss           | 9.47       |
----------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0141     |
| reward_motion           | 0.216      |
| reward_position         | 0.378      |
| reward_torque           | 0.00911    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 67         |
|    time_elapsed         | 245        |
|    total_timesteps      | 8576       |
| train/                  |            |
|    approx_kl            | 0.24765937 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.54       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.366      |
|    value_loss           | 9.59       |
----------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0141    |
| reward_motion           | 0.217     |
| reward_position         | 0.378     |
| reward_torque           | 0.00912   |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 68        |
|    time_elapsed         | 248       |
|    total_timesteps      | 8704      |
| train/                  |           |
|    approx_kl            | 0.4108873 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.592     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.73      |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.366     |
|    value_loss           | 13.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.01       |
| reward_contact          | 0.0142     |
| reward_motion           | 0.217      |
| reward_position         | 0.377      |
| reward_torque           | 0.00912    |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 69         |
|    time_elapsed         | 252        |
|    total_timesteps      | 8832       |
| train/                  |            |
|    approx_kl            | 0.72621864 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.97       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.366      |
|    value_loss           | 12         |
----------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0142    |
| reward_motion           | 0.216     |
| reward_position         | 0.377     |
| reward_torque           | 0.0091    |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 70        |
|    time_elapsed         | 256       |
|    total_timesteps      | 8960      |
| train/                  |           |
|    approx_kl            | 0.4651019 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.04      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -0.12     |
|    std                  | 0.366     |
|    value_loss           | 10.9      |
---------------------------------------
---------------------------------------
| reward                  | 1.01      |
| reward_contact          | 0.0142    |
| reward_motion           | 0.217     |
| reward_position         | 0.377     |
| reward_torque           | 0.00911   |
| reward_velocity         | 0.388     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 71        |
|    time_elapsed         | 259       |
|    total_timesteps      | 9088      |
| train/                  |           |
|    approx_kl            | 0.5619241 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.58      |
|    n_updates            | 1400      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.366     |
|    value_loss           | 12.3      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0141     |
| reward_motion           | 0.217      |
| reward_position         | 0.377      |
| reward_torque           | 0.00913    |
| reward_velocity         | 0.387      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 72         |
|    time_elapsed         | 263        |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.40680775 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0005     |
|    loss                 | 0.501      |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.366      |
|    value_loss           | 10.2       |
----------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0142     |
| reward_motion           | 0.217      |
| reward_position         | 0.376      |
| reward_torque           | 0.00913    |
| reward_velocity         | 0.386      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 73         |
|    time_elapsed         | 267        |
|    total_timesteps      | 9344       |
| train/                  |            |
|    approx_kl            | 0.47490913 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.54       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.366      |
|    value_loss           | 9.52       |
----------------------------------------
---------------------------------------
| reward                  | 1         |
| reward_contact          | 0.0143    |
| reward_motion           | 0.218     |
| reward_position         | 0.377     |
| reward_torque           | 0.00909   |
| reward_velocity         | 0.384     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 74        |
|    time_elapsed         | 270       |
|    total_timesteps      | 9472      |
| train/                  |           |
|    approx_kl            | 0.2552078 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.943     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.4       |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.366     |
|    value_loss           | 9.55      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0142     |
| reward_motion           | 0.218      |
| reward_position         | 0.377      |
| reward_torque           | 0.00908    |
| reward_velocity         | 0.384      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 75         |
|    time_elapsed         | 274        |
|    total_timesteps      | 9600       |
| train/                  |            |
|    approx_kl            | 0.42839548 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.76       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.365      |
|    value_loss           | 14.3       |
----------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0142     |
| reward_motion           | 0.219      |
| reward_position         | 0.376      |
| reward_torque           | 0.00907    |
| reward_velocity         | 0.382      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 76         |
|    time_elapsed         | 278        |
|    total_timesteps      | 9728       |
| train/                  |            |
|    approx_kl            | 0.24552415 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.71       |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.365      |
|    value_loss           | 7.97       |
----------------------------------------
----------------------------------------
| reward                  | 0.999      |
| reward_contact          | 0.0142     |
| reward_motion           | 0.218      |
| reward_position         | 0.376      |
| reward_torque           | 0.00906    |
| reward_velocity         | 0.381      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 77         |
|    time_elapsed         | 281        |
|    total_timesteps      | 9856       |
| train/                  |            |
|    approx_kl            | 0.26148158 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.59       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.365      |
|    value_loss           | 9.86       |
----------------------------------------
---------------------------------------
| reward                  | 1         |
| reward_contact          | 0.0141    |
| reward_motion           | 0.219     |
| reward_position         | 0.376     |
| reward_torque           | 0.00907   |
| reward_velocity         | 0.382     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 78        |
|    time_elapsed         | 285       |
|    total_timesteps      | 9984      |
| train/                  |           |
|    approx_kl            | 0.4188536 |
|    clip_fraction        | 0.466     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.28      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.365     |
|    value_loss           | 12        |
---------------------------------------
---------------------------------------
| reward                  | 0.999     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.219     |
| reward_position         | 0.376     |
| reward_torque           | 0.00906   |
| reward_velocity         | 0.381     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 79        |
|    time_elapsed         | 289       |
|    total_timesteps      | 10112     |
| train/                  |           |
|    approx_kl            | 0.3026377 |
|    clip_fraction        | 0.397     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0005    |
|    loss                 | 0.792     |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.122    |
|    std                  | 0.365     |
|    value_loss           | 8.76      |
---------------------------------------
---------------------------------------
| reward                  | 0.999     |
| reward_contact          | 0.0143    |
| reward_motion           | 0.219     |
| reward_position         | 0.376     |
| reward_torque           | 0.00906   |
| reward_velocity         | 0.381     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 80        |
|    time_elapsed         | 293       |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.3560415 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.11      |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.365     |
|    value_loss           | 21.6      |
---------------------------------------
----------------------------------------
| reward                  | 1          |
| reward_contact          | 0.0142     |
| reward_motion           | 0.219      |
| reward_position         | 0.376      |
| reward_torque           | 0.00906    |
| reward_velocity         | 0.382      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 81         |
|    time_elapsed         | 296        |
|    total_timesteps      | 10368      |
| train/                  |            |
|    approx_kl            | 0.35006475 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.68       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.365      |
|    value_loss           | 34.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.999      |
| reward_contact          | 0.0142     |
| reward_motion           | 0.219      |
| reward_position         | 0.376      |
| reward_torque           | 0.00907    |
| reward_velocity         | 0.381      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 82         |
|    time_elapsed         | 300        |
|    total_timesteps      | 10496      |
| train/                  |            |
|    approx_kl            | 0.30006438 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.09       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.365      |
|    value_loss           | 9.75       |
----------------------------------------
---------------------------------------
| reward                  | 0.997     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.218     |
| reward_position         | 0.376     |
| reward_torque           | 0.00905   |
| reward_velocity         | 0.379     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 83        |
|    time_elapsed         | 303       |
|    total_timesteps      | 10624     |
| train/                  |           |
|    approx_kl            | 0.7875911 |
|    clip_fraction        | 0.539     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.947     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.5       |
|    n_updates            | 1640      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.365     |
|    value_loss           | 9.23      |
---------------------------------------
---------------------------------------
| reward                  | 0.996     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.217     |
| reward_position         | 0.376     |
| reward_torque           | 0.00903   |
| reward_velocity         | 0.38      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 84        |
|    time_elapsed         | 307       |
|    total_timesteps      | 10752     |
| train/                  |           |
|    approx_kl            | 1.2541109 |
|    clip_fraction        | 0.652     |
|    clip_range           | 0.4       |
|    entropy_loss         | -98.7     |
|    explained_variance   | 0.242     |
|    learning_rate        | 0.0005    |
|    loss                 | 24.4      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.175    |
|    std                  | 0.365     |
|    value_loss           | 64.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.994     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.217     |
| reward_position         | 0.376     |
| reward_torque           | 0.00902   |
| reward_velocity         | 0.378     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 121       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 85        |
|    time_elapsed         | 311       |
|    total_timesteps      | 10880     |
| train/                  |           |
|    approx_kl            | 0.7522881 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.4       |
|    entropy_loss         | -102      |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.19      |
|    n_updates            | 1680      |
|    policy_gradient_loss | -0.182    |
|    std                  | 0.365     |
|    value_loss           | 28.9      |
---------------------------------------
----------------------------------------
| reward                  | 0.993      |
| reward_contact          | 0.0143     |
| reward_motion           | 0.217      |
| reward_position         | 0.375      |
| reward_torque           | 0.00901    |
| reward_velocity         | 0.377      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 86         |
|    time_elapsed         | 315        |
|    total_timesteps      | 11008      |
| train/                  |            |
|    approx_kl            | 0.47382364 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.13       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.248     |
|    std                  | 0.365      |
|    value_loss           | 14.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.992     |
| reward_contact          | 0.0143    |
| reward_motion           | 0.218     |
| reward_position         | 0.375     |
| reward_torque           | 0.00899   |
| reward_velocity         | 0.376     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 87        |
|    time_elapsed         | 318       |
|    total_timesteps      | 11136     |
| train/                  |           |
|    approx_kl            | 0.9227168 |
|    clip_fraction        | 0.623     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.11      |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.365     |
|    value_loss           | 12.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.992      |
| reward_contact          | 0.0142     |
| reward_motion           | 0.218      |
| reward_position         | 0.375      |
| reward_torque           | 0.009      |
| reward_velocity         | 0.376      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 88         |
|    time_elapsed         | 322        |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.22903866 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.28       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.365      |
|    value_loss           | 21.2       |
----------------------------------------
----------------------------------------
| reward                  | 0.992      |
| reward_contact          | 0.0141     |
| reward_motion           | 0.218      |
| reward_position         | 0.374      |
| reward_torque           | 0.00901    |
| reward_velocity         | 0.377      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 89         |
|    time_elapsed         | 326        |
|    total_timesteps      | 11392      |
| train/                  |            |
|    approx_kl            | 0.45712984 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.35       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.365      |
|    value_loss           | 13.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.991      |
| reward_contact          | 0.014      |
| reward_motion           | 0.217      |
| reward_position         | 0.373      |
| reward_torque           | 0.00903    |
| reward_velocity         | 0.377      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 90         |
|    time_elapsed         | 329        |
|    total_timesteps      | 11520      |
| train/                  |            |
|    approx_kl            | 0.26511505 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.14       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.365      |
|    value_loss           | 12         |
----------------------------------------
----------------------------------------
| reward                  | 0.989      |
| reward_contact          | 0.0141     |
| reward_motion           | 0.217      |
| reward_position         | 0.373      |
| reward_torque           | 0.00901    |
| reward_velocity         | 0.376      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 91         |
|    time_elapsed         | 333        |
|    total_timesteps      | 11648      |
| train/                  |            |
|    approx_kl            | 0.36786425 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.12       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.365      |
|    value_loss           | 13.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.989     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.217     |
| reward_position         | 0.373     |
| reward_torque           | 0.00902   |
| reward_velocity         | 0.375     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 92        |
|    time_elapsed         | 337       |
|    total_timesteps      | 11776     |
| train/                  |           |
|    approx_kl            | 0.4807947 |
|    clip_fraction        | 0.439     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.23      |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.365     |
|    value_loss           | 12.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.987     |
| reward_contact          | 0.0141    |
| reward_motion           | 0.217     |
| reward_position         | 0.373     |
| reward_torque           | 0.00901   |
| reward_velocity         | 0.373     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 93        |
|    time_elapsed         | 341       |
|    total_timesteps      | 11904     |
| train/                  |           |
|    approx_kl            | 0.3284431 |
|    clip_fraction        | 0.427     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.21      |
|    n_updates            | 1840      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.365     |
|    value_loss           | 5.75      |
---------------------------------------
Num timesteps: 12000
Best mean reward: 121.84 - Last mean reward per episode: 120.12
---------------------------------------
| reward                  | 0.988     |
| reward_contact          | 0.0141    |
| reward_motion           | 0.217     |
| reward_position         | 0.373     |
| reward_torque           | 0.00901   |
| reward_velocity         | 0.374     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 94        |
|    time_elapsed         | 344       |
|    total_timesteps      | 12032     |
| train/                  |           |
|    approx_kl            | 0.5418863 |
|    clip_fraction        | 0.537     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.568     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.76      |
|    n_updates            | 1860      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.365     |
|    value_loss           | 18.6      |
---------------------------------------
---------------------------------------
| reward                  | 0.987     |
| reward_contact          | 0.0142    |
| reward_motion           | 0.218     |
| reward_position         | 0.373     |
| reward_torque           | 0.00899   |
| reward_velocity         | 0.373     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 95        |
|    time_elapsed         | 348       |
|    total_timesteps      | 12160     |
| train/                  |           |
|    approx_kl            | 0.6222862 |
|    clip_fraction        | 0.622     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.271     |
|    learning_rate        | 0.0005    |
|    loss                 | 14        |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.365     |
|    value_loss           | 46        |
---------------------------------------
----------------------------------------
| reward                  | 0.986      |
| reward_contact          | 0.0142     |
| reward_motion           | 0.218      |
| reward_position         | 0.373      |
| reward_torque           | 0.00899    |
| reward_velocity         | 0.372      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 96         |
|    time_elapsed         | 352        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.49796227 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.0666     |
|    learning_rate        | 0.0005     |
|    loss                 | 3.95       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.365      |
|    value_loss           | 19.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.988      |
| reward_contact          | 0.0141     |
| reward_motion           | 0.218      |
| reward_position         | 0.374      |
| reward_torque           | 0.009      |
| reward_velocity         | 0.373      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 97         |
|    time_elapsed         | 355        |
|    total_timesteps      | 12416      |
| train/                  |            |
|    approx_kl            | 0.51596916 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.57       |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.365      |
|    value_loss           | 25.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.987      |
| reward_contact          | 0.0141     |
| reward_motion           | 0.218      |
| reward_position         | 0.373      |
| reward_torque           | 0.00901    |
| reward_velocity         | 0.372      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 98         |
|    time_elapsed         | 359        |
|    total_timesteps      | 12544      |
| train/                  |            |
|    approx_kl            | 0.24257088 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.9        |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.365      |
|    value_loss           | 11.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.988      |
| reward_contact          | 0.014      |
| reward_motion           | 0.218      |
| reward_position         | 0.374      |
| reward_torque           | 0.00901    |
| reward_velocity         | 0.374      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 99         |
|    time_elapsed         | 363        |
|    total_timesteps      | 12672      |
| train/                  |            |
|    approx_kl            | 0.52234304 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.78       |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.365      |
|    value_loss           | 19.3       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp75/PPO_2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 128      |
---------------------------------
---------------------------------------
| reward                  | 0.963     |
| reward_contact          | 0.0148    |
| reward_motion           | 0.177     |
| reward_position         | 0.397     |
| reward_torque           | 0.000996  |
| reward_velocity         | 0.374     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 116       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 2         |
|    time_elapsed         | 7         |
|    total_timesteps      | 256       |
| train/                  |           |
|    approx_kl            | 3.0025377 |
|    clip_fraction        | 0.753     |
|    clip_range           | 0.4       |
|    entropy_loss         | -92.5     |
|    explained_variance   | 0.00167   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.59      |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.00381   |
|    std                  | 0.368     |
|    value_loss           | 8.62      |
---------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0142    |
| reward_motion           | 0.203     |
| reward_position         | 0.367     |
| reward_torque           | 0.00104   |
| reward_velocity         | 0.515     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 119       |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 3         |
|    time_elapsed         | 10        |
|    total_timesteps      | 384       |
| train/                  |           |
|    approx_kl            | 0.5534468 |
|    clip_fraction        | 0.481     |
|    clip_range           | 0.4       |
|    entropy_loss         | -95.1     |
|    explained_variance   | -1.43     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.18      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.144    |
|    std                  | 0.367     |
|    value_loss           | 46.2      |
---------------------------------------
----------------------------------------
| reward                  | 1.18       |
| reward_contact          | 0.013      |
| reward_motion           | 0.185      |
| reward_position         | 0.383      |
| reward_torque           | 0.00109    |
| reward_velocity         | 0.594      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 4          |
|    time_elapsed         | 14         |
|    total_timesteps      | 512        |
| train/                  |            |
|    approx_kl            | 0.51381737 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.4        |
|    entropy_loss         | -95        |
|    explained_variance   | 0.0862     |
|    learning_rate        | 0.0005     |
|    loss                 | 15.2       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.367      |
|    value_loss           | 61.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.23       |
| reward_contact          | 0.0129     |
| reward_motion           | 0.195      |
| reward_position         | 0.38       |
| reward_torque           | 0.00113    |
| reward_velocity         | 0.638      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 119        |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 5          |
|    time_elapsed         | 17         |
|    total_timesteps      | 640        |
| train/                  |            |
|    approx_kl            | 0.38301682 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -98.7      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.9        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.367      |
|    value_loss           | 31.6       |
----------------------------------------
----------------------------------------
| reward                  | 1.15       |
| reward_contact          | 0.0198     |
| reward_motion           | 0.203      |
| reward_position         | 0.372      |
| reward_torque           | 0.00104    |
| reward_velocity         | 0.551      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 118        |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 768        |
| train/                  |            |
|    approx_kl            | 0.34707642 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | -1.9       |
|    learning_rate        | 0.0005     |
|    loss                 | 6          |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.367      |
|    value_loss           | 54.2       |
----------------------------------------
---------------------------------------
| reward                  | 1.11      |
| reward_contact          | 0.0222    |
| reward_motion           | 0.211     |
| reward_position         | 0.369     |
| reward_torque           | 0.000997  |
| reward_velocity         | 0.509     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 120       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 7         |
|    time_elapsed         | 24        |
|    total_timesteps      | 896       |
| train/                  |           |
|    approx_kl            | 0.5183135 |
|    clip_fraction        | 0.545     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.244     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.528     |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.367     |
|    value_loss           | 12.2      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.215      |
| reward_position         | 0.37       |
| reward_torque           | 0.000953   |
| reward_velocity         | 0.465      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 8          |
|    time_elapsed         | 28         |
|    total_timesteps      | 1024       |
| train/                  |            |
|    approx_kl            | 0.36172745 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.35       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0946    |
|    std                  | 0.367      |
|    value_loss           | 30.4       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0227     |
| reward_motion           | 0.219      |
| reward_position         | 0.381      |
| reward_torque           | 0.000988   |
| reward_velocity         | 0.473      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 9          |
|    time_elapsed         | 32         |
|    total_timesteps      | 1152       |
| train/                  |            |
|    approx_kl            | 0.33443734 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.74       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.367      |
|    value_loss           | 26.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.11       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.223      |
| reward_position         | 0.383      |
| reward_torque           | 0.00102    |
| reward_velocity         | 0.477      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 10         |
|    time_elapsed         | 35         |
|    total_timesteps      | 1280       |
| train/                  |            |
|    approx_kl            | 0.35131323 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.25       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0856    |
|    std                  | 0.367      |
|    value_loss           | 22.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.11       |
| reward_contact          | 0.0206     |
| reward_motion           | 0.225      |
| reward_position         | 0.392      |
| reward_torque           | 0.000996   |
| reward_velocity         | 0.468      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 11         |
|    time_elapsed         | 39         |
|    total_timesteps      | 1408       |
| train/                  |            |
|    approx_kl            | 0.38410965 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.7        |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.367      |
|    value_loss           | 21.6       |
----------------------------------------
----------------------------------------
| reward                  | 1.11       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.227      |
| reward_position         | 0.391      |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.475      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 12         |
|    time_elapsed         | 42         |
|    total_timesteps      | 1536       |
| train/                  |            |
|    approx_kl            | 0.29276735 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -101       |
|    explained_variance   | -0.000955  |
|    learning_rate        | 0.0005     |
|    loss                 | 11.3       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.367      |
|    value_loss           | 42.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0203    |
| reward_motion           | 0.229     |
| reward_position         | 0.392     |
| reward_torque           | 0.000972  |
| reward_velocity         | 0.462     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 122       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 13        |
|    time_elapsed         | 46        |
|    total_timesteps      | 1664      |
| train/                  |           |
|    approx_kl            | 0.1897872 |
|    clip_fraction        | 0.287     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.871     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.34      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.107    |
|    std                  | 0.367     |
|    value_loss           | 18.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0202     |
| reward_motion           | 0.228      |
| reward_position         | 0.392      |
| reward_torque           | 0.000966   |
| reward_velocity         | 0.451      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 14         |
|    time_elapsed         | 50         |
|    total_timesteps      | 1792       |
| train/                  |            |
|    approx_kl            | 0.42861506 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.112      |
|    learning_rate        | 0.0005     |
|    loss                 | 11.8       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.367      |
|    value_loss           | 37         |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0201     |
| reward_motion           | 0.227      |
| reward_position         | 0.392      |
| reward_torque           | 0.000952   |
| reward_velocity         | 0.452      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 123        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 15         |
|    time_elapsed         | 53         |
|    total_timesteps      | 1920       |
| train/                  |            |
|    approx_kl            | 0.37356043 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.74       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.367      |
|    value_loss           | 31.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.11       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.229      |
| reward_position         | 0.391      |
| reward_torque           | 0.000971   |
| reward_velocity         | 0.469      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 16         |
|    time_elapsed         | 57         |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.37828964 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -101       |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.44       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.367      |
|    value_loss           | 28.3       |
----------------------------------------
---------------------------------------
| reward                  | 1.13      |
| reward_contact          | 0.0191    |
| reward_motion           | 0.226     |
| reward_position         | 0.39      |
| reward_torque           | 0.000977  |
| reward_velocity         | 0.49      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 17        |
|    time_elapsed         | 60        |
|    total_timesteps      | 2176      |
| train/                  |           |
|    approx_kl            | 0.9861245 |
|    clip_fraction        | 0.599     |
|    clip_range           | 0.4       |
|    entropy_loss         | -99.8     |
|    explained_variance   | 0.0544    |
|    learning_rate        | 0.0005    |
|    loss                 | 18.7      |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.367     |
|    value_loss           | 53.9      |
---------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0186     |
| reward_motion           | 0.228      |
| reward_position         | 0.39       |
| reward_torque           | 0.000965   |
| reward_velocity         | 0.5        |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 18         |
|    time_elapsed         | 64         |
|    total_timesteps      | 2304       |
| train/                  |            |
|    approx_kl            | 0.28149158 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -101       |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.51       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.367      |
|    value_loss           | 29.2       |
----------------------------------------
--------------------------------------
| reward                  | 1.15     |
| reward_contact          | 0.0211   |
| reward_motion           | 0.228    |
| reward_position         | 0.386    |
| reward_torque           | 0.000976 |
| reward_velocity         | 0.512    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 125      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 19       |
|    time_elapsed         | 67       |
|    total_timesteps      | 2432     |
| train/                  |          |
|    approx_kl            | 0.394763 |
|    clip_fraction        | 0.461    |
|    clip_range           | 0.4      |
|    entropy_loss         | -101     |
|    explained_variance   | 0.627    |
|    learning_rate        | 0.0005   |
|    loss                 | 5.24     |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.123   |
|    std                  | 0.367    |
|    value_loss           | 28.7     |
--------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0234     |
| reward_motion           | 0.226      |
| reward_position         | 0.389      |
| reward_torque           | 0.000981   |
| reward_velocity         | 0.505      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 20         |
|    time_elapsed         | 71         |
|    total_timesteps      | 2560       |
| train/                  |            |
|    approx_kl            | 0.23838401 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.29       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.367      |
|    value_loss           | 20.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0243     |
| reward_motion           | 0.227      |
| reward_position         | 0.387      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.501      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 21         |
|    time_elapsed         | 75         |
|    total_timesteps      | 2688       |
| train/                  |            |
|    approx_kl            | 0.24346086 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.92       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.367      |
|    value_loss           | 16.1       |
----------------------------------------
---------------------------------------
| reward                  | 1.13      |
| reward_contact          | 0.0248    |
| reward_motion           | 0.228     |
| reward_position         | 0.388     |
| reward_torque           | 0.000968  |
| reward_velocity         | 0.486     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 22        |
|    time_elapsed         | 78        |
|    total_timesteps      | 2816      |
| train/                  |           |
|    approx_kl            | 0.4520371 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | 0.81      |
|    learning_rate        | 0.0005    |
|    loss                 | 2.21      |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.367     |
|    value_loss           | 16.9      |
---------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0242     |
| reward_motion           | 0.229      |
| reward_position         | 0.387      |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.497      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 23         |
|    time_elapsed         | 82         |
|    total_timesteps      | 2944       |
| train/                  |            |
|    approx_kl            | 0.17892429 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.468      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.48       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.367      |
|    value_loss           | 31         |
----------------------------------------
---------------------------------------
| reward                  | 1.14      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.23      |
| reward_position         | 0.389     |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.496     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 24        |
|    time_elapsed         | 85        |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.3719547 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.14      |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.367     |
|    value_loss           | 9.83      |
---------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.23       |
| reward_position         | 0.387      |
| reward_torque           | 0.000985   |
| reward_velocity         | 0.498      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 25         |
|    time_elapsed         | 89         |
|    total_timesteps      | 3200       |
| train/                  |            |
|    approx_kl            | 0.40977088 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.13       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.367      |
|    value_loss           | 16.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.15      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.23      |
| reward_position         | 0.388     |
| reward_torque           | 0.000985  |
| reward_velocity         | 0.504     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 26        |
|    time_elapsed         | 92        |
|    total_timesteps      | 3328      |
| train/                  |           |
|    approx_kl            | 1.2950157 |
|    clip_fraction        | 0.707     |
|    clip_range           | 0.4       |
|    entropy_loss         | -97.6     |
|    explained_variance   | 0.0224    |
|    learning_rate        | 0.0005    |
|    loss                 | 30.2      |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.14     |
|    std                  | 0.367     |
|    value_loss           | 95        |
---------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0231     |
| reward_motion           | 0.231      |
| reward_position         | 0.387      |
| reward_torque           | 0.000979   |
| reward_velocity         | 0.5        |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 27         |
|    time_elapsed         | 96         |
|    total_timesteps      | 3456       |
| train/                  |            |
|    approx_kl            | 0.58006036 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.37       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.367      |
|    value_loss           | 18.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.14       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.229      |
| reward_position         | 0.387      |
| reward_torque           | 0.000988   |
| reward_velocity         | 0.497      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 28         |
|    time_elapsed         | 100        |
|    total_timesteps      | 3584       |
| train/                  |            |
|    approx_kl            | 0.28333038 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.17       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.367      |
|    value_loss           | 13.4       |
----------------------------------------
----------------------------------------
| reward                  | 1.13       |
| reward_contact          | 0.0226     |
| reward_motion           | 0.227      |
| reward_position         | 0.387      |
| reward_torque           | 0.000981   |
| reward_velocity         | 0.493      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 29         |
|    time_elapsed         | 103        |
|    total_timesteps      | 3712       |
| train/                  |            |
|    approx_kl            | 0.36436623 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.52       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.367      |
|    value_loss           | 13.2       |
----------------------------------------
---------------------------------------
| reward                  | 1.12      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.227     |
| reward_position         | 0.386     |
| reward_torque           | 0.000976  |
| reward_velocity         | 0.486     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 30        |
|    time_elapsed         | 107       |
|    total_timesteps      | 3840      |
| train/                  |           |
|    approx_kl            | 0.4045901 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.84      |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.367     |
|    value_loss           | 14.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.12      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.225     |
| reward_position         | 0.386     |
| reward_torque           | 0.000978  |
| reward_velocity         | 0.482     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 31        |
|    time_elapsed         | 110       |
|    total_timesteps      | 3968      |
| train/                  |           |
|    approx_kl            | 0.3692574 |
|    clip_fraction        | 0.524     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.94      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.62      |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.366     |
|    value_loss           | 11.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.12       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.226      |
| reward_position         | 0.385      |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.483      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 32         |
|    time_elapsed         | 114        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.33658385 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.24       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.366      |
|    value_loss           | 10.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.11       |
| reward_contact          | 0.024      |
| reward_motion           | 0.223      |
| reward_position         | 0.383      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.474      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 33         |
|    time_elapsed         | 117        |
|    total_timesteps      | 4224       |
| train/                  |            |
|    approx_kl            | 0.36512893 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.42       |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.366      |
|    value_loss           | 15.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0241     |
| reward_motion           | 0.224      |
| reward_position         | 0.382      |
| reward_torque           | 0.000981   |
| reward_velocity         | 0.467      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 34         |
|    time_elapsed         | 121        |
|    total_timesteps      | 4352       |
| train/                  |            |
|    approx_kl            | 0.22671045 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.81       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.366      |
|    value_loss           | 13.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0241    |
| reward_motion           | 0.223     |
| reward_position         | 0.382     |
| reward_torque           | 0.000976  |
| reward_velocity         | 0.462     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 35        |
|    time_elapsed         | 124       |
|    total_timesteps      | 4480      |
| train/                  |           |
|    approx_kl            | 0.6917304 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.94      |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.366     |
|    value_loss           | 11.2      |
---------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0239    |
| reward_motion           | 0.221     |
| reward_position         | 0.383     |
| reward_torque           | 0.000974  |
| reward_velocity         | 0.462     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 36        |
|    time_elapsed         | 128       |
|    total_timesteps      | 4608      |
| train/                  |           |
|    approx_kl            | 1.0309976 |
|    clip_fraction        | 0.621     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.57      |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.366     |
|    value_loss           | 23.1      |
---------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0236     |
| reward_motion           | 0.221      |
| reward_position         | 0.384      |
| reward_torque           | 0.000982   |
| reward_velocity         | 0.474      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 37         |
|    time_elapsed         | 132        |
|    total_timesteps      | 4736       |
| train/                  |            |
|    approx_kl            | 0.46979463 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.4        |
|    entropy_loss         | -98.7      |
|    explained_variance   | 0.275      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.51       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.366      |
|    value_loss           | 47.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0233     |
| reward_motion           | 0.222      |
| reward_position         | 0.384      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.472      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 38         |
|    time_elapsed         | 135        |
|    total_timesteps      | 4864       |
| train/                  |            |
|    approx_kl            | 0.32007498 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.78       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.367      |
|    value_loss           | 42.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0236    |
| reward_motion           | 0.222     |
| reward_position         | 0.383     |
| reward_torque           | 0.000979  |
| reward_velocity         | 0.471     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 39        |
|    time_elapsed         | 139       |
|    total_timesteps      | 4992      |
| train/                  |           |
|    approx_kl            | 0.5604662 |
|    clip_fraction        | 0.533     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.851     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.01      |
|    n_updates            | 760       |
|    policy_gradient_loss | -0.142    |
|    std                  | 0.367     |
|    value_loss           | 12.4      |
---------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0239    |
| reward_motion           | 0.222     |
| reward_position         | 0.385     |
| reward_torque           | 0.000972  |
| reward_velocity         | 0.465     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 40        |
|    time_elapsed         | 142       |
|    total_timesteps      | 5120      |
| train/                  |           |
|    approx_kl            | 0.2799111 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.799     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.28      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.366     |
|    value_loss           | 18.9      |
---------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.222     |
| reward_position         | 0.385     |
| reward_torque           | 0.000971  |
| reward_velocity         | 0.461     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 41        |
|    time_elapsed         | 146       |
|    total_timesteps      | 5248      |
| train/                  |           |
|    approx_kl            | 0.5144342 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.72      |
|    n_updates            | 800       |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.366     |
|    value_loss           | 13        |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.223      |
| reward_position         | 0.384      |
| reward_torque           | 0.000967   |
| reward_velocity         | 0.456      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 42         |
|    time_elapsed         | 150        |
|    total_timesteps      | 5376       |
| train/                  |            |
|    approx_kl            | 0.61161196 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.62       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.226     |
|    std                  | 0.366      |
|    value_loss           | 16.2       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.223     |
| reward_position         | 0.384     |
| reward_torque           | 0.000969  |
| reward_velocity         | 0.457     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 43        |
|    time_elapsed         | 154       |
|    total_timesteps      | 5504      |
| train/                  |           |
|    approx_kl            | 0.6317704 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.747     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.55      |
|    n_updates            | 840       |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.366     |
|    value_loss           | 16.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.224      |
| reward_position         | 0.384      |
| reward_torque           | 0.000971   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 44         |
|    time_elapsed         | 157        |
|    total_timesteps      | 5632       |
| train/                  |            |
|    approx_kl            | 0.48328525 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.03       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.366      |
|    value_loss           | 19.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0225     |
| reward_motion           | 0.222      |
| reward_position         | 0.384      |
| reward_torque           | 0.000972   |
| reward_velocity         | 0.457      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 45         |
|    time_elapsed         | 161        |
|    total_timesteps      | 5760       |
| train/                  |            |
|    approx_kl            | 0.47102273 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.4        |
|    entropy_loss         | -100       |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.73       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.366      |
|    value_loss           | 16.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.222     |
| reward_position         | 0.383     |
| reward_torque           | 0.000967  |
| reward_velocity         | 0.453     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 46        |
|    time_elapsed         | 164       |
|    total_timesteps      | 5888      |
| train/                  |           |
|    approx_kl            | 0.5478488 |
|    clip_fraction        | 0.482     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.64      |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.366     |
|    value_loss           | 13.1      |
---------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 124.44
Saving new best model to rl/out_dir/models/exp75/best_model.zip
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0228    |
| reward_motion           | 0.222     |
| reward_position         | 0.383     |
| reward_torque           | 0.000965  |
| reward_velocity         | 0.455     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 47        |
|    time_elapsed         | 168       |
|    total_timesteps      | 6016      |
| train/                  |           |
|    approx_kl            | 0.3556365 |
|    clip_fraction        | 0.481     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.644     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.12      |
|    n_updates            | 920       |
|    policy_gradient_loss | -0.14     |
|    std                  | 0.366     |
|    value_loss           | 21        |
---------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.222     |
| reward_position         | 0.382     |
| reward_torque           | 0.000961  |
| reward_velocity         | 0.45      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 48        |
|    time_elapsed         | 172       |
|    total_timesteps      | 6144      |
| train/                  |           |
|    approx_kl            | 0.2195761 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.71      |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.118    |
|    std                  | 0.366     |
|    value_loss           | 10.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.223      |
| reward_position         | 0.381      |
| reward_torque           | 0.000957   |
| reward_velocity         | 0.45       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 49         |
|    time_elapsed         | 175        |
|    total_timesteps      | 6272       |
| train/                  |            |
|    approx_kl            | 0.31821376 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.12       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.366      |
|    value_loss           | 24         |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.223      |
| reward_position         | 0.382      |
| reward_torque           | 0.000957   |
| reward_velocity         | 0.458      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 50         |
|    time_elapsed         | 179        |
|    total_timesteps      | 6400       |
| train/                  |            |
|    approx_kl            | 0.81778646 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.45       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.366      |
|    value_loss           | 11.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.223      |
| reward_position         | 0.381      |
| reward_torque           | 0.000963   |
| reward_velocity         | 0.461      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 51         |
|    time_elapsed         | 183        |
|    total_timesteps      | 6528       |
| train/                  |            |
|    approx_kl            | 0.62296736 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.1        |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.366      |
|    value_loss           | 15.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.223      |
| reward_position         | 0.382      |
| reward_torque           | 0.000959   |
| reward_velocity         | 0.459      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 52         |
|    time_elapsed         | 186        |
|    total_timesteps      | 6656       |
| train/                  |            |
|    approx_kl            | 0.82669497 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.39       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.366      |
|    value_loss           | 18.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.223      |
| reward_position         | 0.382      |
| reward_torque           | 0.000964   |
| reward_velocity         | 0.463      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 53         |
|    time_elapsed         | 190        |
|    total_timesteps      | 6784       |
| train/                  |            |
|    approx_kl            | 0.24953583 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.86       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.366      |
|    value_loss           | 12.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.224     |
| reward_position         | 0.382     |
| reward_torque           | 0.000969  |
| reward_velocity         | 0.462     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 54        |
|    time_elapsed         | 194       |
|    total_timesteps      | 6912      |
| train/                  |           |
|    approx_kl            | 0.2861852 |
|    clip_fraction        | 0.493     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.79      |
|    n_updates            | 1060      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.366     |
|    value_loss           | 18        |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.224      |
| reward_position         | 0.382      |
| reward_torque           | 0.000974   |
| reward_velocity         | 0.46       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 55         |
|    time_elapsed         | 197        |
|    total_timesteps      | 7040       |
| train/                  |            |
|    approx_kl            | 0.45975977 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.3        |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 18.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.224     |
| reward_position         | 0.382     |
| reward_torque           | 0.000972  |
| reward_velocity         | 0.459     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 56        |
|    time_elapsed         | 201       |
|    total_timesteps      | 7168      |
| train/                  |           |
|    approx_kl            | 0.4477735 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.556     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.15      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.148    |
|    std                  | 0.365     |
|    value_loss           | 17.3      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.225      |
| reward_position         | 0.382      |
| reward_torque           | 0.000974   |
| reward_velocity         | 0.459      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 57         |
|    time_elapsed         | 205        |
|    total_timesteps      | 7296       |
| train/                  |            |
|    approx_kl            | 0.39917165 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.17       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.365      |
|    value_loss           | 15.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.023      |
| reward_motion           | 0.225      |
| reward_position         | 0.382      |
| reward_torque           | 0.000979   |
| reward_velocity         | 0.458      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 58         |
|    time_elapsed         | 208        |
|    total_timesteps      | 7424       |
| train/                  |            |
|    approx_kl            | 0.31431556 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.41       |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.365      |
|    value_loss           | 16.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0228    |
| reward_motion           | 0.225     |
| reward_position         | 0.383     |
| reward_torque           | 0.00098   |
| reward_velocity         | 0.461     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 59        |
|    time_elapsed         | 212       |
|    total_timesteps      | 7552      |
| train/                  |           |
|    approx_kl            | 0.9164225 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -100      |
|    explained_variance   | 0.577     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07      |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.365     |
|    value_loss           | 24        |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.225      |
| reward_position         | 0.382      |
| reward_torque           | 0.000978   |
| reward_velocity         | 0.458      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 60         |
|    time_elapsed         | 215        |
|    total_timesteps      | 7680       |
| train/                  |            |
|    approx_kl            | 0.40021554 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.33       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.365      |
|    value_loss           | 6.83       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0231     |
| reward_motion           | 0.225      |
| reward_position         | 0.382      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.465      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 61         |
|    time_elapsed         | 219        |
|    total_timesteps      | 7808       |
| train/                  |            |
|    approx_kl            | 0.48866066 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -96.1      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.17       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.365      |
|    value_loss           | 26.4       |
----------------------------------------
--------------------------------------
| reward                  | 1.1      |
| reward_contact          | 0.0229   |
| reward_motion           | 0.225    |
| reward_position         | 0.382    |
| reward_torque           | 0.000987 |
| reward_velocity         | 0.473    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 125      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 62       |
|    time_elapsed         | 223      |
|    total_timesteps      | 7936     |
| train/                  |          |
|    approx_kl            | 2.149974 |
|    clip_fraction        | 0.647    |
|    clip_range           | 0.4      |
|    entropy_loss         | -94.2    |
|    explained_variance   | 0.118    |
|    learning_rate        | 0.0005   |
|    loss                 | 17.5     |
|    n_updates            | 1220     |
|    policy_gradient_loss | -0.159   |
|    std                  | 0.365    |
|    value_loss           | 65.5     |
--------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0227    |
| reward_motion           | 0.225     |
| reward_position         | 0.382     |
| reward_torque           | 0.000988  |
| reward_velocity         | 0.473     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 63        |
|    time_elapsed         | 226       |
|    total_timesteps      | 8064      |
| train/                  |           |
|    approx_kl            | 0.4864245 |
|    clip_fraction        | 0.546     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.905     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.66      |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.365     |
|    value_loss           | 9.05      |
---------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0227     |
| reward_motion           | 0.224      |
| reward_position         | 0.382      |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.471      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 64         |
|    time_elapsed         | 230        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.62317085 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.84       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.365      |
|    value_loss           | 18.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0225     |
| reward_motion           | 0.224      |
| reward_position         | 0.382      |
| reward_torque           | 0.000988   |
| reward_velocity         | 0.471      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 65         |
|    time_elapsed         | 233        |
|    total_timesteps      | 8320       |
| train/                  |            |
|    approx_kl            | 0.33440116 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.36       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 19.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0227     |
| reward_motion           | 0.224      |
| reward_position         | 0.381      |
| reward_torque           | 0.000984   |
| reward_velocity         | 0.467      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 66         |
|    time_elapsed         | 237        |
|    total_timesteps      | 8448       |
| train/                  |            |
|    approx_kl            | 0.29595673 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.558      |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 8.85       |
----------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0227    |
| reward_motion           | 0.224     |
| reward_position         | 0.382     |
| reward_torque           | 0.000988  |
| reward_velocity         | 0.47      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 67        |
|    time_elapsed         | 241       |
|    total_timesteps      | 8576      |
| train/                  |           |
|    approx_kl            | 0.3100089 |
|    clip_fraction        | 0.325     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.679     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.19      |
|    n_updates            | 1320      |
|    policy_gradient_loss | -0.106    |
|    std                  | 0.365     |
|    value_loss           | 24.3      |
---------------------------------------
---------------------------------------
| reward                  | 1.1       |
| reward_contact          | 0.0227    |
| reward_motion           | 0.225     |
| reward_position         | 0.382     |
| reward_torque           | 0.000985  |
| reward_velocity         | 0.466     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 68        |
|    time_elapsed         | 244       |
|    total_timesteps      | 8704      |
| train/                  |           |
|    approx_kl            | 0.2882186 |
|    clip_fraction        | 0.427     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.41      |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.12     |
|    std                  | 0.365     |
|    value_loss           | 16.9      |
---------------------------------------
--------------------------------------
| reward                  | 1.09     |
| reward_contact          | 0.0225   |
| reward_motion           | 0.225    |
| reward_position         | 0.382    |
| reward_torque           | 0.000984 |
| reward_velocity         | 0.464    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 125      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 69       |
|    time_elapsed         | 248      |
|    total_timesteps      | 8832     |
| train/                  |          |
|    approx_kl            | 0.581237 |
|    clip_fraction        | 0.406    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.943    |
|    learning_rate        | 0.0005   |
|    loss                 | 0.428    |
|    n_updates            | 1360     |
|    policy_gradient_loss | -0.195   |
|    std                  | 0.365    |
|    value_loss           | 6.32     |
--------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.225      |
| reward_position         | 0.382      |
| reward_torque           | 0.000988   |
| reward_velocity         | 0.463      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 70         |
|    time_elapsed         | 252        |
|    total_timesteps      | 8960       |
| train/                  |            |
|    approx_kl            | 0.23638329 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.96       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 11.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.1        |
| reward_contact          | 0.0237     |
| reward_motion           | 0.226      |
| reward_position         | 0.381      |
| reward_torque           | 0.000989   |
| reward_velocity         | 0.464      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 71         |
|    time_elapsed         | 255        |
|    total_timesteps      | 9088       |
| train/                  |            |
|    approx_kl            | 0.41311106 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.05       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.365      |
|    value_loss           | 6.98       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.226      |
| reward_position         | 0.381      |
| reward_torque           | 0.000986   |
| reward_velocity         | 0.46       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 72         |
|    time_elapsed         | 259        |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.24529849 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0005     |
|    loss                 | 4.41       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.365      |
|    value_loss           | 16.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.225     |
| reward_position         | 0.38      |
| reward_torque           | 0.000987  |
| reward_velocity         | 0.459     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 73        |
|    time_elapsed         | 263       |
|    total_timesteps      | 9344      |
| train/                  |           |
|    approx_kl            | 0.4119126 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.851     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.06      |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.365     |
|    value_loss           | 14.8      |
---------------------------------------
--------------------------------------
| reward                  | 1.09     |
| reward_contact          | 0.0237   |
| reward_motion           | 0.225    |
| reward_position         | 0.381    |
| reward_torque           | 0.000984 |
| reward_velocity         | 0.456    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 124      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 74       |
|    time_elapsed         | 266      |
|    total_timesteps      | 9472     |
| train/                  |          |
|    approx_kl            | 0.318002 |
|    clip_fraction        | 0.375    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.963    |
|    learning_rate        | 0.0005   |
|    loss                 | 0.999    |
|    n_updates            | 1460     |
|    policy_gradient_loss | -0.157   |
|    std                  | 0.365    |
|    value_loss           | 5.48     |
--------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.226     |
| reward_position         | 0.381     |
| reward_torque           | 0.000985  |
| reward_velocity         | 0.454     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 75        |
|    time_elapsed         | 270       |
|    total_timesteps      | 9600      |
| train/                  |           |
|    approx_kl            | 0.2563148 |
|    clip_fraction        | 0.377     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.51      |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.365     |
|    value_loss           | 16.3      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.226      |
| reward_position         | 0.381      |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.452      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 76         |
|    time_elapsed         | 274        |
|    total_timesteps      | 9728       |
| train/                  |            |
|    approx_kl            | 0.64854074 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.51       |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.365      |
|    value_loss           | 31.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0241     |
| reward_motion           | 0.226      |
| reward_position         | 0.38       |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 77         |
|    time_elapsed         | 277        |
|    total_timesteps      | 9856       |
| train/                  |            |
|    approx_kl            | 0.54121757 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.109      |
|    learning_rate        | 0.0005     |
|    loss                 | 13.1       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.365      |
|    value_loss           | 59         |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0239     |
| reward_motion           | 0.226      |
| reward_position         | 0.38       |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 78         |
|    time_elapsed         | 281        |
|    total_timesteps      | 9984       |
| train/                  |            |
|    approx_kl            | 0.24659356 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.18       |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 9.75       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.226     |
| reward_position         | 0.381     |
| reward_torque           | 0.000987  |
| reward_velocity         | 0.455     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 79        |
|    time_elapsed         | 285       |
|    total_timesteps      | 10112     |
| train/                  |           |
|    approx_kl            | 0.2845989 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.724     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.612     |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.365     |
|    value_loss           | 21.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.226      |
| reward_position         | 0.38       |
| reward_torque           | 0.000985   |
| reward_velocity         | 0.454      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 80         |
|    time_elapsed         | 288        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.29728225 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.63       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0939    |
|    std                  | 0.365      |
|    value_loss           | 11.7       |
----------------------------------------
--------------------------------------
| reward                  | 1.09     |
| reward_contact          | 0.0236   |
| reward_motion           | 0.226    |
| reward_position         | 0.38     |
| reward_torque           | 0.000986 |
| reward_velocity         | 0.459    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 124      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 81       |
|    time_elapsed         | 292      |
|    total_timesteps      | 10368    |
| train/                  |          |
|    approx_kl            | 1.02474  |
|    clip_fraction        | 0.636    |
|    clip_range           | 0.4      |
|    entropy_loss         | -101     |
|    explained_variance   | 0.552    |
|    learning_rate        | 0.0005   |
|    loss                 | 3.89     |
|    n_updates            | 1600     |
|    policy_gradient_loss | -0.179   |
|    std                  | 0.365    |
|    value_loss           | 20.8     |
--------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000989   |
| reward_velocity         | 0.458      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 82         |
|    time_elapsed         | 296        |
|    total_timesteps      | 10496      |
| train/                  |            |
|    approx_kl            | 0.35922086 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.981      |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.365      |
|    value_loss           | 5.13       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000988   |
| reward_velocity         | 0.456      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 83         |
|    time_elapsed         | 299        |
|    total_timesteps      | 10624      |
| train/                  |            |
|    approx_kl            | 0.34501147 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.28       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.365      |
|    value_loss           | 7.75       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0236    |
| reward_motion           | 0.227     |
| reward_position         | 0.381     |
| reward_torque           | 0.000985  |
| reward_velocity         | 0.454     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 84        |
|    time_elapsed         | 303       |
|    total_timesteps      | 10752     |
| train/                  |           |
|    approx_kl            | 0.7391174 |
|    clip_fraction        | 0.604     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.22      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.365     |
|    value_loss           | 9.4       |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.453      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 85         |
|    time_elapsed         | 306        |
|    total_timesteps      | 10880      |
| train/                  |            |
|    approx_kl            | 0.48950833 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.23       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.365      |
|    value_loss           | 8.63       |
----------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.227     |
| reward_position         | 0.38      |
| reward_torque           | 0.000983  |
| reward_velocity         | 0.452     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 86        |
|    time_elapsed         | 310       |
|    total_timesteps      | 11008     |
| train/                  |           |
|    approx_kl            | 0.6792623 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.87      |
|    n_updates            | 1700      |
|    policy_gradient_loss | -0.131    |
|    std                  | 0.365     |
|    value_loss           | 14.8      |
---------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.227     |
| reward_position         | 0.38      |
| reward_torque           | 0.000983  |
| reward_velocity         | 0.453     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 87        |
|    time_elapsed         | 314       |
|    total_timesteps      | 11136     |
| train/                  |           |
|    approx_kl            | 0.3728959 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.81      |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.137    |
|    std                  | 0.365     |
|    value_loss           | 20.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000981   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 88         |
|    time_elapsed         | 317        |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.19755553 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.98       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.365      |
|    value_loss           | 21.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0231     |
| reward_motion           | 0.226      |
| reward_position         | 0.381      |
| reward_torque           | 0.000984   |
| reward_velocity         | 0.456      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 89         |
|    time_elapsed         | 321        |
|    total_timesteps      | 11392      |
| train/                  |            |
|    approx_kl            | 0.14026771 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.33       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.365      |
|    value_loss           | 10.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0232    |
| reward_motion           | 0.226     |
| reward_position         | 0.382     |
| reward_torque           | 0.000983  |
| reward_velocity         | 0.455     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 90        |
|    time_elapsed         | 325       |
|    total_timesteps      | 11520     |
| train/                  |           |
|    approx_kl            | 0.2093384 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.689     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.89      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.0945   |
|    std                  | 0.365     |
|    value_loss           | 31        |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0231     |
| reward_motion           | 0.226      |
| reward_position         | 0.382      |
| reward_torque           | 0.000984   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 91         |
|    time_elapsed         | 328        |
|    total_timesteps      | 11648      |
| train/                  |            |
|    approx_kl            | 0.25291848 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.15       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.365      |
|    value_loss           | 8.3        |
----------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.226     |
| reward_position         | 0.382     |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.452     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 92        |
|    time_elapsed         | 332       |
|    total_timesteps      | 11776     |
| train/                  |           |
|    approx_kl            | 0.7653228 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.709     |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.365     |
|    value_loss           | 7.71      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.023      |
| reward_motion           | 0.226      |
| reward_position         | 0.382      |
| reward_torque           | 0.000982   |
| reward_velocity         | 0.452      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 93         |
|    time_elapsed         | 336        |
|    total_timesteps      | 11904      |
| train/                  |            |
|    approx_kl            | 0.45609653 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.19       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.365      |
|    value_loss           | 11.7       |
----------------------------------------
Num timesteps: 12000
Best mean reward: 124.44 - Last mean reward per episode: 124.52
Saving new best model to rl/out_dir/models/exp75/best_model.zip
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0229    |
| reward_motion           | 0.226     |
| reward_position         | 0.382     |
| reward_torque           | 0.000983  |
| reward_velocity         | 0.453     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 94        |
|    time_elapsed         | 339       |
|    total_timesteps      | 12032     |
| train/                  |           |
|    approx_kl            | 0.2592783 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.429     |
|    learning_rate        | 0.0005    |
|    loss                 | 11.7      |
|    n_updates            | 1860      |
|    policy_gradient_loss | -0.127    |
|    std                  | 0.365     |
|    value_loss           | 56.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0227    |
| reward_motion           | 0.227     |
| reward_position         | 0.382     |
| reward_torque           | 0.000983  |
| reward_velocity         | 0.454     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 95        |
|    time_elapsed         | 343       |
|    total_timesteps      | 12160     |
| train/                  |           |
|    approx_kl            | 0.7848507 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.652     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.47      |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.365     |
|    value_loss           | 22.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0226     |
| reward_motion           | 0.227      |
| reward_position         | 0.382      |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.453      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 96         |
|    time_elapsed         | 347        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.21004185 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.21       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0981    |
|    std                  | 0.365      |
|    value_loss           | 26         |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0225     |
| reward_motion           | 0.227      |
| reward_position         | 0.383      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.454      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 97         |
|    time_elapsed         | 350        |
|    total_timesteps      | 12416      |
| train/                  |            |
|    approx_kl            | 0.44434044 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.42       |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.365      |
|    value_loss           | 13         |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0224    |
| reward_motion           | 0.226     |
| reward_position         | 0.383     |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.453     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 98        |
|    time_elapsed         | 354       |
|    total_timesteps      | 12544     |
| train/                  |           |
|    approx_kl            | 0.5531196 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.909     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.541     |
|    n_updates            | 1940      |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.364     |
|    value_loss           | 6.51      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0224     |
| reward_motion           | 0.227      |
| reward_position         | 0.383      |
| reward_torque           | 0.000985   |
| reward_velocity         | 0.457      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 99         |
|    time_elapsed         | 358        |
|    total_timesteps      | 12672      |
| train/                  |            |
|    approx_kl            | 0.27710256 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.09       |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.364      |
|    value_loss           | 33.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0222     |
| reward_motion           | 0.227      |
| reward_position         | 0.383      |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.457      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 100        |
|    time_elapsed         | 361        |
|    total_timesteps      | 12800      |
| train/                  |            |
|    approx_kl            | 0.26154396 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.56       |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.364      |
|    value_loss           | 19.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0223     |
| reward_motion           | 0.227      |
| reward_position         | 0.383      |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.457      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 101        |
|    time_elapsed         | 365        |
|    total_timesteps      | 12928      |
| train/                  |            |
|    approx_kl            | 0.60580504 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.25       |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.364      |
|    value_loss           | 7.51       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0227    |
| reward_motion           | 0.227     |
| reward_position         | 0.382     |
| reward_torque           | 0.000987  |
| reward_velocity         | 0.46      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 102       |
|    time_elapsed         | 369       |
|    total_timesteps      | 13056     |
| train/                  |           |
|    approx_kl            | 0.6064263 |
|    clip_fraction        | 0.639     |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | 0.441     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.44      |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.364     |
|    value_loss           | 37.5      |
---------------------------------------
--------------------------------------
| reward                  | 1.09     |
| reward_contact          | 0.0226   |
| reward_motion           | 0.227    |
| reward_position         | 0.382    |
| reward_torque           | 0.000987 |
| reward_velocity         | 0.457    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 125      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 103      |
|    time_elapsed         | 372      |
|    total_timesteps      | 13184    |
| train/                  |          |
|    approx_kl            | 0.654166 |
|    clip_fraction        | 0.401    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.847    |
|    learning_rate        | 0.0005   |
|    loss                 | 2.51     |
|    n_updates            | 2040     |
|    policy_gradient_loss | -0.144   |
|    std                  | 0.364    |
|    value_loss           | 14.4     |
--------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.228      |
| reward_position         | 0.382      |
| reward_torque           | 0.000987   |
| reward_velocity         | 0.456      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 104        |
|    time_elapsed         | 376        |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.35794625 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.82       |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.364      |
|    value_loss           | 12.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.228      |
| reward_position         | 0.382      |
| reward_torque           | 0.000983   |
| reward_velocity         | 0.452      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 105        |
|    time_elapsed         | 380        |
|    total_timesteps      | 13440      |
| train/                  |            |
|    approx_kl            | 0.48089474 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.32       |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.364      |
|    value_loss           | 13.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0224     |
| reward_motion           | 0.228      |
| reward_position         | 0.383      |
| reward_torque           | 0.000982   |
| reward_velocity         | 0.451      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 106        |
|    time_elapsed         | 383        |
|    total_timesteps      | 13568      |
| train/                  |            |
|    approx_kl            | 0.44440097 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.762      |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.364      |
|    value_loss           | 7.02       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0222    |
| reward_motion           | 0.228     |
| reward_position         | 0.383     |
| reward_torque           | 0.000985  |
| reward_velocity         | 0.454     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 107       |
|    time_elapsed         | 387       |
|    total_timesteps      | 13696     |
| train/                  |           |
|    approx_kl            | 0.4823943 |
|    clip_fraction        | 0.482     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.472     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.96      |
|    n_updates            | 2120      |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.364     |
|    value_loss           | 46.4      |
---------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0221    |
| reward_motion           | 0.228     |
| reward_position         | 0.383     |
| reward_torque           | 0.000988  |
| reward_velocity         | 0.459     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 108       |
|    time_elapsed         | 390       |
|    total_timesteps      | 13824     |
| train/                  |           |
|    approx_kl            | 1.1301922 |
|    clip_fraction        | 0.679     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.965     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.28      |
|    n_updates            | 2140      |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.364     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0224    |
| reward_motion           | 0.228     |
| reward_position         | 0.382     |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.455     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 109       |
|    time_elapsed         | 394       |
|    total_timesteps      | 13952     |
| train/                  |           |
|    approx_kl            | 0.8782006 |
|    clip_fraction        | 0.661     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.16      |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.364     |
|    value_loss           | 26.9      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.227      |
| reward_position         | 0.382      |
| reward_torque           | 0.000977   |
| reward_velocity         | 0.451      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 110        |
|    time_elapsed         | 398        |
|    total_timesteps      | 14080      |
| train/                  |            |
|    approx_kl            | 0.63385475 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0005     |
|    loss                 | 5.29       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.364      |
|    value_loss           | 26.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000978   |
| reward_velocity         | 0.452      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 111        |
|    time_elapsed         | 401        |
|    total_timesteps      | 14208      |
| train/                  |            |
|    approx_kl            | 0.89142215 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.11       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.364      |
|    value_loss           | 6.86       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0229    |
| reward_motion           | 0.227     |
| reward_position         | 0.381     |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.453     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 112       |
|    time_elapsed         | 405       |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.8071065 |
|    clip_fraction        | 0.586     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.79      |
|    n_updates            | 2220      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.364     |
|    value_loss           | 18        |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.227      |
| reward_position         | 0.381      |
| reward_torque           | 0.000986   |
| reward_velocity         | 0.454      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 113        |
|    time_elapsed         | 409        |
|    total_timesteps      | 14464      |
| train/                  |            |
|    approx_kl            | 0.43124312 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.54       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.364      |
|    value_loss           | 20.7       |
----------------------------------------
---------------------------------------
| reward                  | 1.09      |
| reward_contact          | 0.0229    |
| reward_motion           | 0.227     |
| reward_position         | 0.381     |
| reward_torque           | 0.000989  |
| reward_velocity         | 0.455     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 114       |
|    time_elapsed         | 413       |
|    total_timesteps      | 14592     |
| train/                  |           |
|    approx_kl            | 0.2973642 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.664     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.61      |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.364     |
|    value_loss           | 20.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.09       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.228      |
| reward_position         | 0.381      |
| reward_torque           | 0.000994   |
| reward_velocity         | 0.455      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 115        |
|    time_elapsed         | 416        |
|    total_timesteps      | 14720      |
| train/                  |            |
|    approx_kl            | 0.12836033 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.71       |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.364      |
|    value_loss           | 20.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.227      |
| reward_position         | 0.38       |
| reward_torque           | 0.00099    |
| reward_velocity         | 0.45       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 116        |
|    time_elapsed         | 420        |
|    total_timesteps      | 14848      |
| train/                  |            |
|    approx_kl            | 0.74287015 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.678      |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.364      |
|    value_loss           | 7.22       |
----------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.228     |
| reward_position         | 0.38      |
| reward_torque           | 0.000987  |
| reward_velocity         | 0.444     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 117       |
|    time_elapsed         | 423       |
|    total_timesteps      | 14976     |
| train/                  |           |
|    approx_kl            | 1.0565991 |
|    clip_fraction        | 0.676     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.15      |
|    n_updates            | 2320      |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.364     |
|    value_loss           | 10.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.228      |
| reward_position         | 0.38       |
| reward_torque           | 0.000991   |
| reward_velocity         | 0.446      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 118        |
|    time_elapsed         | 427        |
|    total_timesteps      | 15104      |
| train/                  |            |
|    approx_kl            | 0.34065092 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.0005     |
|    loss                 | 9.05       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.364      |
|    value_loss           | 37.1       |
----------------------------------------
--------------------------------------
| reward                  | 1.08     |
| reward_contact          | 0.0228   |
| reward_motion           | 0.228    |
| reward_position         | 0.381    |
| reward_torque           | 0.000989 |
| reward_velocity         | 0.446    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 125      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 119      |
|    time_elapsed         | 431      |
|    total_timesteps      | 15232    |
| train/                  |          |
|    approx_kl            | 1.060564 |
|    clip_fraction        | 0.62     |
|    clip_range           | 0.4      |
|    entropy_loss         | -107     |
|    explained_variance   | 0.743    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.78     |
|    n_updates            | 2360     |
|    policy_gradient_loss | -0.173   |
|    std                  | 0.364    |
|    value_loss           | 13.4     |
--------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0222    |
| reward_motion           | 0.228     |
| reward_position         | 0.381     |
| reward_torque           | 0.000989  |
| reward_velocity         | 0.448     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 120       |
|    time_elapsed         | 434       |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.2988981 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.866     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.67      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.364     |
|    value_loss           | 14.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.228      |
| reward_position         | 0.381      |
| reward_torque           | 0.000991   |
| reward_velocity         | 0.45       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 121        |
|    time_elapsed         | 438        |
|    total_timesteps      | 15488      |
| train/                  |            |
|    approx_kl            | 0.24055493 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.09       |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.364      |
|    value_loss           | 9.47       |
----------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0218    |
| reward_motion           | 0.228     |
| reward_position         | 0.381     |
| reward_torque           | 0.000992  |
| reward_velocity         | 0.451     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 122       |
|    time_elapsed         | 442       |
|    total_timesteps      | 15616     |
| train/                  |           |
|    approx_kl            | 1.0328503 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.36      |
|    n_updates            | 2420      |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.364     |
|    value_loss           | 8.63      |
---------------------------------------
---------------------------------------
| reward                  | 1.08      |
| reward_contact          | 0.0221    |
| reward_motion           | 0.228     |
| reward_position         | 0.38      |
| reward_torque           | 0.000987  |
| reward_velocity         | 0.445     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 123       |
|    time_elapsed         | 445       |
|    total_timesteps      | 15744     |
| train/                  |           |
|    approx_kl            | 1.0931588 |
|    clip_fraction        | 0.746     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.436     |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.364     |
|    value_loss           | 7.09      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0223     |
| reward_motion           | 0.228      |
| reward_position         | 0.38       |
| reward_torque           | 0.000985   |
| reward_velocity         | 0.443      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 124        |
|    time_elapsed         | 449        |
|    total_timesteps      | 15872      |
| train/                  |            |
|    approx_kl            | 0.40621924 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.8        |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.364      |
|    value_loss           | 18.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0222    |
| reward_motion           | 0.228     |
| reward_position         | 0.38      |
| reward_torque           | 0.000982  |
| reward_velocity         | 0.439     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 125       |
|    time_elapsed         | 452       |
|    total_timesteps      | 16000     |
| train/                  |           |
|    approx_kl            | 0.6303283 |
|    clip_fraction        | 0.535     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.05      |
|    n_updates            | 2480      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.364     |
|    value_loss           | 11.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0224     |
| reward_motion           | 0.228      |
| reward_position         | 0.379      |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.435      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 126        |
|    time_elapsed         | 456        |
|    total_timesteps      | 16128      |
| train/                  |            |
|    approx_kl            | 0.20549393 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.06       |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0613    |
|    std                  | 0.364      |
|    value_loss           | 11.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0226     |
| reward_motion           | 0.228      |
| reward_position         | 0.38       |
| reward_torque           | 0.000982   |
| reward_velocity         | 0.434      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 127        |
|    time_elapsed         | 460        |
|    total_timesteps      | 16256      |
| train/                  |            |
|    approx_kl            | 0.33886287 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.999      |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.364      |
|    value_loss           | 10.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0226    |
| reward_motion           | 0.229     |
| reward_position         | 0.38      |
| reward_torque           | 0.000978  |
| reward_velocity         | 0.435     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 128       |
|    time_elapsed         | 463       |
|    total_timesteps      | 16384     |
| train/                  |           |
|    approx_kl            | 0.8438997 |
|    clip_fraction        | 0.694     |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | 0.714     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.12      |
|    n_updates            | 2540      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.364     |
|    value_loss           | 16.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.023     |
| reward_motion           | 0.229     |
| reward_position         | 0.379     |
| reward_torque           | 0.000977  |
| reward_velocity         | 0.434     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 129       |
|    time_elapsed         | 467       |
|    total_timesteps      | 16512     |
| train/                  |           |
|    approx_kl            | 0.5605978 |
|    clip_fraction        | 0.512     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.772     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.6       |
|    n_updates            | 2560      |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.364     |
|    value_loss           | 11.9      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.229      |
| reward_position         | 0.38       |
| reward_torque           | 0.000981   |
| reward_velocity         | 0.438      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 130        |
|    time_elapsed         | 471        |
|    total_timesteps      | 16640      |
| train/                  |            |
|    approx_kl            | 0.44864956 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.59       |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.364      |
|    value_loss           | 24.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.23       |
| reward_position         | 0.38       |
| reward_torque           | 0.00098    |
| reward_velocity         | 0.438      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 131        |
|    time_elapsed         | 474        |
|    total_timesteps      | 16768      |
| train/                  |            |
|    approx_kl            | 0.50316566 |
|    clip_fraction        | 0.623      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.52       |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.363      |
|    value_loss           | 12.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.23       |
| reward_position         | 0.38       |
| reward_torque           | 0.000974   |
| reward_velocity         | 0.436      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 132        |
|    time_elapsed         | 478        |
|    total_timesteps      | 16896      |
| train/                  |            |
|    approx_kl            | 0.40963003 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0005     |
|    loss                 | 1.3        |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.363      |
|    value_loss           | 8.65       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0226     |
| reward_motion           | 0.231      |
| reward_position         | 0.38       |
| reward_torque           | 0.000975   |
| reward_velocity         | 0.437      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 133        |
|    time_elapsed         | 481        |
|    total_timesteps      | 17024      |
| train/                  |            |
|    approx_kl            | 0.40100724 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29       |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.364      |
|    value_loss           | 7.24       |
----------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.023      |
| reward_motion           | 0.231      |
| reward_position         | 0.381      |
| reward_torque           | 0.000976   |
| reward_velocity         | 0.44       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 134        |
|    time_elapsed         | 485        |
|    total_timesteps      | 17152      |
| train/                  |            |
|    approx_kl            | 0.61881614 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.0005     |
|    loss                 | 8.67       |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 31.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.231      |
| reward_position         | 0.381      |
| reward_torque           | 0.000977   |
| reward_velocity         | 0.441      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 135        |
|    time_elapsed         | 489        |
|    total_timesteps      | 17280      |
| train/                  |            |
|    approx_kl            | 0.22164267 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.47       |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.363      |
|    value_loss           | 15.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.08       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.232      |
| reward_position         | 0.38       |
| reward_torque           | 0.000979   |
| reward_velocity         | 0.441      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 136        |
|    time_elapsed         | 492        |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.49809957 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.57       |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.363      |
|    value_loss           | 11.6       |
----------------------------------------
--------------------------------------
| reward                  | 1.07     |
| reward_contact          | 0.0228   |
| reward_motion           | 0.233    |
| reward_position         | 0.379    |
| reward_torque           | 0.000975 |
| reward_velocity         | 0.436    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 124      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 137      |
|    time_elapsed         | 496      |
|    total_timesteps      | 17536    |
| train/                  |          |
|    approx_kl            | 0.451181 |
|    clip_fraction        | 0.499    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.843    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.79     |
|    n_updates            | 2720     |
|    policy_gradient_loss | -0.192   |
|    std                  | 0.363    |
|    value_loss           | 10.9     |
--------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.233      |
| reward_position         | 0.378      |
| reward_torque           | 0.000975   |
| reward_velocity         | 0.435      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 138        |
|    time_elapsed         | 500        |
|    total_timesteps      | 17664      |
| train/                  |            |
|    approx_kl            | 0.25096753 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.86       |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.363      |
|    value_loss           | 13.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0226    |
| reward_motion           | 0.233     |
| reward_position         | 0.378     |
| reward_torque           | 0.000975  |
| reward_velocity         | 0.436     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 139       |
|    time_elapsed         | 503       |
|    total_timesteps      | 17792     |
| train/                  |           |
|    approx_kl            | 0.8884858 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -102      |
|    explained_variance   | 0.212     |
|    learning_rate        | 0.0005    |
|    loss                 | 11.4      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.363     |
|    value_loss           | 45.1      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0227     |
| reward_motion           | 0.232      |
| reward_position         | 0.379      |
| reward_torque           | 0.000974   |
| reward_velocity         | 0.435      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 140        |
|    time_elapsed         | 507        |
|    total_timesteps      | 17920      |
| train/                  |            |
|    approx_kl            | 0.57741845 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.28       |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.363      |
|    value_loss           | 10.4       |
----------------------------------------
Num timesteps: 18000
Best mean reward: 124.52 - Last mean reward per episode: 124.22
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.233      |
| reward_position         | 0.378      |
| reward_torque           | 0.000974   |
| reward_velocity         | 0.435      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 141        |
|    time_elapsed         | 511        |
|    total_timesteps      | 18048      |
| train/                  |            |
|    approx_kl            | 0.55984676 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.18       |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.137     |
|    std                  | 0.363      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.023      |
| reward_motion           | 0.233      |
| reward_position         | 0.379      |
| reward_torque           | 0.000976   |
| reward_velocity         | 0.436      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 142        |
|    time_elapsed         | 514        |
|    total_timesteps      | 18176      |
| train/                  |            |
|    approx_kl            | 0.36241892 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.97       |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.363      |
|    value_loss           | 21.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.233     |
| reward_position         | 0.379     |
| reward_torque           | 0.000974  |
| reward_velocity         | 0.433     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 143       |
|    time_elapsed         | 518       |
|    total_timesteps      | 18304     |
| train/                  |           |
|    approx_kl            | 0.4287812 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.09      |
|    n_updates            | 2840      |
|    policy_gradient_loss | -0.141    |
|    std                  | 0.363     |
|    value_loss           | 6.94      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.232      |
| reward_position         | 0.379      |
| reward_torque           | 0.000972   |
| reward_velocity         | 0.431      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 144        |
|    time_elapsed         | 521        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.31871462 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.91       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.363      |
|    value_loss           | 11.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.233      |
| reward_position         | 0.379      |
| reward_torque           | 0.000969   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 145        |
|    time_elapsed         | 525        |
|    total_timesteps      | 18560      |
| train/                  |            |
|    approx_kl            | 0.62619525 |
|    clip_fraction        | 0.604      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.22       |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.363      |
|    value_loss           | 20.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000972   |
| reward_velocity         | 0.432      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 146        |
|    time_elapsed         | 529        |
|    total_timesteps      | 18688      |
| train/                  |            |
|    approx_kl            | 0.43293452 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.475      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.62       |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.363      |
|    value_loss           | 27.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.233      |
| reward_position         | 0.379      |
| reward_torque           | 0.000973   |
| reward_velocity         | 0.431      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 147        |
|    time_elapsed         | 532        |
|    total_timesteps      | 18816      |
| train/                  |            |
|    approx_kl            | 0.41888046 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.03       |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.363      |
|    value_loss           | 9.9        |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000973   |
| reward_velocity         | 0.432      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 148        |
|    time_elapsed         | 536        |
|    total_timesteps      | 18944      |
| train/                  |            |
|    approx_kl            | 0.29328722 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.449      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.24       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 29         |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000972   |
| reward_velocity         | 0.431      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 149        |
|    time_elapsed         | 540        |
|    total_timesteps      | 19072      |
| train/                  |            |
|    approx_kl            | 0.34997833 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.35       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.137     |
|    std                  | 0.363      |
|    value_loss           | 22         |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.234      |
| reward_position         | 0.38       |
| reward_torque           | 0.000972   |
| reward_velocity         | 0.426      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 150        |
|    time_elapsed         | 543        |
|    total_timesteps      | 19200      |
| train/                  |            |
|    approx_kl            | 0.38304633 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.397      |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 14.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.233     |
| reward_position         | 0.381     |
| reward_torque           | 0.000969  |
| reward_velocity         | 0.423     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 151       |
|    time_elapsed         | 547       |
|    total_timesteps      | 19328     |
| train/                  |           |
|    approx_kl            | 0.8547277 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.903     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.606     |
|    n_updates            | 3000      |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.363     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.232     |
| reward_position         | 0.381     |
| reward_torque           | 0.000974  |
| reward_velocity         | 0.428     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 152       |
|    time_elapsed         | 550       |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.7093239 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -102      |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.0005    |
|    loss                 | 27.4      |
|    n_updates            | 3020      |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.363     |
|    value_loss           | 83.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.232      |
| reward_position         | 0.38       |
| reward_torque           | 0.000969   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 153        |
|    time_elapsed         | 554        |
|    total_timesteps      | 19584      |
| train/                  |            |
|    approx_kl            | 0.53055656 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.2        |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.224     |
|    std                  | 0.363      |
|    value_loss           | 12.1       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.232     |
| reward_position         | 0.381     |
| reward_torque           | 0.000967  |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 154       |
|    time_elapsed         | 558       |
|    total_timesteps      | 19712     |
| train/                  |           |
|    approx_kl            | 0.7568581 |
|    clip_fraction        | 0.503     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.871     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.742     |
|    n_updates            | 3060      |
|    policy_gradient_loss | -0.126    |
|    std                  | 0.363     |
|    value_loss           | 10.8      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.232     |
| reward_position         | 0.38      |
| reward_torque           | 0.000963  |
| reward_velocity         | 0.428     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 155       |
|    time_elapsed         | 561       |
|    total_timesteps      | 19840     |
| train/                  |           |
|    approx_kl            | 1.0580409 |
|    clip_fraction        | 0.692     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.866     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.42      |
|    n_updates            | 3080      |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.363     |
|    value_loss           | 8.41      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.232     |
| reward_position         | 0.381     |
| reward_torque           | 0.000964  |
| reward_velocity         | 0.429     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 156       |
|    time_elapsed         | 565       |
|    total_timesteps      | 19968     |
| train/                  |           |
|    approx_kl            | 0.6203334 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.89      |
|    n_updates            | 3100      |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.363     |
|    value_loss           | 8.8       |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.232     |
| reward_position         | 0.381     |
| reward_torque           | 0.000963  |
| reward_velocity         | 0.427     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 157       |
|    time_elapsed         | 569       |
|    total_timesteps      | 20096     |
| train/                  |           |
|    approx_kl            | 0.4475991 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.889     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.15      |
|    n_updates            | 3120      |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.363     |
|    value_loss           | 9.94      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.232      |
| reward_position         | 0.381      |
| reward_torque           | 0.000958   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 158        |
|    time_elapsed         | 572        |
|    total_timesteps      | 20224      |
| train/                  |            |
|    approx_kl            | 0.36116514 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29       |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.363      |
|    value_loss           | 11.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0238     |
| reward_motion           | 0.232      |
| reward_position         | 0.38       |
| reward_torque           | 0.00096    |
| reward_velocity         | 0.426      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 159        |
|    time_elapsed         | 576        |
|    total_timesteps      | 20352      |
| train/                  |            |
|    approx_kl            | 0.31855926 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.95       |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.363      |
|    value_loss           | 11.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0234     |
| reward_motion           | 0.232      |
| reward_position         | 0.381      |
| reward_torque           | 0.000961   |
| reward_velocity         | 0.43       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 160        |
|    time_elapsed         | 579        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.32660973 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06       |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.363      |
|    value_loss           | 6.47       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.233      |
| reward_position         | 0.381      |
| reward_torque           | 0.000957   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 161        |
|    time_elapsed         | 583        |
|    total_timesteps      | 20608      |
| train/                  |            |
|    approx_kl            | 0.48327056 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.19       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.363      |
|    value_loss           | 9.16       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0236    |
| reward_motion           | 0.233     |
| reward_position         | 0.381     |
| reward_torque           | 0.000957  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 162       |
|    time_elapsed         | 587       |
|    total_timesteps      | 20736     |
| train/                  |           |
|    approx_kl            | 0.5309117 |
|    clip_fraction        | 0.527     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.88      |
|    n_updates            | 3220      |
|    policy_gradient_loss | -0.151    |
|    std                  | 0.363     |
|    value_loss           | 16.4      |
---------------------------------------
--------------------------------------
| reward                  | 1.06     |
| reward_contact          | 0.0236   |
| reward_motion           | 0.233    |
| reward_position         | 0.38     |
| reward_torque           | 0.000956 |
| reward_velocity         | 0.422    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 124      |
| time/                   |          |
|    fps                  | 35       |
|    iterations           | 163      |
|    time_elapsed         | 590      |
|    total_timesteps      | 20864    |
| train/                  |          |
|    approx_kl            | 0.979684 |
|    clip_fraction        | 0.56     |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.961    |
|    learning_rate        | 0.0005   |
|    loss                 | 0.288    |
|    n_updates            | 3240     |
|    policy_gradient_loss | -0.162   |
|    std                  | 0.363    |
|    value_loss           | 3.75     |
--------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.234      |
| reward_position         | 0.381      |
| reward_torque           | 0.000955   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 164        |
|    time_elapsed         | 594        |
|    total_timesteps      | 20992      |
| train/                  |            |
|    approx_kl            | 0.42197642 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.21       |
|    n_updates            | 3260       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.363      |
|    value_loss           | 16.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.234      |
| reward_position         | 0.38       |
| reward_torque           | 0.000957   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 165        |
|    time_elapsed         | 597        |
|    total_timesteps      | 21120      |
| train/                  |            |
|    approx_kl            | 0.63233197 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.4        |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.363      |
|    value_loss           | 18.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.234     |
| reward_position         | 0.381     |
| reward_torque           | 0.000962  |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 166       |
|    time_elapsed         | 601       |
|    total_timesteps      | 21248     |
| train/                  |           |
|    approx_kl            | 0.5364908 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.28      |
|    n_updates            | 3300      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.363     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0238    |
| reward_motion           | 0.234     |
| reward_position         | 0.38      |
| reward_torque           | 0.000957  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 167       |
|    time_elapsed         | 605       |
|    total_timesteps      | 21376     |
| train/                  |           |
|    approx_kl            | 0.6344925 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0005    |
|    loss                 | 6.39      |
|    n_updates            | 3320      |
|    policy_gradient_loss | -0.14     |
|    std                  | 0.363     |
|    value_loss           | 31.7      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0239    |
| reward_motion           | 0.233     |
| reward_position         | 0.38      |
| reward_torque           | 0.000956  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 168       |
|    time_elapsed         | 608       |
|    total_timesteps      | 21504     |
| train/                  |           |
|    approx_kl            | 0.5481432 |
|    clip_fraction        | 0.441     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.901     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.18      |
|    n_updates            | 3340      |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.363     |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.024     |
| reward_motion           | 0.233     |
| reward_position         | 0.38      |
| reward_torque           | 0.000956  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 169       |
|    time_elapsed         | 612       |
|    total_timesteps      | 21632     |
| train/                  |           |
|    approx_kl            | 0.9588952 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.897     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.882     |
|    n_updates            | 3360      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.363     |
|    value_loss           | 9.14      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.232     |
| reward_position         | 0.38      |
| reward_torque           | 0.000954  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 170       |
|    time_elapsed         | 615       |
|    total_timesteps      | 21760     |
| train/                  |           |
|    approx_kl            | 0.5853392 |
|    clip_fraction        | 0.501     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.345     |
|    n_updates            | 3380      |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.363     |
|    value_loss           | 9.75      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.232     |
| reward_position         | 0.38      |
| reward_torque           | 0.000953  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 171       |
|    time_elapsed         | 619       |
|    total_timesteps      | 21888     |
| train/                  |           |
|    approx_kl            | 0.7871096 |
|    clip_fraction        | 0.621     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.968     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.04      |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.183    |
|    std                  | 0.363     |
|    value_loss           | 5.95      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0234     |
| reward_motion           | 0.232      |
| reward_position         | 0.38       |
| reward_torque           | 0.000953   |
| reward_velocity         | 0.422      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 172        |
|    time_elapsed         | 623        |
|    total_timesteps      | 22016      |
| train/                  |            |
|    approx_kl            | 0.87851226 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.31       |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.221     |
|    std                  | 0.363      |
|    value_loss           | 10.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000949   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 173        |
|    time_elapsed         | 626        |
|    total_timesteps      | 22144      |
| train/                  |            |
|    approx_kl            | 0.42560095 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.92       |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.363      |
|    value_loss           | 8.8        |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.233     |
| reward_position         | 0.381     |
| reward_torque           | 0.000949  |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 174       |
|    time_elapsed         | 630       |
|    total_timesteps      | 22272     |
| train/                  |           |
|    approx_kl            | 0.7060157 |
|    clip_fraction        | 0.644     |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.15      |
|    n_updates            | 3460      |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.363     |
|    value_loss           | 28.3      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0239    |
| reward_motion           | 0.233     |
| reward_position         | 0.38      |
| reward_torque           | 0.000946  |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 175       |
|    time_elapsed         | 634       |
|    total_timesteps      | 22400     |
| train/                  |           |
|    approx_kl            | 0.3793116 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.29      |
|    n_updates            | 3480      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.363     |
|    value_loss           | 8.63      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.00095    |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 176        |
|    time_elapsed         | 637        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.35542983 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.07       |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 14.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0232     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000946   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 177        |
|    time_elapsed         | 641        |
|    total_timesteps      | 22656      |
| train/                  |            |
|    approx_kl            | 0.66110957 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.658      |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.363      |
|    value_loss           | 5.86       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.233      |
| reward_position         | 0.38       |
| reward_torque           | 0.000942   |
| reward_velocity         | 0.419      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 178        |
|    time_elapsed         | 645        |
|    total_timesteps      | 22784      |
| train/                  |            |
|    approx_kl            | 0.31810367 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.55       |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.363      |
|    value_loss           | 10.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0234     |
| reward_motion           | 0.234      |
| reward_position         | 0.38       |
| reward_torque           | 0.000937   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 179        |
|    time_elapsed         | 648        |
|    total_timesteps      | 22912      |
| train/                  |            |
|    approx_kl            | 0.40039855 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.768      |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.363      |
|    value_loss           | 10.7       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.233     |
| reward_position         | 0.38      |
| reward_torque           | 0.000937  |
| reward_velocity         | 0.416     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 180       |
|    time_elapsed         | 652       |
|    total_timesteps      | 23040     |
| train/                  |           |
|    approx_kl            | 0.5538343 |
|    clip_fraction        | 0.555     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.459     |
|    n_updates            | 3580      |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.363     |
|    value_loss           | 5.97      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.234     |
| reward_position         | 0.38      |
| reward_torque           | 0.000937  |
| reward_velocity         | 0.413     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 181       |
|    time_elapsed         | 656       |
|    total_timesteps      | 23168     |
| train/                  |           |
|    approx_kl            | 1.0813243 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.82      |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 19.9      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.234     |
| reward_position         | 0.38      |
| reward_torque           | 0.000935  |
| reward_velocity         | 0.415     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 182       |
|    time_elapsed         | 659       |
|    total_timesteps      | 23296     |
| train/                  |           |
|    approx_kl            | 0.8163942 |
|    clip_fraction        | 0.549     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.22      |
|    n_updates            | 3620      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.363     |
|    value_loss           | 14.4      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0235     |
| reward_motion           | 0.234      |
| reward_position         | 0.38       |
| reward_torque           | 0.000937   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 183        |
|    time_elapsed         | 663        |
|    total_timesteps      | 23424      |
| train/                  |            |
|    approx_kl            | 0.37412754 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.707      |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.363      |
|    value_loss           | 7.98       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0236    |
| reward_motion           | 0.234     |
| reward_position         | 0.38      |
| reward_torque           | 0.000937  |
| reward_velocity         | 0.418     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 184       |
|    time_elapsed         | 667       |
|    total_timesteps      | 23552     |
| train/                  |           |
|    approx_kl            | 0.8292922 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.564     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.91      |
|    n_updates            | 3660      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 28.1      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.234      |
| reward_position         | 0.379      |
| reward_torque           | 0.000937   |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 185        |
|    time_elapsed         | 670        |
|    total_timesteps      | 23680      |
| train/                  |            |
|    approx_kl            | 0.50058794 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.82       |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.363      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0236     |
| reward_motion           | 0.234      |
| reward_position         | 0.38       |
| reward_torque           | 0.000935   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 186        |
|    time_elapsed         | 674        |
|    total_timesteps      | 23808      |
| train/                  |            |
|    approx_kl            | 0.49373496 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.96       |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.363      |
|    value_loss           | 12.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.234     |
| reward_position         | 0.379     |
| reward_torque           | 0.000933  |
| reward_velocity         | 0.415     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 187       |
|    time_elapsed         | 678       |
|    total_timesteps      | 23936     |
| train/                  |           |
|    approx_kl            | 0.5673518 |
|    clip_fraction        | 0.531     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.55      |
|    n_updates            | 3720      |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.363     |
|    value_loss           | 12.3      |
---------------------------------------
Num timesteps: 24000
Best mean reward: 124.52 - Last mean reward per episode: 124.60
Saving new best model to rl/out_dir/models/exp75/best_model.zip
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0237    |
| reward_motion           | 0.235     |
| reward_position         | 0.379     |
| reward_torque           | 0.000933  |
| reward_velocity         | 0.417     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 188       |
|    time_elapsed         | 682       |
|    total_timesteps      | 24064     |
| train/                  |           |
|    approx_kl            | 0.7908486 |
|    clip_fraction        | 0.615     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.88      |
|    n_updates            | 3740      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.363     |
|    value_loss           | 21.3      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0237     |
| reward_motion           | 0.235      |
| reward_position         | 0.379      |
| reward_torque           | 0.000931   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 189        |
|    time_elapsed         | 685        |
|    total_timesteps      | 24192      |
| train/                  |            |
|    approx_kl            | 0.90246433 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.39       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.363      |
|    value_loss           | 5.9        |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0235    |
| reward_motion           | 0.236     |
| reward_position         | 0.378     |
| reward_torque           | 0.000933  |
| reward_velocity         | 0.417     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 190       |
|    time_elapsed         | 689       |
|    total_timesteps      | 24320     |
| train/                  |           |
|    approx_kl            | 0.7148757 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.42      |
|    n_updates            | 3780      |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.363     |
|    value_loss           | 7.29      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0234    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000933  |
| reward_velocity         | 0.419     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 191       |
|    time_elapsed         | 693       |
|    total_timesteps      | 24448     |
| train/                  |           |
|    approx_kl            | 0.3053376 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.875     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.06      |
|    n_updates            | 3800      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.363     |
|    value_loss           | 9.02      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000936  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 192       |
|    time_elapsed         | 697       |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.5698827 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0005    |
|    loss                 | 2.73      |
|    n_updates            | 3820      |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.363     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000936  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 193       |
|    time_elapsed         | 700       |
|    total_timesteps      | 24704     |
| train/                  |           |
|    approx_kl            | 0.4618805 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.765     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.08      |
|    n_updates            | 3840      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.363     |
|    value_loss           | 15.9      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000936  |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 194       |
|    time_elapsed         | 704       |
|    total_timesteps      | 24832     |
| train/                  |           |
|    approx_kl            | 0.7024869 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.9       |
|    n_updates            | 3860      |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.363     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000937  |
| reward_velocity         | 0.423     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 195       |
|    time_elapsed         | 708       |
|    total_timesteps      | 24960     |
| train/                  |           |
|    approx_kl            | 0.5529901 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.64      |
|    n_updates            | 3880      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.363     |
|    value_loss           | 6.49      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.236      |
| reward_position         | 0.376      |
| reward_torque           | 0.00094    |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 196        |
|    time_elapsed         | 712        |
|    total_timesteps      | 25088      |
| train/                  |            |
|    approx_kl            | 0.54493374 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.35       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.362      |
|    value_loss           | 9.27       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0233    |
| reward_motion           | 0.236     |
| reward_position         | 0.377     |
| reward_torque           | 0.000937  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 197       |
|    time_elapsed         | 716       |
|    total_timesteps      | 25216     |
| train/                  |           |
|    approx_kl            | 0.3327187 |
|    clip_fraction        | 0.273     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.775     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.4       |
|    n_updates            | 3920      |
|    policy_gradient_loss | -0.119    |
|    std                  | 0.362     |
|    value_loss           | 13.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.237      |
| reward_position         | 0.377      |
| reward_torque           | 0.000938   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 198        |
|    time_elapsed         | 719        |
|    total_timesteps      | 25344      |
| train/                  |            |
|    approx_kl            | 0.42342585 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.98       |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.362      |
|    value_loss           | 8.8        |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0233     |
| reward_motion           | 0.237      |
| reward_position         | 0.376      |
| reward_torque           | 0.000936   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 199        |
|    time_elapsed         | 723        |
|    total_timesteps      | 25472      |
| train/                  |            |
|    approx_kl            | 0.53570247 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0005     |
|    loss                 | 7.74       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.362      |
|    value_loss           | 24.7       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0232    |
| reward_motion           | 0.237     |
| reward_position         | 0.376     |
| reward_torque           | 0.000931  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 200       |
|    time_elapsed         | 727       |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.9744084 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.71      |
|    learning_rate        | 0.0005    |
|    loss                 | 0.906     |
|    n_updates            | 3980      |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.362     |
|    value_loss           | 13.2      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0231    |
| reward_motion           | 0.237     |
| reward_position         | 0.377     |
| reward_torque           | 0.00093   |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 201       |
|    time_elapsed         | 731       |
|    total_timesteps      | 25728     |
| train/                  |           |
|    approx_kl            | 0.6590737 |
|    clip_fraction        | 0.539     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.57      |
|    n_updates            | 4000      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.362     |
|    value_loss           | 9.22      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0228     |
| reward_motion           | 0.237      |
| reward_position         | 0.377      |
| reward_torque           | 0.000928   |
| reward_velocity         | 0.416      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 202        |
|    time_elapsed         | 734        |
|    total_timesteps      | 25856      |
| train/                  |            |
|    approx_kl            | 0.33530515 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.84       |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.362      |
|    value_loss           | 10.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0229     |
| reward_motion           | 0.238      |
| reward_position         | 0.377      |
| reward_torque           | 0.000926   |
| reward_velocity         | 0.414      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 203        |
|    time_elapsed         | 738        |
|    total_timesteps      | 25984      |
| train/                  |            |
|    approx_kl            | 0.24203199 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.81       |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.362      |
|    value_loss           | 7.5        |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.023     |
| reward_motion           | 0.237     |
| reward_position         | 0.377     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.413     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 204       |
|    time_elapsed         | 742       |
|    total_timesteps      | 26112     |
| train/                  |           |
|    approx_kl            | 1.0208263 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.322     |
|    learning_rate        | 0.0005    |
|    loss                 | 21.3      |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.182    |
|    std                  | 0.362     |
|    value_loss           | 63.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0231     |
| reward_motion           | 0.237      |
| reward_position         | 0.377      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.412      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 205        |
|    time_elapsed         | 746        |
|    total_timesteps      | 26240      |
| train/                  |            |
|    approx_kl            | 0.28657812 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.03       |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.362      |
|    value_loss           | 8.03       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.023     |
| reward_motion           | 0.238     |
| reward_position         | 0.377     |
| reward_torque           | 0.000924  |
| reward_velocity         | 0.414     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 206       |
|    time_elapsed         | 750       |
|    total_timesteps      | 26368     |
| train/                  |           |
|    approx_kl            | 0.5840601 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.53      |
|    n_updates            | 4100      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.362     |
|    value_loss           | 12.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0229    |
| reward_motion           | 0.238     |
| reward_position         | 0.377     |
| reward_torque           | 0.000924  |
| reward_velocity         | 0.413     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 207       |
|    time_elapsed         | 753       |
|    total_timesteps      | 26496     |
| train/                  |           |
|    approx_kl            | 0.3686625 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.798     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.73      |
|    n_updates            | 4120      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.362     |
|    value_loss           | 13.2      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0229    |
| reward_motion           | 0.238     |
| reward_position         | 0.376     |
| reward_torque           | 0.000924  |
| reward_velocity         | 0.41      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 208       |
|    time_elapsed         | 757       |
|    total_timesteps      | 26624     |
| train/                  |           |
|    approx_kl            | 0.5107941 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.27      |
|    n_updates            | 4140      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.362     |
|    value_loss           | 10.2      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0226     |
| reward_motion           | 0.238      |
| reward_position         | 0.376      |
| reward_torque           | 0.000925   |
| reward_velocity         | 0.412      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 209        |
|    time_elapsed         | 761        |
|    total_timesteps      | 26752      |
| train/                  |            |
|    approx_kl            | 0.63550496 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.41       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.362      |
|    value_loss           | 13.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.022      |
| reward_motion           | 0.238      |
| reward_position         | 0.377      |
| reward_torque           | 0.000926   |
| reward_velocity         | 0.416      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 210        |
|    time_elapsed         | 765        |
|    total_timesteps      | 26880      |
| train/                  |            |
|    approx_kl            | 0.40079814 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.15       |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.362      |
|    value_loss           | 9.27       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.022     |
| reward_motion           | 0.238     |
| reward_position         | 0.378     |
| reward_torque           | 0.000927  |
| reward_velocity         | 0.415     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 124       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 211       |
|    time_elapsed         | 768       |
|    total_timesteps      | 27008     |
| train/                  |           |
|    approx_kl            | 0.2592423 |
|    clip_fraction        | 0.354     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.14      |
|    n_updates            | 4200      |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.362     |
|    value_loss           | 10.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.022      |
| reward_motion           | 0.237      |
| reward_position         | 0.378      |
| reward_torque           | 0.000927   |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 212        |
|    time_elapsed         | 772        |
|    total_timesteps      | 27136      |
| train/                  |            |
|    approx_kl            | 0.74736696 |
|    clip_fraction        | 0.652      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.0005     |
|    loss                 | 20.1       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.362      |
|    value_loss           | 62.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.022      |
| reward_motion           | 0.237      |
| reward_position         | 0.378      |
| reward_torque           | 0.000925   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 213        |
|    time_elapsed         | 776        |
|    total_timesteps      | 27264      |
| train/                  |            |
|    approx_kl            | 0.24815339 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.21       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.362      |
|    value_loss           | 8.7        |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.237      |
| reward_position         | 0.378      |
| reward_torque           | 0.000925   |
| reward_velocity         | 0.417      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 214        |
|    time_elapsed         | 780        |
|    total_timesteps      | 27392      |
| train/                  |            |
|    approx_kl            | 0.33335865 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.25       |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.362      |
|    value_loss           | 8.65       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.237      |
| reward_position         | 0.379      |
| reward_torque           | 0.000925   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 215        |
|    time_elapsed         | 784        |
|    total_timesteps      | 27520      |
| train/                  |            |
|    approx_kl            | 0.56029546 |
|    clip_fraction        | 0.568      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0005     |
|    loss                 | 22         |
|    n_updates            | 4280       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.362      |
|    value_loss           | 61.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0223     |
| reward_motion           | 0.237      |
| reward_position         | 0.379      |
| reward_torque           | 0.000926   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 216        |
|    time_elapsed         | 787        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.55756706 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0005     |
|    loss                 | 12.2       |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.362      |
|    value_loss           | 39.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.237      |
| reward_position         | 0.379      |
| reward_torque           | 0.000927   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 217        |
|    time_elapsed         | 791        |
|    total_timesteps      | 27776      |
| train/                  |            |
|    approx_kl            | 0.45803708 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.66       |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.362      |
|    value_loss           | 18.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0219    |
| reward_motion           | 0.236     |
| reward_position         | 0.379     |
| reward_torque           | 0.000925  |
| reward_velocity         | 0.425     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 218       |
|    time_elapsed         | 795       |
|    total_timesteps      | 27904     |
| train/                  |           |
|    approx_kl            | 0.4491366 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.661     |
|    learning_rate        | 0.0005    |
|    loss                 | 11.3      |
|    n_updates            | 4340      |
|    policy_gradient_loss | -0.134    |
|    std                  | 0.362     |
|    value_loss           | 40.8      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0221    |
| reward_motion           | 0.237     |
| reward_position         | 0.379     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 219       |
|    time_elapsed         | 798       |
|    total_timesteps      | 28032     |
| train/                  |           |
|    approx_kl            | 0.3161167 |
|    clip_fraction        | 0.382     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.589     |
|    n_updates            | 4360      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.362     |
|    value_loss           | 6.32      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0224     |
| reward_motion           | 0.237      |
| reward_position         | 0.38       |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.416      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 220        |
|    time_elapsed         | 802        |
|    total_timesteps      | 28160      |
| train/                  |            |
|    approx_kl            | 0.64917994 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.0005     |
|    loss                 | 3.73       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.362      |
|    value_loss           | 20         |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0224    |
| reward_motion           | 0.237     |
| reward_position         | 0.379     |
| reward_torque           | 0.000915  |
| reward_velocity         | 0.414     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 125       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 221       |
|    time_elapsed         | 806       |
|    total_timesteps      | 28288     |
| train/                  |           |
|    approx_kl            | 0.5310466 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.06      |
|    n_updates            | 4400      |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.362     |
|    value_loss           | 9.91      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0223     |
| reward_motion           | 0.237      |
| reward_position         | 0.379      |
| reward_torque           | 0.000918   |
| reward_velocity         | 0.416      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 125        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 222        |
|    time_elapsed         | 810        |
|    total_timesteps      | 28416      |
| train/                  |            |
|    approx_kl            | 0.32569802 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.62       |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.362      |
|    value_loss           | 17.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.237      |
| reward_position         | 0.38       |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 223        |
|    time_elapsed         | 814        |
|    total_timesteps      | 28544      |
| train/                  |            |
|    approx_kl            | 0.45068297 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0005     |
|    loss                 | 10.4       |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.362      |
|    value_loss           | 32.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0219    |
| reward_motion           | 0.237     |
| reward_position         | 0.38      |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.418     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 224       |
|    time_elapsed         | 817       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.2898046 |
|    clip_fraction        | 0.439     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.867     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.53      |
|    n_updates            | 4460      |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.362     |
|    value_loss           | 14.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.022      |
| reward_motion           | 0.236      |
| reward_position         | 0.38       |
| reward_torque           | 0.00092    |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 225        |
|    time_elapsed         | 821        |
|    total_timesteps      | 28800      |
| train/                  |            |
|    approx_kl            | 0.47014508 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.11       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.188     |
|    std                  | 0.362      |
|    value_loss           | 10.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0221     |
| reward_motion           | 0.236      |
| reward_position         | 0.38       |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 226        |
|    time_elapsed         | 824        |
|    total_timesteps      | 28928      |
| train/                  |            |
|    approx_kl            | 0.22941932 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0005     |
|    loss                 | 11.7       |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.362      |
|    value_loss           | 44.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0224    |
| reward_motion           | 0.236     |
| reward_position         | 0.381     |
| reward_torque           | 0.000921  |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 126       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 227       |
|    time_elapsed         | 828       |
|    total_timesteps      | 29056     |
| train/                  |           |
|    approx_kl            | 0.4496904 |
|    clip_fraction        | 0.548     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.835     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.56      |
|    n_updates            | 4520      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.362     |
|    value_loss           | 28.3      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0227    |
| reward_motion           | 0.236     |
| reward_position         | 0.382     |
| reward_torque           | 0.000921  |
| reward_velocity         | 0.419     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 228       |
|    time_elapsed         | 832       |
|    total_timesteps      | 29184     |
| train/                  |           |
|    approx_kl            | 1.1239703 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.94      |
|    learning_rate        | 0.0005    |
|    loss                 | 0.232     |
|    n_updates            | 4540      |
|    policy_gradient_loss | -0.151    |
|    std                  | 0.362     |
|    value_loss           | 6.08      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0222     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.422      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 229        |
|    time_elapsed         | 835        |
|    total_timesteps      | 29312      |
| train/                  |            |
|    approx_kl            | 0.31564513 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.875      |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.362      |
|    value_loss           | 5.13       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0221    |
| reward_motion           | 0.236     |
| reward_position         | 0.382     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.423     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 230       |
|    time_elapsed         | 839       |
|    total_timesteps      | 29440     |
| train/                  |           |
|    approx_kl            | 0.8444599 |
|    clip_fraction        | 0.671     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.222     |
|    learning_rate        | 0.0005    |
|    loss                 | 14.5      |
|    n_updates            | 4580      |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.362     |
|    value_loss           | 69.8      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0222     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.424      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 231        |
|    time_elapsed         | 843        |
|    total_timesteps      | 29568      |
| train/                  |            |
|    approx_kl            | 0.30341843 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.42       |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.362      |
|    value_loss           | 16.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0223    |
| reward_motion           | 0.236     |
| reward_position         | 0.381     |
| reward_torque           | 0.000921  |
| reward_velocity         | 0.425     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 232       |
|    time_elapsed         | 847       |
|    total_timesteps      | 29696     |
| train/                  |           |
|    approx_kl            | 0.6250937 |
|    clip_fraction        | 0.561     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.05      |
|    n_updates            | 4620      |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.362     |
|    value_loss           | 12.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0219     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.00092    |
| reward_velocity         | 0.425      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 233        |
|    time_elapsed         | 851        |
|    total_timesteps      | 29824      |
| train/                  |            |
|    approx_kl            | 0.29782102 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.38       |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.362      |
|    value_loss           | 12.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0214     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 234        |
|    time_elapsed         | 854        |
|    total_timesteps      | 29952      |
| train/                  |            |
|    approx_kl            | 0.49549052 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.23       |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.362      |
|    value_loss           | 12.6       |
----------------------------------------
Num timesteps: 30000
Best mean reward: 124.60 - Last mean reward per episode: 126.84
Saving new best model to rl/out_dir/models/exp75/best_model.zip
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0214     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 235        |
|    time_elapsed         | 858        |
|    total_timesteps      | 30080      |
| train/                  |            |
|    approx_kl            | 0.35415766 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.857      |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.188     |
|    std                  | 0.362      |
|    value_loss           | 14.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0215     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 236        |
|    time_elapsed         | 862        |
|    total_timesteps      | 30208      |
| train/                  |            |
|    approx_kl            | 0.39584234 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.15       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.362      |
|    value_loss           | 9.4        |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0214     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 237        |
|    time_elapsed         | 866        |
|    total_timesteps      | 30336      |
| train/                  |            |
|    approx_kl            | 0.96116304 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.68       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.362      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0214     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 238        |
|    time_elapsed         | 870        |
|    total_timesteps      | 30464      |
| train/                  |            |
|    approx_kl            | 0.20471328 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.99       |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.362      |
|    value_loss           | 8.84       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0216     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.419      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 239        |
|    time_elapsed         | 873        |
|    total_timesteps      | 30592      |
| train/                  |            |
|    approx_kl            | 0.45636058 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.06       |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.362      |
|    value_loss           | 13.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0213    |
| reward_motion           | 0.235     |
| reward_position         | 0.382     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 240       |
|    time_elapsed         | 877       |
|    total_timesteps      | 30720     |
| train/                  |           |
|    approx_kl            | 0.8577308 |
|    clip_fraction        | 0.687     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | -0.0302   |
|    learning_rate        | 0.0005    |
|    loss                 | 18.3      |
|    n_updates            | 4780      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.362     |
|    value_loss           | 75.4      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0211     |
| reward_motion           | 0.235      |
| reward_position         | 0.382      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.425      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 241        |
|    time_elapsed         | 881        |
|    total_timesteps      | 30848      |
| train/                  |            |
|    approx_kl            | 0.62056345 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0005     |
|    loss                 | 5.08       |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.362      |
|    value_loss           | 25.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0211    |
| reward_motion           | 0.235     |
| reward_position         | 0.381     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.426     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 35        |
|    iterations           | 242       |
|    time_elapsed         | 884       |
|    total_timesteps      | 30976     |
| train/                  |           |
|    approx_kl            | 0.5827961 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.83      |
|    n_updates            | 4820      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.362     |
|    value_loss           | 15.9      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0211     |
| reward_motion           | 0.236      |
| reward_position         | 0.381      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.427      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 243        |
|    time_elapsed         | 888        |
|    total_timesteps      | 31104      |
| train/                  |            |
|    approx_kl            | 0.34351638 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.362      |
|    value_loss           | 9.53       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.021      |
| reward_motion           | 0.236      |
| reward_position         | 0.381      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.429      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 244        |
|    time_elapsed         | 892        |
|    total_timesteps      | 31232      |
| train/                  |            |
|    approx_kl            | 0.49392557 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.65       |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.362      |
|    value_loss           | 35.6       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0208     |
| reward_motion           | 0.236      |
| reward_position         | 0.381      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 245        |
|    time_elapsed         | 895        |
|    total_timesteps      | 31360      |
| train/                  |            |
|    approx_kl            | 0.40189743 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.69       |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.362      |
|    value_loss           | 10.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0211    |
| reward_motion           | 0.236     |
| reward_position         | 0.381     |
| reward_torque           | 0.000918  |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 246       |
|    time_elapsed         | 899       |
|    total_timesteps      | 31488     |
| train/                  |           |
|    approx_kl            | 0.7034426 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.807     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.07      |
|    n_updates            | 4900      |
|    policy_gradient_loss | -0.168    |
|    std                  | 0.362     |
|    value_loss           | 11.4      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0206     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000918   |
| reward_velocity         | 0.423      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 247        |
|    time_elapsed         | 903        |
|    total_timesteps      | 31616      |
| train/                  |            |
|    approx_kl            | 0.22010806 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.4        |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0998    |
|    std                  | 0.361      |
|    value_loss           | 6.05       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0206     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.429      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 248        |
|    time_elapsed         | 907        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.81143355 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.4        |
|    learning_rate        | 0.0005     |
|    loss                 | 13.3       |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.361      |
|    value_loss           | 41.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.236      |
| reward_position         | 0.382      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.429      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 249        |
|    time_elapsed         | 910        |
|    total_timesteps      | 31872      |
| train/                  |            |
|    approx_kl            | 0.36744237 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0005     |
|    loss                 | 4          |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.361      |
|    value_loss           | 13.7       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0204    |
| reward_motion           | 0.236     |
| reward_position         | 0.381     |
| reward_torque           | 0.000921  |
| reward_velocity         | 0.428     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 250       |
|    time_elapsed         | 914       |
|    total_timesteps      | 32000     |
| train/                  |           |
|    approx_kl            | 0.5697788 |
|    clip_fraction        | 0.418     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.69      |
|    n_updates            | 4980      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.361     |
|    value_loss           | 8.27      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0201    |
| reward_motion           | 0.237     |
| reward_position         | 0.381     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.431     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 251       |
|    time_elapsed         | 918       |
|    total_timesteps      | 32128     |
| train/                  |           |
|    approx_kl            | 1.3560382 |
|    clip_fraction        | 0.661     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.0005    |
|    loss                 | 19.9      |
|    n_updates            | 5000      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.361     |
|    value_loss           | 62.8      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0202    |
| reward_motion           | 0.238     |
| reward_position         | 0.381     |
| reward_torque           | 0.000919  |
| reward_velocity         | 0.425     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 252       |
|    time_elapsed         | 922       |
|    total_timesteps      | 32256     |
| train/                  |           |
|    approx_kl            | 0.3158586 |
|    clip_fraction        | 0.365     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.947     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.628     |
|    n_updates            | 5020      |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.361     |
|    value_loss           | 7.08      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0198    |
| reward_motion           | 0.238     |
| reward_position         | 0.381     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.426     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 253       |
|    time_elapsed         | 926       |
|    total_timesteps      | 32384     |
| train/                  |           |
|    approx_kl            | 0.4256364 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.652     |
|    n_updates            | 5040      |
|    policy_gradient_loss | -0.148    |
|    std                  | 0.361     |
|    value_loss           | 7.75      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.238      |
| reward_position         | 0.381      |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 254        |
|    time_elapsed         | 929        |
|    total_timesteps      | 32512      |
| train/                  |            |
|    approx_kl            | 0.56619114 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.4        |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.361      |
|    value_loss           | 11.9       |
----------------------------------------
--------------------------------------
| reward                  | 1.06     |
| reward_contact          | 0.0199   |
| reward_motion           | 0.238    |
| reward_position         | 0.381    |
| reward_torque           | 0.000921 |
| reward_velocity         | 0.424    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 127      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 255      |
|    time_elapsed         | 933      |
|    total_timesteps      | 32640    |
| train/                  |          |
|    approx_kl            | 0.47866  |
|    clip_fraction        | 0.5      |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.854    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.42     |
|    n_updates            | 5080     |
|    policy_gradient_loss | -0.178   |
|    std                  | 0.361    |
|    value_loss           | 10.1     |
--------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0197    |
| reward_motion           | 0.238     |
| reward_position         | 0.381     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 256       |
|    time_elapsed         | 937       |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.6839499 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.6       |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.361     |
|    value_loss           | 8.91      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.238      |
| reward_position         | 0.38       |
| reward_torque           | 0.000921   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 257        |
|    time_elapsed         | 940        |
|    total_timesteps      | 32896      |
| train/                  |            |
|    approx_kl            | 0.67019534 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.13       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.361      |
|    value_loss           | 7.71       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.02      |
| reward_motion           | 0.237     |
| reward_position         | 0.38      |
| reward_torque           | 0.000923  |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 258       |
|    time_elapsed         | 944       |
|    total_timesteps      | 33024     |
| train/                  |           |
|    approx_kl            | 1.0712593 |
|    clip_fraction        | 0.674     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.28      |
|    learning_rate        | 0.0005    |
|    loss                 | 16.7      |
|    n_updates            | 5140      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.361     |
|    value_loss           | 62.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000918   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 259        |
|    time_elapsed         | 948        |
|    total_timesteps      | 33152      |
| train/                  |            |
|    approx_kl            | 0.35812712 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.69       |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.361      |
|    value_loss           | 27         |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000916   |
| reward_velocity         | 0.419      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 260        |
|    time_elapsed         | 951        |
|    total_timesteps      | 33280      |
| train/                  |            |
|    approx_kl            | 0.28750718 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.43       |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.361      |
|    value_loss           | 25.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0201     |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.421      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 261        |
|    time_elapsed         | 955        |
|    total_timesteps      | 33408      |
| train/                  |            |
|    approx_kl            | 0.62352955 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.69       |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.361      |
|    value_loss           | 20.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0202    |
| reward_motion           | 0.237     |
| reward_position         | 0.382     |
| reward_torque           | 0.000917  |
| reward_velocity         | 0.418     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 262       |
|    time_elapsed         | 959       |
|    total_timesteps      | 33536     |
| train/                  |           |
|    approx_kl            | 0.2509971 |
|    clip_fraction        | 0.299     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.95      |
|    learning_rate        | 0.0005    |
|    loss                 | 1.67      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.131    |
|    std                  | 0.361     |
|    value_loss           | 7.59      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0204     |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000915   |
| reward_velocity         | 0.415      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 263        |
|    time_elapsed         | 962        |
|    total_timesteps      | 33664      |
| train/                  |            |
|    approx_kl            | 0.56601465 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.22       |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.361      |
|    value_loss           | 13.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 264        |
|    time_elapsed         | 966        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.29441184 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.17       |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.361      |
|    value_loss           | 22.1       |
----------------------------------------
--------------------------------------
| reward                  | 1.05     |
| reward_contact          | 0.0203   |
| reward_motion           | 0.237    |
| reward_position         | 0.381    |
| reward_torque           | 0.000914 |
| reward_velocity         | 0.415    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 127      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 265      |
|    time_elapsed         | 970      |
|    total_timesteps      | 33920    |
| train/                  |          |
|    approx_kl            | 0.505898 |
|    clip_fraction        | 0.488    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.928    |
|    learning_rate        | 0.0005   |
|    loss                 | 2.38     |
|    n_updates            | 5280     |
|    policy_gradient_loss | -0.202   |
|    std                  | 0.361    |
|    value_loss           | 10.6     |
--------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0203     |
| reward_motion           | 0.236      |
| reward_position         | 0.381      |
| reward_torque           | 0.000911   |
| reward_velocity         | 0.418      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 266        |
|    time_elapsed         | 974        |
|    total_timesteps      | 34048      |
| train/                  |            |
|    approx_kl            | 0.25192785 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.75       |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.361      |
|    value_loss           | 26.3       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0202    |
| reward_motion           | 0.236     |
| reward_position         | 0.381     |
| reward_torque           | 0.000912  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 127       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 267       |
|    time_elapsed         | 977       |
|    total_timesteps      | 34176     |
| train/                  |           |
|    approx_kl            | 0.5126457 |
|    clip_fraction        | 0.591     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.609     |
|    n_updates            | 5320      |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.361     |
|    value_loss           | 6.92      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.02       |
| reward_motion           | 0.237      |
| reward_position         | 0.381      |
| reward_torque           | 0.000915   |
| reward_velocity         | 0.422      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 268        |
|    time_elapsed         | 981        |
|    total_timesteps      | 34304      |
| train/                  |            |
|    approx_kl            | 0.42610607 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.303      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.89       |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.361      |
|    value_loss           | 44.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0199    |
| reward_motion           | 0.238     |
| reward_position         | 0.381     |
| reward_torque           | 0.000914  |
| reward_velocity         | 0.422     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 269       |
|    time_elapsed         | 985       |
|    total_timesteps      | 34432     |
| train/                  |           |
|    approx_kl            | 0.8480146 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.37      |
|    n_updates            | 5360      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.361     |
|    value_loss           | 7.57      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.02       |
| reward_motion           | 0.238      |
| reward_position         | 0.381      |
| reward_torque           | 0.000913   |
| reward_velocity         | 0.426      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 270        |
|    time_elapsed         | 988        |
|    total_timesteps      | 34560      |
| train/                  |            |
|    approx_kl            | 0.61739767 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.47       |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.224     |
|    std                  | 0.361      |
|    value_loss           | 9.51       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.238      |
| reward_position         | 0.382      |
| reward_torque           | 0.000914   |
| reward_velocity         | 0.43       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 271        |
|    time_elapsed         | 992        |
|    total_timesteps      | 34688      |
| train/                  |            |
|    approx_kl            | 0.76705694 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.0005     |
|    loss                 | 11.4       |
|    n_updates            | 5400       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.361      |
|    value_loss           | 70.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0196     |
| reward_motion           | 0.238      |
| reward_position         | 0.382      |
| reward_torque           | 0.000915   |
| reward_velocity         | 0.428      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 272        |
|    time_elapsed         | 996        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.32114133 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.07       |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.361      |
|    value_loss           | 18.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0193    |
| reward_motion           | 0.237     |
| reward_position         | 0.382     |
| reward_torque           | 0.000916  |
| reward_velocity         | 0.43      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 273       |
|    time_elapsed         | 999       |
|    total_timesteps      | 34944     |
| train/                  |           |
|    approx_kl            | 1.0287082 |
|    clip_fraction        | 0.766     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.116     |
|    learning_rate        | 0.0005    |
|    loss                 | 35.1      |
|    n_updates            | 5440      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.361     |
|    value_loss           | 102       |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0194    |
| reward_motion           | 0.237     |
| reward_position         | 0.382     |
| reward_torque           | 0.000917  |
| reward_velocity         | 0.428     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 274       |
|    time_elapsed         | 1003      |
|    total_timesteps      | 35072     |
| train/                  |           |
|    approx_kl            | 0.6194258 |
|    clip_fraction        | 0.591     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.13      |
|    n_updates            | 5460      |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.361     |
|    value_loss           | 9.48      |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.019      |
| reward_motion           | 0.237      |
| reward_position         | 0.382      |
| reward_torque           | 0.00092    |
| reward_velocity         | 0.43       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 275        |
|    time_elapsed         | 1007       |
|    total_timesteps      | 35200      |
| train/                  |            |
|    approx_kl            | 0.51341367 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.75       |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.361      |
|    value_loss           | 18.6       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.019     |
| reward_motion           | 0.237     |
| reward_position         | 0.382     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.431     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 276       |
|    time_elapsed         | 1011      |
|    total_timesteps      | 35328     |
| train/                  |           |
|    approx_kl            | 0.5013359 |
|    clip_fraction        | 0.496     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.98      |
|    n_updates            | 5500      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.361     |
|    value_loss           | 16        |
---------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0191     |
| reward_motion           | 0.237      |
| reward_position         | 0.383      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.431      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 277        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 35456      |
| train/                  |            |
|    approx_kl            | 0.59673417 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.87       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.361      |
|    value_loss           | 13.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0191    |
| reward_motion           | 0.237     |
| reward_position         | 0.383     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.431     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 278       |
|    time_elapsed         | 1018      |
|    total_timesteps      | 35584     |
| train/                  |           |
|    approx_kl            | 1.0837407 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.598     |
|    learning_rate        | 0.0005    |
|    loss                 | 10.7      |
|    n_updates            | 5540      |
|    policy_gradient_loss | -0.183    |
|    std                  | 0.361     |
|    value_loss           | 48.2      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.019     |
| reward_motion           | 0.237     |
| reward_position         | 0.383     |
| reward_torque           | 0.000926  |
| reward_velocity         | 0.433     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 279       |
|    time_elapsed         | 1022      |
|    total_timesteps      | 35712     |
| train/                  |           |
|    approx_kl            | 0.5367366 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.903     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.981     |
|    n_updates            | 5560      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.361     |
|    value_loss           | 9.07      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0187    |
| reward_motion           | 0.235     |
| reward_position         | 0.383     |
| reward_torque           | 0.000925  |
| reward_velocity         | 0.434     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 280       |
|    time_elapsed         | 1026      |
|    total_timesteps      | 35840     |
| train/                  |           |
|    approx_kl            | 0.5526507 |
|    clip_fraction        | 0.664     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | 2.68      |
|    n_updates            | 5580      |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.361     |
|    value_loss           | 13.7      |
---------------------------------------
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0188    |
| reward_motion           | 0.235     |
| reward_position         | 0.382     |
| reward_torque           | 0.000922  |
| reward_velocity         | 0.433     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 281       |
|    time_elapsed         | 1029      |
|    total_timesteps      | 35968     |
| train/                  |           |
|    approx_kl            | 0.7845228 |
|    clip_fraction        | 0.596     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.48      |
|    n_updates            | 5600      |
|    policy_gradient_loss | -0.166    |
|    std                  | 0.36      |
|    value_loss           | 16.2      |
---------------------------------------
Num timesteps: 36000
Best mean reward: 126.84 - Last mean reward per episode: 128.52
Saving new best model to rl/out_dir/models/exp75/best_model.zip
---------------------------------------
| reward                  | 1.07      |
| reward_contact          | 0.0189    |
| reward_motion           | 0.235     |
| reward_position         | 0.382     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.429     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 282       |
|    time_elapsed         | 1033      |
|    total_timesteps      | 36096     |
| train/                  |           |
|    approx_kl            | 0.4912452 |
|    clip_fraction        | 0.637     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.97      |
|    n_updates            | 5620      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.36      |
|    value_loss           | 9.98      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.019      |
| reward_motion           | 0.235      |
| reward_position         | 0.382      |
| reward_torque           | 0.000917   |
| reward_velocity         | 0.426      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 283        |
|    time_elapsed         | 1037       |
|    total_timesteps      | 36224      |
| train/                  |            |
|    approx_kl            | 0.34064853 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.43       |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.36       |
|    value_loss           | 10.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0189     |
| reward_motion           | 0.235      |
| reward_position         | 0.382      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.426      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 284        |
|    time_elapsed         | 1040       |
|    total_timesteps      | 36352      |
| train/                  |            |
|    approx_kl            | 0.39795738 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.47       |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.36       |
|    value_loss           | 9.35       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0189    |
| reward_motion           | 0.235     |
| reward_position         | 0.383     |
| reward_torque           | 0.00092   |
| reward_velocity         | 0.424     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 285       |
|    time_elapsed         | 1044      |
|    total_timesteps      | 36480     |
| train/                  |           |
|    approx_kl            | 0.3901112 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.08      |
|    n_updates            | 5680      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.36      |
|    value_loss           | 15.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.019      |
| reward_motion           | 0.235      |
| reward_position         | 0.383      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.425      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 286        |
|    time_elapsed         | 1047       |
|    total_timesteps      | 36608      |
| train/                  |            |
|    approx_kl            | 0.41220295 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.76       |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.36       |
|    value_loss           | 18.4       |
----------------------------------------
----------------------------------------
| reward                  | 1.07       |
| reward_contact          | 0.0189     |
| reward_motion           | 0.235      |
| reward_position         | 0.384      |
| reward_torque           | 0.000922   |
| reward_velocity         | 0.427      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 287        |
|    time_elapsed         | 1051       |
|    total_timesteps      | 36736      |
| train/                  |            |
|    approx_kl            | 0.63050467 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.0005     |
|    loss                 | 10.5       |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.36       |
|    value_loss           | 40.1       |
----------------------------------------
--------------------------------------
| reward                  | 1.06     |
| reward_contact          | 0.019    |
| reward_motion           | 0.235    |
| reward_position         | 0.384    |
| reward_torque           | 0.00092  |
| reward_velocity         | 0.421    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 128      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 288      |
|    time_elapsed         | 1055     |
|    total_timesteps      | 36864    |
| train/                  |          |
|    approx_kl            | 0.465714 |
|    clip_fraction        | 0.464    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.804    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.63     |
|    n_updates            | 5740     |
|    policy_gradient_loss | -0.193   |
|    std                  | 0.36     |
|    value_loss           | 11.8     |
--------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.019      |
| reward_motion           | 0.235      |
| reward_position         | 0.384      |
| reward_torque           | 0.000919   |
| reward_velocity         | 0.42       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 289        |
|    time_elapsed         | 1059       |
|    total_timesteps      | 36992      |
| train/                  |            |
|    approx_kl            | 0.27570233 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.48       |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.36       |
|    value_loss           | 16.7       |
----------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.019     |
| reward_motion           | 0.235     |
| reward_position         | 0.383     |
| reward_torque           | 0.000918  |
| reward_velocity         | 0.42      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 290       |
|    time_elapsed         | 1062      |
|    total_timesteps      | 37120     |
| train/                  |           |
|    approx_kl            | 0.5759666 |
|    clip_fraction        | 0.504     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.992     |
|    n_updates            | 5780      |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.36      |
|    value_loss           | 13.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0191    |
| reward_motion           | 0.235     |
| reward_position         | 0.384     |
| reward_torque           | 0.000918  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 291       |
|    time_elapsed         | 1066      |
|    total_timesteps      | 37248     |
| train/                  |           |
|    approx_kl            | 0.9741371 |
|    clip_fraction        | 0.604     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.385     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.8       |
|    n_updates            | 5800      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.36      |
|    value_loss           | 31.3      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0191    |
| reward_motion           | 0.235     |
| reward_position         | 0.384     |
| reward_torque           | 0.000917  |
| reward_velocity         | 0.421     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 292       |
|    time_elapsed         | 1070      |
|    total_timesteps      | 37376     |
| train/                  |           |
|    approx_kl            | 1.3188612 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.761     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.17      |
|    n_updates            | 5820      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.36      |
|    value_loss           | 14.4      |
---------------------------------------
---------------------------------------
| reward                  | 1.06      |
| reward_contact          | 0.0192    |
| reward_motion           | 0.235     |
| reward_position         | 0.385     |
| reward_torque           | 0.000913  |
| reward_velocity         | 0.418     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 293       |
|    time_elapsed         | 1073      |
|    total_timesteps      | 37504     |
| train/                  |           |
|    approx_kl            | 0.6145751 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.931     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.65      |
|    n_updates            | 5840      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.36      |
|    value_loss           | 9.34      |
---------------------------------------
----------------------------------------
| reward                  | 1.06       |
| reward_contact          | 0.0192     |
| reward_motion           | 0.235      |
| reward_position         | 0.385      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.414      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 294        |
|    time_elapsed         | 1077       |
|    total_timesteps      | 37632      |
| train/                  |            |
|    approx_kl            | 0.24263324 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.99       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.36       |
|    value_loss           | 9.77       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0193     |
| reward_motion           | 0.235      |
| reward_position         | 0.385      |
| reward_torque           | 0.000911   |
| reward_velocity         | 0.412      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 295        |
|    time_elapsed         | 1081       |
|    total_timesteps      | 37760      |
| train/                  |            |
|    approx_kl            | 0.30585176 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.36       |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.36       |
|    value_loss           | 9.08       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0194     |
| reward_motion           | 0.235      |
| reward_position         | 0.386      |
| reward_torque           | 0.000909   |
| reward_velocity         | 0.411      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 296        |
|    time_elapsed         | 1085       |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.37373924 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.82       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.36       |
|    value_loss           | 12.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0194     |
| reward_motion           | 0.235      |
| reward_position         | 0.385      |
| reward_torque           | 0.000909   |
| reward_velocity         | 0.41       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 297        |
|    time_elapsed         | 1088       |
|    total_timesteps      | 38016      |
| train/                  |            |
|    approx_kl            | 0.34521553 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.63       |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.36       |
|    value_loss           | 11.2       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.235      |
| reward_position         | 0.385      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.409      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 298        |
|    time_elapsed         | 1092       |
|    total_timesteps      | 38144      |
| train/                  |            |
|    approx_kl            | 0.53577715 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.52       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.36       |
|    value_loss           | 6.81       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.235      |
| reward_position         | 0.386      |
| reward_torque           | 0.000906   |
| reward_velocity         | 0.405      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 299        |
|    time_elapsed         | 1096       |
|    total_timesteps      | 38272      |
| train/                  |            |
|    approx_kl            | 0.41072273 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.47       |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.36       |
|    value_loss           | 12.2       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0195    |
| reward_motion           | 0.235     |
| reward_position         | 0.386     |
| reward_torque           | 0.000908  |
| reward_velocity         | 0.404     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 300       |
|    time_elapsed         | 1099      |
|    total_timesteps      | 38400     |
| train/                  |           |
|    approx_kl            | 0.7999297 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.194     |
|    n_updates            | 5980      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.36      |
|    value_loss           | 4.61      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0196    |
| reward_motion           | 0.235     |
| reward_position         | 0.386     |
| reward_torque           | 0.000908  |
| reward_velocity         | 0.405     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 301       |
|    time_elapsed         | 1103      |
|    total_timesteps      | 38528     |
| train/                  |           |
|    approx_kl            | 0.6572441 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.835     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.59      |
|    n_updates            | 6000      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.36      |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0196    |
| reward_motion           | 0.235     |
| reward_position         | 0.386     |
| reward_torque           | 0.000909  |
| reward_velocity         | 0.405     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 302       |
|    time_elapsed         | 1107      |
|    total_timesteps      | 38656     |
| train/                  |           |
|    approx_kl            | 0.5585085 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.59      |
|    n_updates            | 6020      |
|    policy_gradient_loss | -0.168    |
|    std                  | 0.36      |
|    value_loss           | 8.93      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.235      |
| reward_position         | 0.386      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.406      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 303        |
|    time_elapsed         | 1110       |
|    total_timesteps      | 38784      |
| train/                  |            |
|    approx_kl            | 0.36219296 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.494      |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.36       |
|    value_loss           | 5.04       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0196     |
| reward_motion           | 0.235      |
| reward_position         | 0.387      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.403      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 304        |
|    time_elapsed         | 1114       |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.40007097 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.323      |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.36       |
|    value_loss           | 7.1        |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.235      |
| reward_position         | 0.387      |
| reward_torque           | 0.000911   |
| reward_velocity         | 0.404      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 305        |
|    time_elapsed         | 1118       |
|    total_timesteps      | 39040      |
| train/                  |            |
|    approx_kl            | 0.37745678 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.53       |
|    n_updates            | 6080       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.36       |
|    value_loss           | 5.45       |
----------------------------------------
---------------------------------------
| reward                  | 1.05      |
| reward_contact          | 0.0196    |
| reward_motion           | 0.235     |
| reward_position         | 0.387     |
| reward_torque           | 0.000909  |
| reward_velocity         | 0.405     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 306       |
|    time_elapsed         | 1121      |
|    total_timesteps      | 39168     |
| train/                  |           |
|    approx_kl            | 0.5206138 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.0005    |
|    loss                 | 16.8      |
|    n_updates            | 6100      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.36      |
|    value_loss           | 51.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.235      |
| reward_position         | 0.387      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.404      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 307        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 39296      |
| train/                  |            |
|    approx_kl            | 0.38430774 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0005     |
|    loss                 | 4.11       |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.36       |
|    value_loss           | 17.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.235      |
| reward_position         | 0.387      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.404      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 308        |
|    time_elapsed         | 1129       |
|    total_timesteps      | 39424      |
| train/                  |            |
|    approx_kl            | 0.34883296 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.08       |
|    n_updates            | 6140       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.36       |
|    value_loss           | 14.5       |
----------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0197     |
| reward_motion           | 0.235      |
| reward_position         | 0.387      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.403      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 309        |
|    time_elapsed         | 1133       |
|    total_timesteps      | 39552      |
| train/                  |            |
|    approx_kl            | 0.47300082 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0005     |
|    loss                 | 0.697      |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.36       |
|    value_loss           | 9.1        |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0198     |
| reward_motion           | 0.236      |
| reward_position         | 0.387      |
| reward_torque           | 0.000909   |
| reward_velocity         | 0.401      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 310        |
|    time_elapsed         | 1136       |
|    total_timesteps      | 39680      |
| train/                  |            |
|    approx_kl            | 0.32944053 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0005     |
|    loss                 | 7.84       |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.36       |
|    value_loss           | 31.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0199     |
| reward_motion           | 0.236      |
| reward_position         | 0.386      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.4        |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 311        |
|    time_elapsed         | 1140       |
|    total_timesteps      | 39808      |
| train/                  |            |
|    approx_kl            | 0.32738435 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.687      |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.36       |
|    value_loss           | 9.56       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0199    |
| reward_motion           | 0.237     |
| reward_position         | 0.386     |
| reward_torque           | 0.000905  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 312       |
|    time_elapsed         | 1144      |
|    total_timesteps      | 39936     |
| train/                  |           |
|    approx_kl            | 0.3738733 |
|    clip_fraction        | 0.421     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.86      |
|    n_updates            | 6220      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.36      |
|    value_loss           | 9.45      |
---------------------------------------
--------------------------------------
| reward                  | 1.04     |
| reward_contact          | 0.0199   |
| reward_motion           | 0.237    |
| reward_position         | 0.385    |
| reward_torque           | 0.000904 |
| reward_velocity         | 0.398    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 129      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 313      |
|    time_elapsed         | 1148     |
|    total_timesteps      | 40064    |
| train/                  |          |
|    approx_kl            | 0.505528 |
|    clip_fraction        | 0.499    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.773    |
|    learning_rate        | 0.0005   |
|    loss                 | 2.92     |
|    n_updates            | 6240     |
|    policy_gradient_loss | -0.166   |
|    std                  | 0.36     |
|    value_loss           | 16.1     |
--------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.02       |
| reward_motion           | 0.237      |
| reward_position         | 0.385      |
| reward_torque           | 0.000902   |
| reward_velocity         | 0.398      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 314        |
|    time_elapsed         | 1152       |
|    total_timesteps      | 40192      |
| train/                  |            |
|    approx_kl            | 0.28303647 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.82       |
|    n_updates            | 6260       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.36       |
|    value_loss           | 13.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0199     |
| reward_motion           | 0.237      |
| reward_position         | 0.384      |
| reward_torque           | 0.000899   |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 315        |
|    time_elapsed         | 1155       |
|    total_timesteps      | 40320      |
| train/                  |            |
|    approx_kl            | 0.52612686 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.34       |
|    n_updates            | 6280       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.36       |
|    value_loss           | 6.48       |
----------------------------------------
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0195    |
| reward_motion           | 0.237     |
| reward_position         | 0.384     |
| reward_torque           | 0.000897  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 316       |
|    time_elapsed         | 1159      |
|    total_timesteps      | 40448     |
| train/                  |           |
|    approx_kl            | 1.2292626 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.0005    |
|    loss                 | 24.8      |
|    n_updates            | 6300      |
|    policy_gradient_loss | -0.175    |
|    std                  | 0.36      |
|    value_loss           | 71.2      |
---------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0196     |
| reward_motion           | 0.237      |
| reward_position         | 0.384      |
| reward_torque           | 0.000897   |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 317        |
|    time_elapsed         | 1163       |
|    total_timesteps      | 40576      |
| train/                  |            |
|    approx_kl            | 0.55408144 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.68       |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.36       |
|    value_loss           | 15.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0195    |
| reward_motion           | 0.237     |
| reward_position         | 0.383     |
| reward_torque           | 0.000897  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 318       |
|    time_elapsed         | 1166      |
|    total_timesteps      | 40704     |
| train/                  |           |
|    approx_kl            | 0.6170889 |
|    clip_fraction        | 0.522     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.651     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.87      |
|    n_updates            | 6340      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.36      |
|    value_loss           | 29.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0193    |
| reward_motion           | 0.237     |
| reward_position         | 0.382     |
| reward_torque           | 0.000899  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 319       |
|    time_elapsed         | 1170      |
|    total_timesteps      | 40832     |
| train/                  |           |
|    approx_kl            | 0.3933181 |
|    clip_fraction        | 0.456     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.51      |
|    n_updates            | 6360      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.36      |
|    value_loss           | 31.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0192     |
| reward_motion           | 0.237      |
| reward_position         | 0.382      |
| reward_torque           | 0.000901   |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 320        |
|    time_elapsed         | 1174       |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.70550776 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.37       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.36       |
|    value_loss           | 12.1       |
----------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0194     |
| reward_motion           | 0.236      |
| reward_position         | 0.383      |
| reward_torque           | 0.0009     |
| reward_velocity         | 0.392      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 321        |
|    time_elapsed         | 1178       |
|    total_timesteps      | 41088      |
| train/                  |            |
|    approx_kl            | 0.82293093 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | 2          |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.36       |
|    value_loss           | 12.5       |
----------------------------------------
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0195    |
| reward_motion           | 0.236     |
| reward_position         | 0.383     |
| reward_torque           | 0.000899  |
| reward_velocity         | 0.39      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 322       |
|    time_elapsed         | 1181      |
|    total_timesteps      | 41216     |
| train/                  |           |
|    approx_kl            | 0.3755991 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.875     |
|    n_updates            | 6420      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.36      |
|    value_loss           | 12.1      |
---------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0195     |
| reward_motion           | 0.237      |
| reward_position         | 0.384      |
| reward_torque           | 0.0009     |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 323        |
|    time_elapsed         | 1185       |
|    total_timesteps      | 41344      |
| train/                  |            |
|    approx_kl            | 0.51269674 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.31       |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.36       |
|    value_loss           | 30.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0193     |
| reward_motion           | 0.237      |
| reward_position         | 0.383      |
| reward_torque           | 0.000904   |
| reward_velocity         | 0.396      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 324        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 41472      |
| train/                  |            |
|    approx_kl            | 0.47165433 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.33       |
|    n_updates            | 6460       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.36       |
|    value_loss           | 8.65       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0192     |
| reward_motion           | 0.238      |
| reward_position         | 0.384      |
| reward_torque           | 0.000907   |
| reward_velocity         | 0.397      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 325        |
|    time_elapsed         | 1193       |
|    total_timesteps      | 41600      |
| train/                  |            |
|    approx_kl            | 0.52845705 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.425      |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.36       |
|    value_loss           | 9.06       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.019      |
| reward_motion           | 0.238      |
| reward_position         | 0.384      |
| reward_torque           | 0.000904   |
| reward_velocity         | 0.395      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 326        |
|    time_elapsed         | 1197       |
|    total_timesteps      | 41728      |
| train/                  |            |
|    approx_kl            | 0.35356766 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.13       |
|    n_updates            | 6500       |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.36       |
|    value_loss           | 7.44       |
----------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0184     |
| reward_motion           | 0.238      |
| reward_position         | 0.383      |
| reward_torque           | 0.000905   |
| reward_velocity         | 0.395      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 327        |
|    time_elapsed         | 1200       |
|    total_timesteps      | 41856      |
| train/                  |            |
|    approx_kl            | 0.62681407 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.871      |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.36       |
|    value_loss           | 6.03       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0182     |
| reward_motion           | 0.238      |
| reward_position         | 0.383      |
| reward_torque           | 0.000906   |
| reward_velocity         | 0.395      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 328        |
|    time_elapsed         | 1204       |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.55663586 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.591      |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.36       |
|    value_loss           | 6.69       |
----------------------------------------
Num timesteps: 42000
Best mean reward: 128.52 - Last mean reward per episode: 128.02
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0182    |
| reward_motion           | 0.238     |
| reward_position         | 0.383     |
| reward_torque           | 0.000907  |
| reward_velocity         | 0.394     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 329       |
|    time_elapsed         | 1208      |
|    total_timesteps      | 42112     |
| train/                  |           |
|    approx_kl            | 0.7997998 |
|    clip_fraction        | 0.602     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.0005    |
|    loss                 | 5         |
|    n_updates            | 6560      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.36      |
|    value_loss           | 23.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.03      |
| reward_contact          | 0.0182    |
| reward_motion           | 0.237     |
| reward_position         | 0.383     |
| reward_torque           | 0.000907  |
| reward_velocity         | 0.394     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 330       |
|    time_elapsed         | 1212      |
|    total_timesteps      | 42240     |
| train/                  |           |
|    approx_kl            | 0.6603426 |
|    clip_fraction        | 0.562     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.316     |
|    learning_rate        | 0.0005    |
|    loss                 | 17.3      |
|    n_updates            | 6580      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.36      |
|    value_loss           | 59.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0182     |
| reward_motion           | 0.237      |
| reward_position         | 0.383      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.397      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 331        |
|    time_elapsed         | 1215       |
|    total_timesteps      | 42368      |
| train/                  |            |
|    approx_kl            | 0.56049675 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0005     |
|    loss                 | 10.7       |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.36       |
|    value_loss           | 34.3       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.238      |
| reward_position         | 0.383      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.397      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 332        |
|    time_elapsed         | 1219       |
|    total_timesteps      | 42496      |
| train/                  |            |
|    approx_kl            | 0.42425233 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.94       |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.36       |
|    value_loss           | 14         |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0182     |
| reward_motion           | 0.237      |
| reward_position         | 0.384      |
| reward_torque           | 0.000911   |
| reward_velocity         | 0.397      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 333        |
|    time_elapsed         | 1223       |
|    total_timesteps      | 42624      |
| train/                  |            |
|    approx_kl            | 0.75380015 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.86       |
|    n_updates            | 6640       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.36       |
|    value_loss           | 16         |
----------------------------------------
--------------------------------------
| reward                  | 1.04     |
| reward_contact          | 0.0182   |
| reward_motion           | 0.237    |
| reward_position         | 0.385    |
| reward_torque           | 0.00091  |
| reward_velocity         | 0.397    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 128      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 334      |
|    time_elapsed         | 1226     |
|    total_timesteps      | 42752    |
| train/                  |          |
|    approx_kl            | 0.487331 |
|    clip_fraction        | 0.591    |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.84     |
|    learning_rate        | 0.0005   |
|    loss                 | 1.95     |
|    n_updates            | 6660     |
|    policy_gradient_loss | -0.176   |
|    std                  | 0.36     |
|    value_loss           | 15.7     |
--------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0182     |
| reward_motion           | 0.237      |
| reward_position         | 0.385      |
| reward_torque           | 0.000913   |
| reward_velocity         | 0.398      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 335        |
|    time_elapsed         | 1230       |
|    total_timesteps      | 42880      |
| train/                  |            |
|    approx_kl            | 0.37655196 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.76       |
|    n_updates            | 6680       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.36       |
|    value_loss           | 23.3       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0183    |
| reward_motion           | 0.237     |
| reward_position         | 0.386     |
| reward_torque           | 0.000912  |
| reward_velocity         | 0.397     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 336       |
|    time_elapsed         | 1234      |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.5862402 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.94      |
|    learning_rate        | 0.0005    |
|    loss                 | 2.55      |
|    n_updates            | 6700      |
|    policy_gradient_loss | -0.183    |
|    std                  | 0.36      |
|    value_loss           | 9.15      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0182    |
| reward_motion           | 0.237     |
| reward_position         | 0.386     |
| reward_torque           | 0.000914  |
| reward_velocity         | 0.397     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 337       |
|    time_elapsed         | 1237      |
|    total_timesteps      | 43136     |
| train/                  |           |
|    approx_kl            | 0.8109368 |
|    clip_fraction        | 0.514     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.39      |
|    n_updates            | 6720      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.36      |
|    value_loss           | 29.6      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0184    |
| reward_motion           | 0.237     |
| reward_position         | 0.387     |
| reward_torque           | 0.000911  |
| reward_velocity         | 0.396     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 338       |
|    time_elapsed         | 1241      |
|    total_timesteps      | 43264     |
| train/                  |           |
|    approx_kl            | 0.3575458 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.677     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.72      |
|    n_updates            | 6740      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.36      |
|    value_loss           | 16        |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0182     |
| reward_motion           | 0.237      |
| reward_position         | 0.387      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.397      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 339        |
|    time_elapsed         | 1245       |
|    total_timesteps      | 43392      |
| train/                  |            |
|    approx_kl            | 0.34838977 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.702      |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.36       |
|    value_loss           | 5.49       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0183     |
| reward_motion           | 0.238      |
| reward_position         | 0.387      |
| reward_torque           | 0.000913   |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 340        |
|    time_elapsed         | 1249       |
|    total_timesteps      | 43520      |
| train/                  |            |
|    approx_kl            | 0.29699227 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.64       |
|    n_updates            | 6780       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.36       |
|    value_loss           | 6.93       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0182    |
| reward_motion           | 0.238     |
| reward_position         | 0.386     |
| reward_torque           | 0.000914  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 341       |
|    time_elapsed         | 1252      |
|    total_timesteps      | 43648     |
| train/                  |           |
|    approx_kl            | 0.5491626 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.284     |
|    n_updates            | 6800      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.36      |
|    value_loss           | 3.57      |
---------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0183     |
| reward_motion           | 0.238      |
| reward_position         | 0.386      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.391      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 342        |
|    time_elapsed         | 1256       |
|    total_timesteps      | 43776      |
| train/                  |            |
|    approx_kl            | 0.52286994 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.95       |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.36       |
|    value_loss           | 25.7       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0185     |
| reward_motion           | 0.238      |
| reward_position         | 0.386      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 343        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 43904      |
| train/                  |            |
|    approx_kl            | 0.91610515 |
|    clip_fraction        | 0.634      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0005     |
|    loss                 | 7.25       |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.36       |
|    value_loss           | 27.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0186    |
| reward_motion           | 0.238     |
| reward_position         | 0.387     |
| reward_torque           | 0.000911  |
| reward_velocity         | 0.394     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 344       |
|    time_elapsed         | 1264      |
|    total_timesteps      | 44032     |
| train/                  |           |
|    approx_kl            | 0.3917089 |
|    clip_fraction        | 0.432     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.01      |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.36      |
|    value_loss           | 9.86      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0186    |
| reward_motion           | 0.238     |
| reward_position         | 0.387     |
| reward_torque           | 0.000911  |
| reward_velocity         | 0.395     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 345       |
|    time_elapsed         | 1267      |
|    total_timesteps      | 44160     |
| train/                  |           |
|    approx_kl            | 0.5101238 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.666     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.45      |
|    n_updates            | 6880      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.36      |
|    value_loss           | 23.5      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0184     |
| reward_motion           | 0.238      |
| reward_position         | 0.387      |
| reward_torque           | 0.000913   |
| reward_velocity         | 0.395      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 346        |
|    time_elapsed         | 1271       |
|    total_timesteps      | 44288      |
| train/                  |            |
|    approx_kl            | 0.24248835 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.93       |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.36       |
|    value_loss           | 13.3       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0184    |
| reward_motion           | 0.238     |
| reward_position         | 0.388     |
| reward_torque           | 0.000913  |
| reward_velocity         | 0.396     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 128       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 347       |
|    time_elapsed         | 1275      |
|    total_timesteps      | 44416     |
| train/                  |           |
|    approx_kl            | 1.2013836 |
|    clip_fraction        | 0.714     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.84      |
|    n_updates            | 6920      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.36      |
|    value_loss           | 24.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0184    |
| reward_motion           | 0.238     |
| reward_position         | 0.388     |
| reward_torque           | 0.000911  |
| reward_velocity         | 0.391     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 348       |
|    time_elapsed         | 1278      |
|    total_timesteps      | 44544     |
| train/                  |           |
|    approx_kl            | 0.6468891 |
|    clip_fraction        | 0.509     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.804     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.12      |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.359     |
|    value_loss           | 19.6      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0184     |
| reward_motion           | 0.238      |
| reward_position         | 0.388      |
| reward_torque           | 0.000915   |
| reward_velocity         | 0.398      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 349        |
|    time_elapsed         | 1282       |
|    total_timesteps      | 44672      |
| train/                  |            |
|    approx_kl            | 0.55664754 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.142      |
|    learning_rate        | 0.0005     |
|    loss                 | 27.7       |
|    n_updates            | 6960       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.36       |
|    value_loss           | 84.6       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0183     |
| reward_motion           | 0.238      |
| reward_position         | 0.389      |
| reward_torque           | 0.000914   |
| reward_velocity         | 0.398      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 350        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 44800      |
| train/                  |            |
|    approx_kl            | 0.50303566 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.85       |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.36       |
|    value_loss           | 16.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0184     |
| reward_motion           | 0.238      |
| reward_position         | 0.389      |
| reward_torque           | 0.000912   |
| reward_velocity         | 0.396      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 351        |
|    time_elapsed         | 1289       |
|    total_timesteps      | 44928      |
| train/                  |            |
|    approx_kl            | 0.71091294 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.46       |
|    n_updates            | 7000       |
|    policy_gradient_loss | -0.232     |
|    std                  | 0.36       |
|    value_loss           | 11.4       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0184    |
| reward_motion           | 0.239     |
| reward_position         | 0.389     |
| reward_torque           | 0.000909  |
| reward_velocity         | 0.395     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 352       |
|    time_elapsed         | 1293      |
|    total_timesteps      | 45056     |
| train/                  |           |
|    approx_kl            | 0.8065166 |
|    clip_fraction        | 0.707     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.22      |
|    n_updates            | 7020      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.36      |
|    value_loss           | 26.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0185     |
| reward_motion           | 0.239      |
| reward_position         | 0.388      |
| reward_torque           | 0.000907   |
| reward_velocity         | 0.396      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 353        |
|    time_elapsed         | 1297       |
|    total_timesteps      | 45184      |
| train/                  |            |
|    approx_kl            | 0.80667317 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.84       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.212     |
|    std                  | 0.36       |
|    value_loss           | 9.8        |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0186    |
| reward_motion           | 0.239     |
| reward_position         | 0.388     |
| reward_torque           | 0.000905  |
| reward_velocity         | 0.394     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 354       |
|    time_elapsed         | 1301      |
|    total_timesteps      | 45312     |
| train/                  |           |
|    approx_kl            | 0.6496382 |
|    clip_fraction        | 0.607     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | 1.38      |
|    n_updates            | 7060      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.36      |
|    value_loss           | 9.47      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0185     |
| reward_motion           | 0.239      |
| reward_position         | 0.389      |
| reward_torque           | 0.000908   |
| reward_velocity         | 0.395      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 355        |
|    time_elapsed         | 1304       |
|    total_timesteps      | 45440      |
| train/                  |            |
|    approx_kl            | 0.35110354 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.81       |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.36       |
|    value_loss           | 9.42       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0184     |
| reward_motion           | 0.239      |
| reward_position         | 0.389      |
| reward_torque           | 0.000909   |
| reward_velocity         | 0.396      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 356        |
|    time_elapsed         | 1308       |
|    total_timesteps      | 45568      |
| train/                  |            |
|    approx_kl            | 0.47884446 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.501      |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.226     |
|    std                  | 0.36       |
|    value_loss           | 9.55       |
----------------------------------------
--------------------------------------
| reward                  | 1.04     |
| reward_contact          | 0.0185   |
| reward_motion           | 0.239    |
| reward_position         | 0.39     |
| reward_torque           | 0.000909 |
| reward_velocity         | 0.397    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 129      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 357      |
|    time_elapsed         | 1312     |
|    total_timesteps      | 45696    |
| train/                  |          |
|    approx_kl            | 1.245122 |
|    clip_fraction        | 0.552    |
|    clip_range           | 0.4      |
|    entropy_loss         | -112     |
|    explained_variance   | 0.922    |
|    learning_rate        | 0.0005   |
|    loss                 | 0.697    |
|    n_updates            | 7120     |
|    policy_gradient_loss | -0.243   |
|    std                  | 0.359    |
|    value_loss           | 6.42     |
--------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0182    |
| reward_motion           | 0.24      |
| reward_position         | 0.39      |
| reward_torque           | 0.000907  |
| reward_velocity         | 0.393     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 358       |
|    time_elapsed         | 1315      |
|    total_timesteps      | 45824     |
| train/                  |           |
|    approx_kl            | 0.8893059 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.03      |
|    n_updates            | 7140      |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.359     |
|    value_loss           | 8.59      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.24       |
| reward_position         | 0.389      |
| reward_torque           | 0.000907   |
| reward_velocity         | 0.394      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 359        |
|    time_elapsed         | 1319       |
|    total_timesteps      | 45952      |
| train/                  |            |
|    approx_kl            | 0.74241364 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.0005     |
|    loss                 | 10.3       |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.188     |
|    std                  | 0.359      |
|    value_loss           | 36.9       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.24       |
| reward_position         | 0.389      |
| reward_torque           | 0.000906   |
| reward_velocity         | 0.394      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 360        |
|    time_elapsed         | 1323       |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.63010734 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.52       |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.227     |
|    std                  | 0.359      |
|    value_loss           | 14.6       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.24       |
| reward_position         | 0.39       |
| reward_torque           | 0.000905   |
| reward_velocity         | 0.393      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 361        |
|    time_elapsed         | 1326       |
|    total_timesteps      | 46208      |
| train/                  |            |
|    approx_kl            | 0.39885867 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.63       |
|    n_updates            | 7200       |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.359      |
|    value_loss           | 8.72       |
----------------------------------------
--------------------------------------
| reward                  | 1.04     |
| reward_contact          | 0.018    |
| reward_motion           | 0.24     |
| reward_position         | 0.39     |
| reward_torque           | 0.000905 |
| reward_velocity         | 0.393    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 129      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 362      |
|    time_elapsed         | 1330     |
|    total_timesteps      | 46336    |
| train/                  |          |
|    approx_kl            | 0.46207  |
|    clip_fraction        | 0.469    |
|    clip_range           | 0.4      |
|    entropy_loss         | -112     |
|    explained_variance   | 0.916    |
|    learning_rate        | 0.0005   |
|    loss                 | 3.34     |
|    n_updates            | 7220     |
|    policy_gradient_loss | -0.163   |
|    std                  | 0.359    |
|    value_loss           | 12.6     |
--------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0178     |
| reward_motion           | 0.24       |
| reward_position         | 0.389      |
| reward_torque           | 0.000905   |
| reward_velocity         | 0.394      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 363        |
|    time_elapsed         | 1334       |
|    total_timesteps      | 46464      |
| train/                  |            |
|    approx_kl            | 0.36775616 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06       |
|    n_updates            | 7240       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.359      |
|    value_loss           | 15.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0179    |
| reward_motion           | 0.24      |
| reward_position         | 0.39      |
| reward_torque           | 0.000902  |
| reward_velocity         | 0.391     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 364       |
|    time_elapsed         | 1338      |
|    total_timesteps      | 46592     |
| train/                  |           |
|    approx_kl            | 0.6105561 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.87      |
|    n_updates            | 7260      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.359     |
|    value_loss           | 10.1      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.24       |
| reward_position         | 0.39       |
| reward_torque           | 0.0009     |
| reward_velocity         | 0.391      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 365        |
|    time_elapsed         | 1341       |
|    total_timesteps      | 46720      |
| train/                  |            |
|    approx_kl            | 0.32725585 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.38       |
|    n_updates            | 7280       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.359      |
|    value_loss           | 11         |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.24       |
| reward_position         | 0.391      |
| reward_torque           | 0.000902   |
| reward_velocity         | 0.391      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 366        |
|    time_elapsed         | 1345       |
|    total_timesteps      | 46848      |
| train/                  |            |
|    approx_kl            | 0.45569003 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.0005     |
|    loss                 | 10.6       |
|    n_updates            | 7300       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.359      |
|    value_loss           | 47.8       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0178     |
| reward_motion           | 0.24       |
| reward_position         | 0.392      |
| reward_torque           | 0.0009     |
| reward_velocity         | 0.39       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 367        |
|    time_elapsed         | 1349       |
|    total_timesteps      | 46976      |
| train/                  |            |
|    approx_kl            | 0.51501757 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.58       |
|    n_updates            | 7320       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.359      |
|    value_loss           | 6.81       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0178    |
| reward_motion           | 0.24      |
| reward_position         | 0.392     |
| reward_torque           | 0.000898  |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 368       |
|    time_elapsed         | 1352      |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.4529361 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.97      |
|    learning_rate        | 0.0005    |
|    loss                 | 1.12      |
|    n_updates            | 7340      |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.359     |
|    value_loss           | 6.83      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.018     |
| reward_motion           | 0.24      |
| reward_position         | 0.392     |
| reward_torque           | 0.000899  |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 369       |
|    time_elapsed         | 1356      |
|    total_timesteps      | 47232     |
| train/                  |           |
|    approx_kl            | 0.4941093 |
|    clip_fraction        | 0.546     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.94      |
|    learning_rate        | 0.0005    |
|    loss                 | 1.39      |
|    n_updates            | 7360      |
|    policy_gradient_loss | -0.163    |
|    std                  | 0.359     |
|    value_loss           | 7.19      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.018     |
| reward_motion           | 0.24      |
| reward_position         | 0.392     |
| reward_torque           | 0.000897  |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 370       |
|    time_elapsed         | 1360      |
|    total_timesteps      | 47360     |
| train/                  |           |
|    approx_kl            | 0.5998838 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.866     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.47      |
|    n_updates            | 7380      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.359     |
|    value_loss           | 18.2      |
---------------------------------------
----------------------------------------
| reward                  | 1.03       |
| reward_contact          | 0.0181     |
| reward_motion           | 0.24       |
| reward_position         | 0.393      |
| reward_torque           | 0.000894   |
| reward_velocity         | 0.381      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 371        |
|    time_elapsed         | 1363       |
|    total_timesteps      | 47488      |
| train/                  |            |
|    approx_kl            | 0.37722677 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.47       |
|    n_updates            | 7400       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.359      |
|    value_loss           | 5.84       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0179     |
| reward_motion           | 0.24       |
| reward_position         | 0.392      |
| reward_torque           | 0.000896   |
| reward_velocity         | 0.387      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 372        |
|    time_elapsed         | 1367       |
|    total_timesteps      | 47616      |
| train/                  |            |
|    approx_kl            | 0.49822012 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.21       |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.359      |
|    value_loss           | 19.8       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0179    |
| reward_motion           | 0.241     |
| reward_position         | 0.392     |
| reward_torque           | 0.000899  |
| reward_velocity         | 0.39      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 373       |
|    time_elapsed         | 1371      |
|    total_timesteps      | 47744     |
| train/                  |           |
|    approx_kl            | 0.3175176 |
|    clip_fraction        | 0.376     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.679     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.5       |
|    n_updates            | 7440      |
|    policy_gradient_loss | -0.127    |
|    std                  | 0.359     |
|    value_loss           | 31.7      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0178    |
| reward_motion           | 0.241     |
| reward_position         | 0.392     |
| reward_torque           | 0.000899  |
| reward_velocity         | 0.392     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 374       |
|    time_elapsed         | 1374      |
|    total_timesteps      | 47872     |
| train/                  |           |
|    approx_kl            | 1.1898866 |
|    clip_fraction        | 0.619     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.35      |
|    n_updates            | 7460      |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.359     |
|    value_loss           | 18.3      |
---------------------------------------
Num timesteps: 48000
Best mean reward: 128.52 - Last mean reward per episode: 129.26
Saving new best model to rl/out_dir/models/exp75/best_model.zip
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0179    |
| reward_motion           | 0.241     |
| reward_position         | 0.392     |
| reward_torque           | 0.000898  |
| reward_velocity         | 0.391     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 375       |
|    time_elapsed         | 1378      |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 0.2780778 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.825     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.48      |
|    n_updates            | 7480      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.359     |
|    value_loss           | 18.4      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0179    |
| reward_motion           | 0.241     |
| reward_position         | 0.392     |
| reward_torque           | 0.000896  |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 376       |
|    time_elapsed         | 1382      |
|    total_timesteps      | 48128     |
| train/                  |           |
|    approx_kl            | 0.3691546 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.961     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.471     |
|    n_updates            | 7500      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.359     |
|    value_loss           | 4.18      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0179     |
| reward_motion           | 0.24       |
| reward_position         | 0.392      |
| reward_torque           | 0.000892   |
| reward_velocity         | 0.387      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 377        |
|    time_elapsed         | 1386       |
|    total_timesteps      | 48256      |
| train/                  |            |
|    approx_kl            | 0.28121102 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.11       |
|    n_updates            | 7520       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.359      |
|    value_loss           | 31.1       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0178    |
| reward_motion           | 0.241     |
| reward_position         | 0.392     |
| reward_torque           | 0.000892  |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 378       |
|    time_elapsed         | 1389      |
|    total_timesteps      | 48384     |
| train/                  |           |
|    approx_kl            | 0.5430918 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0005    |
|    loss                 | 0.68      |
|    n_updates            | 7540      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.359     |
|    value_loss           | 5.62      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0179     |
| reward_motion           | 0.241      |
| reward_position         | 0.392      |
| reward_torque           | 0.000888   |
| reward_velocity         | 0.386      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 379        |
|    time_elapsed         | 1393       |
|    total_timesteps      | 48512      |
| train/                  |            |
|    approx_kl            | 0.32809147 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.409      |
|    n_updates            | 7560       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.359      |
|    value_loss           | 5.3        |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0178     |
| reward_motion           | 0.242      |
| reward_position         | 0.392      |
| reward_torque           | 0.000889   |
| reward_velocity         | 0.385      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 380        |
|    time_elapsed         | 1397       |
|    total_timesteps      | 48640      |
| train/                  |            |
|    approx_kl            | 0.46394056 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.25       |
|    n_updates            | 7580       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.359      |
|    value_loss           | 3.99       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0177    |
| reward_motion           | 0.242     |
| reward_position         | 0.394     |
| reward_torque           | 0.000889  |
| reward_velocity         | 0.384     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 381       |
|    time_elapsed         | 1400      |
|    total_timesteps      | 48768     |
| train/                  |           |
|    approx_kl            | 0.3622586 |
|    clip_fraction        | 0.395     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.905     |
|    learning_rate        | 0.0005    |
|    loss                 | 0.537     |
|    n_updates            | 7600      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.359     |
|    value_loss           | 7.89      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0176    |
| reward_motion           | 0.242     |
| reward_position         | 0.393     |
| reward_torque           | 0.000893  |
| reward_velocity         | 0.39      |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 129       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 382       |
|    time_elapsed         | 1404      |
|    total_timesteps      | 48896     |
| train/                  |           |
|    approx_kl            | 0.7973565 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.01      |
|    n_updates            | 7620      |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.359     |
|    value_loss           | 17.5      |
---------------------------------------
--------------------------------------
| reward                  | 1.04     |
| reward_contact          | 0.0175   |
| reward_motion           | 0.242    |
| reward_position         | 0.394    |
| reward_torque           | 0.000892 |
| reward_velocity         | 0.39     |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 130      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 383      |
|    time_elapsed         | 1408     |
|    total_timesteps      | 49024    |
| train/                  |          |
|    approx_kl            | 0.525115 |
|    clip_fraction        | 0.502    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.982    |
|    learning_rate        | 0.0005   |
|    loss                 | 0.0472   |
|    n_updates            | 7640     |
|    policy_gradient_loss | -0.181   |
|    std                  | 0.359    |
|    value_loss           | 2.37     |
--------------------------------------
--------------------------------------
| reward                  | 1.05     |
| reward_contact          | 0.0176   |
| reward_motion           | 0.242    |
| reward_position         | 0.394    |
| reward_torque           | 0.000891 |
| reward_velocity         | 0.391    |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 130      |
| time/                   |          |
|    fps                  | 34       |
|    iterations           | 384      |
|    time_elapsed         | 1412     |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.626008 |
|    clip_fraction        | 0.603    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.951    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.14     |
|    n_updates            | 7660     |
|    policy_gradient_loss | -0.214   |
|    std                  | 0.359    |
|    value_loss           | 8.23     |
--------------------------------------
----------------------------------------
| reward                  | 1.05       |
| reward_contact          | 0.0176     |
| reward_motion           | 0.243      |
| reward_position         | 0.395      |
| reward_torque           | 0.000889   |
| reward_velocity         | 0.391      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 130        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 385        |
|    time_elapsed         | 1415       |
|    total_timesteps      | 49280      |
| train/                  |            |
|    approx_kl            | 0.84858835 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.71       |
|    n_updates            | 7680       |
|    policy_gradient_loss | -0.219     |
|    std                  | 0.359      |
|    value_loss           | 14.9       |
----------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0175    |
| reward_motion           | 0.241     |
| reward_position         | 0.395     |
| reward_torque           | 0.000891  |
| reward_velocity         | 0.389     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 130       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 386       |
|    time_elapsed         | 1419      |
|    total_timesteps      | 49408     |
| train/                  |           |
|    approx_kl            | 0.5779206 |
|    clip_fraction        | 0.538     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.21      |
|    n_updates            | 7700      |
|    policy_gradient_loss | -0.176    |
|    std                  | 0.359     |
|    value_loss           | 26.5      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0176    |
| reward_motion           | 0.241     |
| reward_position         | 0.396     |
| reward_torque           | 0.000888  |
| reward_velocity         | 0.386     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 130       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 387       |
|    time_elapsed         | 1423      |
|    total_timesteps      | 49536     |
| train/                  |           |
|    approx_kl            | 0.3661837 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.98      |
|    n_updates            | 7720      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.359     |
|    value_loss           | 18.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0176    |
| reward_motion           | 0.241     |
| reward_position         | 0.396     |
| reward_torque           | 0.000889  |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 130       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 388       |
|    time_elapsed         | 1427      |
|    total_timesteps      | 49664     |
| train/                  |           |
|    approx_kl            | 0.6475005 |
|    clip_fraction        | 0.552     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.789     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.78      |
|    n_updates            | 7740      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.359     |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| reward                  | 1.04      |
| reward_contact          | 0.0176    |
| reward_motion           | 0.241     |
| reward_position         | 0.396     |
| reward_torque           | 0.00089   |
| reward_velocity         | 0.387     |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 130       |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 389       |
|    time_elapsed         | 1430      |
|    total_timesteps      | 49792     |
| train/                  |           |
|    approx_kl            | 0.3216875 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.95      |
|    learning_rate        | 0.0005    |
|    loss                 | 0.729     |
|    n_updates            | 7760      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.359     |
|    value_loss           | 6.05      |
---------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.0179     |
| reward_motion           | 0.241      |
| reward_position         | 0.397      |
| reward_torque           | 0.000888   |
| reward_velocity         | 0.384      |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 390        |
|    time_elapsed         | 1434       |
|    total_timesteps      | 49920      |
| train/                  |            |
|    approx_kl            | 0.42069572 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.274      |
|    n_updates            | 7780       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.359      |
|    value_loss           | 3.83       |
----------------------------------------
----------------------------------------
| reward                  | 1.04       |
| reward_contact          | 0.018      |
| reward_motion           | 0.241      |
| reward_position         | 0.397      |
| reward_torque           | 0.000886   |
| reward_velocity         | 0.38       |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 391        |
|    time_elapsed         | 1437       |
|    total_timesteps      | 50048      |
| train/                  |            |
|    approx_kl            | 0.87018746 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.268      |
|    n_updates            | 7800       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.359      |
|    value_loss           | 5.79       |
----------------------------------------
