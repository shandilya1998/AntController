/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
usage: ddpg.py [-h] [--experiment EXPERIMENT] [--out_path OUT_PATH]
               [--env ENV] [--env_version ENV_VERSION] [--model_dir MODEL_DIR]
               [--test_env [TEST_ENV]] [--her [HER]]
ddpg.py: error: unrecognized arguments: --get
2021-05-31 11:42:31.795161: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-05-31 11:42:31.795211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
[400, 300]
[400, 300]
Logging to rl/out_dir/models/exp68/TD3_1
2021-05-31 11:44:48.628458: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-05-31 11:44:48.724072: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
[400, 300]
[400, 300]
Logging to rl/out_dir/models/exp68/TD3_2
---------------------------------
| reward             | 1.1      |
| reward_ctrl        | 0.0349   |
| reward_motion      | 0.936    |
| reward_orientation | 0.084    |
| reward_position    | 0.0207   |
| reward_rotation    | 0.0153   |
| reward_velocity    | 0.00931  |
| rollout/           |          |
|    ep_len_mean     | 64.8     |
|    ep_rew_mean     | 45.4     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 32       |
|    time_elapsed    | 8        |
|    total timesteps | 259      |
---------------------------------
---------------------------------
| reward             | 1.05     |
| reward_ctrl        | 0.0316   |
| reward_motion      | 0.915    |
| reward_orientation | 0.084    |
| reward_position    | 0.00691  |
| reward_rotation    | 0.00572  |
| reward_velocity    | 0.0088   |
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | 53.3     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 32       |
|    time_elapsed    | 17       |
|    total timesteps | 563      |
---------------------------------
---------------------------------
| reward             | 0.875    |
| reward_ctrl        | 0.0222   |
| reward_motion      | 0.739    |
| reward_orientation | 0.084    |
| reward_position    | 0.00533  |
| reward_rotation    | 0.00755  |
| reward_velocity    | 0.0167   |
| rollout/           |          |
|    ep_len_mean     | 64.7     |
|    ep_rew_mean     | 46.5     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 34       |
|    time_elapsed    | 22       |
|    total timesteps | 776      |
---------------------------------
---------------------------------
| reward             | 0.878    |
| reward_ctrl        | 0.0198   |
| reward_motion      | 0.743    |
| reward_orientation | 0.084    |
| reward_position    | 0.0056   |
| reward_rotation    | 0.00778  |
| reward_velocity    | 0.0182   |
| rollout/           |          |
|    ep_len_mean     | 54.4     |
|    ep_rew_mean     | 37.2     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 36       |
|    time_elapsed    | 24       |
|    total timesteps | 870      |
---------------------------------
---------------------------------
| reward             | 0.78     |
| reward_ctrl        | 0.015    |
| reward_motion      | 0.652    |
| reward_orientation | 0.084    |
| reward_position    | 0.00504  |
| reward_rotation    | 0.00548  |
| reward_velocity    | 0.0189   |
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | 32.7     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 36       |
|    time_elapsed    | 27       |
|    total timesteps | 1011     |
---------------------------------
---------------------------------
| reward             | 0.773    |
| reward_ctrl        | 0.0157   |
| reward_motion      | 0.645    |
| reward_orientation | 0.084    |
| reward_position    | 0.0048   |
| reward_rotation    | 0.00521  |
| reward_velocity    | 0.0184   |
| rollout/           |          |
|    ep_len_mean     | 65.6     |
|    ep_rew_mean     | 55.4     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 25       |
|    time_elapsed    | 60       |
|    total timesteps | 1575     |
---------------------------------
---------------------------------
| reward             | 0.778    |
| reward_ctrl        | 0.0146   |
| reward_motion      | 0.654    |
| reward_orientation | 0.084    |
| reward_position    | 0.00478  |
| reward_rotation    | 0.00452  |
| reward_velocity    | 0.0166   |
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 73.3     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 23       |
|    time_elapsed    | 90       |
|    total timesteps | 2158     |
---------------------------------
---------------------------------
| reward             | 0.772    |
| reward_ctrl        | 0.0137   |
| reward_motion      | 0.651    |
| reward_orientation | 0.084    |
| reward_position    | 0.00401  |
| reward_rotation    | 0.00517  |
| reward_velocity    | 0.0146   |
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 24       |
|    time_elapsed    | 93       |
|    total timesteps | 2310     |
---------------------------------
---------------------------------
| reward             | 0.779    |
| reward_ctrl        | 0.0133   |
| reward_motion      | 0.659    |
| reward_orientation | 0.084    |
| reward_position    | 0.00366  |
| reward_rotation    | 0.00472  |
| reward_velocity    | 0.0142   |
| rollout/           |          |
|    ep_len_mean     | 74       |
|    ep_rew_mean     | 66.7     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 25       |
|    time_elapsed    | 105      |
|    total timesteps | 2665     |
---------------------------------
---------------------------------
| reward             | 0.792    |
| reward_ctrl        | 0.0138   |
| reward_motion      | 0.674    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00327  |
| reward_rotation    | 0.00422  |
| reward_velocity    | 0.0129   |
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 79.9     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 24       |
|    time_elapsed    | 134      |
|    total timesteps | 3341     |
---------------------------------
---------------------------------
| reward             | 0.788    |
| reward_ctrl        | 0.0135   |
| reward_motion      | 0.672    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00296  |
| reward_rotation    | 0.00419  |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 78.1     |
|    ep_rew_mean     | 73.5     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 25       |
|    time_elapsed    | 135      |
|    total timesteps | 3438     |
---------------------------------
---------------------------------
| reward             | 0.763    |
| reward_ctrl        | 0.0128   |
| reward_motion      | 0.647    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00306  |
| reward_rotation    | 0.00483  |
| reward_velocity    | 0.0115   |
| rollout/           |          |
|    ep_len_mean     | 74       |
|    ep_rew_mean     | 68.3     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 25       |
|    time_elapsed    | 137      |
|    total timesteps | 3554     |
---------------------------------
---------------------------------
| reward             | 0.741    |
| reward_ctrl        | 0.0136   |
| reward_motion      | 0.625    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00295  |
| reward_rotation    | 0.00426  |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 71.9     |
|    ep_rew_mean     | 64.7     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 26       |
|    time_elapsed    | 143      |
|    total timesteps | 3739     |
---------------------------------
---------------------------------
| reward             | 0.753    |
| reward_ctrl        | 0.013    |
| reward_motion      | 0.636    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00279  |
| reward_rotation    | 0.00402  |
| reward_velocity    | 0.0129   |
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | 62.7     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 26       |
|    time_elapsed    | 149      |
|    total timesteps | 3960     |
---------------------------------
---------------------------------
| reward             | 0.753    |
| reward_ctrl        | 0.0121   |
| reward_motion      | 0.639    |
| reward_orientation | 0.0839   |
| reward_position    | 0.0026   |
| reward_rotation    | 0.00372  |
| reward_velocity    | 0.0124   |
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | 59.7     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 26       |
|    time_elapsed    | 154      |
|    total timesteps | 4126     |
---------------------------------
---------------------------------
| reward             | 0.743    |
| reward_ctrl        | 0.0122   |
| reward_motion      | 0.629    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00249  |
| reward_rotation    | 0.00348  |
| reward_velocity    | 0.0119   |
| rollout/           |          |
|    ep_len_mean     | 65.8     |
|    ep_rew_mean     | 56.4     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 27       |
|    time_elapsed    | 155      |
|    total timesteps | 4212     |
---------------------------------
---------------------------------
| reward             | 0.74     |
| reward_ctrl        | 0.0124   |
| reward_motion      | 0.627    |
| reward_orientation | 0.084    |
| reward_position    | 0.00234  |
| reward_rotation    | 0.00328  |
| reward_velocity    | 0.0112   |
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | 59       |
| time/              |          |
|    episodes        | 68       |
|    fps             | 26       |
|    time_elapsed    | 176      |
|    total timesteps | 4647     |
---------------------------------
---------------------------------
| reward             | 0.742    |
| reward_ctrl        | 0.0124   |
| reward_motion      | 0.63     |
| reward_orientation | 0.084    |
| reward_position    | 0.00229  |
| reward_rotation    | 0.00314  |
| reward_velocity    | 0.0108   |
| rollout/           |          |
|    ep_len_mean     | 66.4     |
|    ep_rew_mean     | 56.5     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 26       |
|    time_elapsed    | 180      |
|    total timesteps | 4780     |
---------------------------------
---------------------------------
| reward             | 0.748    |
| reward_ctrl        | 0.0121   |
| reward_motion      | 0.636    |
| reward_orientation | 0.084    |
| reward_position    | 0.00218  |
| reward_rotation    | 0.00297  |
| reward_velocity    | 0.0103   |
| rollout/           |          |
|    ep_len_mean     | 65.4     |
|    ep_rew_mean     | 54.9     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 26       |
|    time_elapsed    | 186      |
|    total timesteps | 4970     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 64.08
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.773    |
| reward_ctrl        | 0.0164   |
| reward_motion      | 0.658    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00204  |
| reward_rotation    | 0.00324  |
| reward_velocity    | 0.00992  |
| rollout/           |          |
|    ep_len_mean     | 77.6     |
|    ep_rew_mean     | 73.7     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 20       |
|    time_elapsed    | 307      |
|    total timesteps | 6208     |
| train/             |          |
|    actor_loss      | -3.74    |
|    critic_loss     | 0.267    |
|    learning_rate   | 0.001    |
|    n_updates       | 1205     |
---------------------------------
---------------------------------
| reward             | 0.821    |
| reward_ctrl        | 0.0239   |
| reward_motion      | 0.693    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00194  |
| reward_rotation    | 0.00866  |
| reward_velocity    | 0.00944  |
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 99.7     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 517      |
|    total timesteps | 8208     |
| train/             |          |
|    actor_loss      | -8.79    |
|    critic_loss     | 0.583    |
|    learning_rate   | 0.001    |
|    n_updates       | 3205     |
---------------------------------
---------------------------------
| reward             | 0.837    |
| reward_ctrl        | 0.0281   |
| reward_motion      | 0.703    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00191  |
| reward_rotation    | 0.0112   |
| reward_velocity    | 0.00925  |
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 596      |
|    total timesteps | 9047     |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.831    |
|    learning_rate   | 0.001    |
|    n_updates       | 4045     |
---------------------------------
---------------------------------
| reward             | 0.862    |
| reward_ctrl        | 0.031    |
| reward_motion      | 0.725    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00185  |
| reward_rotation    | 0.0113   |
| reward_velocity    | 0.00895  |
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 14       |
|    time_elapsed    | 724      |
|    total timesteps | 10365    |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.001    |
|    n_updates       | 5360     |
---------------------------------
---------------------------------
| reward             | 0.881    |
| reward_ctrl        | 0.0336   |
| reward_motion      | 0.739    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00175  |
| reward_rotation    | 0.0149   |
| reward_velocity    | 0.00849  |
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 13       |
|    time_elapsed    | 881      |
|    total timesteps | 11889    |
| train/             |          |
|    actor_loss      | -17.8    |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 6885     |
---------------------------------
Num timesteps: 12000
Best mean reward: 64.08 - Last mean reward per episode: 134.57
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.893    |
| reward_ctrl        | 0.0355   |
| reward_motion      | 0.749    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00168  |
| reward_rotation    | 0.015    |
| reward_velocity    | 0.00816  |
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 13       |
|    time_elapsed    | 1012     |
|    total timesteps | 13242    |
| train/             |          |
|    actor_loss      | -20.1    |
|    critic_loss     | 1.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 8240     |
---------------------------------
---------------------------------
| reward             | 0.912    |
| reward_ctrl        | 0.0366   |
| reward_motion      | 0.768    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00125  |
| reward_rotation    | 0.0146   |
| reward_velocity    | 0.0079   |
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 12       |
|    time_elapsed    | 1126     |
|    total timesteps | 14395    |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 9390     |
---------------------------------
---------------------------------
| reward             | 0.925    |
| reward_ctrl        | 0.0401   |
| reward_motion      | 0.777    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00125  |
| reward_rotation    | 0.015    |
| reward_velocity    | 0.00737  |
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    episodes        | 108      |
|    fps             | 12       |
|    time_elapsed    | 1238     |
|    total timesteps | 15526    |
| train/             |          |
|    actor_loss      | -25      |
|    critic_loss     | 3.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 10525    |
---------------------------------
---------------------------------
| reward             | 0.941    |
| reward_ctrl        | 0.0442   |
| reward_motion      | 0.786    |
| reward_orientation | 0.0839   |
| reward_position    | 0.00118  |
| reward_rotation    | 0.0184   |
| reward_velocity    | 0.00676  |
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 12       |
|    time_elapsed    | 1295     |
|    total timesteps | 16128    |
| train/             |          |
|    actor_loss      | -27.1    |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 11125    |
---------------------------------
Num timesteps: 18000
Best mean reward: 134.57 - Last mean reward per episode: 194.33
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.986    |
| reward_ctrl        | 0.0478   |
| reward_motion      | 0.828    |
| reward_orientation | 0.0838   |
| reward_position    | 0.00085  |
| reward_rotation    | 0.0193   |
| reward_velocity    | 0.00601  |
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 11       |
|    time_elapsed    | 1527     |
|    total timesteps | 18128    |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 13125    |
---------------------------------
---------------------------------
| reward             | 1.02     |
| reward_ctrl        | 0.0515   |
| reward_motion      | 0.855    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000762 |
| reward_rotation    | 0.0201   |
| reward_velocity    | 0.0049   |
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 10       |
|    time_elapsed    | 1796     |
|    total timesteps | 19650    |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 14645    |
---------------------------------
---------------------------------
| reward             | 1.03     |
| reward_ctrl        | 0.0556   |
| reward_motion      | 0.868    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000517 |
| reward_rotation    | 0.0225   |
| reward_velocity    | 0.0043   |
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 10       |
|    time_elapsed    | 1907     |
|    total timesteps | 20757    |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 2.2      |
|    learning_rate   | 0.001    |
|    n_updates       | 15755    |
---------------------------------
---------------------------------
| reward             | 1.04     |
| reward_ctrl        | 0.0588   |
| reward_motion      | 0.873    |
| reward_orientation | 0.0838   |
| reward_position    | 0.00053  |
| reward_rotation    | 0.0225   |
| reward_velocity    | 0.00423  |
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 10       |
|    time_elapsed    | 1962     |
|    total timesteps | 21344    |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 16340    |
---------------------------------
---------------------------------
| reward             | 1.06     |
| reward_ctrl        | 0.061    |
| reward_motion      | 0.881    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000529 |
| reward_rotation    | 0.0265   |
| reward_velocity    | 0.00408  |
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    episodes        | 132      |
|    fps             | 10       |
|    time_elapsed    | 2075     |
|    total timesteps | 22457    |
| train/             |          |
|    actor_loss      | -40.1    |
|    critic_loss     | 5.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 17455    |
---------------------------------
---------------------------------
| reward             | 1.08     |
| reward_ctrl        | 0.0639   |
| reward_motion      | 0.898    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000704 |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.00403  |
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 253      |
| time/              |          |
|    episodes        | 136      |
|    fps             | 10       |
|    time_elapsed    | 2184     |
|    total timesteps | 23503    |
| train/             |          |
|    actor_loss      | -43.2    |
|    critic_loss     | 2.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 18500    |
---------------------------------
Num timesteps: 24000
Best mean reward: 194.33 - Last mean reward per episode: 246.14
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 1.07     |
| reward_ctrl        | 0.0662   |
| reward_motion      | 0.881    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000761 |
| reward_rotation    | 0.029    |
| reward_velocity    | 0.00469  |
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 252      |
| time/              |          |
|    episodes        | 140      |
|    fps             | 10       |
|    time_elapsed    | 2239     |
|    total timesteps | 24074    |
| train/             |          |
|    actor_loss      | -43.6    |
|    critic_loss     | 2.88     |
|    learning_rate   | 0.001    |
|    n_updates       | 19070    |
---------------------------------
---------------------------------
| reward             | 1.08     |
| reward_ctrl        | 0.067    |
| reward_motion      | 0.899    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000632 |
| reward_rotation    | 0.0289   |
| reward_velocity    | 0.00446  |
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 261      |
| time/              |          |
|    episodes        | 144      |
|    fps             | 10       |
|    time_elapsed    | 2311     |
|    total timesteps | 24863    |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 2.46     |
|    learning_rate   | 0.001    |
|    n_updates       | 19860    |
---------------------------------
---------------------------------
| reward             | 1.13     |
| reward_ctrl        | 0.0679   |
| reward_motion      | 0.942    |
| reward_orientation | 0.0838   |
| reward_position    | 0.000502 |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.00399  |
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 10       |
|    time_elapsed    | 2378     |
|    total timesteps | 25640    |
| train/             |          |
|    actor_loss      | -46.2    |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 20635    |
---------------------------------
---------------------------------
| reward             | 1.13     |
| reward_ctrl        | 0.0679   |
| reward_motion      | 0.95     |
| reward_orientation | 0.0838   |
| reward_position    | 0.000502 |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.004    |
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 268      |
| time/              |          |
|    episodes        | 152      |
|    fps             | 10       |
|    time_elapsed    | 2389     |
|    total timesteps | 25799    |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 3.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 20795    |
---------------------------------
---------------------------------
| reward             | 1.12     |
| reward_ctrl        | 0.0678   |
| reward_motion      | 0.935    |
| reward_orientation | 0.0838   |
| reward_position    | 0.00113  |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.00315  |
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 275      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 10       |
|    time_elapsed    | 2447     |
|    total timesteps | 26408    |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 21405    |
---------------------------------
---------------------------------
| reward             | 1.15     |
| reward_ctrl        | 0.0715   |
| reward_motion      | 0.965    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00109  |
| reward_rotation    | 0.0291   |
| reward_velocity    | 0.00313  |
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 294      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 10       |
|    time_elapsed    | 2604     |
|    total timesteps | 27940    |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 2.88     |
|    learning_rate   | 0.001    |
|    n_updates       | 22935    |
---------------------------------
---------------------------------
| reward             | 1.19     |
| reward_ctrl        | 0.0761   |
| reward_motion      | 0.997    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00106  |
| reward_rotation    | 0.0298   |
| reward_velocity    | 0.00279  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    episodes        | 164      |
|    fps             | 10       |
|    time_elapsed    | 2754     |
|    total timesteps | 29423    |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 24420    |
---------------------------------
Num timesteps: 30000
Best mean reward: 246.14 - Last mean reward per episode: 309.78
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 1.19     |
| reward_ctrl        | 0.0755   |
| reward_motion      | 1        |
| reward_orientation | 0.0837   |
| reward_position    | 0.00128  |
| reward_rotation    | 0.0298   |
| reward_velocity    | 0.0028   |
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    episodes        | 168      |
|    fps             | 10       |
|    time_elapsed    | 2814     |
|    total timesteps | 30056    |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 2.27     |
|    learning_rate   | 0.001    |
|    n_updates       | 25055    |
---------------------------------
---------------------------------
| reward             | 1.19     |
| reward_ctrl        | 0.0757   |
| reward_motion      | 1        |
| reward_orientation | 0.0837   |
| reward_position    | 0.00128  |
| reward_rotation    | 0.0298   |
| reward_velocity    | 0.00284  |
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 10       |
|    time_elapsed    | 2822     |
|    total timesteps | 30182    |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 3.78     |
|    learning_rate   | 0.001    |
|    n_updates       | 25180    |
---------------------------------
---------------------------------
| reward             | 1.2      |
| reward_ctrl        | 0.0794   |
| reward_motion      | 1        |
| reward_orientation | 0.0837   |
| reward_position    | 0.00149  |
| reward_rotation    | 0.0314   |
| reward_velocity    | 0.0033   |
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    episodes        | 176      |
|    fps             | 10       |
|    time_elapsed    | 2879     |
|    total timesteps | 30760    |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 2.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 25755    |
---------------------------------
---------------------------------
| reward             | 1.2      |
| reward_ctrl        | 0.0788   |
| reward_motion      | 0.995    |
| reward_orientation | 0.0837   |
| reward_position    | 0.0018   |
| reward_rotation    | 0.0313   |
| reward_velocity    | 0.00452  |
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 304      |
| time/              |          |
|    episodes        | 180      |
|    fps             | 10       |
|    time_elapsed    | 2887     |
|    total timesteps | 30883    |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 4.07     |
|    learning_rate   | 0.001    |
|    n_updates       | 25880    |
---------------------------------
---------------------------------
| reward             | 1.15     |
| reward_ctrl        | 0.0738   |
| reward_motion      | 0.96     |
| reward_orientation | 0.0837   |
| reward_position    | 0.00197  |
| reward_rotation    | 0.029    |
| reward_velocity    | 0.00504  |
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    episodes        | 184      |
|    fps             | 10       |
|    time_elapsed    | 3044     |
|    total timesteps | 32398    |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 3.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 27395    |
---------------------------------
---------------------------------
| reward             | 1.12     |
| reward_ctrl        | 0.0677   |
| reward_motion      | 0.937    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00206  |
| reward_rotation    | 0.0246   |
| reward_velocity    | 0.00499  |
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 290      |
| time/              |          |
|    episodes        | 188      |
|    fps             | 10       |
|    time_elapsed    | 3051     |
|    total timesteps | 32501    |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 2.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 27500    |
---------------------------------
---------------------------------
| reward             | 1.11     |
| reward_ctrl        | 0.066    |
| reward_motion      | 0.93     |
| reward_orientation | 0.0837   |
| reward_position    | 0.00206  |
| reward_rotation    | 0.0242   |
| reward_velocity    | 0.00499  |
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 275      |
| time/              |          |
|    episodes        | 192      |
|    fps             | 10       |
|    time_elapsed    | 3061     |
|    total timesteps | 32655    |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 7.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 27650    |
---------------------------------
---------------------------------
| reward             | 1.09     |
| reward_ctrl        | 0.0664   |
| reward_motion      | 0.91     |
| reward_orientation | 0.0837   |
| reward_position    | 0.00244  |
| reward_rotation    | 0.0229   |
| reward_velocity    | 0.00541  |
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 262      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 10       |
|    time_elapsed    | 3124     |
|    total timesteps | 33296    |
| train/             |          |
|    actor_loss      | -56.8    |
|    critic_loss     | 3.38     |
|    learning_rate   | 0.001    |
|    n_updates       | 28295    |
---------------------------------
---------------------------------
| reward             | 1.09     |
| reward_ctrl        | 0.0667   |
| reward_motion      | 0.904    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00278  |
| reward_rotation    | 0.0202   |
| reward_velocity    | 0.00835  |
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 10       |
|    time_elapsed    | 3291     |
|    total timesteps | 34233    |
| train/             |          |
|    actor_loss      | -56      |
|    critic_loss     | 6.47     |
|    learning_rate   | 0.001    |
|    n_updates       | 29230    |
---------------------------------
---------------------------------
| reward             | 1.06     |
| reward_ctrl        | 0.064    |
| reward_motion      | 0.878    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00348  |
| reward_rotation    | 0.0197   |
| reward_velocity    | 0.00858  |
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 247      |
| time/              |          |
|    episodes        | 204      |
|    fps             | 10       |
|    time_elapsed    | 3389     |
|    total timesteps | 34765    |
| train/             |          |
|    actor_loss      | -57.4    |
|    critic_loss     | 3.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 29760    |
---------------------------------
---------------------------------
| reward             | 1.03     |
| reward_ctrl        | 0.0613   |
| reward_motion      | 0.852    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00421  |
| reward_rotation    | 0.019    |
| reward_velocity    | 0.00845  |
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    episodes        | 208      |
|    fps             | 10       |
|    time_elapsed    | 3394     |
|    total timesteps | 34857    |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 4.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 29855    |
---------------------------------
---------------------------------
| reward             | 1.01     |
| reward_ctrl        | 0.0623   |
| reward_motion      | 0.833    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00445  |
| reward_rotation    | 0.0155   |
| reward_velocity    | 0.00855  |
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    episodes        | 212      |
|    fps             | 10       |
|    time_elapsed    | 3450     |
|    total timesteps | 35448    |
| train/             |          |
|    actor_loss      | -58.3    |
|    critic_loss     | 2.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 30445    |
---------------------------------
Num timesteps: 36000
Best mean reward: 309.78 - Last mean reward per episode: 227.35
---------------------------------
| reward             | 0.993    |
| reward_ctrl        | 0.0594   |
| reward_motion      | 0.82     |
| reward_orientation | 0.0837   |
| reward_position    | 0.00476  |
| reward_rotation    | 0.0163   |
| reward_velocity    | 0.00866  |
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    episodes        | 216      |
|    fps             | 10       |
|    time_elapsed    | 3512     |
|    total timesteps | 36067    |
| train/             |          |
|    actor_loss      | -58.4    |
|    critic_loss     | 3.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 31065    |
---------------------------------
---------------------------------
| reward             | 0.978    |
| reward_ctrl        | 0.056    |
| reward_motion      | 0.81     |
| reward_orientation | 0.0838   |
| reward_position    | 0.00479  |
| reward_rotation    | 0.0147   |
| reward_velocity    | 0.00866  |
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    episodes        | 220      |
|    fps             | 10       |
|    time_elapsed    | 3625     |
|    total timesteps | 37055    |
| train/             |          |
|    actor_loss      | -60.1    |
|    critic_loss     | 3.25     |
|    learning_rate   | 0.001    |
|    n_updates       | 32050    |
---------------------------------
---------------------------------
| reward             | 0.978    |
| reward_ctrl        | 0.0526   |
| reward_motion      | 0.814    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00481  |
| reward_rotation    | 0.014    |
| reward_velocity    | 0.0089   |
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 10       |
|    time_elapsed    | 3779     |
|    total timesteps | 38580    |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 4.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 33575    |
---------------------------------
---------------------------------
| reward             | 0.957    |
| reward_ctrl        | 0.0478   |
| reward_motion      | 0.801    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00479  |
| reward_rotation    | 0.0102   |
| reward_velocity    | 0.00943  |
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    episodes        | 228      |
|    fps             | 10       |
|    time_elapsed    | 3844     |
|    total timesteps | 39279    |
| train/             |          |
|    actor_loss      | -63.6    |
|    critic_loss     | 3.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 34275    |
---------------------------------
---------------------------------
| reward             | 0.958    |
| reward_ctrl        | 0.0465   |
| reward_motion      | 0.803    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00481  |
| reward_rotation    | 0.0102   |
| reward_velocity    | 0.00942  |
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    episodes        | 232      |
|    fps             | 10       |
|    time_elapsed    | 3858     |
|    total timesteps | 39492    |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 2.96     |
|    learning_rate   | 0.001    |
|    n_updates       | 34490    |
---------------------------------
---------------------------------
| reward             | 0.935    |
| reward_ctrl        | 0.0435   |
| reward_motion      | 0.788    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00475  |
| reward_rotation    | 0.00566  |
| reward_velocity    | 0.00917  |
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 10       |
|    time_elapsed    | 3917     |
|    total timesteps | 40104    |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 3.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 35100    |
---------------------------------
---------------------------------
| reward             | 0.942    |
| reward_ctrl        | 0.0419   |
| reward_motion      | 0.796    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00475  |
| reward_rotation    | 0.00682  |
| reward_velocity    | 0.00895  |
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 10       |
|    time_elapsed    | 4023     |
|    total timesteps | 41148    |
| train/             |          |
|    actor_loss      | -65.1    |
|    critic_loss     | 3.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 36145    |
---------------------------------
Num timesteps: 42000
Best mean reward: 309.78 - Last mean reward per episode: 202.87
---------------------------------
| reward             | 0.962    |
| reward_ctrl        | 0.0432   |
| reward_motion      | 0.815    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00474  |
| reward_rotation    | 0.0069   |
| reward_velocity    | 0.00893  |
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 10       |
|    time_elapsed    | 4149     |
|    total timesteps | 42214    |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 3.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 37210    |
---------------------------------
---------------------------------
| reward             | 0.948    |
| reward_ctrl        | 0.0446   |
| reward_motion      | 0.797    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00475  |
| reward_rotation    | 0.00908  |
| reward_velocity    | 0.0088   |
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 10       |
|    time_elapsed    | 4318     |
|    total timesteps | 43254    |
| train/             |          |
|    actor_loss      | -66.4    |
|    critic_loss     | 5.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 38250    |
---------------------------------
---------------------------------
| reward             | 0.942    |
| reward_ctrl        | 0.0466   |
| reward_motion      | 0.788    |
| reward_orientation | 0.0837   |
| reward_position    | 0.00459  |
| reward_rotation    | 0.01     |
| reward_velocity    | 0.00882  |
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 9        |
|    time_elapsed    | 4493     |
|    total timesteps | 44362    |
| train/             |          |
|    actor_loss      | -66.5    |
|    critic_loss     | 3.5      |
|    learning_rate   | 0.001    |
|    n_updates       | 39360    |
---------------------------------
2021-05-31 18:30:00.517563: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin:/home/shandilya/.mujoco/mjpro150/bin
2021-05-31 18:30:00.517629: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_11
---------------------------------
| reward             | 0.133    |
| reward_ctrl        | 0.0556   |
| reward_orientation | 0.0664   |
| reward_position    | 0.00461  |
| reward_rotation    | 0.00142  |
| reward_velocity    | 0.00543  |
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 48       |
|    time_elapsed    | 12       |
|    total timesteps | 590      |
---------------------------------
---------------------------------
| reward             | 0.169    |
| reward_ctrl        | 0.0591   |
| reward_orientation | 0.0663   |
| reward_position    | 0.00231  |
| reward_rotation    | 0.0385   |
| reward_velocity    | 0.00293  |
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 30.5     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 52       |
|    time_elapsed    | 31       |
|    total timesteps | 1675     |
---------------------------------
---------------------------------
| reward             | 0.152    |
| reward_ctrl        | 0.0431   |
| reward_orientation | 0.0609   |
| reward_position    | 0.0014   |
| reward_rotation    | 0.0359   |
| reward_velocity    | 0.0104   |
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 53       |
|    time_elapsed    | 40       |
|    total timesteps | 2151     |
---------------------------------
---------------------------------
| reward             | 0.131    |
| reward_ctrl        | 0.0362   |
| reward_orientation | 0.0607   |
| reward_position    | 0.000933 |
| reward_rotation    | 0.0239   |
| reward_velocity    | 0.0089   |
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 27.9     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 56       |
|    time_elapsed    | 57       |
|    total timesteps | 3265     |
---------------------------------
---------------------------------
| reward             | 0.13     |
| reward_ctrl        | 0.0334   |
| reward_orientation | 0.0608   |
| reward_position    | 0.000737 |
| reward_rotation    | 0.0266   |
| reward_velocity    | 0.00904  |
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 26.8     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 58       |
|    time_elapsed    | 68       |
|    total timesteps | 3999     |
---------------------------------
---------------------------------
| reward             | 0.132    |
| reward_ctrl        | 0.0353   |
| reward_orientation | 0.0573   |
| reward_position    | 0.000608 |
| reward_rotation    | 0.0259   |
| reward_velocity    | 0.0132   |
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 26.9     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 59       |
|    time_elapsed    | 81       |
|    total timesteps | 4854     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 35.12
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.148    |
| reward_ctrl        | 0.0489   |
| reward_orientation | 0.0551   |
| reward_position    | 0.000518 |
| reward_rotation    | 0.0312   |
| reward_velocity    | 0.0125   |
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 40.2     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 33       |
|    time_elapsed    | 192      |
|    total timesteps | 6403     |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 0.117    |
|    learning_rate   | 0.001    |
|    n_updates       | 1400     |
---------------------------------
---------------------------------
| reward             | 0.176    |
| reward_ctrl        | 0.0649   |
| reward_orientation | 0.0554   |
| reward_position    | 0.000451 |
| reward_rotation    | 0.0443   |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 58.4     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 26       |
|    time_elapsed    | 319      |
|    total timesteps | 8403     |
| train/             |          |
|    actor_loss      | -4.39    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 3400     |
---------------------------------
---------------------------------
| reward             | 0.193    |
| reward_ctrl        | 0.0727   |
| reward_orientation | 0.0541   |
| reward_position    | 0.0004   |
| reward_rotation    | 0.0562   |
| reward_velocity    | 0.00999  |
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 72.2     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 23       |
|    time_elapsed    | 446      |
|    total timesteps | 10403    |
| train/             |          |
|    actor_loss      | -6.67    |
|    critic_loss     | 0.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 5400     |
---------------------------------
Num timesteps: 12000
Best mean reward: 35.12 - Last mean reward per episode: 83.89
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.22     |
| reward_ctrl        | 0.083    |
| reward_orientation | 0.0558   |
| reward_position    | 0.000359 |
| reward_rotation    | 0.0706   |
| reward_velocity    | 0.00996  |
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 86.2     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 21       |
|    time_elapsed    | 571      |
|    total timesteps | 12403    |
| train/             |          |
|    actor_loss      | -9.37    |
|    critic_loss     | 0.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 7400     |
---------------------------------
---------------------------------
| reward             | 0.234    |
| reward_ctrl        | 0.0914   |
| reward_orientation | 0.0584   |
| reward_position    | 0.000325 |
| reward_rotation    | 0.0752   |
| reward_velocity    | 0.00911  |
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 96.4     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 20       |
|    time_elapsed    | 707      |
|    total timesteps | 14403    |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.229    |
|    learning_rate   | 0.001    |
|    n_updates       | 9400     |
---------------------------------
---------------------------------
| reward             | 0.248    |
| reward_ctrl        | 0.0983   |
| reward_orientation | 0.0594   |
| reward_position    | 0.000298 |
| reward_rotation    | 0.0799   |
| reward_velocity    | 0.00983  |
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 19       |
|    time_elapsed    | 856      |
|    total timesteps | 16403    |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.211    |
|    learning_rate   | 0.001    |
|    n_updates       | 11400    |
---------------------------------
Num timesteps: 18000
Best mean reward: 83.89 - Last mean reward per episode: 114.42
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.268    |
| reward_ctrl        | 0.104    |
| reward_orientation | 0.0613   |
| reward_position    | 0.000274 |
| reward_rotation    | 0.0914   |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 18       |
|    time_elapsed    | 993      |
|    total timesteps | 18403    |
| train/             |          |
|    actor_loss      | -17.8    |
|    critic_loss     | 0.202    |
|    learning_rate   | 0.001    |
|    n_updates       | 13400    |
---------------------------------
---------------------------------
| reward             | 0.278    |
| reward_ctrl        | 0.109    |
| reward_orientation | 0.063    |
| reward_position    | 0.000254 |
| reward_rotation    | 0.0955   |
| reward_velocity    | 0.00979  |
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 18       |
|    time_elapsed    | 1129     |
|    total timesteps | 20403    |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.255    |
|    learning_rate   | 0.001    |
|    n_updates       | 15400    |
---------------------------------
---------------------------------
| reward             | 0.289    |
| reward_ctrl        | 0.114    |
| reward_orientation | 0.0644   |
| reward_position    | 0.000237 |
| reward_rotation    | 0.102    |
| reward_velocity    | 0.00969  |
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 17       |
|    time_elapsed    | 1266     |
|    total timesteps | 22403    |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.276    |
|    learning_rate   | 0.001    |
|    n_updates       | 17400    |
---------------------------------
Num timesteps: 24000
Best mean reward: 114.42 - Last mean reward per episode: 133.38
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.299    |
| reward_ctrl        | 0.117    |
| reward_orientation | 0.0656   |
| reward_position    | 0.000222 |
| reward_rotation    | 0.104    |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 381      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 17       |
|    time_elapsed    | 1413     |
|    total timesteps | 24403    |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.239    |
|    learning_rate   | 0.001    |
|    n_updates       | 19400    |
---------------------------------
---------------------------------
| reward             | 0.307    |
| reward_ctrl        | 0.121    |
| reward_orientation | 0.0667   |
| reward_position    | 0.000209 |
| reward_rotation    | 0.108    |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 17       |
|    time_elapsed    | 1534     |
|    total timesteps | 26403    |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 21400    |
---------------------------------
---------------------------------
| reward             | 0.313    |
| reward_ctrl        | 0.124    |
| reward_orientation | 0.0676   |
| reward_position    | 0.000197 |
| reward_rotation    | 0.11     |
| reward_velocity    | 0.0113   |
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 17       |
|    time_elapsed    | 1656     |
|    total timesteps | 28403    |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 0.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 23400    |
---------------------------------
Num timesteps: 30000
Best mean reward: 133.38 - Last mean reward per episode: 145.67
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.318    |
| reward_ctrl        | 0.126    |
| reward_orientation | 0.0678   |
| reward_position    | 0.000187 |
| reward_rotation    | 0.113    |
| reward_velocity    | 0.0107   |
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 17       |
|    time_elapsed    | 1783     |
|    total timesteps | 30403    |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 0.211    |
|    learning_rate   | 0.001    |
|    n_updates       | 25400    |
---------------------------------
---------------------------------
| reward             | 0.322    |
| reward_ctrl        | 0.129    |
| reward_orientation | 0.0685   |
| reward_position    | 0.000177 |
| reward_rotation    | 0.113    |
| reward_velocity    | 0.0119   |
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 1909     |
|    total timesteps | 32403    |
| train/             |          |
|    actor_loss      | -36.7    |
|    critic_loss     | 0.298    |
|    learning_rate   | 0.001    |
|    n_updates       | 27400    |
---------------------------------
---------------------------------
| reward             | 0.326    |
| reward_ctrl        | 0.131    |
| reward_orientation | 0.0693   |
| reward_position    | 0.000169 |
| reward_rotation    | 0.114    |
| reward_velocity    | 0.0122   |
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 2037     |
|    total timesteps | 34403    |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 0.289    |
|    learning_rate   | 0.001    |
|    n_updates       | 29400    |
---------------------------------
Num timesteps: 36000
Best mean reward: 145.67 - Last mean reward per episode: 152.43
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.328    |
| reward_ctrl        | 0.133    |
| reward_orientation | 0.0699   |
| reward_position    | 0.000161 |
| reward_rotation    | 0.114    |
| reward_velocity    | 0.0121   |
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 2164     |
|    total timesteps | 36403    |
| train/             |          |
|    actor_loss      | -41.3    |
|    critic_loss     | 0.351    |
|    learning_rate   | 0.001    |
|    n_updates       | 31400    |
---------------------------------
---------------------------------
| reward             | 0.333    |
| reward_ctrl        | 0.135    |
| reward_orientation | 0.0703   |
| reward_position    | 0.000154 |
| reward_rotation    | 0.116    |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 2290     |
|    total timesteps | 38403    |
| train/             |          |
|    actor_loss      | -43.4    |
|    critic_loss     | 0.3      |
|    learning_rate   | 0.001    |
|    n_updates       | 33400    |
---------------------------------
---------------------------------
| reward             | 0.34     |
| reward_ctrl        | 0.136    |
| reward_orientation | 0.0708   |
| reward_position    | 0.000147 |
| reward_rotation    | 0.119    |
| reward_velocity    | 0.0139   |
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 2416     |
|    total timesteps | 40403    |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 0.379    |
|    learning_rate   | 0.001    |
|    n_updates       | 35400    |
---------------------------------
Num timesteps: 42000
Best mean reward: 152.43 - Last mean reward per episode: 160.67
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.342    |
| reward_ctrl        | 0.138    |
| reward_orientation | 0.0713   |
| reward_position    | 0.000141 |
| reward_rotation    | 0.12     |
| reward_velocity    | 0.0134   |
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 2544     |
|    total timesteps | 42403    |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 0.247    |
|    learning_rate   | 0.001    |
|    n_updates       | 37400    |
---------------------------------
---------------------------------
| reward             | 0.349    |
| reward_ctrl        | 0.142    |
| reward_orientation | 0.072    |
| reward_position    | 1.64e-06 |
| reward_rotation    | 0.122    |
| reward_velocity    | 0.0138   |
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 2672     |
|    total timesteps | 44403    |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 0.425    |
|    learning_rate   | 0.001    |
|    n_updates       | 39400    |
---------------------------------
2021-05-31 19:15:14.344367: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin:/home/shandilya/.mujoco/mjpro150/bin
2021-05-31 19:15:14.344474: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_12
---------------------------------
| reward             | 0.899    |
| reward_ctrl        | 0.0157   |
| reward_motion      | 0.75     |
| reward_orientation | 0.0568   |
| reward_position    | 0.054    |
| reward_rotation    | 0.00134  |
| reward_velocity    | 0.0211   |
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 63.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 40       |
|    time_elapsed    | 7        |
|    total timesteps | 294      |
---------------------------------
---------------------------------
| reward             | 0.864    |
| reward_ctrl        | 0.0303   |
| reward_motion      | 0.738    |
| reward_orientation | 0.0541   |
| reward_position    | 0.0155   |
| reward_rotation    | 0.0194   |
| reward_velocity    | 0.0061   |
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 96.3     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 50       |
|    time_elapsed    | 20       |
|    total timesteps | 1046     |
---------------------------------
---------------------------------
| reward             | 0.821    |
| reward_ctrl        | 0.0505   |
| reward_motion      | 0.678    |
| reward_orientation | 0.049    |
| reward_position    | 0.0108   |
| reward_rotation    | 0.0288   |
| reward_velocity    | 0.00436  |
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 82.2     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 53       |
|    time_elapsed    | 24       |
|    total timesteps | 1298     |
---------------------------------
2021-05-31 19:16:24.176006: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin:/home/shandilya/.mujoco/mjpro150/bin
2021-05-31 19:16:24.176073: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_13
---------------------------------
| reward             | 0.73     |
| reward_ctrl        | 0.0593   |
| reward_motion      | 0.6      |
| reward_orientation | 0.0654   |
| reward_position    | 8.03e-05 |
| reward_rotation    | 1.64e-05 |
| reward_velocity    | 0.00551  |
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 98.2     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 59       |
|    time_elapsed    | 9        |
|    total timesteps | 545      |
---------------------------------
---------------------------------
| reward             | 0.732    |
| reward_ctrl        | 0.0465   |
| reward_motion      | 0.6      |
| reward_orientation | 0.0691   |
| reward_position    | 0.00742  |
| reward_rotation    | 0.00622  |
| reward_velocity    | 0.00242  |
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 93.5     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 57       |
|    time_elapsed    | 17       |
|    total timesteps | 1031     |
---------------------------------
---------------------------------
| reward             | 0.73     |
| reward_ctrl        | 0.0592   |
| reward_motion      | 0.581    |
| reward_orientation | 0.0702   |
| reward_position    | 0.0137   |
| reward_rotation    | 0.00396  |
| reward_velocity    | 0.002    |
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 99.1     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 57       |
|    time_elapsed    | 28       |
|    total timesteps | 1622     |
---------------------------------
---------------------------------
| reward             | 0.724    |
| reward_ctrl        | 0.0517   |
| reward_motion      | 0.586    |
| reward_orientation | 0.072    |
| reward_position    | 0.01     |
| reward_rotation    | 0.00302  |
| reward_velocity    | 0.00188  |
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 97.4     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 57       |
|    time_elapsed    | 37       |
|    total timesteps | 2118     |
---------------------------------
---------------------------------
| reward             | 0.713    |
| reward_ctrl        | 0.0497   |
| reward_motion      | 0.575    |
| reward_orientation | 0.0676   |
| reward_position    | 0.00837  |
| reward_rotation    | 0.0101   |
| reward_velocity    | 0.00271  |
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 102      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 56       |
|    time_elapsed    | 50       |
|    total timesteps | 2856     |
---------------------------------
---------------------------------
| reward             | 0.686    |
| reward_ctrl        | 0.0429   |
| reward_motion      | 0.553    |
| reward_orientation | 0.0644   |
| reward_position    | 0.00691  |
| reward_rotation    | 0.0159   |
| reward_velocity    | 0.00268  |
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 56       |
|    time_elapsed    | 77       |
|    total timesteps | 4376     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 144.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.679    |
| reward_ctrl        | 0.0504   |
| reward_motion      | 0.527    |
| reward_orientation | 0.0614   |
| reward_position    | 0.00588  |
| reward_rotation    | 0.0307   |
| reward_velocity    | 0.00318  |
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 34       |
|    time_elapsed    | 182      |
|    total timesteps | 6376     |
| train/             |          |
|    actor_loss      | -4.4     |
|    critic_loss     | 0.132    |
|    learning_rate   | 0.001    |
|    n_updates       | 1375     |
---------------------------------
---------------------------------
| reward             | 0.675    |
| reward_ctrl        | 0.0621   |
| reward_motion      | 0.491    |
| reward_orientation | 0.0608   |
| reward_position    | 0.00512  |
| reward_rotation    | 0.0503   |
| reward_velocity    | 0.00509  |
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 28       |
|    time_elapsed    | 283      |
|    total timesteps | 7960     |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 2955     |
---------------------------------
---------------------------------
| reward             | 0.653    |
| reward_ctrl        | 0.0706   |
| reward_motion      | 0.464    |
| reward_orientation | 0.0588   |
| reward_position    | 0.00454  |
| reward_rotation    | 0.0504   |
| reward_velocity    | 0.00516  |
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 24       |
|    time_elapsed    | 407      |
|    total timesteps | 9898     |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.199    |
|    learning_rate   | 0.001    |
|    n_updates       | 4895     |
---------------------------------
---------------------------------
| reward             | 0.657    |
| reward_ctrl        | 0.072    |
| reward_motion      | 0.464    |
| reward_orientation | 0.0585   |
| reward_position    | 0.00407  |
| reward_rotation    | 0.0533   |
| reward_velocity    | 0.00487  |
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 23       |
|    time_elapsed    | 471      |
|    total timesteps | 10868    |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.206    |
|    learning_rate   | 0.001    |
|    n_updates       | 5865     |
---------------------------------
Num timesteps: 12000
Best mean reward: 144.99 - Last mean reward per episode: 171.83
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.652    |
| reward_ctrl        | 0.0798   |
| reward_motion      | 0.443    |
| reward_orientation | 0.0583   |
| reward_position    | 0.00369  |
| reward_rotation    | 0.0601   |
| reward_velocity    | 0.00711  |
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 21       |
|    time_elapsed    | 610      |
|    total timesteps | 12868    |
| train/             |          |
|    actor_loss      | -17      |
|    critic_loss     | 0.273    |
|    learning_rate   | 0.001    |
|    n_updates       | 7865     |
---------------------------------
---------------------------------
| reward             | 0.65     |
| reward_ctrl        | 0.086    |
| reward_motion      | 0.433    |
| reward_orientation | 0.057    |
| reward_position    | 0.00338  |
| reward_rotation    | 0.0641   |
| reward_velocity    | 0.00693  |
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 19       |
|    time_elapsed    | 746      |
|    total timesteps | 14841    |
| train/             |          |
|    actor_loss      | -20.1    |
|    critic_loss     | 0.196    |
|    learning_rate   | 0.001    |
|    n_updates       | 9840     |
---------------------------------
---------------------------------
| reward             | 0.634    |
| reward_ctrl        | 0.0912   |
| reward_motion      | 0.405    |
| reward_orientation | 0.055    |
| reward_position    | 0.00311  |
| reward_rotation    | 0.071    |
| reward_velocity    | 0.00901  |
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 18       |
|    time_elapsed    | 910      |
|    total timesteps | 16841    |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.338    |
|    learning_rate   | 0.001    |
|    n_updates       | 11840    |
---------------------------------
Num timesteps: 18000
Best mean reward: 171.83 - Last mean reward per episode: 191.30
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.632    |
| reward_ctrl        | 0.0942   |
| reward_motion      | 0.4      |
| reward_orientation | 0.0548   |
| reward_position    | 0.00289  |
| reward_rotation    | 0.0696   |
| reward_velocity    | 0.0104   |
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 18       |
|    time_elapsed    | 989      |
|    total timesteps | 18148    |
| train/             |          |
|    actor_loss      | -25.2    |
|    critic_loss     | 0.584    |
|    learning_rate   | 0.001    |
|    n_updates       | 13145    |
---------------------------------
---------------------------------
| reward             | 0.623    |
| reward_ctrl        | 0.0992   |
| reward_motion      | 0.382    |
| reward_orientation | 0.0534   |
| reward_position    | 0.00269  |
| reward_rotation    | 0.0757   |
| reward_velocity    | 0.0101   |
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 18       |
|    time_elapsed    | 1111     |
|    total timesteps | 20148    |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 0.375    |
|    learning_rate   | 0.001    |
|    n_updates       | 15145    |
---------------------------------
---------------------------------
| reward             | 0.619    |
| reward_ctrl        | 0.101    |
| reward_motion      | 0.373    |
| reward_orientation | 0.0528   |
| reward_position    | 0.00252  |
| reward_rotation    | 0.0787   |
| reward_velocity    | 0.0108   |
| rollout/           |          |
|    ep_len_mean     | 338      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 17       |
|    time_elapsed    | 1203     |
|    total timesteps | 21663    |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 2.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 16660    |
---------------------------------
---------------------------------
| reward             | 0.613    |
| reward_ctrl        | 0.102    |
| reward_motion      | 0.368    |
| reward_orientation | 0.0519   |
| reward_position    | 0.00241  |
| reward_rotation    | 0.0783   |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 17       |
|    time_elapsed    | 1308     |
|    total timesteps | 23190    |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 0.469    |
|    learning_rate   | 0.001    |
|    n_updates       | 18185    |
---------------------------------
Num timesteps: 24000
Best mean reward: 191.30 - Last mean reward per episode: 202.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.61     |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.355    |
| reward_orientation | 0.0519   |
| reward_position    | 0.00224  |
| reward_rotation    | 0.0856   |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 350      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 17       |
|    time_elapsed    | 1438     |
|    total timesteps | 25190    |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 0.325    |
|    learning_rate   | 0.001    |
|    n_updates       | 20185    |
---------------------------------
---------------------------------
| reward             | 0.612    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.355    |
| reward_orientation | 0.0515   |
| reward_position    | 0.00212  |
| reward_rotation    | 0.0857   |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 210      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 17       |
|    time_elapsed    | 1566     |
|    total timesteps | 27190    |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 0.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 22185    |
---------------------------------
---------------------------------
| reward             | 0.601    |
| reward_ctrl        | 0.108    |
| reward_motion      | 0.345    |
| reward_orientation | 0.0511   |
| reward_position    | 0.00201  |
| reward_rotation    | 0.084    |
| reward_velocity    | 0.0106   |
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 17       |
|    time_elapsed    | 1678     |
|    total timesteps | 28937    |
| train/             |          |
|    actor_loss      | -38.9    |
|    critic_loss     | 0.569    |
|    learning_rate   | 0.001    |
|    n_updates       | 23935    |
---------------------------------
---------------------------------
| reward             | 0.602    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.347    |
| reward_orientation | 0.0513   |
| reward_position    | 0.00259  |
| reward_rotation    | 0.0826   |
| reward_velocity    | 0.0103   |
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 17       |
|    time_elapsed    | 1745     |
|    total timesteps | 29975    |
| train/             |          |
|    actor_loss      | -40.1    |
|    critic_loss     | 0.495    |
|    learning_rate   | 0.001    |
|    n_updates       | 24970    |
---------------------------------
Num timesteps: 30000
Best mean reward: 202.18 - Last mean reward per episode: 208.59
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.604    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.35     |
| reward_orientation | 0.0514   |
| reward_position    | 0.00245  |
| reward_rotation    | 0.0815   |
| reward_velocity    | 0.0113   |
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 17       |
|    time_elapsed    | 1875     |
|    total timesteps | 31975    |
| train/             |          |
|    actor_loss      | -42.2    |
|    critic_loss     | 0.588    |
|    learning_rate   | 0.001    |
|    n_updates       | 26970    |
---------------------------------
---------------------------------
| reward             | 0.605    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.354    |
| reward_orientation | 0.0518   |
| reward_position    | 0.00234  |
| reward_rotation    | 0.0797   |
| reward_velocity    | 0.0113   |
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 1973     |
|    total timesteps | 33409    |
| train/             |          |
|    actor_loss      | -42.8    |
|    critic_loss     | 0.579    |
|    learning_rate   | 0.001    |
|    n_updates       | 28405    |
---------------------------------
---------------------------------
| reward             | 0.601    |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.349    |
| reward_orientation | 0.0518   |
| reward_position    | 0.00224  |
| reward_rotation    | 0.0815   |
| reward_velocity    | 0.0109   |
| rollout/           |          |
|    ep_len_mean     | 369      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 2110     |
|    total timesteps | 35409    |
| train/             |          |
|    actor_loss      | -44.7    |
|    critic_loss     | 0.809    |
|    learning_rate   | 0.001    |
|    n_updates       | 30405    |
---------------------------------
Num timesteps: 36000
Best mean reward: 208.59 - Last mean reward per episode: 216.58
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.6      |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.349    |
| reward_orientation | 0.0526   |
| reward_position    | 0.00215  |
| reward_rotation    | 0.0797   |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 369      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 2211     |
|    total timesteps | 36870    |
| train/             |          |
|    actor_loss      | -46.7    |
|    critic_loss     | 0.692    |
|    learning_rate   | 0.001    |
|    n_updates       | 31865    |
---------------------------------
---------------------------------
| reward             | 0.598    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.345    |
| reward_orientation | 0.0522   |
| reward_position    | 0.00213  |
| reward_rotation    | 0.0811   |
| reward_velocity    | 0.0103   |
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 222      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 2320     |
|    total timesteps | 38419    |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 0.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 33415    |
---------------------------------
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.335    |
| reward_orientation | 0.0512   |
| reward_position    | 0.00161  |
| reward_rotation    | 0.0837   |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 224      |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 2391     |
|    total timesteps | 39457    |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 34455    |
---------------------------------
---------------------------------
| reward             | 0.577    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.319    |
| reward_orientation | 0.0495   |
| reward_position    | 0.000623 |
| reward_rotation    | 0.0856   |
| reward_velocity    | 0.0109   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 231      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 2530     |
|    total timesteps | 41457    |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 0.975    |
|    learning_rate   | 0.001    |
|    n_updates       | 36455    |
---------------------------------
Num timesteps: 42000
Best mean reward: 216.58 - Last mean reward per episode: 232.85
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.571    |
| reward_ctrl        | 0.115    |
| reward_motion      | 0.302    |
| reward_orientation | 0.0482   |
| reward_position    | 0.000623 |
| reward_rotation    | 0.0926   |
| reward_velocity    | 0.012    |
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 240      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 2667     |
|    total timesteps | 43457    |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 5.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 38455    |
---------------------------------
---------------------------------
| reward             | 0.569    |
| reward_ctrl        | 0.118    |
| reward_motion      | 0.298    |
| reward_orientation | 0.048    |
| reward_position    | 0.000708 |
| reward_rotation    | 0.0914   |
| reward_velocity    | 0.0118   |
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 2771     |
|    total timesteps | 44974    |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 1.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 39970    |
---------------------------------
---------------------------------
| reward             | 0.569    |
| reward_ctrl        | 0.123    |
| reward_motion      | 0.291    |
| reward_orientation | 0.0476   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0941   |
| reward_velocity    | 0.0118   |
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 247      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 2909     |
|    total timesteps | 46974    |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 1.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 41970    |
---------------------------------
Num timesteps: 48000
Best mean reward: 232.85 - Last mean reward per episode: 246.08
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.561    |
| reward_ctrl        | 0.123    |
| reward_motion      | 0.288    |
| reward_orientation | 0.0476   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0908   |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 3047     |
|    total timesteps | 48974    |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 43970    |
---------------------------------
---------------------------------
| reward             | 0.556    |
| reward_ctrl        | 0.122    |
| reward_motion      | 0.29     |
| reward_orientation | 0.0469   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0839   |
| reward_velocity    | 0.0132   |
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 3159     |
|    total timesteps | 50586    |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 0.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 45585    |
---------------------------------
---------------------------------
| reward             | 0.566    |
| reward_ctrl        | 0.122    |
| reward_motion      | 0.295    |
| reward_orientation | 0.0475   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0856   |
| reward_velocity    | 0.0149   |
| rollout/           |          |
|    ep_len_mean     | 422      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 3265     |
|    total timesteps | 52115    |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 0.808    |
|    learning_rate   | 0.001    |
|    n_updates       | 47110    |
---------------------------------
---------------------------------
| reward             | 0.568    |
| reward_ctrl        | 0.121    |
| reward_motion      | 0.301    |
| reward_orientation | 0.0478   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0835   |
| reward_velocity    | 0.0149   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 3304     |
|    total timesteps | 52688    |
| train/             |          |
|    actor_loss      | -55.6    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 47685    |
---------------------------------
Num timesteps: 54000
Best mean reward: 246.08 - Last mean reward per episode: 244.57
---------------------------------
| reward             | 0.567    |
| reward_ctrl        | 0.119    |
| reward_motion      | 0.305    |
| reward_orientation | 0.0477   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0806   |
| reward_velocity    | 0.0137   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 3442     |
|    total timesteps | 54688    |
| train/             |          |
|    actor_loss      | -56.7    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 49685    |
---------------------------------
---------------------------------
| reward             | 0.565    |
| reward_ctrl        | 0.118    |
| reward_motion      | 0.305    |
| reward_orientation | 0.0481   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0788   |
| reward_velocity    | 0.0143   |
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 3555     |
|    total timesteps | 56209    |
| train/             |          |
|    actor_loss      | -58.1    |
|    critic_loss     | 1.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 51205    |
---------------------------------
---------------------------------
| reward             | 0.572    |
| reward_ctrl        | 0.116    |
| reward_motion      | 0.317    |
| reward_orientation | 0.0491   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0763   |
| reward_velocity    | 0.0132   |
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 3667     |
|    total timesteps | 57902    |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 4.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 52900    |
---------------------------------
---------------------------------
| reward             | 0.564    |
| reward_ctrl        | 0.115    |
| reward_motion      | 0.31     |
| reward_orientation | 0.0483   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0779   |
| reward_velocity    | 0.0125   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 3801     |
|    total timesteps | 59902    |
| train/             |          |
|    actor_loss      | -58.7    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 54900    |
---------------------------------
Num timesteps: 60000
Best mean reward: 246.08 - Last mean reward per episode: 243.76
---------------------------------
| reward             | 0.565    |
| reward_ctrl        | 0.113    |
| reward_motion      | 0.314    |
| reward_orientation | 0.0483   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0745   |
| reward_velocity    | 0.0139   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 3935     |
|    total timesteps | 61902    |
| train/             |          |
|    actor_loss      | -59.7    |
|    critic_loss     | 0.871    |
|    learning_rate   | 0.001    |
|    n_updates       | 56900    |
---------------------------------
---------------------------------
| reward             | 0.573    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.328    |
| reward_orientation | 0.0484   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0697   |
| reward_velocity    | 0.0152   |
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 4008     |
|    total timesteps | 63001    |
| train/             |          |
|    actor_loss      | -59.2    |
|    critic_loss     | 0.844    |
|    learning_rate   | 0.001    |
|    n_updates       | 58000    |
---------------------------------
---------------------------------
| reward             | 0.576    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.331    |
| reward_orientation | 0.0487   |
| reward_position    | 0.000627 |
| reward_rotation    | 0.0701   |
| reward_velocity    | 0.0146   |
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 241      |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 4085     |
|    total timesteps | 64158    |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 59155    |
---------------------------------
---------------------------------
| reward             | 0.578    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.342    |
| reward_orientation | 0.0488   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.0638   |
| reward_velocity    | 0.0157   |
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 15       |
|    time_elapsed    | 4164     |
|    total timesteps | 65341    |
| train/             |          |
|    actor_loss      | -60.6    |
|    critic_loss     | 9.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 60340    |
---------------------------------
Num timesteps: 66000
Best mean reward: 246.08 - Last mean reward per episode: 234.70
---------------------------------
| reward             | 0.576    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.341    |
| reward_orientation | 0.0491   |
| reward_position    | 0.000628 |
| reward_rotation    | 0.064    |
| reward_velocity    | 0.0162   |
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 176      |
|    fps             | 15       |
|    time_elapsed    | 4266     |
|    total timesteps | 66856    |
| train/             |          |
|    actor_loss      | -60.5    |
|    critic_loss     | 1.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 61855    |
---------------------------------
---------------------------------
| reward             | 0.584    |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.353    |
| reward_orientation | 0.0496   |
| reward_position    | 0.000702 |
| reward_rotation    | 0.0615   |
| reward_velocity    | 0.016    |
| rollout/           |          |
|    ep_len_mean     | 391      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 4346     |
|    total timesteps | 68041    |
| train/             |          |
|    actor_loss      | -60.6    |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 63040    |
---------------------------------
---------------------------------
| reward             | 0.582    |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.352    |
| reward_orientation | 0.0491   |
| reward_position    | 0.000164 |
| reward_rotation    | 0.0605   |
| reward_velocity    | 0.016    |
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 184      |
|    fps             | 15       |
|    time_elapsed    | 4448     |
|    total timesteps | 69567    |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 1.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 64565    |
---------------------------------
---------------------------------
| reward             | 0.576    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.35     |
| reward_orientation | 0.0486   |
| reward_position    | 0.00021  |
| reward_rotation    | 0.058    |
| reward_velocity    | 0.0154   |
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 188      |
|    fps             | 15       |
|    time_elapsed    | 4582     |
|    total timesteps | 71567    |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 2.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 66565    |
---------------------------------
Num timesteps: 72000
Best mean reward: 246.08 - Last mean reward per episode: 232.82
---------------------------------
| reward             | 0.577    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.349    |
| reward_orientation | 0.0475   |
| reward_position    | 0.00021  |
| reward_rotation    | 0.059    |
| reward_velocity    | 0.015    |
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 237      |
| time/              |          |
|    episodes        | 192      |
|    fps             | 15       |
|    time_elapsed    | 4716     |
|    total timesteps | 73567    |
| train/             |          |
|    actor_loss      | -62.7    |
|    critic_loss     | 1.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 68565    |
---------------------------------
---------------------------------
| reward             | 0.575    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.351    |
| reward_orientation | 0.047    |
| reward_position    | 0.00021  |
| reward_rotation    | 0.0544   |
| reward_velocity    | 0.0149   |
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 237      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 15       |
|    time_elapsed    | 4855     |
|    total timesteps | 75567    |
| train/             |          |
|    actor_loss      | -62.2    |
|    critic_loss     | 1.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 70565    |
---------------------------------
---------------------------------
| reward             | 0.577    |
| reward_ctrl        | 0.108    |
| reward_motion      | 0.349    |
| reward_orientation | 0.0459   |
| reward_position    | 0.000544 |
| reward_rotation    | 0.0561   |
| reward_velocity    | 0.0164   |
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 15       |
|    time_elapsed    | 4929     |
|    total timesteps | 76618    |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 2.42     |
|    learning_rate   | 0.001    |
|    n_updates       | 71615    |
---------------------------------
Num timesteps: 78000
Best mean reward: 246.08 - Last mean reward per episode: 236.27
---------------------------------
| reward             | 0.572    |
| reward_ctrl        | 0.108    |
| reward_motion      | 0.344    |
| reward_orientation | 0.0463   |
| reward_position    | 0.000544 |
| reward_rotation    | 0.056    |
| reward_velocity    | 0.0165   |
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    episodes        | 204      |
|    fps             | 15       |
|    time_elapsed    | 5058     |
|    total timesteps | 78406    |
| train/             |          |
|    actor_loss      | -61.7    |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 73405    |
---------------------------------
---------------------------------
| reward             | 0.586    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.356    |
| reward_orientation | 0.0475   |
| reward_position    | 0.00174  |
| reward_rotation    | 0.0548   |
| reward_velocity    | 0.0184   |
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 240      |
| time/              |          |
|    episodes        | 208      |
|    fps             | 15       |
|    time_elapsed    | 5154     |
|    total timesteps | 79920    |
| train/             |          |
|    actor_loss      | -62.8    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 74915    |
---------------------------------
---------------------------------
| reward             | 0.591    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.361    |
| reward_orientation | 0.0475   |
| reward_position    | 0.00174  |
| reward_rotation    | 0.0566   |
| reward_velocity    | 0.0181   |
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    episodes        | 212      |
|    fps             | 15       |
|    time_elapsed    | 5251     |
|    total timesteps | 81454    |
| train/             |          |
|    actor_loss      | -63.6    |
|    critic_loss     | 8.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 76450    |
---------------------------------
---------------------------------
| reward             | 0.588    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.366    |
| reward_orientation | 0.0475   |
| reward_position    | 0.00174  |
| reward_rotation    | 0.052    |
| reward_velocity    | 0.017    |
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 216      |
|    fps             | 15       |
|    time_elapsed    | 5351     |
|    total timesteps | 82999    |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.001    |
|    n_updates       | 77995    |
---------------------------------
Num timesteps: 84000
Best mean reward: 246.08 - Last mean reward per episode: 235.32
---------------------------------
| reward             | 0.584    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.355    |
| reward_orientation | 0.0473   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0564   |
| reward_velocity    | 0.0182   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 220      |
|    fps             | 15       |
|    time_elapsed    | 5467     |
|    total timesteps | 84806    |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 79805    |
---------------------------------
---------------------------------
| reward             | 0.583    |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.353    |
| reward_orientation | 0.0473   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0551   |
| reward_velocity    | 0.0207   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 15       |
|    time_elapsed    | 5605     |
|    total timesteps | 86806    |
| train/             |          |
|    actor_loss      | -62.9    |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.001    |
|    n_updates       | 81805    |
---------------------------------
---------------------------------
| reward             | 0.581    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.348    |
| reward_orientation | 0.0469   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0568   |
| reward_velocity    | 0.0217   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 228      |
|    fps             | 15       |
|    time_elapsed    | 5741     |
|    total timesteps | 88806    |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 2.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 83805    |
---------------------------------
Num timesteps: 90000
Best mean reward: 246.08 - Last mean reward per episode: 231.71
---------------------------------
| reward             | 0.579    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.35     |
| reward_orientation | 0.0465   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0577   |
| reward_velocity    | 0.0195   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    episodes        | 232      |
|    fps             | 15       |
|    time_elapsed    | 5849     |
|    total timesteps | 90377    |
| train/             |          |
|    actor_loss      | -63.4    |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 85375    |
---------------------------------
---------------------------------
| reward             | 0.573    |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.34     |
| reward_orientation | 0.0462   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0617   |
| reward_velocity    | 0.018    |
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | 231      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 15       |
|    time_elapsed    | 5961     |
|    total timesteps | 92006    |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 87005    |
---------------------------------
---------------------------------
| reward             | 0.574    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.34     |
| reward_orientation | 0.0458   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0618   |
| reward_velocity    | 0.0183   |
| rollout/           |          |
|    ep_len_mean     | 409      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 15       |
|    time_elapsed    | 6069     |
|    total timesteps | 93545    |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 88540    |
---------------------------------
---------------------------------
| reward             | 0.572    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.34     |
| reward_orientation | 0.0453   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0621   |
| reward_velocity    | 0.0186   |
| rollout/           |          |
|    ep_len_mean     | 404      |
|    ep_rew_mean     | 232      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 15       |
|    time_elapsed    | 6174     |
|    total timesteps | 95085    |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 90080    |
---------------------------------
Num timesteps: 96000
Best mean reward: 246.08 - Last mean reward per episode: 226.38
---------------------------------
| reward             | 0.574    |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.345    |
| reward_orientation | 0.045    |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0616   |
| reward_velocity    | 0.0185   |
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 15       |
|    time_elapsed    | 6251     |
|    total timesteps | 96194    |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 91190    |
---------------------------------
---------------------------------
| reward             | 0.576    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.345    |
| reward_orientation | 0.0444   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0609   |
| reward_velocity    | 0.02     |
| rollout/           |          |
|    ep_len_mean     | 403      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 15       |
|    time_elapsed    | 6388     |
|    total timesteps | 98194    |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 93190    |
---------------------------------
---------------------------------
| reward             | 0.583    |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.353    |
| reward_orientation | 0.045    |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0586   |
| reward_velocity    | 0.021    |
| rollout/           |          |
|    ep_len_mean     | 403      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 256      |
|    fps             | 15       |
|    time_elapsed    | 6528     |
|    total timesteps | 100194   |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 95190    |
---------------------------------
Num timesteps: 102000
Best mean reward: 246.08 - Last mean reward per episode: 232.33
---------------------------------
| reward             | 0.578    |
| reward_ctrl        | 0.1      |
| reward_motion      | 0.354    |
| reward_orientation | 0.0454   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0576   |
| reward_velocity    | 0.0196   |
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 260      |
|    fps             | 15       |
|    time_elapsed    | 6655     |
|    total timesteps | 102046   |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 1.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 97045    |
---------------------------------
---------------------------------
| reward             | 0.574    |
| reward_ctrl        | 0.101    |
| reward_motion      | 0.348    |
| reward_orientation | 0.0454   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0588   |
| reward_velocity    | 0.0195   |
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 234      |
| time/              |          |
|    episodes        | 264      |
|    fps             | 15       |
|    time_elapsed    | 6761     |
|    total timesteps | 103592   |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 2.82     |
|    learning_rate   | 0.001    |
|    n_updates       | 98590    |
---------------------------------
---------------------------------
| reward             | 0.569    |
| reward_ctrl        | 0.101    |
| reward_motion      | 0.348    |
| reward_orientation | 0.0453   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.056    |
| reward_velocity    | 0.0169   |
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 237      |
| time/              |          |
|    episodes        | 268      |
|    fps             | 15       |
|    time_elapsed    | 6872     |
|    total timesteps | 105208   |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 100205   |
---------------------------------
---------------------------------
| reward             | 0.564    |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.341    |
| reward_orientation | 0.0447   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0571   |
| reward_velocity    | 0.0166   |
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    episodes        | 272      |
|    fps             | 15       |
|    time_elapsed    | 6976     |
|    total timesteps | 106749   |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 101745   |
---------------------------------
Num timesteps: 108000
Best mean reward: 246.08 - Last mean reward per episode: 234.64
---------------------------------
| reward             | 0.556    |
| reward_ctrl        | 0.102    |
| reward_motion      | 0.334    |
| reward_orientation | 0.0444   |
| reward_position    | 0.00165  |
| reward_rotation    | 0.0571   |
| reward_velocity    | 0.0161   |
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 237      |
| time/              |          |
|    episodes        | 276      |
|    fps             | 15       |
|    time_elapsed    | 7081     |
|    total timesteps | 108274   |
| train/             |          |
|    actor_loss      | -63.9    |
|    critic_loss     | 1.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 103270   |
---------------------------------
---------------------------------
| reward             | 0.555    |
| reward_ctrl        | 0.101    |
| reward_motion      | 0.335    |
| reward_orientation | 0.0438   |
| reward_position    | 0.00158  |
| reward_rotation    | 0.0567   |
| reward_velocity    | 0.0163   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 239      |
| time/              |          |
|    episodes        | 280      |
|    fps             | 15       |
|    time_elapsed    | 7186     |
|    total timesteps | 109809   |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 104805   |
---------------------------------
---------------------------------
| reward             | 0.556    |
| reward_ctrl        | 0.0993   |
| reward_motion      | 0.336    |
| reward_orientation | 0.044    |
| reward_position    | 0.00168  |
| reward_rotation    | 0.0584   |
| reward_velocity    | 0.0167   |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 238      |
| time/              |          |
|    episodes        | 284      |
|    fps             | 15       |
|    time_elapsed    | 7291     |
|    total timesteps | 111332   |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 106330   |
---------------------------------
---------------------------------
| reward             | 0.555    |
| reward_ctrl        | 0.0991   |
| reward_motion      | 0.336    |
| reward_orientation | 0.0435   |
| reward_position    | 0.00164  |
| reward_rotation    | 0.059    |
| reward_velocity    | 0.016    |
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    episodes        | 288      |
|    fps             | 15       |
|    time_elapsed    | 7428     |
|    total timesteps | 113332   |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 1.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 108330   |
---------------------------------
Num timesteps: 114000
Best mean reward: 246.08 - Last mean reward per episode: 235.20
---------------------------------
| reward             | 0.55     |
| reward_ctrl        | 0.0969   |
| reward_motion      | 0.335    |
| reward_orientation | 0.0435   |
| reward_position    | 0.00164  |
| reward_rotation    | 0.0566   |
| reward_velocity    | 0.016    |
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 233      |
| time/              |          |
|    episodes        | 292      |
|    fps             | 15       |
|    time_elapsed    | 7541     |
|    total timesteps | 114939   |
| train/             |          |
|    actor_loss      | -65.5    |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 109935   |
---------------------------------
---------------------------------
| reward             | 0.555    |
| reward_ctrl        | 0.0963   |
| reward_motion      | 0.338    |
| reward_orientation | 0.0439   |
| reward_position    | 0.0017   |
| reward_rotation    | 0.0584   |
| reward_velocity    | 0.0174   |
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    episodes        | 296      |
|    fps             | 15       |
|    time_elapsed    | 7619     |
|    total timesteps | 116076   |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 1.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 111075   |
---------------------------------
---------------------------------
| reward             | 0.551    |
| reward_ctrl        | 0.0954   |
| reward_motion      | 0.336    |
| reward_orientation | 0.0439   |
| reward_position    | 0.00136  |
| reward_rotation    | 0.0581   |
| reward_velocity    | 0.0161   |
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 229      |
| time/              |          |
|    episodes        | 300      |
|    fps             | 15       |
|    time_elapsed    | 7723     |
|    total timesteps | 117585   |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 1.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 112580   |
---------------------------------
---------------------------------
| reward             | 0.55     |
| reward_ctrl        | 0.0948   |
| reward_motion      | 0.334    |
| reward_orientation | 0.0431   |
| reward_position    | 0.00136  |
| reward_rotation    | 0.0589   |
| reward_velocity    | 0.018    |
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 227      |
| time/              |          |
|    episodes        | 304      |
|    fps             | 15       |
|    time_elapsed    | 7827     |
|    total timesteps | 119107   |
| train/             |          |
|    actor_loss      | -66.2    |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 114105   |
---------------------------------
Num timesteps: 120000
Best mean reward: 246.08 - Last mean reward per episode: 227.30
---------------------------------
| reward             | 0.54     |
| reward_ctrl        | 0.0922   |
| reward_motion      | 0.329    |
| reward_orientation | 0.0422   |
| reward_position    | 0.000686 |
| reward_rotation    | 0.0596   |
| reward_velocity    | 0.0163   |
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 227      |
| time/              |          |
|    episodes        | 308      |
|    fps             | 15       |
|    time_elapsed    | 7933     |
|    total timesteps | 120633   |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 115630   |
---------------------------------
---------------------------------
| reward             | 0.541    |
| reward_ctrl        | 0.0934   |
| reward_motion      | 0.33     |
| reward_orientation | 0.0421   |
| reward_position    | 0.000708 |
| reward_rotation    | 0.0592   |
| reward_velocity    | 0.0162   |
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    episodes        | 312      |
|    fps             | 15       |
|    time_elapsed    | 8038     |
|    total timesteps | 122162   |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 1.4      |
|    learning_rate   | 0.001    |
|    n_updates       | 117160   |
---------------------------------
---------------------------------
| reward             | 0.55     |
| reward_ctrl        | 0.0948   |
| reward_motion      | 0.337    |
| reward_orientation | 0.0432   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0581   |
| reward_velocity    | 0.0163   |
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 220      |
| time/              |          |
|    episodes        | 316      |
|    fps             | 15       |
|    time_elapsed    | 8081     |
|    total timesteps | 122786   |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 8.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 117785   |
---------------------------------
---------------------------------
| reward             | 0.55     |
| reward_ctrl        | 0.091    |
| reward_motion      | 0.345    |
| reward_orientation | 0.0437   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0536   |
| reward_velocity    | 0.0152   |
| rollout/           |          |
|    ep_len_mean     | 391      |
|    ep_rew_mean     | 216      |
| time/              |          |
|    episodes        | 320      |
|    fps             | 15       |
|    time_elapsed    | 8158     |
|    total timesteps | 123902   |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 7.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 118900   |
---------------------------------
---------------------------------
| reward             | 0.55     |
| reward_ctrl        | 0.0905   |
| reward_motion      | 0.35     |
| reward_orientation | 0.0436   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0518   |
| reward_velocity    | 0.0127   |
| rollout/           |          |
|    ep_len_mean     | 382      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    episodes        | 324      |
|    fps             | 15       |
|    time_elapsed    | 8246     |
|    total timesteps | 124963   |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 119960   |
---------------------------------
Num timesteps: 126000
Best mean reward: 246.08 - Last mean reward per episode: 209.63
---------------------------------
| reward             | 0.555    |
| reward_ctrl        | 0.0876   |
| reward_motion      | 0.36     |
| reward_orientation | 0.0442   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0509   |
| reward_velocity    | 0.0117   |
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    episodes        | 328      |
|    fps             | 15       |
|    time_elapsed    | 8328     |
|    total timesteps | 126147   |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 121145   |
---------------------------------
---------------------------------
| reward             | 0.557    |
| reward_ctrl        | 0.0878   |
| reward_motion      | 0.36     |
| reward_orientation | 0.0442   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0523   |
| reward_velocity    | 0.0118   |
| rollout/           |          |
|    ep_len_mean     | 375      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    episodes        | 332      |
|    fps             | 15       |
|    time_elapsed    | 8448     |
|    total timesteps | 127866   |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 2.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 122865   |
---------------------------------
---------------------------------
| reward             | 0.563    |
| reward_ctrl        | 0.0826   |
| reward_motion      | 0.377    |
| reward_orientation | 0.0444   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.045    |
| reward_velocity    | 0.0127   |
| rollout/           |          |
|    ep_len_mean     | 374      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    episodes        | 336      |
|    fps             | 15       |
|    time_elapsed    | 8561     |
|    total timesteps | 129453   |
| train/             |          |
|    actor_loss      | -64.3    |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.001    |
|    n_updates       | 124450   |
---------------------------------
---------------------------------
| reward             | 0.556    |
| reward_ctrl        | 0.0837   |
| reward_motion      | 0.369    |
| reward_orientation | 0.0435   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0463   |
| reward_velocity    | 0.0127   |
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 214      |
| time/              |          |
|    episodes        | 340      |
|    fps             | 15       |
|    time_elapsed    | 8699     |
|    total timesteps | 131453   |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.001    |
|    n_updates       | 126450   |
---------------------------------
Num timesteps: 132000
Best mean reward: 246.08 - Last mean reward per episode: 214.00
---------------------------------
| reward             | 0.553    |
| reward_ctrl        | 0.083    |
| reward_motion      | 0.367    |
| reward_orientation | 0.0436   |
| reward_position    | 0.000893 |
| reward_rotation    | 0.0456   |
| reward_velocity    | 0.0125   |
| rollout/           |          |
|    ep_len_mean     | 375      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    episodes        | 344      |
|    fps             | 15       |
|    time_elapsed    | 8774     |
|    total timesteps | 132548   |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 127545   |
---------------------------------
---------------------------------
| reward             | 0.553    |
| reward_ctrl        | 0.0808   |
| reward_motion      | 0.372    |
| reward_orientation | 0.0437   |
| reward_position    | 0.000892 |
| reward_rotation    | 0.0434   |
| reward_velocity    | 0.0121   |
| rollout/           |          |
|    ep_len_mean     | 375      |
|    ep_rew_mean     | 211      |
| time/              |          |
|    episodes        | 348      |
|    fps             | 15       |
|    time_elapsed    | 8852     |
|    total timesteps | 133673   |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 2.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 128670   |
---------------------------------
---------------------------------
| reward             | 0.545    |
| reward_ctrl        | 0.0777   |
| reward_motion      | 0.371    |
| reward_orientation | 0.0432   |
| reward_position    | 0.000892 |
| reward_rotation    | 0.0418   |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    episodes        | 352      |
|    fps             | 15       |
|    time_elapsed    | 8962     |
|    total timesteps | 135245   |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.001    |
|    n_updates       | 130240   |
---------------------------------
---------------------------------
| reward             | 0.544    |
| reward_ctrl        | 0.0768   |
| reward_motion      | 0.371    |
| reward_orientation | 0.0435   |
| reward_position    | 0.000892 |
| reward_rotation    | 0.042    |
| reward_velocity    | 0.00974  |
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | 200      |
| time/              |          |
|    episodes        | 356      |
|    fps             | 15       |
|    time_elapsed    | 9006     |
|    total timesteps | 135896   |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 3.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 130895   |
---------------------------------
---------------------------------
| reward             | 0.548    |
| reward_ctrl        | 0.0755   |
| reward_motion      | 0.38     |
| reward_orientation | 0.0436   |
| reward_position    | 0.000892 |
| reward_rotation    | 0.0391   |
| reward_velocity    | 0.00893  |
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    episodes        | 360      |
|    fps             | 15       |
|    time_elapsed    | 9111     |
|    total timesteps | 137421   |
| train/             |          |
|    actor_loss      | -61.8    |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 132420   |
---------------------------------
Num timesteps: 138000
Best mean reward: 246.08 - Last mean reward per episode: 193.22
---------------------------------
| reward             | 0.558    |
| reward_ctrl        | 0.0735   |
| reward_motion      | 0.39     |
| reward_orientation | 0.0446   |
| reward_position    | 0.000985 |
| reward_rotation    | 0.0398   |
| reward_velocity    | 0.00892  |
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    episodes        | 364      |
|    fps             | 15       |
|    time_elapsed    | 9155     |
|    total timesteps | 138051   |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 133050   |
---------------------------------
---------------------------------
| reward             | 0.561    |
| reward_ctrl        | 0.069    |
| reward_motion      | 0.396    |
| reward_orientation | 0.0444   |
| reward_position    | 0.000985 |
| reward_rotation    | 0.0411   |
| reward_velocity    | 0.00876  |
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    episodes        | 368      |
|    fps             | 15       |
|    time_elapsed    | 9233     |
|    total timesteps | 139119   |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 134115   |
---------------------------------
---------------------------------
| reward             | 0.568    |
| reward_ctrl        | 0.0672   |
| reward_motion      | 0.407    |
| reward_orientation | 0.0454   |
| reward_position    | 0.000986 |
| reward_rotation    | 0.0385   |
| reward_velocity    | 0.00865  |
| rollout/           |          |
|    ep_len_mean     | 326      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    episodes        | 372      |
|    fps             | 15       |
|    time_elapsed    | 9245     |
|    total timesteps | 139306   |
| train/             |          |
|    actor_loss      | -60.6    |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.001    |
|    n_updates       | 134305   |
---------------------------------
---------------------------------
| reward             | 0.577    |
| reward_ctrl        | 0.0654   |
| reward_motion      | 0.417    |
| reward_orientation | 0.0462   |
| reward_position    | 0.000987 |
| reward_rotation    | 0.0383   |
| reward_velocity    | 0.00855  |
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    episodes        | 376      |
|    fps             | 15       |
|    time_elapsed    | 9309     |
|    total timesteps | 140354   |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 135350   |
---------------------------------
---------------------------------
| reward             | 0.584    |
| reward_ctrl        | 0.0665   |
| reward_motion      | 0.424    |
| reward_orientation | 0.0461   |
| reward_position    | 0.000987 |
| reward_rotation    | 0.0371   |
| reward_velocity    | 0.00937  |
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    episodes        | 380      |
|    fps             | 15       |
|    time_elapsed    | 9408     |
|    total timesteps | 141968   |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 136965   |
---------------------------------
---------------------------------
| reward             | 0.586    |
| reward_ctrl        | 0.0655   |
| reward_motion      | 0.428    |
| reward_orientation | 0.0466   |
| reward_position    | 0.000987 |
| reward_rotation    | 0.0356   |
| reward_velocity    | 0.00984  |
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    episodes        | 384      |
|    fps             | 15       |
|    time_elapsed    | 9476     |
|    total timesteps | 143072   |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 138070   |
---------------------------------
Num timesteps: 144000
Best mean reward: 246.08 - Last mean reward per episode: 175.99
---------------------------------
| reward             | 0.588    |
| reward_ctrl        | 0.0629   |
| reward_motion      | 0.432    |
| reward_orientation | 0.0471   |
| reward_position    | 0.000894 |
| reward_rotation    | 0.0355   |
| reward_velocity    | 0.00946  |
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    episodes        | 388      |
|    fps             | 15       |
|    time_elapsed    | 9541     |
|    total timesteps | 144142   |
| train/             |          |
|    actor_loss      | -60.9    |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 139140   |
---------------------------------
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.0621   |
| reward_motion      | 0.436    |
| reward_orientation | 0.0477   |
| reward_position    | 0.000918 |
| reward_rotation    | 0.0361   |
| reward_velocity    | 0.00945  |
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    episodes        | 392      |
|    fps             | 15       |
|    time_elapsed    | 9609     |
|    total timesteps | 145228   |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 140225   |
---------------------------------
---------------------------------
| reward             | 0.599    |
| reward_ctrl        | 0.0605   |
| reward_motion      | 0.446    |
| reward_orientation | 0.0485   |
| reward_position    | 0.00126  |
| reward_rotation    | 0.0344   |
| reward_velocity    | 0.00821  |
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    episodes        | 396      |
|    fps             | 15       |
|    time_elapsed    | 9626     |
|    total timesteps | 145496   |
| train/             |          |
|    actor_loss      | -59.8    |
|    critic_loss     | 2        |
|    learning_rate   | 0.001    |
|    n_updates       | 140495   |
---------------------------------
---------------------------------
| reward             | 0.604    |
| reward_ctrl        | 0.057    |
| reward_motion      | 0.458    |
| reward_orientation | 0.0487   |
| reward_position    | 0.0012   |
| reward_rotation    | 0.0316   |
| reward_velocity    | 0.00778  |
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 400      |
|    fps             | 15       |
|    time_elapsed    | 9667     |
|    total timesteps | 146183   |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 141180   |
---------------------------------
---------------------------------
| reward             | 0.611    |
| reward_ctrl        | 0.0538   |
| reward_motion      | 0.472    |
| reward_orientation | 0.0491   |
| reward_position    | 0.00121  |
| reward_rotation    | 0.0295   |
| reward_velocity    | 0.0058   |
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 404      |
|    fps             | 15       |
|    time_elapsed    | 9681     |
|    total timesteps | 146425   |
| train/             |          |
|    actor_loss      | -60.4    |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 141420   |
---------------------------------
---------------------------------
| reward             | 0.611    |
| reward_ctrl        | 0.0531   |
| reward_motion      | 0.476    |
| reward_orientation | 0.0491   |
| reward_position    | 0.00069  |
| reward_rotation    | 0.0269   |
| reward_velocity    | 0.00594  |
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 408      |
|    fps             | 15       |
|    time_elapsed    | 9776     |
|    total timesteps | 147945   |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 142940   |
---------------------------------
---------------------------------
| reward             | 0.613    |
| reward_ctrl        | 0.0519   |
| reward_motion      | 0.481    |
| reward_orientation | 0.0497   |
| reward_position    | 0.000668 |
| reward_rotation    | 0.0244   |
| reward_velocity    | 0.00597  |
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 412      |
|    fps             | 15       |
|    time_elapsed    | 9827     |
|    total timesteps | 148712   |
| train/             |          |
|    actor_loss      | -58.6    |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 143710   |
---------------------------------
Num timesteps: 150000
Best mean reward: 246.08 - Last mean reward per episode: 159.69
---------------------------------
| reward             | 0.608    |
| reward_ctrl        | 0.0487   |
| reward_motion      | 0.48     |
| reward_orientation | 0.0488   |
| reward_position    | 0.000483 |
| reward_rotation    | 0.0233   |
| reward_velocity    | 0.00659  |
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    episodes        | 416      |
|    fps             | 15       |
|    time_elapsed    | 9928     |
|    total timesteps | 150244   |
| train/             |          |
|    actor_loss      | -59.7    |
|    critic_loss     | 2.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 145240   |
---------------------------------
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0485   |
| reward_motion      | 0.478    |
| reward_orientation | 0.0485   |
| reward_position    | 0.000483 |
| reward_rotation    | 0.0269   |
| reward_velocity    | 0.00656  |
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    episodes        | 420      |
|    fps             | 15       |
|    time_elapsed    | 10003    |
|    total timesteps | 151388   |
| train/             |          |
|    actor_loss      | -58.3    |
|    critic_loss     | 2.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 146385   |
---------------------------------
---------------------------------
| reward             | 0.618    |
| reward_ctrl        | 0.0464   |
| reward_motion      | 0.489    |
| reward_orientation | 0.0495   |
| reward_position    | 0.000483 |
| reward_rotation    | 0.0257   |
| reward_velocity    | 0.00644  |
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    episodes        | 424      |
|    fps             | 15       |
|    time_elapsed    | 10068    |
|    total timesteps | 152324   |
| train/             |          |
|    actor_loss      | -58      |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 147320   |
---------------------------------
---------------------------------
| reward             | 0.617    |
| reward_ctrl        | 0.0478   |
| reward_motion      | 0.487    |
| reward_orientation | 0.0493   |
| reward_position    | 0.000672 |
| reward_rotation    | 0.0258   |
| reward_velocity    | 0.00639  |
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 428      |
|    fps             | 15       |
|    time_elapsed    | 10170    |
|    total timesteps | 153841   |
| train/             |          |
|    actor_loss      | -57.6    |
|    critic_loss     | 2.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 148840   |
---------------------------------
---------------------------------
| reward             | 0.62     |
| reward_ctrl        | 0.046    |
| reward_motion      | 0.491    |
| reward_orientation | 0.0508   |
| reward_position    | 0.000678 |
| reward_rotation    | 0.0248   |
| reward_velocity    | 0.00749  |
| rollout/           |          |
|    ep_len_mean     | 262      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 432      |
|    fps             | 15       |
|    time_elapsed    | 10183    |
|    total timesteps | 154031   |
| train/             |          |
|    actor_loss      | -58.4    |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 149030   |
---------------------------------
---------------------------------
| reward             | 0.624    |
| reward_ctrl        | 0.0481   |
| reward_motion      | 0.492    |
| reward_orientation | 0.0518   |
| reward_position    | 0.00068  |
| reward_rotation    | 0.0243   |
| reward_velocity    | 0.00697  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 436      |
|    fps             | 15       |
|    time_elapsed    | 10227    |
|    total timesteps | 154653   |
| train/             |          |
|    actor_loss      | -57.9    |
|    critic_loss     | 10.1     |
|    learning_rate   | 0.001    |
|    n_updates       | 149650   |
---------------------------------
Num timesteps: 156000
Best mean reward: 246.08 - Last mean reward per episode: 142.66
---------------------------------
| reward             | 0.625    |
| reward_ctrl        | 0.0458   |
| reward_motion      | 0.494    |
| reward_orientation | 0.0522   |
| reward_position    | 0.000681 |
| reward_rotation    | 0.0256   |
| reward_velocity    | 0.00676  |
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 440      |
|    fps             | 15       |
|    time_elapsed    | 10332    |
|    total timesteps | 156227   |
| train/             |          |
|    actor_loss      | -57.4    |
|    critic_loss     | 2.5      |
|    learning_rate   | 0.001    |
|    n_updates       | 151225   |
---------------------------------
---------------------------------
| reward             | 0.627    |
| reward_ctrl        | 0.0469   |
| reward_motion      | 0.494    |
| reward_orientation | 0.0519   |
| reward_position    | 0.000681 |
| reward_rotation    | 0.0263   |
| reward_velocity    | 0.0074   |
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 444      |
|    fps             | 15       |
|    time_elapsed    | 10411    |
|    total timesteps | 157386   |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 152385   |
---------------------------------
---------------------------------
| reward             | 0.624    |
| reward_ctrl        | 0.0464   |
| reward_motion      | 0.491    |
| reward_orientation | 0.0524   |
| reward_position    | 0.00068  |
| reward_rotation    | 0.0255   |
| reward_velocity    | 0.00765  |
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 448      |
|    fps             | 15       |
|    time_elapsed    | 10494    |
|    total timesteps | 158623   |
| train/             |          |
|    actor_loss      | -57.1    |
|    critic_loss     | 2.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 153620   |
---------------------------------
---------------------------------
| reward             | 0.63     |
| reward_ctrl        | 0.047    |
| reward_motion      | 0.497    |
| reward_orientation | 0.0524   |
| reward_position    | 0.00068  |
| reward_rotation    | 0.0243   |
| reward_velocity    | 0.00773  |
| rollout/           |          |
|    ep_len_mean     | 244      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 452      |
|    fps             | 15       |
|    time_elapsed    | 10565    |
|    total timesteps | 159686   |
| train/             |          |
|    actor_loss      | -58.8    |
|    critic_loss     | 2.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 154685   |
---------------------------------
---------------------------------
| reward             | 0.63     |
| reward_ctrl        | 0.0447   |
| reward_motion      | 0.5      |
| reward_orientation | 0.0526   |
| reward_position    | 0.000719 |
| reward_rotation    | 0.024    |
| reward_velocity    | 0.00773  |
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 456      |
|    fps             | 15       |
|    time_elapsed    | 10639    |
|    total timesteps | 160772   |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 155770   |
---------------------------------
Num timesteps: 162000
Best mean reward: 246.08 - Last mean reward per episode: 144.80
---------------------------------
| reward             | 0.633    |
| reward_ctrl        | 0.0438   |
| reward_motion      | 0.504    |
| reward_orientation | 0.0523   |
| reward_position    | 0.000719 |
| reward_rotation    | 0.0249   |
| reward_velocity    | 0.00767  |
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 460      |
|    fps             | 15       |
|    time_elapsed    | 10775    |
|    total timesteps | 162772   |
| train/             |          |
|    actor_loss      | -57.5    |
|    critic_loss     | 1.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 157770   |
---------------------------------
---------------------------------
| reward             | 0.63     |
| reward_ctrl        | 0.0447   |
| reward_motion      | 0.501    |
| reward_orientation | 0.0515   |
| reward_position    | 0.000915 |
| reward_rotation    | 0.0239   |
| reward_velocity    | 0.00776  |
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    episodes        | 464      |
|    fps             | 15       |
|    time_elapsed    | 10822    |
|    total timesteps | 163465   |
| train/             |          |
|    actor_loss      | -55.5    |
|    critic_loss     | 2.5      |
|    learning_rate   | 0.001    |
|    n_updates       | 158460   |
---------------------------------
---------------------------------
| reward             | 0.629    |
| reward_ctrl        | 0.0445   |
| reward_motion      | 0.501    |
| reward_orientation | 0.0521   |
| reward_position    | 0.000915 |
| reward_rotation    | 0.022    |
| reward_velocity    | 0.00771  |
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 468      |
|    fps             | 15       |
|    time_elapsed    | 10889    |
|    total timesteps | 164464   |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 159460   |
---------------------------------
---------------------------------
| reward             | 0.621    |
| reward_ctrl        | 0.0434   |
| reward_motion      | 0.493    |
| reward_orientation | 0.0515   |
| reward_position    | 0.000984 |
| reward_rotation    | 0.0243   |
| reward_velocity    | 0.00793  |
| rollout/           |          |
|    ep_len_mean     | 262      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 472      |
|    fps             | 15       |
|    time_elapsed    | 10954    |
|    total timesteps | 165520   |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.001    |
|    n_updates       | 160515   |
---------------------------------
---------------------------------
| reward             | 0.625    |
| reward_ctrl        | 0.0431   |
| reward_motion      | 0.496    |
| reward_orientation | 0.0514   |
| reward_position    | 0.000982 |
| reward_rotation    | 0.0246   |
| reward_velocity    | 0.00858  |
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 476      |
|    fps             | 15       |
|    time_elapsed    | 11023    |
|    total timesteps | 166641   |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 2.32     |
|    learning_rate   | 0.001    |
|    n_updates       | 161640   |
---------------------------------
---------------------------------
| reward             | 0.623    |
| reward_ctrl        | 0.042    |
| reward_motion      | 0.494    |
| reward_orientation | 0.0525   |
| reward_position    | 0.00135  |
| reward_rotation    | 0.0248   |
| reward_velocity    | 0.00793  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 480      |
|    fps             | 15       |
|    time_elapsed    | 11058    |
|    total timesteps | 167217   |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 4.45     |
|    learning_rate   | 0.001    |
|    n_updates       | 162215   |
---------------------------------
Num timesteps: 168000
Best mean reward: 246.08 - Last mean reward per episode: 150.11
---------------------------------
| reward             | 0.616    |
| reward_ctrl        | 0.0429   |
| reward_motion      | 0.487    |
| reward_orientation | 0.052    |
| reward_position    | 0.00135  |
| reward_rotation    | 0.0245   |
| reward_velocity    | 0.00793  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 484      |
|    fps             | 15       |
|    time_elapsed    | 11123    |
|    total timesteps | 168286   |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 2.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 163285   |
---------------------------------
---------------------------------
| reward             | 0.62     |
| reward_ctrl        | 0.0418   |
| reward_motion      | 0.492    |
| reward_orientation | 0.0524   |
| reward_position    | 0.00169  |
| reward_rotation    | 0.0243   |
| reward_velocity    | 0.00758  |
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 488      |
|    fps             | 15       |
|    time_elapsed    | 11191    |
|    total timesteps | 169395   |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 2.63     |
|    learning_rate   | 0.001    |
|    n_updates       | 164390   |
---------------------------------
---------------------------------
| reward             | 0.619    |
| reward_ctrl        | 0.043    |
| reward_motion      | 0.49     |
| reward_orientation | 0.0523   |
| reward_position    | 0.00166  |
| reward_rotation    | 0.0251   |
| reward_velocity    | 0.00757  |
| rollout/           |          |
|    ep_len_mean     | 257      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    episodes        | 492      |
|    fps             | 15       |
|    time_elapsed    | 11285    |
|    total timesteps | 170921   |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 165920   |
---------------------------------
---------------------------------
| reward             | 0.615    |
| reward_ctrl        | 0.0441   |
| reward_motion      | 0.483    |
| reward_orientation | 0.0517   |
| reward_position    | 0.00144  |
| reward_rotation    | 0.0269   |
| reward_velocity    | 0.00769  |
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 496      |
|    fps             | 15       |
|    time_elapsed    | 11354    |
|    total timesteps | 172036   |
| train/             |          |
|    actor_loss      | -54.7    |
|    critic_loss     | 2.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 167035   |
---------------------------------
---------------------------------
| reward             | 0.61     |
| reward_ctrl        | 0.0429   |
| reward_motion      | 0.477    |
| reward_orientation | 0.0515   |
| reward_position    | 0.00144  |
| reward_rotation    | 0.0293   |
| reward_velocity    | 0.0079   |
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    episodes        | 500      |
|    fps             | 15       |
|    time_elapsed    | 11419    |
|    total timesteps | 173115   |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 168110   |
---------------------------------
Num timesteps: 174000
Best mean reward: 246.08 - Last mean reward per episode: 159.37
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0427   |
| reward_motion      | 0.476    |
| reward_orientation | 0.0515   |
| reward_position    | 0.00144  |
| reward_rotation    | 0.0294   |
| reward_velocity    | 0.008    |
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 504      |
|    fps             | 15       |
|    time_elapsed    | 11498    |
|    total timesteps | 174394   |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 3.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 169390   |
---------------------------------
---------------------------------
| reward             | 0.602    |
| reward_ctrl        | 0.0432   |
| reward_motion      | 0.469    |
| reward_orientation | 0.0514   |
| reward_position    | 0.00156  |
| reward_rotation    | 0.0284   |
| reward_velocity    | 0.00783  |
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 508      |
|    fps             | 15       |
|    time_elapsed    | 11629    |
|    total timesteps | 176394   |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 171390   |
---------------------------------
---------------------------------
| reward             | 0.599    |
| reward_ctrl        | 0.0443   |
| reward_motion      | 0.461    |
| reward_orientation | 0.0511   |
| reward_position    | 0.00194  |
| reward_rotation    | 0.0309   |
| reward_velocity    | 0.00921  |
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 512      |
|    fps             | 15       |
|    time_elapsed    | 11704    |
|    total timesteps | 177512   |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 172510   |
---------------------------------
---------------------------------
| reward             | 0.594    |
| reward_ctrl        | 0.0444   |
| reward_motion      | 0.454    |
| reward_orientation | 0.0511   |
| reward_position    | 0.00194  |
| reward_rotation    | 0.0339   |
| reward_velocity    | 0.00857  |
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 516      |
|    fps             | 15       |
|    time_elapsed    | 11809    |
|    total timesteps | 179065   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 174060   |
---------------------------------
---------------------------------
| reward             | 0.594    |
| reward_ctrl        | 0.043    |
| reward_motion      | 0.459    |
| reward_orientation | 0.0511   |
| reward_position    | 0.00194  |
| reward_rotation    | 0.0303   |
| reward_velocity    | 0.0085   |
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 520      |
|    fps             | 15       |
|    time_elapsed    | 11862    |
|    total timesteps | 179872   |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 2.34     |
|    learning_rate   | 0.001    |
|    n_updates       | 174870   |
---------------------------------
Num timesteps: 180000
Best mean reward: 246.08 - Last mean reward per episode: 160.25
---------------------------------
| reward             | 0.587    |
| reward_ctrl        | 0.0442   |
| reward_motion      | 0.449    |
| reward_orientation | 0.0503   |
| reward_position    | 0.00194  |
| reward_rotation    | 0.0321   |
| reward_velocity    | 0.00915  |
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    episodes        | 524      |
|    fps             | 15       |
|    time_elapsed    | 11963    |
|    total timesteps | 181385   |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 5.4      |
|    learning_rate   | 0.001    |
|    n_updates       | 176380   |
---------------------------------
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.0432   |
| reward_motion      | 0.457    |
| reward_orientation | 0.0504   |
| reward_position    | 0.00218  |
| reward_rotation    | 0.0305   |
| reward_velocity    | 0.00908  |
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    episodes        | 528      |
|    fps             | 15       |
|    time_elapsed    | 12033    |
|    total timesteps | 182412   |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 177410   |
---------------------------------
---------------------------------
| reward             | 0.594    |
| reward_ctrl        | 0.0425   |
| reward_motion      | 0.463    |
| reward_orientation | 0.05     |
| reward_position    | 0.00218  |
| reward_rotation    | 0.029    |
| reward_velocity    | 0.0079   |
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    episodes        | 532      |
|    fps             | 15       |
|    time_elapsed    | 12100    |
|    total timesteps | 183410   |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 178405   |
---------------------------------
---------------------------------
| reward             | 0.583    |
| reward_ctrl        | 0.0424   |
| reward_motion      | 0.453    |
| reward_orientation | 0.0485   |
| reward_position    | 0.00218  |
| reward_rotation    | 0.0303   |
| reward_velocity    | 0.00717  |
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    episodes        | 536      |
|    fps             | 15       |
|    time_elapsed    | 12203    |
|    total timesteps | 184929   |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 2.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 179925   |
---------------------------------
---------------------------------
| reward             | 0.588    |
| reward_ctrl        | 0.0408   |
| reward_motion      | 0.46     |
| reward_orientation | 0.0494   |
| reward_position    | 0.00313  |
| reward_rotation    | 0.0278   |
| reward_velocity    | 0.00735  |
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    episodes        | 540      |
|    fps             | 15       |
|    time_elapsed    | 12242    |
|    total timesteps | 185510   |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 180505   |
---------------------------------
Num timesteps: 186000
Best mean reward: 246.08 - Last mean reward per episode: 167.38
---------------------------------
| reward             | 0.589    |
| reward_ctrl        | 0.0396   |
| reward_motion      | 0.462    |
| reward_orientation | 0.0498   |
| reward_position    | 0.00313  |
| reward_rotation    | 0.0281   |
| reward_velocity    | 0.00669  |
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    episodes        | 544      |
|    fps             | 15       |
|    time_elapsed    | 12345    |
|    total timesteps | 187036   |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 182035   |
---------------------------------
---------------------------------
| reward             | 0.591    |
| reward_ctrl        | 0.041    |
| reward_motion      | 0.463    |
| reward_orientation | 0.0494   |
| reward_position    | 0.00313  |
| reward_rotation    | 0.0279   |
| reward_velocity    | 0.00608  |
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 548      |
|    fps             | 15       |
|    time_elapsed    | 12389    |
|    total timesteps | 187676   |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 3.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 182675   |
---------------------------------
---------------------------------
| reward             | 0.589    |
| reward_ctrl        | 0.0412   |
| reward_motion      | 0.458    |
| reward_orientation | 0.0491   |
| reward_position    | 0.00313  |
| reward_rotation    | 0.0303   |
| reward_velocity    | 0.00668  |
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    episodes        | 552      |
|    fps             | 15       |
|    time_elapsed    | 12429    |
|    total timesteps | 188269   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 183265   |
---------------------------------
---------------------------------
| reward             | 0.589    |
| reward_ctrl        | 0.0423   |
| reward_motion      | 0.457    |
| reward_orientation | 0.0486   |
| reward_position    | 0.0031   |
| reward_rotation    | 0.0308   |
| reward_velocity    | 0.00671  |
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 556      |
|    fps             | 15       |
|    time_elapsed    | 12534    |
|    total timesteps | 189824   |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 2.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 184820   |
---------------------------------
---------------------------------
| reward             | 0.584    |
| reward_ctrl        | 0.0445   |
| reward_motion      | 0.448    |
| reward_orientation | 0.0486   |
| reward_position    | 0.0031   |
| reward_rotation    | 0.033    |
| reward_velocity    | 0.00702  |
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    episodes        | 560      |
|    fps             | 15       |
|    time_elapsed    | 12645    |
|    total timesteps | 191506   |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 2.2      |
|    learning_rate   | 0.001    |
|    n_updates       | 186505   |
---------------------------------
Num timesteps: 192000
Best mean reward: 246.08 - Last mean reward per episode: 163.43
---------------------------------
| reward             | 0.583    |
| reward_ctrl        | 0.0436   |
| reward_motion      | 0.448    |
| reward_orientation | 0.0492   |
| reward_position    | 0.00297  |
| reward_rotation    | 0.0318   |
| reward_velocity    | 0.00698  |
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    episodes        | 564      |
|    fps             | 15       |
|    time_elapsed    | 12747    |
|    total timesteps | 193031   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 188030   |
---------------------------------
---------------------------------
| reward             | 0.583    |
| reward_ctrl        | 0.0445   |
| reward_motion      | 0.448    |
| reward_orientation | 0.0486   |
| reward_position    | 0.00298  |
| reward_rotation    | 0.0318   |
| reward_velocity    | 0.00714  |
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 568      |
|    fps             | 15       |
|    time_elapsed    | 12784    |
|    total timesteps | 193626   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 2.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 188625   |
---------------------------------
---------------------------------
| reward             | 0.586    |
| reward_ctrl        | 0.0459   |
| reward_motion      | 0.452    |
| reward_orientation | 0.0486   |
| reward_position    | 0.00297  |
| reward_rotation    | 0.0301   |
| reward_velocity    | 0.00689  |
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 572      |
|    fps             | 15       |
|    time_elapsed    | 12834    |
|    total timesteps | 194438   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 4.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 189435   |
---------------------------------
---------------------------------
| reward             | 0.586    |
| reward_ctrl        | 0.0458   |
| reward_motion      | 0.452    |
| reward_orientation | 0.0485   |
| reward_position    | 0.00291  |
| reward_rotation    | 0.0301   |
| reward_velocity    | 0.0068   |
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    episodes        | 576      |
|    fps             | 15       |
|    time_elapsed    | 12873    |
|    total timesteps | 195075   |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 2.56     |
|    learning_rate   | 0.001    |
|    n_updates       | 190070   |
---------------------------------
---------------------------------
| reward             | 0.58     |
| reward_ctrl        | 0.0453   |
| reward_motion      | 0.447    |
| reward_orientation | 0.048    |
| reward_position    | 0.00259  |
| reward_rotation    | 0.0294   |
| reward_velocity    | 0.00804  |
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    episodes        | 580      |
|    fps             | 15       |
|    time_elapsed    | 12967    |
|    total timesteps | 196604   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 2.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 191600   |
---------------------------------
Num timesteps: 198000
Best mean reward: 246.08 - Last mean reward per episode: 165.67
---------------------------------
| reward             | 0.579    |
| reward_ctrl        | 0.0444   |
| reward_motion      | 0.447    |
| reward_orientation | 0.0474   |
| reward_position    | 0.00224  |
| reward_rotation    | 0.0305   |
| reward_velocity    | 0.00824  |
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 584      |
|    fps             | 15       |
|    time_elapsed    | 13090    |
|    total timesteps | 198604   |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 3.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 193600   |
---------------------------------
---------------------------------
| reward             | 0.582    |
| reward_ctrl        | 0.0445   |
| reward_motion      | 0.449    |
| reward_orientation | 0.0479   |
| reward_position    | 0.00228  |
| reward_rotation    | 0.0294   |
| reward_velocity    | 0.00813  |
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    episodes        | 588      |
|    fps             | 15       |
|    time_elapsed    | 13132    |
|    total timesteps | 199282   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 2.34     |
|    learning_rate   | 0.001    |
|    n_updates       | 194280   |
---------------------------------
---------------------------------
| reward             | 0.585    |
| reward_ctrl        | 0.0428   |
| reward_motion      | 0.455    |
| reward_orientation | 0.0479   |
| reward_position    | 0.00228  |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.00825  |
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 592      |
|    fps             | 15       |
|    time_elapsed    | 13154    |
|    total timesteps | 199636   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 3.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 194635   |
---------------------------------
---------------------------------
| reward             | 0.58     |
| reward_ctrl        | 0.0419   |
| reward_motion      | 0.454    |
| reward_orientation | 0.048    |
| reward_position    | 0.00216  |
| reward_rotation    | 0.0261   |
| reward_velocity    | 0.00817  |
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 596      |
|    fps             | 15       |
|    time_elapsed    | 13230    |
|    total timesteps | 200864   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 2.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 195860   |
---------------------------------
---------------------------------
| reward             | 0.586    |
| reward_ctrl        | 0.0415   |
| reward_motion      | 0.461    |
| reward_orientation | 0.0485   |
| reward_position    | 0.00266  |
| reward_rotation    | 0.0236   |
| reward_velocity    | 0.00901  |
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 600      |
|    fps             | 15       |
|    time_elapsed    | 13240    |
|    total timesteps | 201034   |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 196030   |
---------------------------------
---------------------------------
| reward             | 0.584    |
| reward_ctrl        | 0.0421   |
| reward_motion      | 0.455    |
| reward_orientation | 0.0484   |
| reward_position    | 0.00266  |
| reward_rotation    | 0.0251   |
| reward_velocity    | 0.0101   |
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    episodes        | 604      |
|    fps             | 15       |
|    time_elapsed    | 13334    |
|    total timesteps | 202563   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 197560   |
---------------------------------
---------------------------------
| reward             | 0.593    |
| reward_ctrl        | 0.0408   |
| reward_motion      | 0.466    |
| reward_orientation | 0.0489   |
| reward_position    | 0.00254  |
| reward_rotation    | 0.0248   |
| reward_velocity    | 0.0102   |
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 608      |
|    fps             | 15       |
|    time_elapsed    | 13376    |
|    total timesteps | 203236   |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 2.47     |
|    learning_rate   | 0.001    |
|    n_updates       | 198235   |
---------------------------------
Num timesteps: 204000
Best mean reward: 246.08 - Last mean reward per episode: 154.35
---------------------------------
| reward             | 0.59     |
| reward_ctrl        | 0.0391   |
| reward_motion      | 0.468    |
| reward_orientation | 0.0488   |
| reward_position    | 0.00216  |
| reward_rotation    | 0.023    |
| reward_velocity    | 0.00878  |
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 612      |
|    fps             | 15       |
|    time_elapsed    | 13473    |
|    total timesteps | 204817   |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 2.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 199815   |
---------------------------------
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.0373   |
| reward_motion      | 0.474    |
| reward_orientation | 0.0488   |
| reward_position    | 0.00216  |
| reward_rotation    | 0.0209   |
| reward_velocity    | 0.00888  |
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 616      |
|    fps             | 15       |
|    time_elapsed    | 13541    |
|    total timesteps | 205842   |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 8.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 200840   |
---------------------------------
---------------------------------
| reward             | 0.597    |
| reward_ctrl        | 0.038    |
| reward_motion      | 0.478    |
| reward_orientation | 0.0492   |
| reward_position    | 0.00223  |
| reward_rotation    | 0.0207   |
| reward_velocity    | 0.0091   |
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 620      |
|    fps             | 15       |
|    time_elapsed    | 13592    |
|    total timesteps | 206584   |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 3.81     |
|    learning_rate   | 0.001    |
|    n_updates       | 201580   |
---------------------------------
---------------------------------
| reward             | 0.603    |
| reward_ctrl        | 0.0353   |
| reward_motion      | 0.485    |
| reward_orientation | 0.0491   |
| reward_position    | 0.00249  |
| reward_rotation    | 0.0213   |
| reward_velocity    | 0.00915  |
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 624      |
|    fps             | 15       |
|    time_elapsed    | 13693    |
|    total timesteps | 208103   |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 2.35     |
|    learning_rate   | 0.001    |
|    n_updates       | 203100   |
---------------------------------
---------------------------------
| reward             | 0.599    |
| reward_ctrl        | 0.0339   |
| reward_motion      | 0.483    |
| reward_orientation | 0.0494   |
| reward_position    | 0.00206  |
| reward_rotation    | 0.0211   |
| reward_velocity    | 0.00919  |
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 628      |
|    fps             | 15       |
|    time_elapsed    | 13768    |
|    total timesteps | 209239   |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 2.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 204235   |
---------------------------------
Num timesteps: 210000
Best mean reward: 246.08 - Last mean reward per episode: 153.25
---------------------------------
| reward             | 0.6      |
| reward_ctrl        | 0.0351   |
| reward_motion      | 0.483    |
| reward_orientation | 0.0487   |
| reward_position    | 0.00206  |
| reward_rotation    | 0.0214   |
| reward_velocity    | 0.00968  |
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 632      |
|    fps             | 15       |
|    time_elapsed    | 13842    |
|    total timesteps | 210313   |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 3.62     |
|    learning_rate   | 0.001    |
|    n_updates       | 205310   |
---------------------------------
---------------------------------
| reward             | 0.608    |
| reward_ctrl        | 0.033    |
| reward_motion      | 0.494    |
| reward_orientation | 0.0502   |
| reward_position    | 0.00215  |
| reward_rotation    | 0.02     |
| reward_velocity    | 0.00931  |
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    episodes        | 636      |
|    fps             | 15       |
|    time_elapsed    | 13891    |
|    total timesteps | 211035   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.001    |
|    n_updates       | 206030   |
---------------------------------
---------------------------------
| reward             | 0.601    |
| reward_ctrl        | 0.0339   |
| reward_motion      | 0.485    |
| reward_orientation | 0.0488   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.0232   |
| reward_velocity    | 0.00947  |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 640      |
|    fps             | 15       |
|    time_elapsed    | 13996    |
|    total timesteps | 212580   |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 2.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 207575   |
---------------------------------
---------------------------------
| reward             | 0.598    |
| reward_ctrl        | 0.0362   |
| reward_motion      | 0.479    |
| reward_orientation | 0.0485   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.0231   |
| reward_velocity    | 0.00963  |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 644      |
|    fps             | 15       |
|    time_elapsed    | 14099    |
|    total timesteps | 214112   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 2.97     |
|    learning_rate   | 0.001    |
|    n_updates       | 209110   |
---------------------------------
---------------------------------
| reward             | 0.602    |
| reward_ctrl        | 0.0345   |
| reward_motion      | 0.484    |
| reward_orientation | 0.0484   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.0234   |
| reward_velocity    | 0.00961  |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    episodes        | 648      |
|    fps             | 15       |
|    time_elapsed    | 14145    |
|    total timesteps | 214787   |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 209785   |
---------------------------------
---------------------------------
| reward             | 0.599    |
| reward_ctrl        | 0.0351   |
| reward_motion      | 0.48     |
| reward_orientation | 0.0488   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.0239   |
| reward_velocity    | 0.00931  |
| rollout/           |          |
|    ep_len_mean     | 276      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 652      |
|    fps             | 15       |
|    time_elapsed    | 14215    |
|    total timesteps | 215836   |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 210835   |
---------------------------------
Num timesteps: 216000
Best mean reward: 246.08 - Last mean reward per episode: 157.12
---------------------------------
| reward             | 0.594    |
| reward_ctrl        | 0.0344   |
| reward_motion      | 0.475    |
| reward_orientation | 0.0486   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.025    |
| reward_velocity    | 0.00944  |
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    episodes        | 656      |
|    fps             | 15       |
|    time_elapsed    | 14351    |
|    total timesteps | 217836   |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 212835   |
---------------------------------
---------------------------------
| reward             | 0.604    |
| reward_ctrl        | 0.0332   |
| reward_motion      | 0.488    |
| reward_orientation | 0.0493   |
| reward_position    | 0.00119  |
| reward_rotation    | 0.0227   |
| reward_velocity    | 0.00917  |
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    episodes        | 660      |
|    fps             | 15       |
|    time_elapsed    | 14425    |
|    total timesteps | 218982   |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 213980   |
---------------------------------
---------------------------------
| reward             | 0.603    |
| reward_ctrl        | 0.0345   |
| reward_motion      | 0.485    |
| reward_orientation | 0.0487   |
| reward_position    | 0.00103  |
| reward_rotation    | 0.0233   |
| reward_velocity    | 0.00985  |
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 664      |
|    fps             | 15       |
|    time_elapsed    | 14466    |
|    total timesteps | 219633   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 2.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 214630   |
---------------------------------
---------------------------------
| reward             | 0.603    |
| reward_ctrl        | 0.0325   |
| reward_motion      | 0.486    |
| reward_orientation | 0.0487   |
| reward_position    | 0.00102  |
| reward_rotation    | 0.0233   |
| reward_velocity    | 0.0113   |
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    episodes        | 668      |
|    fps             | 15       |
|    time_elapsed    | 14503    |
|    total timesteps | 220223   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 2.24     |
|    learning_rate   | 0.001    |
|    n_updates       | 215220   |
---------------------------------
---------------------------------
| reward             | 0.604    |
| reward_ctrl        | 0.0329   |
| reward_motion      | 0.485    |
| reward_orientation | 0.0495   |
| reward_position    | 0.00102  |
| reward_rotation    | 0.025    |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 672      |
|    fps             | 15       |
|    time_elapsed    | 14566    |
|    total timesteps | 221045   |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 2.25     |
|    learning_rate   | 0.001    |
|    n_updates       | 216040   |
---------------------------------
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0323   |
| reward_motion      | 0.489    |
| reward_orientation | 0.0499   |
| reward_position    | 0.00102  |
| reward_rotation    | 0.0247   |
| reward_velocity    | 0.012    |
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    episodes        | 676      |
|    fps             | 15       |
|    time_elapsed    | 14575    |
|    total timesteps | 221176   |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 2.34     |
|    learning_rate   | 0.001    |
|    n_updates       | 216175   |
---------------------------------
---------------------------------
| reward             | 0.612    |
| reward_ctrl        | 0.0331   |
| reward_motion      | 0.491    |
| reward_orientation | 0.0501   |
| reward_position    | 0.00217  |
| reward_rotation    | 0.0253   |
| reward_velocity    | 0.00968  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 680      |
|    fps             | 15       |
|    time_elapsed    | 14617    |
|    total timesteps | 221817   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 4.4      |
|    learning_rate   | 0.001    |
|    n_updates       | 216815   |
---------------------------------
Num timesteps: 222000
Best mean reward: 246.08 - Last mean reward per episode: 143.57
---------------------------------
| reward             | 0.615    |
| reward_ctrl        | 0.0335   |
| reward_motion      | 0.495    |
| reward_orientation | 0.0504   |
| reward_position    | 0.00217  |
| reward_rotation    | 0.0242   |
| reward_velocity    | 0.00944  |
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    episodes        | 684      |
|    fps             | 15       |
|    time_elapsed    | 14717    |
|    total timesteps | 223336   |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 2.45     |
|    learning_rate   | 0.001    |
|    n_updates       | 218335   |
---------------------------------
---------------------------------
| reward             | 0.616    |
| reward_ctrl        | 0.0326   |
| reward_motion      | 0.497    |
| reward_orientation | 0.0506   |
| reward_position    | 0.00213  |
| reward_rotation    | 0.0246   |
| reward_velocity    | 0.00939  |
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    episodes        | 688      |
|    fps             | 15       |
|    time_elapsed    | 14758    |
|    total timesteps | 223938   |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.001    |
|    n_updates       | 218935   |
---------------------------------
---------------------------------
| reward             | 0.614    |
| reward_ctrl        | 0.0345   |
| reward_motion      | 0.491    |
| reward_orientation | 0.0503   |
| reward_position    | 0.00213  |
| reward_rotation    | 0.0269   |
| reward_velocity    | 0.00959  |
| rollout/           |          |
|    ep_len_mean     | 259      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 692      |
|    fps             | 15       |
|    time_elapsed    | 14859    |
|    total timesteps | 225487   |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.001    |
|    n_updates       | 220485   |
---------------------------------
---------------------------------
| reward             | 0.614    |
| reward_ctrl        | 0.0348   |
| reward_motion      | 0.489    |
| reward_orientation | 0.0504   |
| reward_position    | 0.00231  |
| reward_rotation    | 0.0278   |
| reward_velocity    | 0.00971  |
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    episodes        | 696      |
|    fps             | 15       |
|    time_elapsed    | 14966    |
|    total timesteps | 227003   |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 222000   |
---------------------------------
Num timesteps: 228000
Best mean reward: 246.08 - Last mean reward per episode: 150.93
---------------------------------
| reward             | 0.607    |
| reward_ctrl        | 0.0367   |
| reward_motion      | 0.482    |
| reward_orientation | 0.0496   |
| reward_position    | 0.00181  |
| reward_rotation    | 0.0278   |
| reward_velocity    | 0.00864  |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 700      |
|    fps             | 15       |
|    time_elapsed    | 15041    |
|    total timesteps | 228175   |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 223170   |
---------------------------------
---------------------------------
| reward             | 0.605    |
| reward_ctrl        | 0.0362   |
| reward_motion      | 0.484    |
| reward_orientation | 0.0501   |
| reward_position    | 0.00181  |
| reward_rotation    | 0.0264   |
| reward_velocity    | 0.00727  |
| rollout/           |          |
|    ep_len_mean     | 264      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    episodes        | 704      |
|    fps             | 15       |
|    time_elapsed    | 15089    |
|    total timesteps | 228917   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 2.55     |
|    learning_rate   | 0.001    |
|    n_updates       | 223915   |
---------------------------------
---------------------------------
| reward             | 0.603    |
| reward_ctrl        | 0.0368   |
| reward_motion      | 0.481    |
| reward_orientation | 0.0498   |
| reward_position    | 0.00181  |
| reward_rotation    | 0.0265   |
| reward_velocity    | 0.00733  |
| rollout/           |          |
|    ep_len_mean     | 260      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 708      |
|    fps             | 15       |
|    time_elapsed    | 15111    |
|    total timesteps | 229243   |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.001    |
|    n_updates       | 224240   |
---------------------------------
---------------------------------
| reward             | 0.61     |
| reward_ctrl        | 0.0389   |
| reward_motion      | 0.486    |
| reward_orientation | 0.0504   |
| reward_position    | 0.00181  |
| reward_rotation    | 0.0258   |
| reward_velocity    | 0.007    |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 712      |
|    fps             | 15       |
|    time_elapsed    | 15153    |
|    total timesteps | 229895   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 2.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 224890   |
---------------------------------
---------------------------------
| reward             | 0.615    |
| reward_ctrl        | 0.0409   |
| reward_motion      | 0.486    |
| reward_orientation | 0.0509   |
| reward_position    | 0.00181  |
| reward_rotation    | 0.0283   |
| reward_velocity    | 0.0069   |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 716      |
|    fps             | 15       |
|    time_elapsed    | 15222    |
|    total timesteps | 230965   |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 2.35     |
|    learning_rate   | 0.001    |
|    n_updates       | 225960   |
---------------------------------
---------------------------------
| reward             | 0.619    |
| reward_ctrl        | 0.0411   |
| reward_motion      | 0.489    |
| reward_orientation | 0.051    |
| reward_position    | 0.00287  |
| reward_rotation    | 0.0279   |
| reward_velocity    | 0.00743  |
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 720      |
|    fps             | 15       |
|    time_elapsed    | 15260    |
|    total timesteps | 231540   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 226535   |
---------------------------------
---------------------------------
| reward             | 0.617    |
| reward_ctrl        | 0.042    |
| reward_motion      | 0.489    |
| reward_orientation | 0.0506   |
| reward_position    | 0.0026   |
| reward_rotation    | 0.0257   |
| reward_velocity    | 0.00705  |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 724      |
|    fps             | 15       |
|    time_elapsed    | 15369    |
|    total timesteps | 233219   |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 228215   |
---------------------------------
Num timesteps: 234000
Best mean reward: 246.08 - Last mean reward per episode: 141.86
---------------------------------
| reward             | 0.619    |
| reward_ctrl        | 0.0438   |
| reward_motion      | 0.489    |
| reward_orientation | 0.0513   |
| reward_position    | 0.0026   |
| reward_rotation    | 0.0259   |
| reward_velocity    | 0.00702  |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    episodes        | 728      |
|    fps             | 15       |
|    time_elapsed    | 15439    |
|    total timesteps | 234316   |
| train/             |          |
|    actor_loss      | -46.6    |
|    critic_loss     | 2.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 229315   |
---------------------------------
---------------------------------
| reward             | 0.612    |
| reward_ctrl        | 0.0443   |
| reward_motion      | 0.481    |
| reward_orientation | 0.0511   |
| reward_position    | 0.0026   |
| reward_rotation    | 0.0268   |
| reward_velocity    | 0.00671  |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    episodes        | 732      |
|    fps             | 15       |
|    time_elapsed    | 15508    |
|    total timesteps | 235390   |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 2.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 230385   |
---------------------------------
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0439   |
| reward_motion      | 0.471    |
| reward_orientation | 0.0508   |
| reward_position    | 0.0038   |
| reward_rotation    | 0.0305   |
| reward_velocity    | 0.00872  |
| rollout/           |          |
|    ep_len_mean     | 255      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 736      |
|    fps             | 15       |
|    time_elapsed    | 15589    |
|    total timesteps | 236541   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.001    |
|    n_updates       | 231540   |
---------------------------------
---------------------------------
| reward             | 0.606    |
| reward_ctrl        | 0.0446   |
| reward_motion      | 0.471    |
| reward_orientation | 0.0504   |
| reward_position    | 0.0038   |
| reward_rotation    | 0.0281   |
| reward_velocity    | 0.00872  |
| rollout/           |          |
|    ep_len_mean     | 256      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 740      |
|    fps             | 15       |
|    time_elapsed    | 15699    |
|    total timesteps | 238137   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 2.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 233135   |
---------------------------------
---------------------------------
| reward             | 0.606    |
| reward_ctrl        | 0.0439   |
| reward_motion      | 0.47     |
| reward_orientation | 0.0508   |
| reward_position    | 0.0038   |
| reward_rotation    | 0.0286   |
| reward_velocity    | 0.00861  |
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    episodes        | 744      |
|    fps             | 15       |
|    time_elapsed    | 15776    |
|    total timesteps | 239231   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 2.22     |
|    learning_rate   | 0.001    |
|    n_updates       | 234230   |
---------------------------------
Num timesteps: 240000
Best mean reward: 246.08 - Last mean reward per episode: 143.11
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0419   |
| reward_motion      | 0.474    |
| reward_orientation | 0.052    |
| reward_position    | 0.00383  |
| reward_rotation    | 0.0288   |
| reward_velocity    | 0.00831  |
| rollout/           |          |
|    ep_len_mean     | 255      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    episodes        | 748      |
|    fps             | 15       |
|    time_elapsed    | 15850    |
|    total timesteps | 240297   |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 235295   |
---------------------------------
---------------------------------
| reward             | 0.606    |
| reward_ctrl        | 0.0422   |
| reward_motion      | 0.472    |
| reward_orientation | 0.0515   |
| reward_position    | 0.00383  |
| reward_rotation    | 0.028    |
| reward_velocity    | 0.00828  |
| rollout/           |          |
|    ep_len_mean     | 260      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    episodes        | 752      |
|    fps             | 15       |
|    time_elapsed    | 15959    |
|    total timesteps | 241835   |
| train/             |          |
|    actor_loss      | -47.1    |
|    critic_loss     | 5.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 236830   |
---------------------------------
---------------------------------
| reward             | 0.61     |
| reward_ctrl        | 0.0444   |
| reward_motion      | 0.471    |
| reward_orientation | 0.0519   |
| reward_position    | 0.00383  |
| reward_rotation    | 0.0309   |
| reward_velocity    | 0.00869  |
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 756      |
|    fps             | 15       |
|    time_elapsed    | 16040    |
|    total timesteps | 243006   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 2.72     |
|    learning_rate   | 0.001    |
|    n_updates       | 238005   |
---------------------------------
---------------------------------
| reward             | 0.609    |
| reward_ctrl        | 0.0426   |
| reward_motion      | 0.472    |
| reward_orientation | 0.0524   |
| reward_position    | 0.00383  |
| reward_rotation    | 0.0299   |
| reward_velocity    | 0.00863  |
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    episodes        | 760      |
|    fps             | 15       |
|    time_elapsed    | 16091    |
|    total timesteps | 243728   |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 2.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 238725   |
---------------------------------
---------------------------------
| reward             | 0.611    |
| reward_ctrl        | 0.0418   |
| reward_motion      | 0.474    |
| reward_orientation | 0.0531   |
| reward_position    | 0.00389  |
| reward_rotation    | 0.0299   |
| reward_velocity    | 0.00811  |
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    episodes        | 764      |
|    fps             | 15       |
|    time_elapsed    | 16105    |
|    total timesteps | 243916   |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 3.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 238915   |
---------------------------------
---------------------------------
| reward             | 0.605    |
| reward_ctrl        | 0.0441   |
| reward_motion      | 0.467    |
| reward_orientation | 0.0532   |
| reward_position    | 0.00389  |
| reward_rotation    | 0.0299   |
| reward_velocity    | 0.00641  |
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    episodes        | 768      |
|    fps             | 15       |
|    time_elapsed    | 16185    |
|    total timesteps | 245060   |
| train/             |          |
|    actor_loss      | -47.1    |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 240055   |
---------------------------------
Num timesteps: 246000
Best mean reward: 246.08 - Last mean reward per episode: 142.59
---------------------------------
| reward             | 0.603    |
| reward_ctrl        | 0.0442   |
| reward_motion      | 0.465    |
| reward_orientation | 0.0525   |
| reward_position    | 0.00389  |
| reward_rotation    | 0.0301   |
| reward_velocity    | 0.00706  |
| rollout/           |          |
|    ep_len_mean     | 260      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 772      |
|    fps             | 15       |
|    time_elapsed    | 16324    |
|    total timesteps | 247060   |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 242055   |
---------------------------------
---------------------------------
| reward             | 0.595    |
| reward_ctrl        | 0.0446   |
| reward_motion      | 0.458    |
| reward_orientation | 0.0517   |
| reward_position    | 0.00389  |
| reward_rotation    | 0.0305   |
| reward_velocity    | 0.00655  |
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 776      |
|    fps             | 15       |
|    time_elapsed    | 16394    |
|    total timesteps | 248136   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.001    |
|    n_updates       | 243135   |
---------------------------------
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.0441   |
| reward_motion      | 0.456    |
| reward_orientation | 0.0514   |
| reward_position    | 0.00275  |
| reward_rotation    | 0.0305   |
| reward_velocity    | 0.00696  |
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    episodes        | 780      |
|    fps             | 15       |
|    time_elapsed    | 16434    |
|    total timesteps | 248752   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 2.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 243750   |
---------------------------------
---------------------------------
| reward             | 0.591    |
| reward_ctrl        | 0.0433   |
| reward_motion      | 0.455    |
| reward_orientation | 0.0521   |
| reward_position    | 0.00281  |
| reward_rotation    | 0.03     |
| reward_velocity    | 0.00719  |
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    episodes        | 784      |
|    fps             | 15       |
|    time_elapsed    | 16532    |
|    total timesteps | 250273   |
| train/             |          |
|    actor_loss      | -47.5    |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 245270   |
---------------------------------
---------------------------------
| reward             | 0.581    |
| reward_ctrl        | 0.0443   |
| reward_motion      | 0.446    |
| reward_orientation | 0.0509   |
| reward_position    | 0.00281  |
| reward_rotation    | 0.0306   |
| reward_velocity    | 0.00717  |
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 788      |
|    fps             | 15       |
|    time_elapsed    | 16644    |
|    total timesteps | 251802   |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 4.84     |
|    learning_rate   | 0.001    |
|    n_updates       | 246800   |
---------------------------------
Num timesteps: 252000
Best mean reward: 246.08 - Last mean reward per episode: 152.08
---------------------------------
| reward             | 0.582    |
| reward_ctrl        | 0.045    |
| reward_motion      | 0.445    |
| reward_orientation | 0.0514   |
| reward_position    | 0.00301  |
| reward_rotation    | 0.0302   |
| reward_velocity    | 0.00776  |
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    episodes        | 792      |
|    fps             | 15       |
|    time_elapsed    | 16720    |
|    total timesteps | 252957   |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 247955   |
---------------------------------
---------------------------------
| reward             | 0.585    |
| reward_ctrl        | 0.0442   |
| reward_motion      | 0.449    |
| reward_orientation | 0.0512   |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0297   |
| reward_velocity    | 0.00763  |
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 796      |
|    fps             | 15       |
|    time_elapsed    | 16799    |
|    total timesteps | 254167   |
| train/             |          |
|    actor_loss      | -46.2    |
|    critic_loss     | 2.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 249165   |
---------------------------------
---------------------------------
| reward             | 0.589    |
| reward_ctrl        | 0.0446   |
| reward_motion      | 0.45     |
| reward_orientation | 0.0517   |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0314   |
| reward_velocity    | 0.00844  |
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 800      |
|    fps             | 15       |
|    time_elapsed    | 16866    |
|    total timesteps | 255212   |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 2.83     |
|    learning_rate   | 0.001    |
|    n_updates       | 250210   |
---------------------------------
---------------------------------
| reward             | 0.595    |
| reward_ctrl        | 0.0455   |
| reward_motion      | 0.453    |
| reward_orientation | 0.052    |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0321   |
| reward_velocity    | 0.00943  |
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 804      |
|    fps             | 15       |
|    time_elapsed    | 16911    |
|    total timesteps | 255881   |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 250880   |
---------------------------------
---------------------------------
| reward             | 0.599    |
| reward_ctrl        | 0.0456   |
| reward_motion      | 0.457    |
| reward_orientation | 0.0523   |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0321   |
| reward_velocity    | 0.00936  |
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 808      |
|    fps             | 15       |
|    time_elapsed    | 16982    |
|    total timesteps | 256983   |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 2.35     |
|    learning_rate   | 0.001    |
|    n_updates       | 251980   |
---------------------------------
Num timesteps: 258000
Best mean reward: 246.08 - Last mean reward per episode: 161.28
---------------------------------
| reward             | 0.598    |
| reward_ctrl        | 0.0464   |
| reward_motion      | 0.453    |
| reward_orientation | 0.0514   |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0345   |
| reward_velocity    | 0.00945  |
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    episodes        | 812      |
|    fps             | 15       |
|    time_elapsed    | 17082    |
|    total timesteps | 258509   |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 2.47     |
|    learning_rate   | 0.001    |
|    n_updates       | 253505   |
---------------------------------
---------------------------------
| reward             | 0.59     |
| reward_ctrl        | 0.044    |
| reward_motion      | 0.448    |
| reward_orientation | 0.0509   |
| reward_position    | 0.00283  |
| reward_rotation    | 0.0342   |
| reward_velocity    | 0.00962  |
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 816      |
|    fps             | 15       |
|    time_elapsed    | 17223    |
|    total timesteps | 260509   |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 3.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 255505   |
---------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 194, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 00:05:21.598831: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 00:05:21.598882: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_14
---------------------------------
| reward             | 0.709    |
| reward_ctrl        | 0.092    |
| reward_motion      | 0.474    |
| reward_orientation | 0.0598   |
| reward_position    | 0        |
| reward_rotation    | 0.0741   |
| reward_velocity    | 0.00896  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 359      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 72       |
|    time_elapsed    | 27       |
|    total timesteps | 2000     |
---------------------------------
----------------------------------
| reward             | 0.756     |
| reward_ctrl        | 0.0814    |
| reward_motion      | 0.526     |
| reward_orientation | 0.0663    |
| reward_position    | 8.93e-319 |
| reward_rotation    | 0.0754    |
| reward_velocity    | 0.00695   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 368       |
| time/              |           |
|    episodes        | 8         |
|    fps             | 74        |
|    time_elapsed    | 53        |
|    total timesteps | 4000      |
----------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 356.34
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.0892    |
| reward_motion      | 0.486     |
| reward_orientation | 0.072     |
| reward_position    | 5.68e-319 |
| reward_rotation    | 0.0985    |
| reward_velocity    | 0.00682   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 356       |
| time/              |           |
|    episodes        | 12        |
|    fps             | 54        |
|    time_elapsed    | 109       |
|    total timesteps | 6000      |
| train/             |           |
|    actor_loss      | -3.64     |
|    critic_loss     | 0.087     |
|    learning_rate   | 0.001     |
|    n_updates       | 995       |
----------------------------------
----------------------------------
| reward             | 0.726     |
| reward_ctrl        | 0.108     |
| reward_motion      | 0.423     |
| reward_orientation | 0.0742    |
| reward_position    | 4.17e-319 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.00519   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 342       |
| time/              |           |
|    episodes        | 16        |
|    fps             | 33        |
|    time_elapsed    | 238       |
|    total timesteps | 8000      |
| train/             |           |
|    actor_loss      | -8.38     |
|    critic_loss     | 0.0808    |
|    learning_rate   | 0.001     |
|    n_updates       | 2995      |
----------------------------------
----------------------------------
| reward             | 0.659     |
| reward_ctrl        | 0.119     |
| reward_motion      | 0.35      |
| reward_orientation | 0.0736    |
| reward_position    | 3.29e-319 |
| reward_rotation    | 0.112     |
| reward_velocity    | 0.00496   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 333       |
| time/              |           |
|    episodes        | 20        |
|    fps             | 27        |
|    time_elapsed    | 365       |
|    total timesteps | 10000     |
| train/             |           |
|    actor_loss      | -12.6     |
|    critic_loss     | 0.0973    |
|    learning_rate   | 0.001     |
|    n_updates       | 4995      |
----------------------------------
Num timesteps: 12000
Best mean reward: 356.34 - Last mean reward per episode: 334.57
----------------------------------
| reward             | 0.648     |
| reward_ctrl        | 0.122     |
| reward_motion      | 0.344     |
| reward_orientation | 0.075     |
| reward_position    | 2.72e-319 |
| reward_rotation    | 0.103     |
| reward_velocity    | 0.00518   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 335       |
| time/              |           |
|    episodes        | 24        |
|    fps             | 24        |
|    time_elapsed    | 492       |
|    total timesteps | 12000     |
| train/             |           |
|    actor_loss      | -16.4     |
|    critic_loss     | 0.108     |
|    learning_rate   | 0.001     |
|    n_updates       | 6995      |
----------------------------------
----------------------------------
| reward             | 0.63      |
| reward_ctrl        | 0.123     |
| reward_motion      | 0.322     |
| reward_orientation | 0.0756    |
| reward_position    | 2.32e-319 |
| reward_rotation    | 0.104     |
| reward_velocity    | 0.00496   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 328       |
| time/              |           |
|    episodes        | 28        |
|    fps             | 22        |
|    time_elapsed    | 619       |
|    total timesteps | 14000     |
| train/             |           |
|    actor_loss      | -20.2     |
|    critic_loss     | 0.103     |
|    learning_rate   | 0.001     |
|    n_updates       | 8995      |
----------------------------------
----------------------------------
| reward             | 0.638     |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.322     |
| reward_orientation | 0.075     |
| reward_position    | 2.02e-319 |
| reward_rotation    | 0.11      |
| reward_velocity    | 0.00451   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 328       |
| time/              |           |
|    episodes        | 32        |
|    fps             | 21        |
|    time_elapsed    | 750       |
|    total timesteps | 16000     |
| train/             |           |
|    actor_loss      | -23.8     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 10995     |
----------------------------------
Num timesteps: 18000
Best mean reward: 356.34 - Last mean reward per episode: 328.82
----------------------------------
| reward             | 0.643     |
| reward_ctrl        | 0.125     |
| reward_motion      | 0.317     |
| reward_orientation | 0.074     |
| reward_position    | 3.77e-294 |
| reward_rotation    | 0.117     |
| reward_velocity    | 0.0087    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 329       |
| time/              |           |
|    episodes        | 36        |
|    fps             | 20        |
|    time_elapsed    | 888       |
|    total timesteps | 18000     |
| train/             |           |
|    actor_loss      | -27       |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 12995     |
----------------------------------
----------------------------------
| reward             | 0.64      |
| reward_ctrl        | 0.128     |
| reward_motion      | 0.308     |
| reward_orientation | 0.0737    |
| reward_position    | 3.38e-294 |
| reward_rotation    | 0.122     |
| reward_velocity    | 0.00801   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 328       |
| time/              |           |
|    episodes        | 40        |
|    fps             | 19        |
|    time_elapsed    | 1025      |
|    total timesteps | 20000     |
| train/             |           |
|    actor_loss      | -30.4     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 14995     |
----------------------------------
----------------------------------
| reward             | 0.648     |
| reward_ctrl        | 0.125     |
| reward_motion      | 0.32      |
| reward_orientation | 0.0745    |
| reward_position    | 3.06e-294 |
| reward_rotation    | 0.121     |
| reward_velocity    | 0.00747   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 331       |
| time/              |           |
|    episodes        | 44        |
|    fps             | 18        |
|    time_elapsed    | 1162      |
|    total timesteps | 22000     |
| train/             |           |
|    actor_loss      | -33.9     |
|    critic_loss     | 0.147     |
|    learning_rate   | 0.001     |
|    n_updates       | 16995     |
----------------------------------
Num timesteps: 24000
Best mean reward: 356.34 - Last mean reward per episode: 332.33
---------------------------------
| reward             | 0.646    |
| reward_ctrl        | 0.125    |
| reward_motion      | 0.319    |
| reward_orientation | 0.075    |
| reward_position    | 2.8e-294 |
| reward_rotation    | 0.119    |
| reward_velocity    | 0.00869  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 332      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 18       |
|    time_elapsed    | 1300     |
|    total timesteps | 24000    |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 0.127    |
|    learning_rate   | 0.001    |
|    n_updates       | 18995    |
---------------------------------
----------------------------------
| reward             | 0.65      |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.316     |
| reward_orientation | 0.0755    |
| reward_position    | 2.58e-294 |
| reward_rotation    | 0.12      |
| reward_velocity    | 0.0121    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 333       |
| time/              |           |
|    episodes        | 52        |
|    fps             | 18        |
|    time_elapsed    | 1438      |
|    total timesteps | 26000     |
| train/             |           |
|    actor_loss      | -39.3     |
|    critic_loss     | 0.142     |
|    learning_rate   | 0.001     |
|    n_updates       | 20995     |
----------------------------------
---------------------------------
| reward             | 0.65     |
| reward_ctrl        | 0.127    |
| reward_motion      | 0.32     |
| reward_orientation | 0.0758   |
| reward_position    | 2.4e-294 |
| reward_rotation    | 0.116    |
| reward_velocity    | 0.0112   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 332      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 17       |
|    time_elapsed    | 1576     |
|    total timesteps | 28000    |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 0.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 22995    |
---------------------------------
Num timesteps: 30000
Best mean reward: 356.34 - Last mean reward per episode: 332.82
----------------------------------
| reward             | 0.65      |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.319     |
| reward_orientation | 0.0763    |
| reward_position    | 8.25e-289 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.012     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 333       |
| time/              |           |
|    episodes        | 60        |
|    fps             | 17        |
|    time_elapsed    | 1713      |
|    total timesteps | 30000     |
| train/             |           |
|    actor_loss      | -44.8     |
|    critic_loss     | 0.141     |
|    learning_rate   | 0.001     |
|    n_updates       | 24995     |
----------------------------------
----------------------------------
| reward             | 0.66      |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.334     |
| reward_orientation | 0.0766    |
| reward_position    | 7.73e-289 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0113    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 335       |
| time/              |           |
|    episodes        | 64        |
|    fps             | 17        |
|    time_elapsed    | 1847      |
|    total timesteps | 32000     |
| train/             |           |
|    actor_loss      | -47.2     |
|    critic_loss     | 0.176     |
|    learning_rate   | 0.001     |
|    n_updates       | 26995     |
----------------------------------
----------------------------------
| reward             | 0.673     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.339     |
| reward_orientation | 0.0768    |
| reward_position    | 7.48e-230 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.0154    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 68        |
|    fps             | 17        |
|    time_elapsed    | 1974      |
|    total timesteps | 34000     |
| train/             |           |
|    actor_loss      | -49.4     |
|    critic_loss     | 0.144     |
|    learning_rate   | 0.001     |
|    n_updates       | 28995     |
----------------------------------
Num timesteps: 36000
Best mean reward: 356.34 - Last mean reward per episode: 337.13
----------------------------------
| reward             | 0.676     |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.345     |
| reward_orientation | 0.0771    |
| reward_position    | 7.06e-230 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0151    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 72        |
|    fps             | 17        |
|    time_elapsed    | 2102      |
|    total timesteps | 36000     |
| train/             |           |
|    actor_loss      | -51.2     |
|    critic_loss     | 0.138     |
|    learning_rate   | 0.001     |
|    n_updates       | 30995     |
----------------------------------
----------------------------------
| reward             | 0.67      |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.34      |
| reward_orientation | 0.0767    |
| reward_position    | 6.69e-230 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0145    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 76        |
|    fps             | 17        |
|    time_elapsed    | 2230      |
|    total timesteps | 38000     |
| train/             |           |
|    actor_loss      | -53.4     |
|    critic_loss     | 0.154     |
|    learning_rate   | 0.001     |
|    n_updates       | 32995     |
----------------------------------
----------------------------------
| reward             | 0.674     |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.343     |
| reward_orientation | 0.077     |
| reward_position    | 6.35e-230 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.0139    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 80        |
|    fps             | 16        |
|    time_elapsed    | 2358      |
|    total timesteps | 40000     |
| train/             |           |
|    actor_loss      | -55.3     |
|    critic_loss     | 0.161     |
|    learning_rate   | 0.001     |
|    n_updates       | 34995     |
----------------------------------
Num timesteps: 42000
Best mean reward: 356.34 - Last mean reward per episode: 336.41
----------------------------------
| reward             | 0.672     |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.34      |
| reward_orientation | 0.077     |
| reward_position    | 6.04e-230 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.0142    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 336       |
| time/              |           |
|    episodes        | 84        |
|    fps             | 16        |
|    time_elapsed    | 2486      |
|    total timesteps | 42000     |
| train/             |           |
|    actor_loss      | -56.3     |
|    critic_loss     | 0.192     |
|    learning_rate   | 0.001     |
|    n_updates       | 36995     |
----------------------------------
----------------------------------
| reward             | 0.668     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.334     |
| reward_orientation | 0.0771    |
| reward_position    | 5.76e-230 |
| reward_rotation    | 0.117     |
| reward_velocity    | 0.0136    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 88        |
|    fps             | 16        |
|    time_elapsed    | 2619      |
|    total timesteps | 44000     |
| train/             |           |
|    actor_loss      | -57.5     |
|    critic_loss     | 0.198     |
|    learning_rate   | 0.001     |
|    n_updates       | 38995     |
----------------------------------
----------------------------------
| reward             | 0.664     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.332     |
| reward_orientation | 0.0769    |
| reward_position    | 5.51e-230 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 336       |
| time/              |           |
|    episodes        | 92        |
|    fps             | 16        |
|    time_elapsed    | 2755      |
|    total timesteps | 46000     |
| train/             |           |
|    actor_loss      | -59.3     |
|    critic_loss     | 0.181     |
|    learning_rate   | 0.001     |
|    n_updates       | 40995     |
----------------------------------
Num timesteps: 48000
Best mean reward: 356.34 - Last mean reward per episode: 336.48
----------------------------------
| reward             | 0.669     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.342     |
| reward_orientation | 0.077     |
| reward_position    | 5.28e-230 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.0127    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 336       |
| time/              |           |
|    episodes        | 96        |
|    fps             | 16        |
|    time_elapsed    | 2893      |
|    total timesteps | 48000     |
| train/             |           |
|    actor_loss      | -61.2     |
|    critic_loss     | 0.163     |
|    learning_rate   | 0.001     |
|    n_updates       | 42995     |
----------------------------------
----------------------------------
| reward             | 0.666     |
| reward_ctrl        | 0.126     |
| reward_motion      | 0.339     |
| reward_orientation | 0.077     |
| reward_position    | 5.07e-230 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0122    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 100       |
|    fps             | 16        |
|    time_elapsed    | 3031      |
|    total timesteps | 50000     |
| train/             |           |
|    actor_loss      | -62.1     |
|    critic_loss     | 0.155     |
|    learning_rate   | 0.001     |
|    n_updates       | 44995     |
----------------------------------
----------------------------------
| reward             | 0.664     |
| reward_ctrl        | 0.125     |
| reward_motion      | 0.337     |
| reward_orientation | 0.0776    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0119    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 104       |
|    fps             | 16        |
|    time_elapsed    | 3169      |
|    total timesteps | 52000     |
| train/             |           |
|    actor_loss      | -63.9     |
|    critic_loss     | 0.167     |
|    learning_rate   | 0.001     |
|    n_updates       | 46995     |
----------------------------------
Num timesteps: 54000
Best mean reward: 356.34 - Last mean reward per episode: 335.92
----------------------------------
| reward             | 0.659     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.326     |
| reward_orientation | 0.078     |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 336       |
| time/              |           |
|    episodes        | 108       |
|    fps             | 16        |
|    time_elapsed    | 3305      |
|    total timesteps | 54000     |
| train/             |           |
|    actor_loss      | -65.1     |
|    critic_loss     | 0.169     |
|    learning_rate   | 0.001     |
|    n_updates       | 48995     |
----------------------------------
----------------------------------
| reward             | 0.662     |
| reward_ctrl        | 0.129     |
| reward_motion      | 0.332     |
| reward_orientation | 0.0776    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 337       |
| time/              |           |
|    episodes        | 112       |
|    fps             | 16        |
|    time_elapsed    | 3433      |
|    total timesteps | 56000     |
| train/             |           |
|    actor_loss      | -66.6     |
|    critic_loss     | 0.155     |
|    learning_rate   | 0.001     |
|    n_updates       | 50995     |
----------------------------------
----------------------------------
| reward             | 0.667     |
| reward_ctrl        | 0.127     |
| reward_motion      | 0.339     |
| reward_orientation | 0.0776    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.11      |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 340       |
| time/              |           |
|    episodes        | 116       |
|    fps             | 2         |
|    time_elapsed    | 26091     |
|    total timesteps | 58000     |
| train/             |           |
|    actor_loss      | -67.6     |
|    critic_loss     | 0.155     |
|    learning_rate   | 0.001     |
|    n_updates       | 52995     |
----------------------------------
Num timesteps: 60000
Best mean reward: 356.34 - Last mean reward per episode: 342.13
----------------------------------
| reward             | 0.678     |
| reward_ctrl        | 0.123     |
| reward_motion      | 0.355     |
| reward_orientation | 0.077     |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.109     |
| reward_velocity    | 0.0143    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 342       |
| time/              |           |
|    episodes        | 120       |
|    fps             | 2         |
|    time_elapsed    | 26224     |
|    total timesteps | 60000     |
| train/             |           |
|    actor_loss      | -68.6     |
|    critic_loss     | 0.146     |
|    learning_rate   | 0.001     |
|    n_updates       | 54995     |
----------------------------------
----------------------------------
| reward             | 0.68      |
| reward_ctrl        | 0.121     |
| reward_motion      | 0.359     |
| reward_orientation | 0.0767    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.109     |
| reward_velocity    | 0.015     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 343       |
| time/              |           |
|    episodes        | 124       |
|    fps             | 2         |
|    time_elapsed    | 26341     |
|    total timesteps | 62000     |
| train/             |           |
|    actor_loss      | -70.1     |
|    critic_loss     | 0.182     |
|    learning_rate   | 0.001     |
|    n_updates       | 56995     |
----------------------------------
----------------------------------
| reward             | 0.69      |
| reward_ctrl        | 0.119     |
| reward_motion      | 0.37      |
| reward_orientation | 0.0768    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.108     |
| reward_velocity    | 0.0172    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 346       |
| time/              |           |
|    episodes        | 128       |
|    fps             | 2         |
|    time_elapsed    | 26460     |
|    total timesteps | 64000     |
| train/             |           |
|    actor_loss      | -71.1     |
|    critic_loss     | 0.162     |
|    learning_rate   | 0.001     |
|    n_updates       | 58995     |
----------------------------------
Num timesteps: 66000
Best mean reward: 356.34 - Last mean reward per episode: 347.87
----------------------------------
| reward             | 0.69      |
| reward_ctrl        | 0.118     |
| reward_motion      | 0.373     |
| reward_orientation | 0.0772    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.104     |
| reward_velocity    | 0.0181    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 348       |
| time/              |           |
|    episodes        | 132       |
|    fps             | 2         |
|    time_elapsed    | 26584     |
|    total timesteps | 66000     |
| train/             |           |
|    actor_loss      | -72.2     |
|    critic_loss     | 0.165     |
|    learning_rate   | 0.001     |
|    n_updates       | 60995     |
----------------------------------
----------------------------------
| reward             | 0.688     |
| reward_ctrl        | 0.117     |
| reward_motion      | 0.376     |
| reward_orientation | 0.0777    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0166    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 349       |
| time/              |           |
|    episodes        | 136       |
|    fps             | 2         |
|    time_elapsed    | 26712     |
|    total timesteps | 68000     |
| train/             |           |
|    actor_loss      | -73.8     |
|    critic_loss     | 0.154     |
|    learning_rate   | 0.001     |
|    n_updates       | 62995     |
----------------------------------
----------------------------------
| reward             | 0.687     |
| reward_ctrl        | 0.114     |
| reward_motion      | 0.383     |
| reward_orientation | 0.0779    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.0958    |
| reward_velocity    | 0.0165    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 350       |
| time/              |           |
|    episodes        | 140       |
|    fps             | 2         |
|    time_elapsed    | 26840     |
|    total timesteps | 70000     |
| train/             |           |
|    actor_loss      | -74.6     |
|    critic_loss     | 0.145     |
|    learning_rate   | 0.001     |
|    n_updates       | 64995     |
----------------------------------
Num timesteps: 72000
Best mean reward: 356.34 - Last mean reward per episode: 351.01
----------------------------------
| reward             | 0.687     |
| reward_ctrl        | 0.114     |
| reward_motion      | 0.387     |
| reward_orientation | 0.078     |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.092     |
| reward_velocity    | 0.0165    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 351       |
| time/              |           |
|    episodes        | 144       |
|    fps             | 2         |
|    time_elapsed    | 26969     |
|    total timesteps | 72000     |
| train/             |           |
|    actor_loss      | -75.7     |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 66995     |
----------------------------------
----------------------------------
| reward             | 0.685     |
| reward_ctrl        | 0.111     |
| reward_motion      | 0.387     |
| reward_orientation | 0.078     |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.0923    |
| reward_velocity    | 0.0157    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 352       |
| time/              |           |
|    episodes        | 148       |
|    fps             | 2         |
|    time_elapsed    | 27098     |
|    total timesteps | 74000     |
| train/             |           |
|    actor_loss      | -76.7     |
|    critic_loss     | 0.148     |
|    learning_rate   | 0.001     |
|    n_updates       | 68995     |
----------------------------------
----------------------------------
| reward             | 0.686     |
| reward_ctrl        | 0.11      |
| reward_motion      | 0.396     |
| reward_orientation | 0.0776    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.089     |
| reward_velocity    | 0.0138    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 353       |
| time/              |           |
|    episodes        | 152       |
|    fps             | 2         |
|    time_elapsed    | 27227     |
|    total timesteps | 76000     |
| train/             |           |
|    actor_loss      | -77.9     |
|    critic_loss     | 0.152     |
|    learning_rate   | 0.001     |
|    n_updates       | 70995     |
----------------------------------
Num timesteps: 78000
Best mean reward: 356.34 - Last mean reward per episode: 355.12
----------------------------------
| reward             | 0.685     |
| reward_ctrl        | 0.108     |
| reward_motion      | 0.396     |
| reward_orientation | 0.0776    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.0894    |
| reward_velocity    | 0.0141    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 355       |
| time/              |           |
|    episodes        | 156       |
|    fps             | 2         |
|    time_elapsed    | 27354     |
|    total timesteps | 78000     |
| train/             |           |
|    actor_loss      | -78.4     |
|    critic_loss     | 0.136     |
|    learning_rate   | 0.001     |
|    n_updates       | 72995     |
----------------------------------
----------------------------------
| reward             | 0.69      |
| reward_ctrl        | 0.108     |
| reward_motion      | 0.399     |
| reward_orientation | 0.0775    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.0903    |
| reward_velocity    | 0.0147    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 357       |
| time/              |           |
|    episodes        | 160       |
|    fps             | 2         |
|    time_elapsed    | 27482     |
|    total timesteps | 80000     |
| train/             |           |
|    actor_loss      | -79       |
|    critic_loss     | 0.147     |
|    learning_rate   | 0.001     |
|    n_updates       | 74995     |
----------------------------------
----------------------------------
| reward             | 0.69      |
| reward_ctrl        | 0.108     |
| reward_motion      | 0.401     |
| reward_orientation | 0.0771    |
| reward_position    | 5.01e-230 |
| reward_rotation    | 0.0896    |
| reward_velocity    | 0.0147    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 357       |
| time/              |           |
|    episodes        | 164       |
|    fps             | 2         |
|    time_elapsed    | 27604     |
|    total timesteps | 82000     |
| train/             |           |
|    actor_loss      | -79.8     |
|    critic_loss     | 0.144     |
|    learning_rate   | 0.001     |
|    n_updates       | 76995     |
----------------------------------
Num timesteps: 84000
Best mean reward: 356.34 - Last mean reward per episode: 357.39
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.683     |
| reward_ctrl        | 0.104     |
| reward_motion      | 0.403     |
| reward_orientation | 0.0772    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0868    |
| reward_velocity    | 0.0116    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 357       |
| time/              |           |
|    episodes        | 168       |
|    fps             | 3         |
|    time_elapsed    | 27722     |
|    total timesteps | 84000     |
| train/             |           |
|    actor_loss      | -80.1     |
|    critic_loss     | 0.142     |
|    learning_rate   | 0.001     |
|    n_updates       | 78995     |
----------------------------------
----------------------------------
| reward             | 0.681     |
| reward_ctrl        | 0.104     |
| reward_motion      | 0.404     |
| reward_orientation | 0.077     |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0844    |
| reward_velocity    | 0.0113    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 359       |
| time/              |           |
|    episodes        | 172       |
|    fps             | 3         |
|    time_elapsed    | 27840     |
|    total timesteps | 86000     |
| train/             |           |
|    actor_loss      | -81.1     |
|    critic_loss     | 0.123     |
|    learning_rate   | 0.001     |
|    n_updates       | 80995     |
----------------------------------
----------------------------------
| reward             | 0.683     |
| reward_ctrl        | 0.103     |
| reward_motion      | 0.41      |
| reward_orientation | 0.0767    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0814    |
| reward_velocity    | 0.0117    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 360       |
| time/              |           |
|    episodes        | 176       |
|    fps             | 3         |
|    time_elapsed    | 27958     |
|    total timesteps | 88000     |
| train/             |           |
|    actor_loss      | -81.8     |
|    critic_loss     | 0.152     |
|    learning_rate   | 0.001     |
|    n_updates       | 82995     |
----------------------------------
Num timesteps: 90000
Best mean reward: 357.39 - Last mean reward per episode: 361.01
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.685     |
| reward_ctrl        | 0.102     |
| reward_motion      | 0.418     |
| reward_orientation | 0.0762    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0774    |
| reward_velocity    | 0.0121    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 361       |
| time/              |           |
|    episodes        | 180       |
|    fps             | 3         |
|    time_elapsed    | 28076     |
|    total timesteps | 90000     |
| train/             |           |
|    actor_loss      | -82.3     |
|    critic_loss     | 0.138     |
|    learning_rate   | 0.001     |
|    n_updates       | 84995     |
----------------------------------
----------------------------------
| reward             | 0.689     |
| reward_ctrl        | 0.0999    |
| reward_motion      | 0.425     |
| reward_orientation | 0.0762    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.076     |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 363       |
| time/              |           |
|    episodes        | 184       |
|    fps             | 3         |
|    time_elapsed    | 28196     |
|    total timesteps | 92000     |
| train/             |           |
|    actor_loss      | -82.5     |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 86995     |
----------------------------------
----------------------------------
| reward             | 0.689     |
| reward_ctrl        | 0.0967    |
| reward_motion      | 0.433     |
| reward_orientation | 0.0762    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0708    |
| reward_velocity    | 0.0119    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 364       |
| time/              |           |
|    episodes        | 188       |
|    fps             | 3         |
|    time_elapsed    | 28314     |
|    total timesteps | 94000     |
| train/             |           |
|    actor_loss      | -84       |
|    critic_loss     | 0.147     |
|    learning_rate   | 0.001     |
|    n_updates       | 88995     |
----------------------------------
Num timesteps: 96000
Best mean reward: 361.01 - Last mean reward per episode: 366.41
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.694     |
| reward_ctrl        | 0.0958    |
| reward_motion      | 0.44      |
| reward_orientation | 0.0765    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0698    |
| reward_velocity    | 0.0123    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 366       |
| time/              |           |
|    episodes        | 192       |
|    fps             | 3         |
|    time_elapsed    | 28433     |
|    total timesteps | 96000     |
| train/             |           |
|    actor_loss      | -84.2     |
|    critic_loss     | 0.151     |
|    learning_rate   | 0.001     |
|    n_updates       | 90995     |
----------------------------------
----------------------------------
| reward             | 0.692     |
| reward_ctrl        | 0.0933    |
| reward_motion      | 0.44      |
| reward_orientation | 0.0761    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0701    |
| reward_velocity    | 0.0124    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 367       |
| time/              |           |
|    episodes        | 196       |
|    fps             | 3         |
|    time_elapsed    | 28562     |
|    total timesteps | 98000     |
| train/             |           |
|    actor_loss      | -85.3     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 92995     |
----------------------------------
----------------------------------
| reward             | 0.698     |
| reward_ctrl        | 0.0922    |
| reward_motion      | 0.449     |
| reward_orientation | 0.0762    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0672    |
| reward_velocity    | 0.013     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 369       |
| time/              |           |
|    episodes        | 200       |
|    fps             | 3         |
|    time_elapsed    | 28690     |
|    total timesteps | 100000    |
| train/             |           |
|    actor_loss      | -85       |
|    critic_loss     | 0.131     |
|    learning_rate   | 0.001     |
|    n_updates       | 94995     |
----------------------------------
Num timesteps: 102000
Best mean reward: 366.41 - Last mean reward per episode: 369.35
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.7       |
| reward_ctrl        | 0.0914    |
| reward_motion      | 0.455     |
| reward_orientation | 0.0756    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0648    |
| reward_velocity    | 0.013     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 369       |
| time/              |           |
|    episodes        | 204       |
|    fps             | 3         |
|    time_elapsed    | 28822     |
|    total timesteps | 102000    |
| train/             |           |
|    actor_loss      | -85.5     |
|    critic_loss     | 0.128     |
|    learning_rate   | 0.001     |
|    n_updates       | 96995     |
----------------------------------
----------------------------------
| reward             | 0.707     |
| reward_ctrl        | 0.0904    |
| reward_motion      | 0.465     |
| reward_orientation | 0.0748    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.065     |
| reward_velocity    | 0.0113    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 370       |
| time/              |           |
|    episodes        | 208       |
|    fps             | 3         |
|    time_elapsed    | 28952     |
|    total timesteps | 104000    |
| train/             |           |
|    actor_loss      | -86.1     |
|    critic_loss     | 0.158     |
|    learning_rate   | 0.001     |
|    n_updates       | 98995     |
----------------------------------
----------------------------------
| reward             | 0.708     |
| reward_ctrl        | 0.0881    |
| reward_motion      | 0.465     |
| reward_orientation | 0.0751    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0684    |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 370       |
| time/              |           |
|    episodes        | 212       |
|    fps             | 3         |
|    time_elapsed    | 29082     |
|    total timesteps | 106000    |
| train/             |           |
|    actor_loss      | -86.4     |
|    critic_loss     | 0.14      |
|    learning_rate   | 0.001     |
|    n_updates       | 100995    |
----------------------------------
Num timesteps: 108000
Best mean reward: 369.35 - Last mean reward per episode: 370.20
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.709     |
| reward_ctrl        | 0.0855    |
| reward_motion      | 0.471     |
| reward_orientation | 0.0747    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0656    |
| reward_velocity    | 0.0117    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 370       |
| time/              |           |
|    episodes        | 216       |
|    fps             | 3         |
|    time_elapsed    | 29212     |
|    total timesteps | 108000    |
| train/             |           |
|    actor_loss      | -87.4     |
|    critic_loss     | 0.122     |
|    learning_rate   | 0.001     |
|    n_updates       | 102995    |
----------------------------------
----------------------------------
| reward             | 0.71      |
| reward_ctrl        | 0.0854    |
| reward_motion      | 0.475     |
| reward_orientation | 0.0753    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0638    |
| reward_velocity    | 0.0108    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 371       |
| time/              |           |
|    episodes        | 220       |
|    fps             | 3         |
|    time_elapsed    | 29346     |
|    total timesteps | 110000    |
| train/             |           |
|    actor_loss      | -87       |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 104995    |
----------------------------------
----------------------------------
| reward             | 0.71      |
| reward_ctrl        | 0.0837    |
| reward_motion      | 0.476     |
| reward_orientation | 0.0753    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0655    |
| reward_velocity    | 0.00998   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 224       |
|    fps             | 3         |
|    time_elapsed    | 29479     |
|    total timesteps | 112000    |
| train/             |           |
|    actor_loss      | -87.4     |
|    critic_loss     | 0.142     |
|    learning_rate   | 0.001     |
|    n_updates       | 106995    |
----------------------------------
Num timesteps: 114000
Best mean reward: 370.20 - Last mean reward per episode: 371.46
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.706     |
| reward_ctrl        | 0.0842    |
| reward_motion      | 0.476     |
| reward_orientation | 0.0749    |
| reward_position    | 1.96e-241 |
| reward_rotation    | 0.0633    |
| reward_velocity    | 0.00769   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 371       |
| time/              |           |
|    episodes        | 228       |
|    fps             | 3         |
|    time_elapsed    | 29613     |
|    total timesteps | 114000    |
| train/             |           |
|    actor_loss      | -88.2     |
|    critic_loss     | 0.14      |
|    learning_rate   | 0.001     |
|    n_updates       | 108995    |
----------------------------------
----------------------------------
| reward             | 0.709     |
| reward_ctrl        | 0.0833    |
| reward_motion      | 0.479     |
| reward_orientation | 0.0747    |
| reward_position    | 1.98e-323 |
| reward_rotation    | 0.0636    |
| reward_velocity    | 0.00827   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 232       |
|    fps             | 3         |
|    time_elapsed    | 29748     |
|    total timesteps | 116000    |
| train/             |           |
|    actor_loss      | -88.2     |
|    critic_loss     | 0.145     |
|    learning_rate   | 0.001     |
|    n_updates       | 110995    |
----------------------------------
----------------------------------
| reward             | 0.715     |
| reward_ctrl        | 0.0816    |
| reward_motion      | 0.489     |
| reward_orientation | 0.0748    |
| reward_position    | 1.98e-323 |
| reward_rotation    | 0.0609    |
| reward_velocity    | 0.00827   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 236       |
|    fps             | 3         |
|    time_elapsed    | 29882     |
|    total timesteps | 118000    |
| train/             |           |
|    actor_loss      | -88.2     |
|    critic_loss     | 0.123     |
|    learning_rate   | 0.001     |
|    n_updates       | 112995    |
----------------------------------
Num timesteps: 120000
Best mean reward: 371.46 - Last mean reward per episode: 372.93
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.719     |
| reward_ctrl        | 0.0804    |
| reward_motion      | 0.493     |
| reward_orientation | 0.0743    |
| reward_position    | 1.98e-323 |
| reward_rotation    | 0.062     |
| reward_velocity    | 0.00882   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 240       |
|    fps             | 3         |
|    time_elapsed    | 30018     |
|    total timesteps | 120000    |
| train/             |           |
|    actor_loss      | -88.3     |
|    critic_loss     | 0.14      |
|    learning_rate   | 0.001     |
|    n_updates       | 114995    |
----------------------------------
----------------------------------
| reward             | 0.719     |
| reward_ctrl        | 0.0796    |
| reward_motion      | 0.493     |
| reward_orientation | 0.0739    |
| reward_position    | 1.98e-323 |
| reward_rotation    | 0.0639    |
| reward_velocity    | 0.00901   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 244       |
|    fps             | 4         |
|    time_elapsed    | 30154     |
|    total timesteps | 122000    |
| train/             |           |
|    actor_loss      | -89       |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 116995    |
----------------------------------
----------------------------------
| reward             | 0.724     |
| reward_ctrl        | 0.0809    |
| reward_motion      | 0.495     |
| reward_orientation | 0.0734    |
| reward_position    | 1.98e-323 |
| reward_rotation    | 0.0649    |
| reward_velocity    | 0.00908   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 248       |
|    fps             | 4         |
|    time_elapsed    | 30279     |
|    total timesteps | 123852    |
| train/             |           |
|    actor_loss      | -89.3     |
|    critic_loss     | 0.138     |
|    learning_rate   | 0.001     |
|    n_updates       | 118850    |
----------------------------------
----------------------------------
| reward             | 0.726     |
| reward_ctrl        | 0.079     |
| reward_motion      | 0.5       |
| reward_orientation | 0.0735    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.063     |
| reward_velocity    | 0.0105    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 252       |
|    fps             | 4         |
|    time_elapsed    | 30415     |
|    total timesteps | 125852    |
| train/             |           |
|    actor_loss      | -89.5     |
|    critic_loss     | 0.14      |
|    learning_rate   | 0.001     |
|    n_updates       | 120850    |
----------------------------------
Num timesteps: 126000
Best mean reward: 372.93 - Last mean reward per episode: 372.60
----------------------------------
| reward             | 0.732     |
| reward_ctrl        | 0.0797    |
| reward_motion      | 0.507     |
| reward_orientation | 0.0729    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0626    |
| reward_velocity    | 0.0102    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 256       |
|    fps             | 4         |
|    time_elapsed    | 30550     |
|    total timesteps | 127852    |
| train/             |           |
|    actor_loss      | -89.7     |
|    critic_loss     | 0.132     |
|    learning_rate   | 0.001     |
|    n_updates       | 122850    |
----------------------------------
----------------------------------
| reward             | 0.727     |
| reward_ctrl        | 0.0779    |
| reward_motion      | 0.506     |
| reward_orientation | 0.0722    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0618    |
| reward_velocity    | 0.00877   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 260       |
|    fps             | 4         |
|    time_elapsed    | 30687     |
|    total timesteps | 129852    |
| train/             |           |
|    actor_loss      | -90       |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 124850    |
----------------------------------
----------------------------------
| reward             | 0.726     |
| reward_ctrl        | 0.0763    |
| reward_motion      | 0.507     |
| reward_orientation | 0.0722    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.062     |
| reward_velocity    | 0.00878   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 264       |
|    fps             | 4         |
|    time_elapsed    | 30822     |
|    total timesteps | 131852    |
| train/             |           |
|    actor_loss      | -89.5     |
|    critic_loss     | 0.137     |
|    learning_rate   | 0.001     |
|    n_updates       | 126850    |
----------------------------------
Num timesteps: 132000
Best mean reward: 372.93 - Last mean reward per episode: 372.23
----------------------------------
| reward             | 0.729     |
| reward_ctrl        | 0.0766    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0713    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0608    |
| reward_velocity    | 0.00883   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 372       |
| time/              |           |
|    episodes        | 268       |
|    fps             | 4         |
|    time_elapsed    | 30958     |
|    total timesteps | 133852    |
| train/             |           |
|    actor_loss      | -90.3     |
|    critic_loss     | 0.137     |
|    learning_rate   | 0.001     |
|    n_updates       | 128850    |
----------------------------------
----------------------------------
| reward             | 0.737     |
| reward_ctrl        | 0.0746    |
| reward_motion      | 0.516     |
| reward_orientation | 0.071     |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0654    |
| reward_velocity    | 0.01      |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 272       |
|    fps             | 4         |
|    time_elapsed    | 31095     |
|    total timesteps | 135852    |
| train/             |           |
|    actor_loss      | -89.9     |
|    critic_loss     | 0.113     |
|    learning_rate   | 0.001     |
|    n_updates       | 130850    |
----------------------------------
----------------------------------
| reward             | 0.735     |
| reward_ctrl        | 0.0718    |
| reward_motion      | 0.515     |
| reward_orientation | 0.0708    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0679    |
| reward_velocity    | 0.00944   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 276       |
|    fps             | 4         |
|    time_elapsed    | 31230     |
|    total timesteps | 137852    |
| train/             |           |
|    actor_loss      | -90.5     |
|    critic_loss     | 0.145     |
|    learning_rate   | 0.001     |
|    n_updates       | 132850    |
----------------------------------
Num timesteps: 138000
Best mean reward: 372.93 - Last mean reward per episode: 372.83
----------------------------------
| reward             | 0.73      |
| reward_ctrl        | 0.0729    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0712    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0685    |
| reward_velocity    | 0.0092    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 280       |
|    fps             | 4         |
|    time_elapsed    | 31365     |
|    total timesteps | 139852    |
| train/             |           |
|    actor_loss      | -91       |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 134850    |
----------------------------------
----------------------------------
| reward             | 0.73      |
| reward_ctrl        | 0.0721    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0716    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0669    |
| reward_velocity    | 0.00878   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 373       |
| time/              |           |
|    episodes        | 284       |
|    fps             | 4         |
|    time_elapsed    | 31502     |
|    total timesteps | 141852    |
| train/             |           |
|    actor_loss      | -91.1     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 136850    |
----------------------------------
----------------------------------
| reward             | 0.738     |
| reward_ctrl        | 0.0737    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0711    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0723    |
| reward_velocity    | 0.0104    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 374       |
| time/              |           |
|    episodes        | 288       |
|    fps             | 4         |
|    time_elapsed    | 31637     |
|    total timesteps | 143852    |
| train/             |           |
|    actor_loss      | -91.6     |
|    critic_loss     | 0.14      |
|    learning_rate   | 0.001     |
|    n_updates       | 138850    |
----------------------------------
Num timesteps: 144000
Best mean reward: 372.93 - Last mean reward per episode: 373.73
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.742     |
| reward_ctrl        | 0.0742    |
| reward_motion      | 0.512     |
| reward_orientation | 0.0711    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0748    |
| reward_velocity    | 0.00983   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 374       |
| time/              |           |
|    episodes        | 292       |
|    fps             | 4         |
|    time_elapsed    | 31774     |
|    total timesteps | 145852    |
| train/             |           |
|    actor_loss      | -92       |
|    critic_loss     | 0.132     |
|    learning_rate   | 0.001     |
|    n_updates       | 140850    |
----------------------------------
----------------------------------
| reward             | 0.744     |
| reward_ctrl        | 0.0749    |
| reward_motion      | 0.513     |
| reward_orientation | 0.0715    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.074     |
| reward_velocity    | 0.00987   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 374       |
| time/              |           |
|    episodes        | 296       |
|    fps             | 4         |
|    time_elapsed    | 31912     |
|    total timesteps | 147852    |
| train/             |           |
|    actor_loss      | -91.4     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 142850    |
----------------------------------
----------------------------------
| reward             | 0.745     |
| reward_ctrl        | 0.0738    |
| reward_motion      | 0.515     |
| reward_orientation | 0.0711    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0756    |
| reward_velocity    | 0.00953   |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 374       |
| time/              |           |
|    episodes        | 300       |
|    fps             | 4         |
|    time_elapsed    | 32047     |
|    total timesteps | 149852    |
| train/             |           |
|    actor_loss      | -91.5     |
|    critic_loss     | 0.104     |
|    learning_rate   | 0.001     |
|    n_updates       | 144850    |
----------------------------------
Num timesteps: 150000
Best mean reward: 373.73 - Last mean reward per episode: 373.97
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.75      |
| reward_ctrl        | 0.074     |
| reward_motion      | 0.515     |
| reward_orientation | 0.0717    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0799    |
| reward_velocity    | 0.0103    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 374       |
| time/              |           |
|    episodes        | 304       |
|    fps             | 4         |
|    time_elapsed    | 32184     |
|    total timesteps | 151852    |
| train/             |           |
|    actor_loss      | -91.7     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 146850    |
----------------------------------
----------------------------------
| reward             | 0.744     |
| reward_ctrl        | 0.0728    |
| reward_motion      | 0.512     |
| reward_orientation | 0.0719    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0769    |
| reward_velocity    | 0.0106    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 375       |
| time/              |           |
|    episodes        | 308       |
|    fps             | 4         |
|    time_elapsed    | 32322     |
|    total timesteps | 153852    |
| train/             |           |
|    actor_loss      | -91.4     |
|    critic_loss     | 0.131     |
|    learning_rate   | 0.001     |
|    n_updates       | 148850    |
----------------------------------
----------------------------------
| reward             | 0.746     |
| reward_ctrl        | 0.0742    |
| reward_motion      | 0.514     |
| reward_orientation | 0.072     |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0755    |
| reward_velocity    | 0.0101    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 375       |
| time/              |           |
|    episodes        | 312       |
|    fps             | 4         |
|    time_elapsed    | 32459     |
|    total timesteps | 155852    |
| train/             |           |
|    actor_loss      | -91.9     |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 150850    |
----------------------------------
Num timesteps: 156000
Best mean reward: 373.97 - Last mean reward per episode: 375.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.738     |
| reward_ctrl        | 0.0754    |
| reward_motion      | 0.507     |
| reward_orientation | 0.0723    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0736    |
| reward_velocity    | 0.0102    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 375       |
| time/              |           |
|    episodes        | 316       |
|    fps             | 4         |
|    time_elapsed    | 32596     |
|    total timesteps | 157852    |
| train/             |           |
|    actor_loss      | -92.6     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 152850    |
----------------------------------
----------------------------------
| reward             | 0.735     |
| reward_ctrl        | 0.0761    |
| reward_motion      | 0.502     |
| reward_orientation | 0.0722    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0742    |
| reward_velocity    | 0.0104    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 375       |
| time/              |           |
|    episodes        | 320       |
|    fps             | 4         |
|    time_elapsed    | 32732     |
|    total timesteps | 159852    |
| train/             |           |
|    actor_loss      | -92.6     |
|    critic_loss     | 0.135     |
|    learning_rate   | 0.001     |
|    n_updates       | 154850    |
----------------------------------
----------------------------------
| reward             | 0.742     |
| reward_ctrl        | 0.0778    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0715    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0737    |
| reward_velocity    | 0.0111    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 376       |
| time/              |           |
|    episodes        | 324       |
|    fps             | 4         |
|    time_elapsed    | 32871     |
|    total timesteps | 161852    |
| train/             |           |
|    actor_loss      | -92.1     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 156850    |
----------------------------------
Num timesteps: 162000
Best mean reward: 375.18 - Last mean reward per episode: 375.82
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.746     |
| reward_ctrl        | 0.0771    |
| reward_motion      | 0.51      |
| reward_orientation | 0.0711    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0757    |
| reward_velocity    | 0.0123    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 376       |
| time/              |           |
|    episodes        | 328       |
|    fps             | 4         |
|    time_elapsed    | 33008     |
|    total timesteps | 163852    |
| train/             |           |
|    actor_loss      | -92.7     |
|    critic_loss     | 0.123     |
|    learning_rate   | 0.001     |
|    n_updates       | 158850    |
----------------------------------
----------------------------------
| reward             | 0.75      |
| reward_ctrl        | 0.0761    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0706    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0806    |
| reward_velocity    | 0.0114    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 332       |
|    fps             | 5         |
|    time_elapsed    | 33140     |
|    total timesteps | 165852    |
| train/             |           |
|    actor_loss      | -92.5     |
|    critic_loss     | 0.119     |
|    learning_rate   | 0.001     |
|    n_updates       | 160850    |
----------------------------------
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.0787    |
| reward_motion      | 0.51      |
| reward_orientation | 0.0701    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0825    |
| reward_velocity    | 0.0121    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 336       |
|    fps             | 5         |
|    time_elapsed    | 33266     |
|    total timesteps | 167852    |
| train/             |           |
|    actor_loss      | -92.1     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 162850    |
----------------------------------
Num timesteps: 168000
Best mean reward: 375.82 - Last mean reward per episode: 376.66
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.758     |
| reward_ctrl        | 0.0799    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0705    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0852    |
| reward_velocity    | 0.012     |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 340       |
|    fps             | 5         |
|    time_elapsed    | 33394     |
|    total timesteps | 169852    |
| train/             |           |
|    actor_loss      | -92.9     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 164850    |
----------------------------------
----------------------------------
| reward             | 0.762     |
| reward_ctrl        | 0.0808    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0707    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.086     |
| reward_velocity    | 0.0134    |
| rollout/           |           |
|    ep_len_mean     | 499       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 344       |
|    fps             | 5         |
|    time_elapsed    | 33519     |
|    total timesteps | 171852    |
| train/             |           |
|    actor_loss      | -92.4     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 166850    |
----------------------------------
----------------------------------
| reward             | 0.766     |
| reward_ctrl        | 0.0783    |
| reward_motion      | 0.519     |
| reward_orientation | 0.0702    |
| reward_position    | 5.38e-171 |
| reward_rotation    | 0.0838    |
| reward_velocity    | 0.0142    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 348       |
|    fps             | 5         |
|    time_elapsed    | 33646     |
|    total timesteps | 173852    |
| train/             |           |
|    actor_loss      | -92.9     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 168850    |
----------------------------------
Num timesteps: 174000
Best mean reward: 376.66 - Last mean reward per episode: 378.75
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.763     |
| reward_ctrl        | 0.0793    |
| reward_motion      | 0.515     |
| reward_orientation | 0.0706    |
| reward_position    | 2.61e-187 |
| reward_rotation    | 0.085     |
| reward_velocity    | 0.0126    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 352       |
|    fps             | 5         |
|    time_elapsed    | 33772     |
|    total timesteps | 175852    |
| train/             |           |
|    actor_loss      | -93.2     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 170850    |
----------------------------------
----------------------------------
| reward             | 0.763     |
| reward_ctrl        | 0.0779    |
| reward_motion      | 0.514     |
| reward_orientation | 0.0707    |
| reward_position    | 2.61e-187 |
| reward_rotation    | 0.0864    |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 356       |
|    fps             | 5         |
|    time_elapsed    | 33902     |
|    total timesteps | 177852    |
| train/             |           |
|    actor_loss      | -92.9     |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 172850    |
----------------------------------
----------------------------------
| reward             | 0.771     |
| reward_ctrl        | 0.0774    |
| reward_motion      | 0.521     |
| reward_orientation | 0.071     |
| reward_position    | 2.61e-187 |
| reward_rotation    | 0.0882    |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 360       |
|    fps             | 5         |
|    time_elapsed    | 34039     |
|    total timesteps | 179852    |
| train/             |           |
|    actor_loss      | -93.2     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 174850    |
----------------------------------
Num timesteps: 180000
Best mean reward: 378.75 - Last mean reward per episode: 378.01
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0784    |
| reward_motion      | 0.517     |
| reward_orientation | 0.071     |
| reward_position    | 2.61e-187 |
| reward_rotation    | 0.0892    |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 364       |
|    fps             | 5         |
|    time_elapsed    | 34175     |
|    total timesteps | 181852    |
| train/             |           |
|    actor_loss      | -93.3     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 176850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0804    |
| reward_motion      | 0.515     |
| reward_orientation | 0.0715    |
| reward_position    | 2.61e-187 |
| reward_rotation    | 0.0907    |
| reward_velocity    | 0.0137    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 368       |
|    fps             | 5         |
|    time_elapsed    | 34319     |
|    total timesteps | 183852    |
| train/             |           |
|    actor_loss      | -93.4     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 178850    |
----------------------------------
----------------------------------
| reward             | 0.762     |
| reward_ctrl        | 0.0819    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0718    |
| reward_position    | 1.97e-258 |
| reward_rotation    | 0.087     |
| reward_velocity    | 0.0127    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 372       |
|    fps             | 5         |
|    time_elapsed    | 34439     |
|    total timesteps | 185852    |
| train/             |           |
|    actor_loss      | -94       |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 180850    |
----------------------------------
Num timesteps: 186000
Best mean reward: 378.75 - Last mean reward per episode: 377.39
----------------------------------
| reward             | 0.767     |
| reward_ctrl        | 0.0818    |
| reward_motion      | 0.514     |
| reward_orientation | 0.0724    |
| reward_position    | 1.97e-258 |
| reward_rotation    | 0.0862    |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 376       |
|    fps             | 5         |
|    time_elapsed    | 34559     |
|    total timesteps | 187852    |
| train/             |           |
|    actor_loss      | -93.3     |
|    critic_loss     | 0.139     |
|    learning_rate   | 0.001     |
|    n_updates       | 182850    |
----------------------------------
----------------------------------
| reward             | 0.77      |
| reward_ctrl        | 0.0813    |
| reward_motion      | 0.515     |
| reward_orientation | 0.0721    |
| reward_position    | 1.97e-258 |
| reward_rotation    | 0.0878    |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 380       |
|    fps             | 5         |
|    time_elapsed    | 34680     |
|    total timesteps | 189852    |
| train/             |           |
|    actor_loss      | -93.4     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 184850    |
----------------------------------
----------------------------------
| reward             | 0.776     |
| reward_ctrl        | 0.0823    |
| reward_motion      | 0.518     |
| reward_orientation | 0.0719    |
| reward_position    | 1.97e-258 |
| reward_rotation    | 0.0899    |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 384       |
|    fps             | 5         |
|    time_elapsed    | 34800     |
|    total timesteps | 191852    |
| train/             |           |
|    actor_loss      | -93.9     |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 186850    |
----------------------------------
Num timesteps: 192000
Best mean reward: 378.75 - Last mean reward per episode: 378.59
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0826    |
| reward_motion      | 0.514     |
| reward_orientation | 0.072     |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.089     |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 388       |
|    fps             | 5         |
|    time_elapsed    | 34922     |
|    total timesteps | 193852    |
| train/             |           |
|    actor_loss      | -93.2     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 188850    |
----------------------------------
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0823    |
| reward_motion      | 0.515     |
| reward_orientation | 0.072     |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0875    |
| reward_velocity    | 0.0119    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 392       |
|    fps             | 5         |
|    time_elapsed    | 35046     |
|    total timesteps | 195852    |
| train/             |           |
|    actor_loss      | -93.7     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 190850    |
----------------------------------
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0834    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0718    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0908    |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 396       |
|    fps             | 5         |
|    time_elapsed    | 35178     |
|    total timesteps | 197852    |
| train/             |           |
|    actor_loss      | -93.7     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 192850    |
----------------------------------
Num timesteps: 198000
Best mean reward: 378.75 - Last mean reward per episode: 378.24
----------------------------------
| reward             | 0.765     |
| reward_ctrl        | 0.0849    |
| reward_motion      | 0.503     |
| reward_orientation | 0.0714    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0935    |
| reward_velocity    | 0.0117    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 400       |
|    fps             | 5         |
|    time_elapsed    | 35312     |
|    total timesteps | 199852    |
| train/             |           |
|    actor_loss      | -93.6     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 194850    |
----------------------------------
----------------------------------
| reward             | 0.763     |
| reward_ctrl        | 0.0864    |
| reward_motion      | 0.503     |
| reward_orientation | 0.0709    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0915    |
| reward_velocity    | 0.0111    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 404       |
|    fps             | 5         |
|    time_elapsed    | 35444     |
|    total timesteps | 201852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.122     |
|    learning_rate   | 0.001     |
|    n_updates       | 196850    |
----------------------------------
----------------------------------
| reward             | 0.766     |
| reward_ctrl        | 0.0874    |
| reward_motion      | 0.505     |
| reward_orientation | 0.0713    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0921    |
| reward_velocity    | 0.0109    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 408       |
|    fps             | 5         |
|    time_elapsed    | 35578     |
|    total timesteps | 203852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 198850    |
----------------------------------
Num timesteps: 204000
Best mean reward: 378.75 - Last mean reward per episode: 377.06
----------------------------------
| reward             | 0.764     |
| reward_ctrl        | 0.0862    |
| reward_motion      | 0.504     |
| reward_orientation | 0.0708    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0912    |
| reward_velocity    | 0.0127    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 412       |
|    fps             | 5         |
|    time_elapsed    | 35710     |
|    total timesteps | 205852    |
| train/             |           |
|    actor_loss      | -93.8     |
|    critic_loss     | 0.142     |
|    learning_rate   | 0.001     |
|    n_updates       | 200850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0863    |
| reward_motion      | 0.511     |
| reward_orientation | 0.0705    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0913    |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 416       |
|    fps             | 5         |
|    time_elapsed    | 35842     |
|    total timesteps | 207852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.119     |
|    learning_rate   | 0.001     |
|    n_updates       | 202850    |
----------------------------------
----------------------------------
| reward             | 0.778     |
| reward_ctrl        | 0.0868    |
| reward_motion      | 0.514     |
| reward_orientation | 0.0698    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0943    |
| reward_velocity    | 0.0128    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 420       |
|    fps             | 5         |
|    time_elapsed    | 35963     |
|    total timesteps | 209852    |
| train/             |           |
|    actor_loss      | -94.1     |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 204850    |
----------------------------------
Num timesteps: 210000
Best mean reward: 378.75 - Last mean reward per episode: 376.98
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.0878    |
| reward_motion      | 0.51      |
| reward_orientation | 0.0704    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0929    |
| reward_velocity    | 0.0124    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 424       |
|    fps             | 5         |
|    time_elapsed    | 36085     |
|    total timesteps | 211852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.134     |
|    learning_rate   | 0.001     |
|    n_updates       | 206850    |
----------------------------------
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.0882    |
| reward_motion      | 0.506     |
| reward_orientation | 0.0707    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.097     |
| reward_velocity    | 0.0124    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 428       |
|    fps             | 5         |
|    time_elapsed    | 36208     |
|    total timesteps | 213852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 208850    |
----------------------------------
----------------------------------
| reward             | 0.775     |
| reward_ctrl        | 0.088     |
| reward_motion      | 0.509     |
| reward_orientation | 0.0714    |
| reward_position    | 1.64e-289 |
| reward_rotation    | 0.0949    |
| reward_velocity    | 0.0121    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 432       |
|    fps             | 5         |
|    time_elapsed    | 36329     |
|    total timesteps | 215852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.128     |
|    learning_rate   | 0.001     |
|    n_updates       | 210850    |
----------------------------------
Num timesteps: 216000
Best mean reward: 378.75 - Last mean reward per episode: 377.04
---------------------------------
| reward             | 0.773    |
| reward_ctrl        | 0.0872   |
| reward_motion      | 0.508    |
| reward_orientation | 0.0712   |
| reward_position    | 0        |
| reward_rotation    | 0.0948   |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    episodes        | 436      |
|    fps             | 5        |
|    time_elapsed    | 36454    |
|    total timesteps | 217852   |
| train/             |          |
|    actor_loss      | -93.8    |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.001    |
|    n_updates       | 212850   |
---------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0892    |
| reward_motion      | 0.505     |
| reward_orientation | 0.0713    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.094     |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 440       |
|    fps             | 6         |
|    time_elapsed    | 36589     |
|    total timesteps | 219852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.119     |
|    learning_rate   | 0.001     |
|    n_updates       | 214850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0877    |
| reward_motion      | 0.507     |
| reward_orientation | 0.0709    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0948    |
| reward_velocity    | 0.0116    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 444       |
|    fps             | 6         |
|    time_elapsed    | 36724     |
|    total timesteps | 221852    |
| train/             |           |
|    actor_loss      | -93.8     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 216850    |
----------------------------------
Num timesteps: 222000
Best mean reward: 378.75 - Last mean reward per episode: 377.50
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.0896    |
| reward_motion      | 0.505     |
| reward_orientation | 0.0715    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0971    |
| reward_velocity    | 0.0114    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 448       |
|    fps             | 6         |
|    time_elapsed    | 36859     |
|    total timesteps | 223852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 218850    |
----------------------------------
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.0885    |
| reward_motion      | 0.506     |
| reward_orientation | 0.0715    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0115    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 452       |
|    fps             | 6         |
|    time_elapsed    | 36992     |
|    total timesteps | 225852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 220850    |
----------------------------------
----------------------------------
| reward             | 0.773     |
| reward_ctrl        | 0.09      |
| reward_motion      | 0.501     |
| reward_orientation | 0.0713    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0983    |
| reward_velocity    | 0.0126    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 456       |
|    fps             | 6         |
|    time_elapsed    | 37115     |
|    total timesteps | 227852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.131     |
|    learning_rate   | 0.001     |
|    n_updates       | 222850    |
----------------------------------
Num timesteps: 228000
Best mean reward: 378.75 - Last mean reward per episode: 377.96
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0911    |
| reward_motion      | 0.499     |
| reward_orientation | 0.0716    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0971    |
| reward_velocity    | 0.0131    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 460       |
|    fps             | 6         |
|    time_elapsed    | 37239     |
|    total timesteps | 229852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 224850    |
----------------------------------
----------------------------------
| reward             | 0.766     |
| reward_ctrl        | 0.0909    |
| reward_motion      | 0.49      |
| reward_orientation | 0.0716    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0999    |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 464       |
|    fps             | 6         |
|    time_elapsed    | 37360     |
|    total timesteps | 231852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 226850    |
----------------------------------
----------------------------------
| reward             | 0.766     |
| reward_ctrl        | 0.0891    |
| reward_motion      | 0.492     |
| reward_orientation | 0.0715    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 468       |
|    fps             | 6         |
|    time_elapsed    | 37484     |
|    total timesteps | 233852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 228850    |
----------------------------------
Num timesteps: 234000
Best mean reward: 378.75 - Last mean reward per episode: 379.23
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0891    |
| reward_motion      | 0.494     |
| reward_orientation | 0.0713    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 472       |
|    fps             | 6         |
|    time_elapsed    | 37608     |
|    total timesteps | 235852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.139     |
|    learning_rate   | 0.001     |
|    n_updates       | 230850    |
----------------------------------
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.0901    |
| reward_motion      | 0.495     |
| reward_orientation | 0.0714    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0998    |
| reward_velocity    | 0.013     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 476       |
|    fps             | 6         |
|    time_elapsed    | 37732     |
|    total timesteps | 237852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 232850    |
----------------------------------
----------------------------------
| reward             | 0.761     |
| reward_ctrl        | 0.0903    |
| reward_motion      | 0.489     |
| reward_orientation | 0.071     |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0974    |
| reward_velocity    | 0.0126    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 480       |
|    fps             | 6         |
|    time_elapsed    | 37869     |
|    total timesteps | 239852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.127     |
|    learning_rate   | 0.001     |
|    n_updates       | 234850    |
----------------------------------
Num timesteps: 240000
Best mean reward: 379.23 - Last mean reward per episode: 379.09
----------------------------------
| reward             | 0.755     |
| reward_ctrl        | 0.0918    |
| reward_motion      | 0.483     |
| reward_orientation | 0.0709    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0967    |
| reward_velocity    | 0.0125    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 484       |
|    fps             | 6         |
|    time_elapsed    | 38005     |
|    total timesteps | 241852    |
| train/             |           |
|    actor_loss      | -94.1     |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 236850    |
----------------------------------
----------------------------------
| reward             | 0.767     |
| reward_ctrl        | 0.0913    |
| reward_motion      | 0.496     |
| reward_orientation | 0.0711    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0966    |
| reward_velocity    | 0.0127    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 488       |
|    fps             | 6         |
|    time_elapsed    | 38140     |
|    total timesteps | 243852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 238850    |
----------------------------------
----------------------------------
| reward             | 0.767     |
| reward_ctrl        | 0.0895    |
| reward_motion      | 0.497     |
| reward_orientation | 0.0711    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0965    |
| reward_velocity    | 0.0129    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 492       |
|    fps             | 6         |
|    time_elapsed    | 38278     |
|    total timesteps | 245852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.109     |
|    learning_rate   | 0.001     |
|    n_updates       | 240850    |
----------------------------------
Num timesteps: 246000
Best mean reward: 379.23 - Last mean reward per episode: 378.96
----------------------------------
| reward             | 0.76      |
| reward_ctrl        | 0.0888    |
| reward_motion      | 0.49      |
| reward_orientation | 0.0706    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0974    |
| reward_velocity    | 0.0129    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 496       |
|    fps             | 6         |
|    time_elapsed    | 38415     |
|    total timesteps | 247852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 242850    |
----------------------------------
----------------------------------
| reward             | 0.76      |
| reward_ctrl        | 0.0881    |
| reward_motion      | 0.492     |
| reward_orientation | 0.0714    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0961    |
| reward_velocity    | 0.0128    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 500       |
|    fps             | 6         |
|    time_elapsed    | 38554     |
|    total timesteps | 249852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.113     |
|    learning_rate   | 0.001     |
|    n_updates       | 244850    |
----------------------------------
----------------------------------
| reward             | 0.756     |
| reward_ctrl        | 0.0881    |
| reward_motion      | 0.486     |
| reward_orientation | 0.0713    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0972    |
| reward_velocity    | 0.013     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 504       |
|    fps             | 6         |
|    time_elapsed    | 38694     |
|    total timesteps | 251852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 246850    |
----------------------------------
Num timesteps: 252000
Best mean reward: 379.23 - Last mean reward per episode: 379.42
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.0874    |
| reward_motion      | 0.482     |
| reward_orientation | 0.071     |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0988    |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 508       |
|    fps             | 6         |
|    time_elapsed    | 38825     |
|    total timesteps | 253852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.116     |
|    learning_rate   | 0.001     |
|    n_updates       | 248850    |
----------------------------------
----------------------------------
| reward             | 0.748     |
| reward_ctrl        | 0.0873    |
| reward_motion      | 0.479     |
| reward_orientation | 0.0713    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0982    |
| reward_velocity    | 0.012     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 512       |
|    fps             | 6         |
|    time_elapsed    | 38951     |
|    total timesteps | 255852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.135     |
|    learning_rate   | 0.001     |
|    n_updates       | 250850    |
----------------------------------
----------------------------------
| reward             | 0.741     |
| reward_ctrl        | 0.0868    |
| reward_motion      | 0.47      |
| reward_orientation | 0.0718    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0985    |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 516       |
|    fps             | 6         |
|    time_elapsed    | 39079     |
|    total timesteps | 257852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.117     |
|    learning_rate   | 0.001     |
|    n_updates       | 252850    |
----------------------------------
Num timesteps: 258000
Best mean reward: 379.42 - Last mean reward per episode: 379.29
----------------------------------
| reward             | 0.741     |
| reward_ctrl        | 0.0867    |
| reward_motion      | 0.47      |
| reward_orientation | 0.0728    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0969    |
| reward_velocity    | 0.0149    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 520       |
|    fps             | 6         |
|    time_elapsed    | 39208     |
|    total timesteps | 259852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.122     |
|    learning_rate   | 0.001     |
|    n_updates       | 254850    |
----------------------------------
----------------------------------
| reward             | 0.739     |
| reward_ctrl        | 0.0856    |
| reward_motion      | 0.466     |
| reward_orientation | 0.073     |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0995    |
| reward_velocity    | 0.0152    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 524       |
|    fps             | 6         |
|    time_elapsed    | 39338     |
|    total timesteps | 261852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 256850    |
----------------------------------
----------------------------------
| reward             | 0.739     |
| reward_ctrl        | 0.0847    |
| reward_motion      | 0.473     |
| reward_orientation | 0.0734    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.0936    |
| reward_velocity    | 0.0148    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 528       |
|    fps             | 6         |
|    time_elapsed    | 39477     |
|    total timesteps | 263852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 258850    |
----------------------------------
Num timesteps: 264000
Best mean reward: 379.42 - Last mean reward per episode: 379.81
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.733     |
| reward_ctrl        | 0.0837    |
| reward_motion      | 0.469     |
| reward_orientation | 0.0733    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.092     |
| reward_velocity    | 0.0145    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 532       |
|    fps             | 6         |
|    time_elapsed    | 39616     |
|    total timesteps | 265852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 260850    |
----------------------------------
----------------------------------
| reward             | 0.736     |
| reward_ctrl        | 0.0823    |
| reward_motion      | 0.471     |
| reward_orientation | 0.0736    |
| reward_position    | 3.45e-297 |
| reward_rotation    | 0.093     |
| reward_velocity    | 0.0166    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 536       |
|    fps             | 6         |
|    time_elapsed    | 39756     |
|    total timesteps | 267852    |
| train/             |           |
|    actor_loss      | -95.3     |
|    critic_loss     | 0.128     |
|    learning_rate   | 0.001     |
|    n_updates       | 262850    |
----------------------------------
---------------------------------
| reward             | 0.728    |
| reward_ctrl        | 0.0801   |
| reward_motion      | 0.468    |
| reward_orientation | 0.0734   |
| reward_position    | 0        |
| reward_rotation    | 0.092    |
| reward_velocity    | 0.0144   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 540      |
|    fps             | 6        |
|    time_elapsed    | 39901    |
|    total timesteps | 269852   |
| train/             |          |
|    actor_loss      | -94.9    |
|    critic_loss     | 0.116    |
|    learning_rate   | 0.001    |
|    n_updates       | 264850   |
---------------------------------
Num timesteps: 270000
Best mean reward: 379.81 - Last mean reward per episode: 379.35
---------------------------------
| reward             | 0.724    |
| reward_ctrl        | 0.0814   |
| reward_motion      | 0.462    |
| reward_orientation | 0.0739   |
| reward_position    | 0        |
| reward_rotation    | 0.0908   |
| reward_velocity    | 0.0152   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 544      |
|    fps             | 6        |
|    time_elapsed    | 40036    |
|    total timesteps | 271852   |
| train/             |          |
|    actor_loss      | -94.4    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 266850   |
---------------------------------
---------------------------------
| reward             | 0.717    |
| reward_ctrl        | 0.0812   |
| reward_motion      | 0.461    |
| reward_orientation | 0.0738   |
| reward_position    | 0        |
| reward_rotation    | 0.0864   |
| reward_velocity    | 0.0145   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 548      |
|    fps             | 6        |
|    time_elapsed    | 40162    |
|    total timesteps | 273852   |
| train/             |          |
|    actor_loss      | -95      |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 268850   |
---------------------------------
---------------------------------
| reward             | 0.719    |
| reward_ctrl        | 0.0823   |
| reward_motion      | 0.463    |
| reward_orientation | 0.0731   |
| reward_position    | 0        |
| reward_rotation    | 0.0863   |
| reward_velocity    | 0.0145   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 552      |
|    fps             | 6        |
|    time_elapsed    | 40290    |
|    total timesteps | 275852   |
| train/             |          |
|    actor_loss      | -94.9    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 270850   |
---------------------------------
Num timesteps: 276000
Best mean reward: 379.81 - Last mean reward per episode: 378.95
---------------------------------
| reward             | 0.719    |
| reward_ctrl        | 0.0812   |
| reward_motion      | 0.463    |
| reward_orientation | 0.0733   |
| reward_position    | 0        |
| reward_rotation    | 0.0879   |
| reward_velocity    | 0.0133   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 556      |
|    fps             | 6        |
|    time_elapsed    | 40416    |
|    total timesteps | 277852   |
| train/             |          |
|    actor_loss      | -94.5    |
|    critic_loss     | 0.117    |
|    learning_rate   | 0.001    |
|    n_updates       | 272850   |
---------------------------------
---------------------------------
| reward             | 0.714    |
| reward_ctrl        | 0.0807   |
| reward_motion      | 0.463    |
| reward_orientation | 0.0733   |
| reward_position    | 0        |
| reward_rotation    | 0.0846   |
| reward_velocity    | 0.0125   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    episodes        | 560      |
|    fps             | 6        |
|    time_elapsed    | 40543    |
|    total timesteps | 279852   |
| train/             |          |
|    actor_loss      | -94.9    |
|    critic_loss     | 0.126    |
|    learning_rate   | 0.001    |
|    n_updates       | 274850   |
---------------------------------
---------------------------------
| reward             | 0.722    |
| reward_ctrl        | 0.082    |
| reward_motion      | 0.472    |
| reward_orientation | 0.0731   |
| reward_position    | 0        |
| reward_rotation    | 0.0827   |
| reward_velocity    | 0.0126   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    episodes        | 564      |
|    fps             | 6        |
|    time_elapsed    | 40672    |
|    total timesteps | 281852   |
| train/             |          |
|    actor_loss      | -94.6    |
|    critic_loss     | 0.129    |
|    learning_rate   | 0.001    |
|    n_updates       | 276850   |
---------------------------------
Num timesteps: 282000
Best mean reward: 379.81 - Last mean reward per episode: 378.14
----------------------------------
| reward             | 0.724     |
| reward_ctrl        | 0.082     |
| reward_motion      | 0.47      |
| reward_orientation | 0.0726    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0836    |
| reward_velocity    | 0.0155    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 568       |
|    fps             | 6         |
|    time_elapsed    | 40799     |
|    total timesteps | 283852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.11      |
|    learning_rate   | 0.001     |
|    n_updates       | 278850    |
----------------------------------
----------------------------------
| reward             | 0.728     |
| reward_ctrl        | 0.0812    |
| reward_motion      | 0.472     |
| reward_orientation | 0.0729    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0863    |
| reward_velocity    | 0.0155    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 572       |
|    fps             | 6         |
|    time_elapsed    | 40925     |
|    total timesteps | 285852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 280850    |
----------------------------------
----------------------------------
| reward             | 0.726     |
| reward_ctrl        | 0.082     |
| reward_motion      | 0.468     |
| reward_orientation | 0.0732    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0875    |
| reward_velocity    | 0.0155    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 576       |
|    fps             | 7         |
|    time_elapsed    | 41050     |
|    total timesteps | 287852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.138     |
|    learning_rate   | 0.001     |
|    n_updates       | 282850    |
----------------------------------
Num timesteps: 288000
Best mean reward: 379.81 - Last mean reward per episode: 379.72
----------------------------------
| reward             | 0.739     |
| reward_ctrl        | 0.0824    |
| reward_motion      | 0.474     |
| reward_orientation | 0.0737    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0926    |
| reward_velocity    | 0.0159    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 580       |
|    fps             | 7         |
|    time_elapsed    | 41175     |
|    total timesteps | 289852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.109     |
|    learning_rate   | 0.001     |
|    n_updates       | 284850    |
----------------------------------
----------------------------------
| reward             | 0.74      |
| reward_ctrl        | 0.0793    |
| reward_motion      | 0.479     |
| reward_orientation | 0.0739    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0922    |
| reward_velocity    | 0.0162    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 584       |
|    fps             | 7         |
|    time_elapsed    | 41300     |
|    total timesteps | 291852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 286850    |
----------------------------------
----------------------------------
| reward             | 0.734     |
| reward_ctrl        | 0.0781    |
| reward_motion      | 0.474     |
| reward_orientation | 0.0738    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0912    |
| reward_velocity    | 0.016     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 588       |
|    fps             | 7         |
|    time_elapsed    | 41427     |
|    total timesteps | 293852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.113     |
|    learning_rate   | 0.001     |
|    n_updates       | 288850    |
----------------------------------
Num timesteps: 294000
Best mean reward: 379.81 - Last mean reward per episode: 380.16
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.73      |
| reward_ctrl        | 0.0779    |
| reward_motion      | 0.47      |
| reward_orientation | 0.0736    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0921    |
| reward_velocity    | 0.0158    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 592       |
|    fps             | 7         |
|    time_elapsed    | 41553     |
|    total timesteps | 295852    |
| train/             |           |
|    actor_loss      | -94       |
|    critic_loss     | 0.106     |
|    learning_rate   | 0.001     |
|    n_updates       | 290850    |
----------------------------------
----------------------------------
| reward             | 0.739     |
| reward_ctrl        | 0.079     |
| reward_motion      | 0.477     |
| reward_orientation | 0.0746    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0925    |
| reward_velocity    | 0.0159    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 596       |
|    fps             | 7         |
|    time_elapsed    | 41679     |
|    total timesteps | 297852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.127     |
|    learning_rate   | 0.001     |
|    n_updates       | 292850    |
----------------------------------
----------------------------------
| reward             | 0.745     |
| reward_ctrl        | 0.0804    |
| reward_motion      | 0.483     |
| reward_orientation | 0.0744    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0914    |
| reward_velocity    | 0.0158    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 600       |
|    fps             | 7         |
|    time_elapsed    | 41807     |
|    total timesteps | 299852    |
| train/             |           |
|    actor_loss      | -95.1     |
|    critic_loss     | 0.401     |
|    learning_rate   | 0.001     |
|    n_updates       | 294850    |
----------------------------------
Num timesteps: 300000
Best mean reward: 380.16 - Last mean reward per episode: 380.05
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.079     |
| reward_motion      | 0.491     |
| reward_orientation | 0.0748    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0916    |
| reward_velocity    | 0.016     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 604       |
|    fps             | 7         |
|    time_elapsed    | 41935     |
|    total timesteps | 301852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.109     |
|    learning_rate   | 0.001     |
|    n_updates       | 296850    |
----------------------------------
----------------------------------
| reward             | 0.757     |
| reward_ctrl        | 0.0789    |
| reward_motion      | 0.497     |
| reward_orientation | 0.0743    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.091     |
| reward_velocity    | 0.0157    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 608       |
|    fps             | 7         |
|    time_elapsed    | 42064     |
|    total timesteps | 303852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 298850    |
----------------------------------
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.0781    |
| reward_motion      | 0.497     |
| reward_orientation | 0.0734    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0894    |
| reward_velocity    | 0.0152    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 612       |
|    fps             | 7         |
|    time_elapsed    | 42193     |
|    total timesteps | 305852    |
| train/             |           |
|    actor_loss      | -95.2     |
|    critic_loss     | 0.117     |
|    learning_rate   | 0.001     |
|    n_updates       | 300850    |
----------------------------------
Num timesteps: 306000
Best mean reward: 380.16 - Last mean reward per episode: 379.65
----------------------------------
| reward             | 0.763     |
| reward_ctrl        | 0.0789    |
| reward_motion      | 0.506     |
| reward_orientation | 0.0734    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.091     |
| reward_velocity    | 0.0134    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 616       |
|    fps             | 7         |
|    time_elapsed    | 42334     |
|    total timesteps | 307852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 302850    |
----------------------------------
----------------------------------
| reward             | 0.758     |
| reward_ctrl        | 0.0782    |
| reward_motion      | 0.505     |
| reward_orientation | 0.0734    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0892    |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 620       |
|    fps             | 7         |
|    time_elapsed    | 42474     |
|    total timesteps | 309852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 304850    |
----------------------------------
----------------------------------
| reward             | 0.762     |
| reward_ctrl        | 0.0782    |
| reward_motion      | 0.512     |
| reward_orientation | 0.0729    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0879    |
| reward_velocity    | 0.0114    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 624       |
|    fps             | 7         |
|    time_elapsed    | 42605     |
|    total timesteps | 311852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.13      |
|    learning_rate   | 0.001     |
|    n_updates       | 306850    |
----------------------------------
Num timesteps: 312000
Best mean reward: 380.16 - Last mean reward per episode: 379.47
----------------------------------
| reward             | 0.761     |
| reward_ctrl        | 0.0801    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0728    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0891    |
| reward_velocity    | 0.0109    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 628       |
|    fps             | 7         |
|    time_elapsed    | 42736     |
|    total timesteps | 313852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.129     |
|    learning_rate   | 0.001     |
|    n_updates       | 308850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0827    |
| reward_motion      | 0.513     |
| reward_orientation | 0.0715    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0905    |
| reward_velocity    | 0.0137    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 632       |
|    fps             | 7         |
|    time_elapsed    | 42869     |
|    total timesteps | 315852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.117     |
|    learning_rate   | 0.001     |
|    n_updates       | 310850    |
----------------------------------
----------------------------------
| reward             | 0.773     |
| reward_ctrl        | 0.0843    |
| reward_motion      | 0.514     |
| reward_orientation | 0.0721    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0902    |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 636       |
|    fps             | 7         |
|    time_elapsed    | 43002     |
|    total timesteps | 317852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 312850    |
----------------------------------
Num timesteps: 318000
Best mean reward: 380.16 - Last mean reward per episode: 379.88
----------------------------------
| reward             | 0.78      |
| reward_ctrl        | 0.0849    |
| reward_motion      | 0.517     |
| reward_orientation | 0.072     |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0927    |
| reward_velocity    | 0.0137    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 640       |
|    fps             | 7         |
|    time_elapsed    | 43134     |
|    total timesteps | 319852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 314850    |
----------------------------------
----------------------------------
| reward             | 0.78      |
| reward_ctrl        | 0.0839    |
| reward_motion      | 0.517     |
| reward_orientation | 0.0715    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0947    |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 644       |
|    fps             | 7         |
|    time_elapsed    | 43269     |
|    total timesteps | 321852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.104     |
|    learning_rate   | 0.001     |
|    n_updates       | 316850    |
----------------------------------
----------------------------------
| reward             | 0.777     |
| reward_ctrl        | 0.0837    |
| reward_motion      | 0.513     |
| reward_orientation | 0.071     |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0952    |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 648       |
|    fps             | 7         |
|    time_elapsed    | 43402     |
|    total timesteps | 323852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 318850    |
----------------------------------
Num timesteps: 324000
Best mean reward: 380.16 - Last mean reward per episode: 380.95
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.083     |
| reward_motion      | 0.515     |
| reward_orientation | 0.071     |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0909    |
| reward_velocity    | 0.0142    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 652       |
|    fps             | 7         |
|    time_elapsed    | 43547     |
|    total timesteps | 325852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.114     |
|    learning_rate   | 0.001     |
|    n_updates       | 320850    |
----------------------------------
----------------------------------
| reward             | 0.783     |
| reward_ctrl        | 0.0844    |
| reward_motion      | 0.52      |
| reward_orientation | 0.0716    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0935    |
| reward_velocity    | 0.0137    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 656       |
|    fps             | 7         |
|    time_elapsed    | 43695     |
|    total timesteps | 327852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 322850    |
----------------------------------
----------------------------------
| reward             | 0.789     |
| reward_ctrl        | 0.0862    |
| reward_motion      | 0.522     |
| reward_orientation | 0.0712    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0957    |
| reward_velocity    | 0.0143    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 660       |
|    fps             | 7         |
|    time_elapsed    | 43841     |
|    total timesteps | 329852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.127     |
|    learning_rate   | 0.001     |
|    n_updates       | 324850    |
----------------------------------
Num timesteps: 330000
Best mean reward: 380.95 - Last mean reward per episode: 383.36
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.787     |
| reward_ctrl        | 0.0842    |
| reward_motion      | 0.523     |
| reward_orientation | 0.0712    |
| reward_position    | 1.56e-190 |
| reward_rotation    | 0.0946    |
| reward_velocity    | 0.014     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 664       |
|    fps             | 7         |
|    time_elapsed    | 43987     |
|    total timesteps | 331852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.104     |
|    learning_rate   | 0.001     |
|    n_updates       | 326850    |
----------------------------------
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.0856    |
| reward_motion      | 0.518     |
| reward_orientation | 0.0721    |
| reward_position    | 2.78e-300 |
| reward_rotation    | 0.0923    |
| reward_velocity    | 0.0109    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 668       |
|    fps             | 7         |
|    time_elapsed    | 44132     |
|    total timesteps | 333852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.117     |
|    learning_rate   | 0.001     |
|    n_updates       | 328850    |
----------------------------------
----------------------------------
| reward             | 0.783     |
| reward_ctrl        | 0.0873    |
| reward_motion      | 0.52      |
| reward_orientation | 0.0717    |
| reward_position    | 2.78e-300 |
| reward_rotation    | 0.0926    |
| reward_velocity    | 0.0106    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 672       |
|    fps             | 7         |
|    time_elapsed    | 44264     |
|    total timesteps | 335852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.122     |
|    learning_rate   | 0.001     |
|    n_updates       | 330850    |
----------------------------------
Num timesteps: 336000
Best mean reward: 383.36 - Last mean reward per episode: 382.26
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.0886    |
| reward_motion      | 0.513     |
| reward_orientation | 0.0719    |
| reward_position    | 2.78e-300 |
| reward_rotation    | 0.0951    |
| reward_velocity    | 0.011     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 676       |
|    fps             | 7         |
|    time_elapsed    | 44397     |
|    total timesteps | 337852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 332850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0878    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0714    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0929    |
| reward_velocity    | 0.0118    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 680       |
|    fps             | 7         |
|    time_elapsed    | 44529     |
|    total timesteps | 339852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.114     |
|    learning_rate   | 0.001     |
|    n_updates       | 334850    |
----------------------------------
----------------------------------
| reward             | 0.776     |
| reward_ctrl        | 0.0876    |
| reward_motion      | 0.509     |
| reward_orientation | 0.0714    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0944    |
| reward_velocity    | 0.0139    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 684       |
|    fps             | 7         |
|    time_elapsed    | 44672     |
|    total timesteps | 341852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.0999    |
|    learning_rate   | 0.001     |
|    n_updates       | 336850    |
----------------------------------
Num timesteps: 342000
Best mean reward: 383.36 - Last mean reward per episode: 382.28
----------------------------------
| reward             | 0.775     |
| reward_ctrl        | 0.0874    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0715    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0936    |
| reward_velocity    | 0.014     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 688       |
|    fps             | 7         |
|    time_elapsed    | 44817     |
|    total timesteps | 343852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.137     |
|    learning_rate   | 0.001     |
|    n_updates       | 338850    |
----------------------------------
----------------------------------
| reward             | 0.78      |
| reward_ctrl        | 0.0889    |
| reward_motion      | 0.51      |
| reward_orientation | 0.0719    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0946    |
| reward_velocity    | 0.0143    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 692       |
|    fps             | 7         |
|    time_elapsed    | 44964     |
|    total timesteps | 345852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 340850    |
----------------------------------
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.0891    |
| reward_motion      | 0.508     |
| reward_orientation | 0.0709    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0915    |
| reward_velocity    | 0.0145    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 696       |
|    fps             | 7         |
|    time_elapsed    | 45109     |
|    total timesteps | 347852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 342850    |
----------------------------------
Num timesteps: 348000
Best mean reward: 383.36 - Last mean reward per episode: 382.08
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.0875    |
| reward_motion      | 0.508     |
| reward_orientation | 0.071     |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0907    |
| reward_velocity    | 0.0145    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 700       |
|    fps             | 7         |
|    time_elapsed    | 45258     |
|    total timesteps | 349852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.11      |
|    learning_rate   | 0.001     |
|    n_updates       | 344850    |
----------------------------------
----------------------------------
| reward             | 0.767     |
| reward_ctrl        | 0.0877    |
| reward_motion      | 0.503     |
| reward_orientation | 0.0705    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0922    |
| reward_velocity    | 0.0138    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 704       |
|    fps             | 7         |
|    time_elapsed    | 45403     |
|    total timesteps | 351852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 346850    |
----------------------------------
----------------------------------
| reward             | 0.772     |
| reward_ctrl        | 0.09      |
| reward_motion      | 0.503     |
| reward_orientation | 0.0708    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0938    |
| reward_velocity    | 0.0138    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 708       |
|    fps             | 7         |
|    time_elapsed    | 45549     |
|    total timesteps | 353852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 348850    |
----------------------------------
Num timesteps: 354000
Best mean reward: 383.36 - Last mean reward per episode: 383.03
----------------------------------
| reward             | 0.778     |
| reward_ctrl        | 0.0906    |
| reward_motion      | 0.503     |
| reward_orientation | 0.0717    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.098     |
| reward_velocity    | 0.0141    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 712       |
|    fps             | 7         |
|    time_elapsed    | 45685     |
|    total timesteps | 355852    |
| train/             |           |
|    actor_loss      | -95.1     |
|    critic_loss     | 0.134     |
|    learning_rate   | 0.001     |
|    n_updates       | 350850    |
----------------------------------
----------------------------------
| reward             | 0.773     |
| reward_ctrl        | 0.0929    |
| reward_motion      | 0.492     |
| reward_orientation | 0.0717    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0992    |
| reward_velocity    | 0.0163    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 716       |
|    fps             | 7         |
|    time_elapsed    | 45819     |
|    total timesteps | 357852    |
| train/             |           |
|    actor_loss      | -95       |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 352850    |
----------------------------------
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.0933    |
| reward_motion      | 0.497     |
| reward_orientation | 0.0718    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0164    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 384       |
| time/              |           |
|    episodes        | 720       |
|    fps             | 7         |
|    time_elapsed    | 45966     |
|    total timesteps | 359852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.132     |
|    learning_rate   | 0.001     |
|    n_updates       | 354850    |
----------------------------------
Num timesteps: 360000
Best mean reward: 383.36 - Last mean reward per episode: 384.02
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.776     |
| reward_ctrl        | 0.0934    |
| reward_motion      | 0.493     |
| reward_orientation | 0.072     |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0162    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 384       |
| time/              |           |
|    episodes        | 724       |
|    fps             | 7         |
|    time_elapsed    | 46113     |
|    total timesteps | 361852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 356850    |
----------------------------------
----------------------------------
| reward             | 0.773     |
| reward_ctrl        | 0.0919    |
| reward_motion      | 0.488     |
| reward_orientation | 0.0711    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.104     |
| reward_velocity    | 0.0177    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 728       |
|    fps             | 7         |
|    time_elapsed    | 46261     |
|    total timesteps | 363852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.123     |
|    learning_rate   | 0.001     |
|    n_updates       | 358850    |
----------------------------------
----------------------------------
| reward             | 0.762     |
| reward_ctrl        | 0.0913    |
| reward_motion      | 0.477     |
| reward_orientation | 0.0722    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.103     |
| reward_velocity    | 0.0177    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 383       |
| time/              |           |
|    episodes        | 732       |
|    fps             | 7         |
|    time_elapsed    | 46407     |
|    total timesteps | 365852    |
| train/             |           |
|    actor_loss      | -95.1     |
|    critic_loss     | 0.101     |
|    learning_rate   | 0.001     |
|    n_updates       | 360850    |
----------------------------------
Num timesteps: 366000
Best mean reward: 384.02 - Last mean reward per episode: 382.77
----------------------------------
| reward             | 0.755     |
| reward_ctrl        | 0.0914    |
| reward_motion      | 0.473     |
| reward_orientation | 0.0718    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.102     |
| reward_velocity    | 0.0162    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 736       |
|    fps             | 7         |
|    time_elapsed    | 46557     |
|    total timesteps | 367852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 362850    |
----------------------------------
----------------------------------
| reward             | 0.753     |
| reward_ctrl        | 0.0927    |
| reward_motion      | 0.473     |
| reward_orientation | 0.0722    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0997    |
| reward_velocity    | 0.0158    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 740       |
|    fps             | 7         |
|    time_elapsed    | 46693     |
|    total timesteps | 369852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 364850    |
----------------------------------
----------------------------------
| reward             | 0.759     |
| reward_ctrl        | 0.0948    |
| reward_motion      | 0.476     |
| reward_orientation | 0.0725    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.0993    |
| reward_velocity    | 0.0158    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 744       |
|    fps             | 7         |
|    time_elapsed    | 46823     |
|    total timesteps | 371852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 366850    |
----------------------------------
Num timesteps: 372000
Best mean reward: 384.02 - Last mean reward per episode: 380.26
----------------------------------
| reward             | 0.764     |
| reward_ctrl        | 0.096     |
| reward_motion      | 0.477     |
| reward_orientation | 0.0733    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.102     |
| reward_velocity    | 0.0162    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 748       |
|    fps             | 7         |
|    time_elapsed    | 46953     |
|    total timesteps | 373852    |
| train/             |           |
|    actor_loss      | -95       |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 368850    |
----------------------------------
----------------------------------
| reward             | 0.766     |
| reward_ctrl        | 0.0967    |
| reward_motion      | 0.475     |
| reward_orientation | 0.0734    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.105     |
| reward_velocity    | 0.0156    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 752       |
|    fps             | 7         |
|    time_elapsed    | 47093     |
|    total timesteps | 375852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 370850    |
----------------------------------
----------------------------------
| reward             | 0.757     |
| reward_ctrl        | 0.0961    |
| reward_motion      | 0.469     |
| reward_orientation | 0.0727    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.103     |
| reward_velocity    | 0.0157    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 756       |
|    fps             | 7         |
|    time_elapsed    | 47236     |
|    total timesteps | 377852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 372850    |
----------------------------------
Num timesteps: 378000
Best mean reward: 384.02 - Last mean reward per episode: 377.43
----------------------------------
| reward             | 0.744     |
| reward_ctrl        | 0.095     |
| reward_motion      | 0.458     |
| reward_orientation | 0.0726    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.104     |
| reward_velocity    | 0.0151    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 377       |
| time/              |           |
|    episodes        | 760       |
|    fps             | 8         |
|    time_elapsed    | 47377     |
|    total timesteps | 379852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 374850    |
----------------------------------
----------------------------------
| reward             | 0.749     |
| reward_ctrl        | 0.0942    |
| reward_motion      | 0.46      |
| reward_orientation | 0.0728    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.105     |
| reward_velocity    | 0.0164    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 764       |
|    fps             | 8         |
|    time_elapsed    | 47519     |
|    total timesteps | 381852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.132     |
|    learning_rate   | 0.001     |
|    n_updates       | 376850    |
----------------------------------
----------------------------------
| reward             | 0.759     |
| reward_ctrl        | 0.0935    |
| reward_motion      | 0.466     |
| reward_orientation | 0.0725    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.109     |
| reward_velocity    | 0.0176    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 768       |
|    fps             | 8         |
|    time_elapsed    | 47663     |
|    total timesteps | 383852    |
| train/             |           |
|    actor_loss      | -94.9     |
|    critic_loss     | 0.12      |
|    learning_rate   | 0.001     |
|    n_updates       | 378850    |
----------------------------------
Num timesteps: 384000
Best mean reward: 384.02 - Last mean reward per episode: 378.70
----------------------------------
| reward             | 0.752     |
| reward_ctrl        | 0.0939    |
| reward_motion      | 0.46      |
| reward_orientation | 0.0725    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.108     |
| reward_velocity    | 0.0177    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 772       |
|    fps             | 8         |
|    time_elapsed    | 47803     |
|    total timesteps | 385852    |
| train/             |           |
|    actor_loss      | -95       |
|    critic_loss     | 0.144     |
|    learning_rate   | 0.001     |
|    n_updates       | 380850    |
----------------------------------
----------------------------------
| reward             | 0.762     |
| reward_ctrl        | 0.0935    |
| reward_motion      | 0.471     |
| reward_orientation | 0.0724    |
| reward_position    | 8.11e-253 |
| reward_rotation    | 0.107     |
| reward_velocity    | 0.018     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 776       |
|    fps             | 8         |
|    time_elapsed    | 47946     |
|    total timesteps | 387852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.134     |
|    learning_rate   | 0.001     |
|    n_updates       | 382850    |
----------------------------------
----------------------------------
| reward             | 0.774     |
| reward_ctrl        | 0.094     |
| reward_motion      | 0.479     |
| reward_orientation | 0.0725    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.11      |
| reward_velocity    | 0.0183    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 780       |
|    fps             | 8         |
|    time_elapsed    | 48091     |
|    total timesteps | 389852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 384850    |
----------------------------------
Num timesteps: 390000
Best mean reward: 384.02 - Last mean reward per episode: 379.25
----------------------------------
| reward             | 0.771     |
| reward_ctrl        | 0.0977    |
| reward_motion      | 0.475     |
| reward_orientation | 0.0721    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.016     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 784       |
|    fps             | 8         |
|    time_elapsed    | 48234     |
|    total timesteps | 391852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.126     |
|    learning_rate   | 0.001     |
|    n_updates       | 386850    |
----------------------------------
----------------------------------
| reward             | 0.777     |
| reward_ctrl        | 0.0986    |
| reward_motion      | 0.479     |
| reward_orientation | 0.0722    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.0171    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 788       |
|    fps             | 8         |
|    time_elapsed    | 48376     |
|    total timesteps | 393852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.133     |
|    learning_rate   | 0.001     |
|    n_updates       | 388850    |
----------------------------------
----------------------------------
| reward             | 0.775     |
| reward_ctrl        | 0.0995    |
| reward_motion      | 0.474     |
| reward_orientation | 0.0715    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0173    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 792       |
|    fps             | 8         |
|    time_elapsed    | 48521     |
|    total timesteps | 395852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.114     |
|    learning_rate   | 0.001     |
|    n_updates       | 390850    |
----------------------------------
Num timesteps: 396000
Best mean reward: 384.02 - Last mean reward per episode: 380.19
----------------------------------
| reward             | 0.783     |
| reward_ctrl        | 0.1       |
| reward_motion      | 0.477     |
| reward_orientation | 0.0724    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.117     |
| reward_velocity    | 0.017     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 796       |
|    fps             | 8         |
|    time_elapsed    | 48672     |
|    total timesteps | 397852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.148     |
|    learning_rate   | 0.001     |
|    n_updates       | 392850    |
----------------------------------
----------------------------------
| reward             | 0.783     |
| reward_ctrl        | 0.0998    |
| reward_motion      | 0.48      |
| reward_orientation | 0.0724    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.0171    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 800       |
|    fps             | 8         |
|    time_elapsed    | 48801     |
|    total timesteps | 399852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.143     |
|    learning_rate   | 0.001     |
|    n_updates       | 394850    |
----------------------------------
----------------------------------
| reward             | 0.79      |
| reward_ctrl        | 0.101     |
| reward_motion      | 0.485     |
| reward_orientation | 0.0727    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.0177    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 804       |
|    fps             | 8         |
|    time_elapsed    | 48932     |
|    total timesteps | 401852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 396850    |
----------------------------------
Num timesteps: 402000
Best mean reward: 384.02 - Last mean reward per episode: 382.49
----------------------------------
| reward             | 0.788     |
| reward_ctrl        | 0.0992    |
| reward_motion      | 0.485     |
| reward_orientation | 0.0731    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0184    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 382       |
| time/              |           |
|    episodes        | 808       |
|    fps             | 8         |
|    time_elapsed    | 49060     |
|    total timesteps | 403852    |
| train/             |           |
|    actor_loss      | -94.2     |
|    critic_loss     | 0.128     |
|    learning_rate   | 0.001     |
|    n_updates       | 398850    |
----------------------------------
----------------------------------
| reward             | 0.776     |
| reward_ctrl        | 0.101     |
| reward_motion      | 0.473     |
| reward_orientation | 0.0732    |
| reward_position    | 9.66e-273 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.0182    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 812       |
|    fps             | 8         |
|    time_elapsed    | 49189     |
|    total timesteps | 405852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.119     |
|    learning_rate   | 0.001     |
|    n_updates       | 400850    |
----------------------------------
----------------------------------
| reward             | 0.782     |
| reward_ctrl        | 0.0999    |
| reward_motion      | 0.481     |
| reward_orientation | 0.0725    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0164    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 816       |
|    fps             | 8         |
|    time_elapsed    | 49320     |
|    total timesteps | 407852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.119     |
|    learning_rate   | 0.001     |
|    n_updates       | 402850    |
----------------------------------
Num timesteps: 408000
Best mean reward: 384.02 - Last mean reward per episode: 381.21
----------------------------------
| reward             | 0.78      |
| reward_ctrl        | 0.101     |
| reward_motion      | 0.475     |
| reward_orientation | 0.0725    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.0163    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 820       |
|    fps             | 8         |
|    time_elapsed    | 49460     |
|    total timesteps | 409852    |
| train/             |           |
|    actor_loss      | -94.3     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 404850    |
----------------------------------
----------------------------------
| reward             | 0.781     |
| reward_ctrl        | 0.102     |
| reward_motion      | 0.477     |
| reward_orientation | 0.0724    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.0162    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 824       |
|    fps             | 8         |
|    time_elapsed    | 49598     |
|    total timesteps | 411852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 406850    |
----------------------------------
----------------------------------
| reward             | 0.778     |
| reward_ctrl        | 0.105     |
| reward_motion      | 0.475     |
| reward_orientation | 0.0729    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.11      |
| reward_velocity    | 0.0146    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 381       |
| time/              |           |
|    episodes        | 828       |
|    fps             | 8         |
|    time_elapsed    | 49738     |
|    total timesteps | 413852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.124     |
|    learning_rate   | 0.001     |
|    n_updates       | 408850    |
----------------------------------
Num timesteps: 414000
Best mean reward: 384.02 - Last mean reward per episode: 380.61
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.106     |
| reward_motion      | 0.477     |
| reward_orientation | 0.0732    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.111     |
| reward_velocity    | 0.0117    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 832       |
|    fps             | 8         |
|    time_elapsed    | 49877     |
|    total timesteps | 415852    |
| train/             |           |
|    actor_loss      | -95.2     |
|    critic_loss     | 0.127     |
|    learning_rate   | 0.001     |
|    n_updates       | 410850    |
----------------------------------
----------------------------------
| reward             | 0.779     |
| reward_ctrl        | 0.105     |
| reward_motion      | 0.474     |
| reward_orientation | 0.0728    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.0124    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 836       |
|    fps             | 8         |
|    time_elapsed    | 50014     |
|    total timesteps | 417852    |
| train/             |           |
|    actor_loss      | -95.1     |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 412850    |
----------------------------------
----------------------------------
| reward             | 0.778     |
| reward_ctrl        | 0.104     |
| reward_motion      | 0.473     |
| reward_orientation | 0.0727    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.116     |
| reward_velocity    | 0.0125    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 840       |
|    fps             | 8         |
|    time_elapsed    | 50152     |
|    total timesteps | 419852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.116     |
|    learning_rate   | 0.001     |
|    n_updates       | 414850    |
----------------------------------
Num timesteps: 420000
Best mean reward: 384.02 - Last mean reward per episode: 380.17
----------------------------------
| reward             | 0.771     |
| reward_ctrl        | 0.105     |
| reward_motion      | 0.465     |
| reward_orientation | 0.0727    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.014     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 378       |
| time/              |           |
|    episodes        | 844       |
|    fps             | 8         |
|    time_elapsed    | 50283     |
|    total timesteps | 421852    |
| train/             |           |
|    actor_loss      | -94.5     |
|    critic_loss     | 0.109     |
|    learning_rate   | 0.001     |
|    n_updates       | 416850    |
----------------------------------
----------------------------------
| reward             | 0.771     |
| reward_ctrl        | 0.106     |
| reward_motion      | 0.462     |
| reward_orientation | 0.0729    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.116     |
| reward_velocity    | 0.0136    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 848       |
|    fps             | 8         |
|    time_elapsed    | 50406     |
|    total timesteps | 423852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.118     |
|    learning_rate   | 0.001     |
|    n_updates       | 418850    |
----------------------------------
----------------------------------
| reward             | 0.765     |
| reward_ctrl        | 0.104     |
| reward_motion      | 0.456     |
| reward_orientation | 0.0733    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.118     |
| reward_velocity    | 0.0135    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 852       |
|    fps             | 8         |
|    time_elapsed    | 50531     |
|    total timesteps | 425852    |
| train/             |           |
|    actor_loss      | -94.8     |
|    critic_loss     | 0.116     |
|    learning_rate   | 0.001     |
|    n_updates       | 420850    |
----------------------------------
Num timesteps: 426000
Best mean reward: 384.02 - Last mean reward per episode: 378.80
----------------------------------
| reward             | 0.769     |
| reward_ctrl        | 0.105     |
| reward_motion      | 0.459     |
| reward_orientation | 0.0739    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.116     |
| reward_velocity    | 0.0143    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 856       |
|    fps             | 8         |
|    time_elapsed    | 50656     |
|    total timesteps | 427852    |
| train/             |           |
|    actor_loss      | -94.6     |
|    critic_loss     | 0.138     |
|    learning_rate   | 0.001     |
|    n_updates       | 422850    |
----------------------------------
----------------------------------
| reward             | 0.782     |
| reward_ctrl        | 0.105     |
| reward_motion      | 0.471     |
| reward_orientation | 0.0744    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.118     |
| reward_velocity    | 0.0146    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 860       |
|    fps             | 8         |
|    time_elapsed    | 50793     |
|    total timesteps | 429852    |
| train/             |           |
|    actor_loss      | -95.1     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 424850    |
----------------------------------
----------------------------------
| reward             | 0.777     |
| reward_ctrl        | 0.106     |
| reward_motion      | 0.462     |
| reward_orientation | 0.0747    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.121     |
| reward_velocity    | 0.0136    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 379       |
| time/              |           |
|    episodes        | 864       |
|    fps             | 8         |
|    time_elapsed    | 50931     |
|    total timesteps | 431852    |
| train/             |           |
|    actor_loss      | -94.7     |
|    critic_loss     | 0.134     |
|    learning_rate   | 0.001     |
|    n_updates       | 426850    |
----------------------------------
Num timesteps: 432000
Best mean reward: 384.02 - Last mean reward per episode: 379.14
----------------------------------
| reward             | 0.771     |
| reward_ctrl        | 0.108     |
| reward_motion      | 0.456     |
| reward_orientation | 0.0752    |
| reward_position    | 2.16e-274 |
| reward_rotation    | 0.12      |
| reward_velocity    | 0.0125    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 380       |
| time/              |           |
|    episodes        | 868       |
|    fps             | 8         |
|    time_elapsed    | 51071     |
|    total timesteps | 433852    |
| train/             |           |
|    actor_loss      | -94.4     |
|    critic_loss     | 0.128     |
|    learning_rate   | 0.001     |
|    n_updates       | 428850    |
----------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 194, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 467, in __init__
    super(AntEnvV4, self).__init__(path)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 84, in __init__
    mujoco_env.MujocoEnv.__init__(self, path, 5)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 64, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 599, in step
    self.render()
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 160, in render
    self._get_viewer(mode).render()
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 172, in _get_viewer
    self.viewer = mujoco_py.MjViewer(self.sim)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/mjviewer.py", line 133, in __init__
    super().__init__(sim)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/mjviewer.py", line 26, in __init__
    super().__init__(sim)
  File "mujoco_py/mjrendercontext.pyx", line 278, in mujoco_py.cymj.MjRenderContextWindow.__init__
  File "mujoco_py/mjrendercontext.pyx", line 43, in mujoco_py.cymj.MjRenderContext.__init__
  File "mujoco_py/mjrendercontext.pyx", line 96, in mujoco_py.cymj.MjRenderContext._setup_opengl_context
  File "mujoco_py/opengl_context.pyx", line 44, in mujoco_py.cymj.GlfwContext.__init__
  File "mujoco_py/opengl_context.pyx", line 59, in mujoco_py.cymj.GlfwContext._init_glfw
  File "/usr/local/lib/python3.8/dist-packages/glfw-2.1.0-py3.8-linux-x86_64.egg/glfw/__init__.py", line 847, in set_error_callback
    def set_error_callback(cbfun):
KeyboardInterrupt
2021-06-01 14:54:40.706280: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 14:54:40.706344: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_22
---------------------------------
| reward             | 0.659    |
| reward_ctrl        | 0.0641   |
| reward_motion      | 0.439    |
| reward_orientation | 0.0743   |
| reward_position    | 0        |
| reward_rotation    | 0.0811   |
| reward_velocity    | 6.02e-05 |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 368      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 63       |
|    time_elapsed    | 31       |
|    total timesteps | 2000     |
---------------------------------
---------------------------------
| reward             | 0.696    |
| reward_ctrl        | 0.0713   |
| reward_motion      | 0.492    |
| reward_orientation | 0.0665   |
| reward_position    | 0        |
| reward_rotation    | 0.0651   |
| reward_velocity    | 0.000558 |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 370      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 65       |
|    time_elapsed    | 60       |
|    total timesteps | 4000     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 355.73
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.707     |
| reward_ctrl        | 0.0862    |
| reward_motion      | 0.444     |
| reward_orientation | 0.0675    |
| reward_position    | 6.11e-294 |
| reward_rotation    | 0.101     |
| reward_velocity    | 0.0088    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 356       |
| time/              |           |
|    episodes        | 12        |
|    fps             | 49        |
|    time_elapsed    | 120       |
|    total timesteps | 6000      |
| train/             |           |
|    actor_loss      | -2.72     |
|    critic_loss     | 0.111     |
|    learning_rate   | 0.001     |
|    n_updates       | 995       |
----------------------------------
----------------------------------
| reward             | 0.668     |
| reward_ctrl        | 0.109     |
| reward_motion      | 0.37      |
| reward_orientation | 0.0718    |
| reward_position    | 1.68e-249 |
| reward_rotation    | 0.106     |
| reward_velocity    | 0.0114    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 340       |
| time/              |           |
|    episodes        | 16        |
|    fps             | 37        |
|    time_elapsed    | 211       |
|    total timesteps | 8000      |
| train/             |           |
|    actor_loss      | -7.34     |
|    critic_loss     | 0.11      |
|    learning_rate   | 0.001     |
|    n_updates       | 2995      |
----------------------------------
----------------------------------
| reward             | 0.626     |
| reward_ctrl        | 0.123     |
| reward_motion      | 0.307     |
| reward_orientation | 0.0738    |
| reward_position    | 1.32e-249 |
| reward_rotation    | 0.113     |
| reward_velocity    | 0.00959   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 327       |
| time/              |           |
|    episodes        | 20        |
|    fps             | 33        |
|    time_elapsed    | 300       |
|    total timesteps | 10000     |
| train/             |           |
|    actor_loss      | -11.4     |
|    critic_loss     | 0.1       |
|    learning_rate   | 0.001     |
|    n_updates       | 4995      |
----------------------------------
Num timesteps: 12000
Best mean reward: 355.73 - Last mean reward per episode: 318.55
----------------------------------
| reward             | 0.636     |
| reward_ctrl        | 0.131     |
| reward_motion      | 0.306     |
| reward_orientation | 0.0754    |
| reward_position    | 1.09e-249 |
| reward_rotation    | 0.11      |
| reward_velocity    | 0.0133    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 319       |
| time/              |           |
|    episodes        | 24        |
|    fps             | 30        |
|    time_elapsed    | 390       |
|    total timesteps | 12000     |
| train/             |           |
|    actor_loss      | -15.3     |
|    critic_loss     | 0.112     |
|    learning_rate   | 0.001     |
|    n_updates       | 6995      |
----------------------------------
----------------------------------
| reward             | 0.613     |
| reward_ctrl        | 0.137     |
| reward_motion      | 0.274     |
| reward_orientation | 0.0765    |
| reward_position    | 9.31e-250 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.0114    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 314       |
| time/              |           |
|    episodes        | 28        |
|    fps             | 29        |
|    time_elapsed    | 481       |
|    total timesteps | 14000     |
| train/             |           |
|    actor_loss      | -19.1     |
|    critic_loss     | 0.115     |
|    learning_rate   | 0.001     |
|    n_updates       | 8995      |
----------------------------------
----------------------------------
| reward             | 0.608     |
| reward_ctrl        | 0.142     |
| reward_motion      | 0.25      |
| reward_orientation | 0.0774    |
| reward_position    | 8.11e-250 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.0104    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 312       |
| time/              |           |
|    episodes        | 32        |
|    fps             | 27        |
|    time_elapsed    | 573       |
|    total timesteps | 16000     |
| train/             |           |
|    actor_loss      | -22.3     |
|    critic_loss     | 0.11      |
|    learning_rate   | 0.001     |
|    n_updates       | 10995     |
----------------------------------
Num timesteps: 18000
Best mean reward: 355.73 - Last mean reward per episode: 310.66
----------------------------------
| reward             | 0.597     |
| reward_ctrl        | 0.145     |
| reward_motion      | 0.229     |
| reward_orientation | 0.0781    |
| reward_position    | 7.18e-250 |
| reward_rotation    | 0.134     |
| reward_velocity    | 0.0113    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 311       |
| time/              |           |
|    episodes        | 36        |
|    fps             | 26        |
|    time_elapsed    | 670       |
|    total timesteps | 18000     |
| train/             |           |
|    actor_loss      | -25.6     |
|    critic_loss     | 0.121     |
|    learning_rate   | 0.001     |
|    n_updates       | 12995     |
----------------------------------
----------------------------------
| reward             | 0.585     |
| reward_ctrl        | 0.148     |
| reward_motion      | 0.209     |
| reward_orientation | 0.0786    |
| reward_position    | 6.44e-250 |
| reward_rotation    | 0.139     |
| reward_velocity    | 0.0105    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 309       |
| time/              |           |
|    episodes        | 40        |
|    fps             | 26        |
|    time_elapsed    | 766       |
|    total timesteps | 20000     |
| train/             |           |
|    actor_loss      | -28.9     |
|    critic_loss     | 0.122     |
|    learning_rate   | 0.001     |
|    n_updates       | 14995     |
----------------------------------
----------------------------------
| reward             | 0.581     |
| reward_ctrl        | 0.15      |
| reward_motion      | 0.209     |
| reward_orientation | 0.0791    |
| reward_position    | 5.84e-250 |
| reward_rotation    | 0.134     |
| reward_velocity    | 0.00998   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 303       |
| time/              |           |
|    episodes        | 44        |
|    fps             | 25        |
|    time_elapsed    | 862       |
|    total timesteps | 22000     |
| train/             |           |
|    actor_loss      | -31.6     |
|    critic_loss     | 0.132     |
|    learning_rate   | 0.001     |
|    n_updates       | 16995     |
----------------------------------
Num timesteps: 24000
Best mean reward: 355.73 - Last mean reward per episode: 301.07
----------------------------------
| reward             | 0.588     |
| reward_ctrl        | 0.151     |
| reward_motion      | 0.216     |
| reward_orientation | 0.0792    |
| reward_position    | 5.35e-250 |
| reward_rotation    | 0.132     |
| reward_velocity    | 0.00926   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 301       |
| time/              |           |
|    episodes        | 48        |
|    fps             | 25        |
|    time_elapsed    | 957       |
|    total timesteps | 24000     |
| train/             |           |
|    actor_loss      | -34.3     |
|    critic_loss     | 0.147     |
|    learning_rate   | 0.001     |
|    n_updates       | 18995     |
----------------------------------
----------------------------------
| reward             | 0.586     |
| reward_ctrl        | 0.153     |
| reward_motion      | 0.218     |
| reward_orientation | 0.0795    |
| reward_position    | 4.93e-250 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.00896   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 299       |
| time/              |           |
|    episodes        | 52        |
|    fps             | 24        |
|    time_elapsed    | 1052      |
|    total timesteps | 26000     |
| train/             |           |
|    actor_loss      | -37       |
|    critic_loss     | 0.144     |
|    learning_rate   | 0.001     |
|    n_updates       | 20995     |
----------------------------------
----------------------------------
| reward             | 0.585     |
| reward_ctrl        | 0.154     |
| reward_motion      | 0.215     |
| reward_orientation | 0.0796    |
| reward_position    | 4.57e-250 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.00949   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 299       |
| time/              |           |
|    episodes        | 56        |
|    fps             | 24        |
|    time_elapsed    | 1149      |
|    total timesteps | 28000     |
| train/             |           |
|    actor_loss      | -39.3     |
|    critic_loss     | 0.143     |
|    learning_rate   | 0.001     |
|    n_updates       | 22995     |
----------------------------------
Num timesteps: 30000
Best mean reward: 355.73 - Last mean reward per episode: 297.96
----------------------------------
| reward             | 0.578     |
| reward_ctrl        | 0.155     |
| reward_motion      | 0.203     |
| reward_orientation | 0.0799    |
| reward_position    | 4.26e-250 |
| reward_rotation    | 0.131     |
| reward_velocity    | 0.00909   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 298       |
| time/              |           |
|    episodes        | 60        |
|    fps             | 24        |
|    time_elapsed    | 1247      |
|    total timesteps | 30000     |
| train/             |           |
|    actor_loss      | -41.6     |
|    critic_loss     | 0.145     |
|    learning_rate   | 0.001     |
|    n_updates       | 24995     |
----------------------------------
----------------------------------
| reward             | 0.566     |
| reward_ctrl        | 0.156     |
| reward_motion      | 0.192     |
| reward_orientation | 0.0799    |
| reward_position    | 3.99e-250 |
| reward_rotation    | 0.129     |
| reward_velocity    | 0.00893   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 296       |
| time/              |           |
|    episodes        | 64        |
|    fps             | 23        |
|    time_elapsed    | 1343      |
|    total timesteps | 32000     |
| train/             |           |
|    actor_loss      | -43.9     |
|    critic_loss     | 0.146     |
|    learning_rate   | 0.001     |
|    n_updates       | 26995     |
----------------------------------
----------------------------------
| reward             | 0.556     |
| reward_ctrl        | 0.157     |
| reward_motion      | 0.183     |
| reward_orientation | 0.0801    |
| reward_position    | 3.75e-250 |
| reward_rotation    | 0.126     |
| reward_velocity    | 0.00883   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 294       |
| time/              |           |
|    episodes        | 68        |
|    fps             | 23        |
|    time_elapsed    | 1438      |
|    total timesteps | 34000     |
| train/             |           |
|    actor_loss      | -46       |
|    critic_loss     | 0.149     |
|    learning_rate   | 0.001     |
|    n_updates       | 28995     |
----------------------------------
Num timesteps: 36000
Best mean reward: 355.73 - Last mean reward per episode: 292.35
----------------------------------
| reward             | 0.553     |
| reward_ctrl        | 0.158     |
| reward_motion      | 0.179     |
| reward_orientation | 0.0802    |
| reward_position    | 3.54e-250 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0088    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 292       |
| time/              |           |
|    episodes        | 72        |
|    fps             | 23        |
|    time_elapsed    | 1528      |
|    total timesteps | 36000     |
| train/             |           |
|    actor_loss      | -48       |
|    critic_loss     | 0.152     |
|    learning_rate   | 0.001     |
|    n_updates       | 30995     |
----------------------------------
2021-06-01 15:21:34.286069: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 15:21:34.286255: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_23
---------------------------------
| reward             | 0.665    |
| reward_ctrl        | 0.0666   |
| reward_motion      | 0.296    |
| reward_orientation | 0.0761   |
| reward_position    | 0        |
| reward_rotation    | 0.161    |
| reward_velocity    | 0.0648   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 62       |
|    time_elapsed    | 32       |
|    total timesteps | 2000     |
---------------------------------
---------------------------------
| reward             | 0.572    |
| reward_ctrl        | 0.0801   |
| reward_motion      | 0.211    |
| reward_orientation | 0.0785   |
| reward_position    | 0        |
| reward_rotation    | 0.172    |
| reward_velocity    | 0.0304   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 65       |
|    time_elapsed    | 61       |
|    total timesteps | 4000     |
---------------------------------
2021-06-01 15:23:58.342514: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 15:23:58.342573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_24
---------------------------------
| reward             | 0.604    |
| reward_ctrl        | 0.0762   |
| reward_motion      | 0.269    |
| reward_orientation | 0.0768   |
| reward_position    | 0        |
| reward_rotation    | 0.157    |
| reward_velocity    | 0.0251   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 295      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 60       |
|    time_elapsed    | 32       |
|    total timesteps | 2000     |
---------------------------------
---------------------------------
| reward             | 0.589    |
| reward_ctrl        | 0.0696   |
| reward_motion      | 0.288    |
| reward_orientation | 0.0738   |
| reward_position    | 0        |
| reward_rotation    | 0.146    |
| reward_velocity    | 0.0109   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 282      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 65       |
|    time_elapsed    | 60       |
|    total timesteps | 4000     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 278.68
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.592    |
| reward_ctrl        | 0.0707   |
| reward_motion      | 0.284    |
| reward_orientation | 0.0758   |
| reward_position    | 0        |
| reward_rotation    | 0.153    |
| reward_velocity    | 0.00802  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 279      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 55       |
|    time_elapsed    | 107      |
|    total timesteps | 6000     |
| train/             |          |
|    actor_loss      | -3.69    |
|    critic_loss     | 0.156    |
|    learning_rate   | 0.001    |
|    n_updates       | 995      |
---------------------------------
---------------------------------
| reward             | 0.559    |
| reward_ctrl        | 0.0799   |
| reward_motion      | 0.224    |
| reward_orientation | 0.078    |
| reward_position    | 0        |
| reward_rotation    | 0.17     |
| reward_velocity    | 0.00668  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 45       |
|    time_elapsed    | 174      |
|    total timesteps | 8000     |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.144    |
|    learning_rate   | 0.001    |
|    n_updates       | 2995     |
---------------------------------
---------------------------------
| reward             | 0.538    |
| reward_ctrl        | 0.093    |
| reward_motion      | 0.187    |
| reward_orientation | 0.0792   |
| reward_position    | 0        |
| reward_rotation    | 0.171    |
| reward_velocity    | 0.00768  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 41       |
|    time_elapsed    | 241      |
|    total timesteps | 10000    |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.146    |
|    learning_rate   | 0.001    |
|    n_updates       | 4995     |
---------------------------------
Num timesteps: 12000
Best mean reward: 278.68 - Last mean reward per episode: 258.76
---------------------------------
| reward             | 0.524    |
| reward_ctrl        | 0.0979   |
| reward_motion      | 0.166    |
| reward_orientation | 0.0799   |
| reward_position    | 0        |
| reward_rotation    | 0.173    |
| reward_velocity    | 0.00758  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 259      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 38       |
|    time_elapsed    | 308      |
|    total timesteps | 12000    |
| train/             |          |
|    actor_loss      | -16.5    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.001    |
|    n_updates       | 6995     |
---------------------------------
---------------------------------
| reward             | 0.518    |
| reward_ctrl        | 0.104    |
| reward_motion      | 0.147    |
| reward_orientation | 0.0805   |
| reward_position    | 0        |
| reward_rotation    | 0.18     |
| reward_velocity    | 0.00667  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 262      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 37       |
|    time_elapsed    | 374      |
|    total timesteps | 14000    |
| train/             |          |
|    actor_loss      | -20.1    |
|    critic_loss     | 0.154    |
|    learning_rate   | 0.001    |
|    n_updates       | 8995     |
---------------------------------
---------------------------------
| reward             | 0.515    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.14     |
| reward_orientation | 0.0809   |
| reward_position    | 0        |
| reward_rotation    | 0.178    |
| reward_velocity    | 0.00922  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 36       |
|    time_elapsed    | 442      |
|    total timesteps | 16000    |
| train/             |          |
|    actor_loss      | -23.7    |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 10995    |
---------------------------------
Num timesteps: 18000
Best mean reward: 278.68 - Last mean reward per episode: 253.69
---------------------------------
| reward             | 0.5      |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.13     |
| reward_orientation | 0.0813   |
| reward_position    | 0        |
| reward_rotation    | 0.174    |
| reward_velocity    | 0.00836  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 254      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 35       |
|    time_elapsed    | 509      |
|    total timesteps | 18000    |
| train/             |          |
|    actor_loss      | -27.1    |
|    critic_loss     | 0.171    |
|    learning_rate   | 0.001    |
|    n_updates       | 12995    |
---------------------------------
---------------------------------
| reward             | 0.484    |
| reward_ctrl        | 0.106    |
| reward_motion      | 0.123    |
| reward_orientation | 0.0816   |
| reward_position    | 0        |
| reward_rotation    | 0.166    |
| reward_velocity    | 0.00787  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 251      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 34       |
|    time_elapsed    | 576      |
|    total timesteps | 20000    |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 0.189    |
|    learning_rate   | 0.001    |
|    n_updates       | 14995    |
---------------------------------
---------------------------------
| reward             | 0.483    |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.119    |
| reward_orientation | 0.0817   |
| reward_position    | 0        |
| reward_rotation    | 0.167    |
| reward_velocity    | 0.0083   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 250      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 34       |
|    time_elapsed    | 642      |
|    total timesteps | 22000    |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 0.167    |
|    learning_rate   | 0.001    |
|    n_updates       | 16995    |
---------------------------------
Num timesteps: 24000
Best mean reward: 278.68 - Last mean reward per episode: 246.68
---------------------------------
| reward             | 0.472    |
| reward_ctrl        | 0.108    |
| reward_motion      | 0.113    |
| reward_orientation | 0.0819   |
| reward_position    | 0        |
| reward_rotation    | 0.161    |
| reward_velocity    | 0.00796  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 247      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 33       |
|    time_elapsed    | 708      |
|    total timesteps | 24000    |
| train/             |          |
|    actor_loss      | -36.2    |
|    critic_loss     | 0.157    |
|    learning_rate   | 0.001    |
|    n_updates       | 18995    |
---------------------------------
---------------------------------
| reward             | 0.474    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.107    |
| reward_orientation | 0.082    |
| reward_position    | 0        |
| reward_rotation    | 0.166    |
| reward_velocity    | 0.00946  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 247      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 33       |
|    time_elapsed    | 775      |
|    total timesteps | 26000    |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.161    |
|    learning_rate   | 0.001    |
|    n_updates       | 20995    |
---------------------------------
---------------------------------
| reward             | 0.469    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.103    |
| reward_orientation | 0.0821   |
| reward_position    | 0        |
| reward_rotation    | 0.164    |
| reward_velocity    | 0.0088   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 33       |
|    time_elapsed    | 842      |
|    total timesteps | 28000    |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 0.148    |
|    learning_rate   | 0.001    |
|    n_updates       | 22995    |
---------------------------------
Num timesteps: 30000
Best mean reward: 278.68 - Last mean reward per episode: 243.29
---------------------------------
| reward             | 0.465    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.104    |
| reward_orientation | 0.0822   |
| reward_position    | 0        |
| reward_rotation    | 0.161    |
| reward_velocity    | 0.00886  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 32       |
|    time_elapsed    | 909      |
|    total timesteps | 30000    |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 0.161    |
|    learning_rate   | 0.001    |
|    n_updates       | 24995    |
---------------------------------
---------------------------------
| reward             | 0.462    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.0995   |
| reward_orientation | 0.0822   |
| reward_position    | 0        |
| reward_rotation    | 0.162    |
| reward_velocity    | 0.00864  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 32       |
|    time_elapsed    | 975      |
|    total timesteps | 32000    |
| train/             |          |
|    actor_loss      | -46.1    |
|    critic_loss     | 0.165    |
|    learning_rate   | 0.001    |
|    n_updates       | 26995    |
---------------------------------
---------------------------------
| reward             | 0.46     |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.0956   |
| reward_orientation | 0.0823   |
| reward_position    | 0        |
| reward_rotation    | 0.163    |
| reward_velocity    | 0.00821  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 32       |
|    time_elapsed    | 1042     |
|    total timesteps | 34000    |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 0.171    |
|    learning_rate   | 0.001    |
|    n_updates       | 28995    |
---------------------------------
Num timesteps: 36000
Best mean reward: 278.68 - Last mean reward per episode: 244.22
---------------------------------
| reward             | 0.462    |
| reward_ctrl        | 0.112    |
| reward_motion      | 0.0934   |
| reward_orientation | 0.0824   |
| reward_position    | 0        |
| reward_rotation    | 0.166    |
| reward_velocity    | 0.00797  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 244      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 32       |
|    time_elapsed    | 1108     |
|    total timesteps | 36000    |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 0.152    |
|    learning_rate   | 0.001    |
|    n_updates       | 30995    |
---------------------------------
---------------------------------
| reward             | 0.457    |
| reward_ctrl        | 0.112    |
| reward_motion      | 0.0901   |
| reward_orientation | 0.0825   |
| reward_position    | 0        |
| reward_rotation    | 0.165    |
| reward_velocity    | 0.0076   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 32       |
|    time_elapsed    | 1175     |
|    total timesteps | 38000    |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.19     |
|    learning_rate   | 0.001    |
|    n_updates       | 32995    |
---------------------------------
---------------------------------
| reward             | 0.458    |
| reward_ctrl        | 0.113    |
| reward_motion      | 0.0879   |
| reward_orientation | 0.0826   |
| reward_position    | 0        |
| reward_rotation    | 0.167    |
| reward_velocity    | 0.00747  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 32       |
|    time_elapsed    | 1242     |
|    total timesteps | 40000    |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.173    |
|    learning_rate   | 0.001    |
|    n_updates       | 34995    |
---------------------------------
Num timesteps: 42000
Best mean reward: 278.68 - Last mean reward per episode: 245.23
---------------------------------
| reward             | 0.455    |
| reward_ctrl        | 0.112    |
| reward_motion      | 0.0847   |
| reward_orientation | 0.0826   |
| reward_position    | 0        |
| reward_rotation    | 0.168    |
| reward_velocity    | 0.00811  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 32       |
|    time_elapsed    | 1308     |
|    total timesteps | 42000    |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.164    |
|    learning_rate   | 0.001    |
|    n_updates       | 36995    |
---------------------------------
---------------------------------
| reward             | 0.457    |
| reward_ctrl        | 0.112    |
| reward_motion      | 0.0844   |
| reward_orientation | 0.0826   |
| reward_position    | 0        |
| reward_rotation    | 0.171    |
| reward_velocity    | 0.00798  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 31       |
|    time_elapsed    | 1375     |
|    total timesteps | 44000    |
| train/             |          |
|    actor_loss      | -57.7    |
|    critic_loss     | 0.161    |
|    learning_rate   | 0.001    |
|    n_updates       | 38995    |
---------------------------------
---------------------------------
| reward             | 0.451    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.0816   |
| reward_orientation | 0.0826   |
| reward_position    | 0        |
| reward_rotation    | 0.167    |
| reward_velocity    | 0.00894  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 243      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 31       |
|    time_elapsed    | 1442     |
|    total timesteps | 46000    |
| train/             |          |
|    actor_loss      | -59.5    |
|    critic_loss     | 0.174    |
|    learning_rate   | 0.001    |
|    n_updates       | 40995    |
---------------------------------
Num timesteps: 48000
Best mean reward: 278.68 - Last mean reward per episode: 242.00
---------------------------------
| reward             | 0.448    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.0811   |
| reward_orientation | 0.0827   |
| reward_position    | 0        |
| reward_rotation    | 0.164    |
| reward_velocity    | 0.00902  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 31       |
|    time_elapsed    | 1508     |
|    total timesteps | 48000    |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 42995    |
---------------------------------
---------------------------------
| reward             | 0.446    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.0785   |
| reward_orientation | 0.0827   |
| reward_position    | 0        |
| reward_rotation    | 0.165    |
| reward_velocity    | 0.00918  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 242      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 31       |
|    time_elapsed    | 1577     |
|    total timesteps | 50000    |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 0.157    |
|    learning_rate   | 0.001    |
|    n_updates       | 44995    |
---------------------------------
2021-06-01 15:51:27.008375: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 15:51:27.008423: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_25
---------------------------------
| reward             | 0.659    |
| reward_ctrl        | 0.0291   |
| reward_motion      | 0.547    |
| reward_orientation | 0.0553   |
| reward_position    | 0        |
| reward_rotation    | 0.0224   |
| reward_velocity    | 0.00506  |
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | 333      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 73       |
|    time_elapsed    | 25       |
|    total timesteps | 1863     |
---------------------------------
---------------------------------
| reward             | 0.653    |
| reward_ctrl        | 0.0497   |
| reward_motion      | 0.493    |
| reward_orientation | 0.0547   |
| reward_position    | 0        |
| reward_rotation    | 0.0506   |
| reward_velocity    | 0.00408  |
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 331      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 75       |
|    time_elapsed    | 51       |
|    total timesteps | 3863     |
---------------------------------
---------------------------------
| reward             | 0.689    |
| reward_ctrl        | 0.0774   |
| reward_motion      | 0.512    |
| reward_orientation | 0.0565   |
| reward_position    | 0        |
| reward_rotation    | 0.0395   |
| reward_velocity    | 0.00384  |
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | 331      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 61       |
|    time_elapsed    | 95       |
|    total timesteps | 5863     |
| train/             |          |
|    actor_loss      | -2.7     |
|    critic_loss     | 0.0799   |
|    learning_rate   | 0.001    |
|    n_updates       | 860      |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 331.10
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.618    |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.384    |
| reward_orientation | 0.0602   |
| reward_position    | 0        |
| reward_rotation    | 0.0666   |
| reward_velocity    | 0.00458  |
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 321      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 48       |
|    time_elapsed    | 163      |
|    total timesteps | 7863     |
| train/             |          |
|    actor_loss      | -7.04    |
|    critic_loss     | 0.0792   |
|    learning_rate   | 0.001    |
|    n_updates       | 2860     |
---------------------------------
---------------------------------
| reward             | 0.575    |
| reward_ctrl        | 0.117    |
| reward_motion      | 0.312    |
| reward_orientation | 0.0599   |
| reward_position    | 0        |
| reward_rotation    | 0.0802   |
| reward_velocity    | 0.00614  |
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 42       |
|    time_elapsed    | 230      |
|    total timesteps | 9863     |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0882   |
|    learning_rate   | 0.001    |
|    n_updates       | 4860     |
---------------------------------
---------------------------------
| reward             | 0.551    |
| reward_ctrl        | 0.127    |
| reward_motion      | 0.273    |
| reward_orientation | 0.0626   |
| reward_position    | 0        |
| reward_rotation    | 0.0831   |
| reward_velocity    | 0.00585  |
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | 303      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 39       |
|    time_elapsed    | 298      |
|    total timesteps | 11863    |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 6860     |
---------------------------------
2021-06-01 15:57:19.120735: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 15:57:19.120791: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_26
----------------------------------
| reward             | 0.697     |
| reward_ctrl        | 0.0453    |
| reward_motion      | 0.456     |
| reward_orientation | 0.0661    |
| reward_position    | 3.16e-229 |
| reward_rotation    | 0.0813    |
| reward_velocity    | 0.0485    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 371       |
| time/              |           |
|    episodes        | 4         |
|    fps             | 73        |
|    time_elapsed    | 27        |
|    total timesteps | 2000      |
----------------------------------
----------------------------------
| reward             | 0.757     |
| reward_ctrl        | 0.0464    |
| reward_motion      | 0.51      |
| reward_orientation | 0.0639    |
| reward_position    | 1.35e-229 |
| reward_rotation    | 0.116     |
| reward_velocity    | 0.0215    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 365       |
| time/              |           |
|    episodes        | 8         |
|    fps             | 74        |
|    time_elapsed    | 53        |
|    total timesteps | 4000      |
----------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 347.52
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.725     |
| reward_ctrl        | 0.0554    |
| reward_motion      | 0.494     |
| reward_orientation | 0.0663    |
| reward_position    | 8.62e-230 |
| reward_rotation    | 0.0942    |
| reward_velocity    | 0.0156    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 348       |
| time/              |           |
|    episodes        | 12        |
|    fps             | 60        |
|    time_elapsed    | 98        |
|    total timesteps | 6000      |
| train/             |           |
|    actor_loss      | -2.99     |
|    critic_loss     | 0.0686    |
|    learning_rate   | 0.001     |
|    n_updates       | 995       |
----------------------------------
----------------------------------
| reward             | 0.656     |
| reward_ctrl        | 0.086     |
| reward_motion      | 0.394     |
| reward_orientation | 0.0637    |
| reward_position    | 6.32e-230 |
| reward_rotation    | 0.1       |
| reward_velocity    | 0.012     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 332       |
| time/              |           |
|    episodes        | 16        |
|    fps             | 49        |
|    time_elapsed    | 163       |
|    total timesteps | 8000      |
| train/             |           |
|    actor_loss      | -7.11     |
|    critic_loss     | 0.0809    |
|    learning_rate   | 0.001     |
|    n_updates       | 2995      |
----------------------------------
----------------------------------
| reward             | 0.635     |
| reward_ctrl        | 0.104     |
| reward_motion      | 0.34      |
| reward_orientation | 0.0649    |
| reward_position    | 4.99e-230 |
| reward_rotation    | 0.115     |
| reward_velocity    | 0.00967   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 330       |
| time/              |           |
|    episodes        | 20        |
|    fps             | 43        |
|    time_elapsed    | 229       |
|    total timesteps | 10000     |
| train/             |           |
|    actor_loss      | -10.8     |
|    critic_loss     | 0.103     |
|    learning_rate   | 0.001     |
|    n_updates       | 4995      |
----------------------------------
Num timesteps: 12000
Best mean reward: 347.52 - Last mean reward per episode: 318.32
----------------------------------
| reward             | 0.61      |
| reward_ctrl        | 0.115     |
| reward_motion      | 0.306     |
| reward_orientation | 0.0656    |
| reward_position    | 4.12e-230 |
| reward_rotation    | 0.114     |
| reward_velocity    | 0.00962   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 318       |
| time/              |           |
|    episodes        | 24        |
|    fps             | 40        |
|    time_elapsed    | 294       |
|    total timesteps | 12000     |
| train/             |           |
|    actor_loss      | -14.3     |
|    critic_loss     | 0.0928    |
|    learning_rate   | 0.001     |
|    n_updates       | 6995      |
----------------------------------
----------------------------------
| reward             | 0.591     |
| reward_ctrl        | 0.123     |
| reward_motion      | 0.282     |
| reward_orientation | 0.0681    |
| reward_position    | 3.51e-230 |
| reward_rotation    | 0.109     |
| reward_velocity    | 0.00833   |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 316       |
| time/              |           |
|    episodes        | 28        |
|    fps             | 38        |
|    time_elapsed    | 360       |
|    total timesteps | 14000     |
| train/             |           |
|    actor_loss      | -18       |
|    critic_loss     | 0.109     |
|    learning_rate   | 0.001     |
|    n_updates       | 8995      |
----------------------------------
2021-06-01 16:04:09.607525: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 16:04:09.718439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_27
---------------------------------
| reward             | 0.787    |
| reward_ctrl        | 0.057    |
| reward_motion      | 0.6      |
| reward_orientation | 0.0789   |
| reward_position    | 9.04e-48 |
| reward_rotation    | 0.0444   |
| reward_velocity    | 0.00699  |
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 295      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 71       |
|    time_elapsed    | 22       |
|    total timesteps | 1620     |
---------------------------------
---------------------------------
| reward             | 0.761    |
| reward_ctrl        | 0.047    |
| reward_motion      | 0.6      |
| reward_orientation | 0.076    |
| reward_position    | 3.87e-48 |
| reward_rotation    | 0.033    |
| reward_velocity    | 0.00487  |
| rollout/           |          |
|    ep_len_mean     | 452      |
|    ep_rew_mean     | 330      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 74       |
|    time_elapsed    | 48       |
|    total timesteps | 3620     |
---------------------------------
---------------------------------
| reward             | 0.766    |
| reward_ctrl        | 0.065    |
| reward_motion      | 0.583    |
| reward_orientation | 0.0717   |
| reward_position    | 2.46e-48 |
| reward_rotation    | 0.0337   |
| reward_velocity    | 0.0123   |
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | 329      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 65       |
|    time_elapsed    | 86       |
|    total timesteps | 5620     |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 0.0684   |
|    learning_rate   | 0.001    |
|    n_updates       | 615      |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 328.86
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.705    |
| reward_ctrl        | 0.0933   |
| reward_motion      | 0.476    |
| reward_orientation | 0.0646   |
| reward_position    | 1.81e-48 |
| reward_rotation    | 0.0549   |
| reward_velocity    | 0.0165   |
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 323      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 50       |
|    time_elapsed    | 151      |
|    total timesteps | 7620     |
| train/             |          |
|    actor_loss      | -6.48    |
|    critic_loss     | 0.178    |
|    learning_rate   | 0.001    |
|    n_updates       | 2615     |
---------------------------------
---------------------------------
| reward             | 0.663    |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.407    |
| reward_orientation | 0.0642   |
| reward_position    | 1.43e-48 |
| reward_rotation    | 0.0691   |
| reward_velocity    | 0.0132   |
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 44       |
|    time_elapsed    | 216      |
|    total timesteps | 9620     |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0945   |
|    learning_rate   | 0.001    |
|    n_updates       | 4615     |
---------------------------------
---------------------------------
| reward             | 0.637    |
| reward_ctrl        | 0.12     |
| reward_motion      | 0.361    |
| reward_orientation | 0.0632   |
| reward_position    | 1.18e-48 |
| reward_rotation    | 0.0761   |
| reward_velocity    | 0.0162   |
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 41       |
|    time_elapsed    | 282      |
|    total timesteps | 11620    |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.109    |
|    learning_rate   | 0.001    |
|    n_updates       | 6615     |
---------------------------------
2021-06-01 17:03:10.497433: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 17:03:10.497479: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_40
---------------------------------
| reward             | 0.683    |
| reward_ctrl        | 0.025    |
| reward_motion      | 0.562    |
| reward_orientation | 0.0649   |
| reward_position    | 4.51e-11 |
| reward_rotation    | 0.024    |
| reward_velocity    | 0.00652  |
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 68       |
|    time_elapsed    | 10       |
|    total timesteps | 706      |
---------------------------------
---------------------------------
| reward             | 0.637    |
| reward_ctrl        | 0.021    |
| reward_motion      | 0.532    |
| reward_orientation | 0.0624   |
| reward_position    | 2.25e-11 |
| reward_rotation    | 0.0176   |
| reward_velocity    | 0.00418  |
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 74.6     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 70       |
|    time_elapsed    | 13       |
|    total timesteps | 933      |
---------------------------------
---------------------------------
| reward             | 0.657    |
| reward_ctrl        | 0.0174   |
| reward_motion      | 0.559    |
| reward_orientation | 0.0665   |
| reward_position    | 2.15e-11 |
| reward_rotation    | 0.011    |
| reward_velocity    | 0.00251  |
| rollout/           |          |
|    ep_len_mean     | 98.9     |
|    ep_rew_mean     | 65       |
| time/              |          |
|    episodes        | 12       |
|    fps             | 72       |
|    time_elapsed    | 16       |
|    total timesteps | 1187     |
---------------------------------
---------------------------------
| reward             | 0.683    |
| reward_ctrl        | 0.0197   |
| reward_motion      | 0.573    |
| reward_orientation | 0.0657   |
| reward_position    | 0.00108  |
| reward_rotation    | 0.0179   |
| reward_velocity    | 0.00572  |
| rollout/           |          |
|    ep_len_mean     | 94.5     |
|    ep_rew_mean     | 63.2     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 73       |
|    time_elapsed    | 20       |
|    total timesteps | 1512     |
---------------------------------
---------------------------------
| reward             | 0.681    |
| reward_ctrl        | 0.0161   |
| reward_motion      | 0.578    |
| reward_orientation | 0.0668   |
| reward_position    | 0.000854 |
| reward_rotation    | 0.0141   |
| reward_velocity    | 0.00496  |
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 62.9     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 74       |
|    time_elapsed    | 25       |
|    total timesteps | 1859     |
---------------------------------
---------------------------------
| reward             | 0.674    |
| reward_ctrl        | 0.0147   |
| reward_motion      | 0.569    |
| reward_orientation | 0.066    |
| reward_position    | 0.00332  |
| reward_rotation    | 0.014    |
| reward_velocity    | 0.00625  |
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 71.6     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 75       |
|    time_elapsed    | 34       |
|    total timesteps | 2620     |
---------------------------------
---------------------------------
| reward             | 0.677    |
| reward_ctrl        | 0.0163   |
| reward_motion      | 0.573    |
| reward_orientation | 0.0673   |
| reward_position    | 0.00294  |
| reward_rotation    | 0.0124   |
| reward_velocity    | 0.00554  |
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 67.1     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 75       |
|    time_elapsed    | 37       |
|    total timesteps | 2839     |
---------------------------------
---------------------------------
| reward             | 0.679    |
| reward_ctrl        | 0.0154   |
| reward_motion      | 0.577    |
| reward_orientation | 0.0677   |
| reward_position    | 0.00253  |
| reward_rotation    | 0.0119   |
| reward_velocity    | 0.00464  |
| rollout/           |          |
|    ep_len_mean     | 95.7     |
|    ep_rew_mean     | 63.8     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 75       |
|    time_elapsed    | 40       |
|    total timesteps | 3063     |
---------------------------------
---------------------------------
| reward             | 0.679    |
| reward_ctrl        | 0.0157   |
| reward_motion      | 0.579    |
| reward_orientation | 0.0663   |
| reward_position    | 0.00231  |
| reward_rotation    | 0.0108   |
| reward_velocity    | 0.00424  |
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 67.8     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 75       |
|    time_elapsed    | 48       |
|    total timesteps | 3695     |
---------------------------------
---------------------------------
| reward             | 0.68     |
| reward_ctrl        | 0.0144   |
| reward_motion      | 0.582    |
| reward_orientation | 0.0644   |
| reward_position    | 0.00563  |
| reward_rotation    | 0.0096   |
| reward_velocity    | 0.00384  |
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 72       |
| time/              |          |
|    episodes        | 40       |
|    fps             | 75       |
|    time_elapsed    | 57       |
|    total timesteps | 4378     |
---------------------------------
---------------------------------
| reward             | 0.677    |
| reward_ctrl        | 0.0137   |
| reward_motion      | 0.584    |
| reward_orientation | 0.0616   |
| reward_position    | 0.00511  |
| reward_rotation    | 0.00872  |
| reward_velocity    | 0.00446  |
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 79.3     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 66       |
|    time_elapsed    | 82       |
|    total timesteps | 5492     |
| train/             |          |
|    actor_loss      | -1.95    |
|    critic_loss     | 0.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 490      |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 82.08
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.659    |
| reward_ctrl        | 0.0267   |
| reward_motion      | 0.55     |
| reward_orientation | 0.06     |
| reward_position    | 0.00467  |
| reward_rotation    | 0.0136   |
| reward_velocity    | 0.00417  |
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 95       |
| time/              |          |
|    episodes        | 48       |
|    fps             | 41       |
|    time_elapsed    | 182      |
|    total timesteps | 7492     |
| train/             |          |
|    actor_loss      | -6.18    |
|    critic_loss     | 0.363    |
|    learning_rate   | 0.001    |
|    n_updates       | 2490     |
---------------------------------
---------------------------------
| reward             | 0.65     |
| reward_ctrl        | 0.0373   |
| reward_motion      | 0.516    |
| reward_orientation | 0.0597   |
| reward_position    | 0.00431  |
| reward_rotation    | 0.0271   |
| reward_velocity    | 0.00599  |
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 33       |
|    time_elapsed    | 279      |
|    total timesteps | 9492     |
| train/             |          |
|    actor_loss      | -9.76    |
|    critic_loss     | 0.292    |
|    learning_rate   | 0.001    |
|    n_updates       | 4490     |
---------------------------------
---------------------------------
| reward             | 0.647    |
| reward_ctrl        | 0.0468   |
| reward_motion      | 0.496    |
| reward_orientation | 0.0595   |
| reward_position    | 0.00399  |
| reward_rotation    | 0.0336   |
| reward_velocity    | 0.00639  |
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 30       |
|    time_elapsed    | 375      |
|    total timesteps | 11492    |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.298    |
|    learning_rate   | 0.001    |
|    n_updates       | 6490     |
---------------------------------
Num timesteps: 12000
Best mean reward: 82.08 - Last mean reward per episode: 124.45
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.627    |
| reward_ctrl        | 0.055    |
| reward_motion      | 0.469    |
| reward_orientation | 0.0576   |
| reward_position    | 0.00372  |
| reward_rotation    | 0.0359   |
| reward_velocity    | 0.00611  |
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 28       |
|    time_elapsed    | 470      |
|    total timesteps | 13492    |
| train/             |          |
|    actor_loss      | -16      |
|    critic_loss     | 0.454    |
|    learning_rate   | 0.001    |
|    n_updates       | 8490     |
---------------------------------
---------------------------------
| reward             | 0.618    |
| reward_ctrl        | 0.0619   |
| reward_motion      | 0.446    |
| reward_orientation | 0.0568   |
| reward_position    | 0.00349  |
| reward_rotation    | 0.0437   |
| reward_velocity    | 0.00611  |
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 27       |
|    time_elapsed    | 567      |
|    total timesteps | 15492    |
| train/             |          |
|    actor_loss      | -18.5    |
|    critic_loss     | 0.518    |
|    learning_rate   | 0.001    |
|    n_updates       | 10490    |
---------------------------------
---------------------------------
| reward             | 0.614    |
| reward_ctrl        | 0.0679   |
| reward_motion      | 0.431    |
| reward_orientation | 0.0562   |
| reward_position    | 0.00328  |
| reward_rotation    | 0.0472   |
| reward_velocity    | 0.00799  |
| rollout/           |          |
|    ep_len_mean     | 257      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 26       |
|    time_elapsed    | 663      |
|    total timesteps | 17492    |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 0.533    |
|    learning_rate   | 0.001    |
|    n_updates       | 12490    |
---------------------------------
Num timesteps: 18000
Best mean reward: 124.45 - Last mean reward per episode: 153.44
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.605    |
| reward_ctrl        | 0.0734   |
| reward_motion      | 0.415    |
| reward_orientation | 0.0556   |
| reward_position    | 0.00309  |
| reward_rotation    | 0.0498   |
| reward_velocity    | 0.00803  |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 25       |
|    time_elapsed    | 760      |
|    total timesteps | 19492    |
| train/             |          |
|    actor_loss      | -23.4    |
|    critic_loss     | 0.716    |
|    learning_rate   | 0.001    |
|    n_updates       | 14490    |
---------------------------------
---------------------------------
| reward             | 0.598    |
| reward_ctrl        | 0.0784   |
| reward_motion      | 0.4      |
| reward_orientation | 0.0543   |
| reward_position    | 0.00293  |
| reward_rotation    | 0.0547   |
| reward_velocity    | 0.00791  |
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 24       |
|    time_elapsed    | 861      |
|    total timesteps | 21492    |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 0.788    |
|    learning_rate   | 0.001    |
|    n_updates       | 16490    |
---------------------------------
---------------------------------
| reward             | 0.591    |
| reward_ctrl        | 0.0828   |
| reward_motion      | 0.386    |
| reward_orientation | 0.0538   |
| reward_position    | 0.00278  |
| reward_rotation    | 0.058    |
| reward_velocity    | 0.00763  |
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 24       |
|    time_elapsed    | 966      |
|    total timesteps | 23492    |
| train/             |          |
|    actor_loss      | -27.8    |
|    critic_loss     | 0.781    |
|    learning_rate   | 0.001    |
|    n_updates       | 18490    |
---------------------------------
Num timesteps: 24000
Best mean reward: 153.44 - Last mean reward per episode: 172.48
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.582    |
| reward_ctrl        | 0.087    |
| reward_motion      | 0.371    |
| reward_orientation | 0.0533   |
| reward_position    | 0.00265  |
| reward_rotation    | 0.0603   |
| reward_velocity    | 0.00727  |
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 23       |
|    time_elapsed    | 1068     |
|    total timesteps | 25492    |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 20490    |
---------------------------------
---------------------------------
| reward             | 0.579    |
| reward_ctrl        | 0.0907   |
| reward_motion      | 0.361    |
| reward_orientation | 0.053    |
| reward_position    | 0.00252  |
| reward_rotation    | 0.0633   |
| reward_velocity    | 0.00857  |
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 23       |
|    time_elapsed    | 1172     |
|    total timesteps | 27492    |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.777    |
|    learning_rate   | 0.001    |
|    n_updates       | 22490    |
---------------------------------
---------------------------------
| reward             | 0.575    |
| reward_ctrl        | 0.094    |
| reward_motion      | 0.348    |
| reward_orientation | 0.0527   |
| reward_position    | 0.00241  |
| reward_rotation    | 0.0689   |
| reward_velocity    | 0.00922  |
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 23       |
|    time_elapsed    | 1275     |
|    total timesteps | 29492    |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 0.662    |
|    learning_rate   | 0.001    |
|    n_updates       | 24490    |
---------------------------------
Num timesteps: 30000
Best mean reward: 172.48 - Last mean reward per episode: 185.95
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.569    |
| reward_ctrl        | 0.0969   |
| reward_motion      | 0.337    |
| reward_orientation | 0.0524   |
| reward_position    | 0.00231  |
| reward_rotation    | 0.0717   |
| reward_velocity    | 0.00923  |
| rollout/           |          |
|    ep_len_mean     | 328      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 22       |
|    time_elapsed    | 1377     |
|    total timesteps | 31492    |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 0.743    |
|    learning_rate   | 0.001    |
|    n_updates       | 26490    |
---------------------------------
---------------------------------
| reward             | 0.564    |
| reward_ctrl        | 0.0997   |
| reward_motion      | 0.328    |
| reward_orientation | 0.0516   |
| reward_position    | 0.00222  |
| reward_rotation    | 0.0735   |
| reward_velocity    | 0.00892  |
| rollout/           |          |
|    ep_len_mean     | 335      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 22       |
|    time_elapsed    | 1480     |
|    total timesteps | 33492    |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 0.581    |
|    learning_rate   | 0.001    |
|    n_updates       | 28490    |
---------------------------------
---------------------------------
| reward             | 0.558    |
| reward_ctrl        | 0.105    |
| reward_motion      | 0.311    |
| reward_orientation | 0.051    |
| reward_position    | 0.0022   |
| reward_rotation    | 0.0789   |
| reward_velocity    | 0.00937  |
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 22       |
|    time_elapsed    | 1582     |
|    total timesteps | 35492    |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.001    |
|    n_updates       | 30490    |
---------------------------------
Num timesteps: 36000
Best mean reward: 185.95 - Last mean reward per episode: 202.03
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.549    |
| reward_ctrl        | 0.111    |
| reward_motion      | 0.293    |
| reward_orientation | 0.0498   |
| reward_position    | 0.0022   |
| reward_rotation    | 0.0822   |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | 210      |
| time/              |          |
|    episodes        | 108      |
|    fps             | 22       |
|    time_elapsed    | 1685     |
|    total timesteps | 37492    |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.788    |
|    learning_rate   | 0.001    |
|    n_updates       | 32490    |
---------------------------------
---------------------------------
| reward             | 0.542    |
| reward_ctrl        | 0.117    |
| reward_motion      | 0.276    |
| reward_orientation | 0.0494   |
| reward_position    | 0.0022   |
| reward_rotation    | 0.0857   |
| reward_velocity    | 0.0118   |
| rollout/           |          |
|    ep_len_mean     | 383      |
|    ep_rew_mean     | 218      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 22       |
|    time_elapsed    | 1787     |
|    total timesteps | 39492    |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 0.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 34490    |
---------------------------------
---------------------------------
| reward             | 0.528    |
| reward_ctrl        | 0.122    |
| reward_motion      | 0.256    |
| reward_orientation | 0.0483   |
| reward_position    | 0.00203  |
| reward_rotation    | 0.0871   |
| reward_velocity    | 0.0123   |
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 228      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 21       |
|    time_elapsed    | 1891     |
|    total timesteps | 41492    |
| train/             |          |
|    actor_loss      | -41.1    |
|    critic_loss     | 0.704    |
|    learning_rate   | 0.001    |
|    n_updates       | 36490    |
---------------------------------
Num timesteps: 42000
Best mean reward: 202.03 - Last mean reward per episode: 231.15
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.517    |
| reward_ctrl        | 0.128    |
| reward_motion      | 0.235    |
| reward_orientation | 0.047    |
| reward_position    | 0.00203  |
| reward_rotation    | 0.092    |
| reward_velocity    | 0.0124   |
| rollout/           |          |
|    ep_len_mean     | 416      |
|    ep_rew_mean     | 237      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 21       |
|    time_elapsed    | 1995     |
|    total timesteps | 43492    |
| train/             |          |
|    actor_loss      | -41.9    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 38490    |
---------------------------------
---------------------------------
| reward             | 0.514    |
| reward_ctrl        | 0.135    |
| reward_motion      | 0.222    |
| reward_orientation | 0.046    |
| reward_position    | 0.00143  |
| reward_rotation    | 0.0969   |
| reward_velocity    | 0.013    |
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 21       |
|    time_elapsed    | 2097     |
|    total timesteps | 45492    |
| train/             |          |
|    actor_loss      | -43.1    |
|    critic_loss     | 0.754    |
|    learning_rate   | 0.001    |
|    n_updates       | 40490    |
---------------------------------
---------------------------------
| reward             | 0.504    |
| reward_ctrl        | 0.14     |
| reward_motion      | 0.201    |
| reward_orientation | 0.0446   |
| reward_position    | 0.00141  |
| reward_rotation    | 0.104    |
| reward_velocity    | 0.0131   |
| rollout/           |          |
|    ep_len_mean     | 447      |
|    ep_rew_mean     | 255      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 21       |
|    time_elapsed    | 2201     |
|    total timesteps | 47492    |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 0.752    |
|    learning_rate   | 0.001    |
|    n_updates       | 42490    |
---------------------------------
Num timesteps: 48000
Best mean reward: 231.15 - Last mean reward per episode: 258.22
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.497    |
| reward_ctrl        | 0.147    |
| reward_motion      | 0.183    |
| reward_orientation | 0.0439   |
| reward_position    | 0.00141  |
| reward_rotation    | 0.109    |
| reward_velocity    | 0.0132   |
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    episodes        | 132      |
|    fps             | 21       |
|    time_elapsed    | 2302     |
|    total timesteps | 49492    |
| train/             |          |
|    actor_loss      | -44.7    |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 44490    |
---------------------------------
---------------------------------
| reward             | 0.492    |
| reward_ctrl        | 0.153    |
| reward_motion      | 0.169    |
| reward_orientation | 0.0433   |
| reward_position    | 0.00141  |
| reward_rotation    | 0.112    |
| reward_velocity    | 0.0134   |
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 273      |
| time/              |          |
|    episodes        | 136      |
|    fps             | 21       |
|    time_elapsed    | 2405     |
|    total timesteps | 51492    |
| train/             |          |
|    actor_loss      | -46.4    |
|    critic_loss     | 0.648    |
|    learning_rate   | 0.001    |
|    n_updates       | 46490    |
---------------------------------
---------------------------------
| reward             | 0.481    |
| reward_ctrl        | 0.159    |
| reward_motion      | 0.146    |
| reward_orientation | 0.0424   |
| reward_position    | 2.28e-17 |
| reward_rotation    | 0.119    |
| reward_velocity    | 0.0133   |
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 280      |
| time/              |          |
|    episodes        | 140      |
|    fps             | 21       |
|    time_elapsed    | 2509     |
|    total timesteps | 53492    |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 0.713    |
|    learning_rate   | 0.001    |
|    n_updates       | 48490    |
---------------------------------
Num timesteps: 54000
Best mean reward: 258.22 - Last mean reward per episode: 279.64
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.474     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.125     |
| reward_orientation | 0.0423    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.126     |
| reward_velocity    | 0.015     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 144       |
|    fps             | 21        |
|    time_elapsed    | 2612      |
|    total timesteps | 55492     |
| train/             |           |
|    actor_loss      | -47.6     |
|    critic_loss     | 0.705     |
|    learning_rate   | 0.001     |
|    n_updates       | 50490     |
----------------------------------
----------------------------------
| reward             | 0.475     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.123     |
| reward_orientation | 0.042     |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.129     |
| reward_velocity    | 0.0153    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 148       |
|    fps             | 21        |
|    time_elapsed    | 2716      |
|    total timesteps | 57492     |
| train/             |           |
|    actor_loss      | -48.3     |
|    critic_loss     | 0.821     |
|    learning_rate   | 0.001     |
|    n_updates       | 52490     |
----------------------------------
----------------------------------
| reward             | 0.471     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.124     |
| reward_orientation | 0.041     |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.126     |
| reward_velocity    | 0.0147    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 152       |
|    fps             | 21        |
|    time_elapsed    | 2822      |
|    total timesteps | 59492     |
| train/             |           |
|    actor_loss      | -50.2     |
|    critic_loss     | 0.784     |
|    learning_rate   | 0.001     |
|    n_updates       | 54490     |
----------------------------------
Num timesteps: 60000
Best mean reward: 279.64 - Last mean reward per episode: 284.07
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.468     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.122     |
| reward_orientation | 0.0405    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.126     |
| reward_velocity    | 0.0146    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 156       |
|    fps             | 21        |
|    time_elapsed    | 2925      |
|    total timesteps | 61492     |
| train/             |           |
|    actor_loss      | -50.8     |
|    critic_loss     | 0.592     |
|    learning_rate   | 0.001     |
|    n_updates       | 56490     |
----------------------------------
----------------------------------
| reward             | 0.471     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.121     |
| reward_orientation | 0.0405    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.129     |
| reward_velocity    | 0.0148    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 160       |
|    fps             | 20        |
|    time_elapsed    | 3029      |
|    total timesteps | 63492     |
| train/             |           |
|    actor_loss      | -52       |
|    critic_loss     | 0.799     |
|    learning_rate   | 0.001     |
|    n_updates       | 58490     |
----------------------------------
----------------------------------
| reward             | 0.469     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.119     |
| reward_orientation | 0.04      |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.016     |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 164       |
|    fps             | 20        |
|    time_elapsed    | 3132      |
|    total timesteps | 65492     |
| train/             |           |
|    actor_loss      | -52.9     |
|    critic_loss     | 0.706     |
|    learning_rate   | 0.001     |
|    n_updates       | 60490     |
----------------------------------
Num timesteps: 66000
Best mean reward: 284.07 - Last mean reward per episode: 283.42
----------------------------------
| reward             | 0.465     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.115     |
| reward_orientation | 0.0398    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.131     |
| reward_velocity    | 0.0148    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 284       |
| time/              |           |
|    episodes        | 168       |
|    fps             | 20        |
|    time_elapsed    | 3235      |
|    total timesteps | 67492     |
| train/             |           |
|    actor_loss      | -52.6     |
|    critic_loss     | 0.543     |
|    learning_rate   | 0.001     |
|    n_updates       | 62490     |
----------------------------------
----------------------------------
| reward             | 0.459     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.111     |
| reward_orientation | 0.0393    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.0147    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 284       |
| time/              |           |
|    episodes        | 172       |
|    fps             | 20        |
|    time_elapsed    | 3339      |
|    total timesteps | 69492     |
| train/             |           |
|    actor_loss      | -53.5     |
|    critic_loss     | 0.857     |
|    learning_rate   | 0.001     |
|    n_updates       | 64490     |
----------------------------------
----------------------------------
| reward             | 0.454     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.109     |
| reward_orientation | 0.0399    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.125     |
| reward_velocity    | 0.0147    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 176       |
|    fps             | 20        |
|    time_elapsed    | 3443      |
|    total timesteps | 71492     |
| train/             |           |
|    actor_loss      | -54.8     |
|    critic_loss     | 0.518     |
|    learning_rate   | 0.001     |
|    n_updates       | 66490     |
----------------------------------
Num timesteps: 72000
Best mean reward: 284.07 - Last mean reward per episode: 284.82
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.454     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.111     |
| reward_orientation | 0.0394    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.124     |
| reward_velocity    | 0.0148    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 284       |
| time/              |           |
|    episodes        | 180       |
|    fps             | 20        |
|    time_elapsed    | 3545      |
|    total timesteps | 73492     |
| train/             |           |
|    actor_loss      | -54.9     |
|    critic_loss     | 0.843     |
|    learning_rate   | 0.001     |
|    n_updates       | 68490     |
----------------------------------
----------------------------------
| reward             | 0.458     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.112     |
| reward_orientation | 0.0398    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.125     |
| reward_velocity    | 0.0148    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 184       |
|    fps             | 20        |
|    time_elapsed    | 3648      |
|    total timesteps | 75492     |
| train/             |           |
|    actor_loss      | -55.9     |
|    critic_loss     | 0.519     |
|    learning_rate   | 0.001     |
|    n_updates       | 70490     |
----------------------------------
----------------------------------
| reward             | 0.456     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.112     |
| reward_orientation | 0.0398    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.126     |
| reward_velocity    | 0.0138    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 286       |
| time/              |           |
|    episodes        | 188       |
|    fps             | 20        |
|    time_elapsed    | 3751      |
|    total timesteps | 77492     |
| train/             |           |
|    actor_loss      | -56.5     |
|    critic_loss     | 0.539     |
|    learning_rate   | 0.001     |
|    n_updates       | 72490     |
----------------------------------
Num timesteps: 78000
Best mean reward: 284.82 - Last mean reward per episode: 285.72
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------
| reward             | 0.457     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.112     |
| reward_orientation | 0.0398    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0129    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 192       |
|    fps             | 20        |
|    time_elapsed    | 3853      |
|    total timesteps | 79492     |
| train/             |           |
|    actor_loss      | -57.9     |
|    critic_loss     | 0.407     |
|    learning_rate   | 0.001     |
|    n_updates       | 74490     |
----------------------------------
----------------------------------
| reward             | 0.46      |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.114     |
| reward_orientation | 0.0404    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0129    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 285       |
| time/              |           |
|    episodes        | 196       |
|    fps             | 20        |
|    time_elapsed    | 3956      |
|    total timesteps | 81492     |
| train/             |           |
|    actor_loss      | -57.6     |
|    critic_loss     | 0.649     |
|    learning_rate   | 0.001     |
|    n_updates       | 76490     |
----------------------------------
----------------------------------
| reward             | 0.463     |
| reward_ctrl        | 0.165     |
| reward_motion      | 0.115     |
| reward_orientation | 0.0405    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.0134    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 200       |
|    fps             | 20        |
|    time_elapsed    | 4059      |
|    total timesteps | 83492     |
| train/             |           |
|    actor_loss      | -59       |
|    critic_loss     | 0.795     |
|    learning_rate   | 0.001     |
|    n_updates       | 78490     |
----------------------------------
Num timesteps: 84000
Best mean reward: 285.72 - Last mean reward per episode: 284.07
----------------------------------
| reward             | 0.461     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.114     |
| reward_orientation | 0.0399    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.129     |
| reward_velocity    | 0.0132    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 204       |
|    fps             | 20        |
|    time_elapsed    | 4162      |
|    total timesteps | 85492     |
| train/             |           |
|    actor_loss      | -58.6     |
|    critic_loss     | 0.731     |
|    learning_rate   | 0.001     |
|    n_updates       | 80490     |
----------------------------------
----------------------------------
| reward             | 0.461     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.115     |
| reward_orientation | 0.0403    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.0122    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 282       |
| time/              |           |
|    episodes        | 208       |
|    fps             | 20        |
|    time_elapsed    | 4266      |
|    total timesteps | 87492     |
| train/             |           |
|    actor_loss      | -59.9     |
|    critic_loss     | 0.8       |
|    learning_rate   | 0.001     |
|    n_updates       | 82490     |
----------------------------------
----------------------------------
| reward             | 0.465     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.117     |
| reward_orientation | 0.0403    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.13      |
| reward_velocity    | 0.0119    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 284       |
| time/              |           |
|    episodes        | 212       |
|    fps             | 20        |
|    time_elapsed    | 4369      |
|    total timesteps | 89492     |
| train/             |           |
|    actor_loss      | -59.1     |
|    critic_loss     | 0.556     |
|    learning_rate   | 0.001     |
|    n_updates       | 84490     |
----------------------------------
Num timesteps: 90000
Best mean reward: 285.72 - Last mean reward per episode: 283.66
----------------------------------
| reward             | 0.47      |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.122     |
| reward_orientation | 0.0412    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.13      |
| reward_velocity    | 0.0108    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 216       |
|    fps             | 20        |
|    time_elapsed    | 4470      |
|    total timesteps | 91492     |
| train/             |           |
|    actor_loss      | -61.5     |
|    critic_loss     | 0.538     |
|    learning_rate   | 0.001     |
|    n_updates       | 86490     |
----------------------------------
----------------------------------
| reward             | 0.472     |
| reward_ctrl        | 0.167     |
| reward_motion      | 0.123     |
| reward_orientation | 0.0414    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.129     |
| reward_velocity    | 0.0115    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 220       |
|    fps             | 20        |
|    time_elapsed    | 4575      |
|    total timesteps | 93492     |
| train/             |           |
|    actor_loss      | -60.6     |
|    critic_loss     | 0.469     |
|    learning_rate   | 0.001     |
|    n_updates       | 88490     |
----------------------------------
----------------------------------
| reward             | 0.466     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.121     |
| reward_orientation | 0.0413    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0106    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 282       |
| time/              |           |
|    episodes        | 224       |
|    fps             | 20        |
|    time_elapsed    | 4678      |
|    total timesteps | 95492     |
| train/             |           |
|    actor_loss      | -61.1     |
|    critic_loss     | 0.457     |
|    learning_rate   | 0.001     |
|    n_updates       | 90490     |
----------------------------------
Num timesteps: 96000
Best mean reward: 285.72 - Last mean reward per episode: 282.39
----------------------------------
| reward             | 0.472     |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.126     |
| reward_orientation | 0.0413    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0111    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 283       |
| time/              |           |
|    episodes        | 228       |
|    fps             | 20        |
|    time_elapsed    | 4780      |
|    total timesteps | 97492     |
| train/             |           |
|    actor_loss      | -61.8     |
|    critic_loss     | 0.522     |
|    learning_rate   | 0.001     |
|    n_updates       | 92490     |
----------------------------------
----------------------------------
| reward             | 0.47      |
| reward_ctrl        | 0.166     |
| reward_motion      | 0.124     |
| reward_orientation | 0.0409    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.128     |
| reward_velocity    | 0.0112    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 282       |
| time/              |           |
|    episodes        | 232       |
|    fps             | 20        |
|    time_elapsed    | 4883      |
|    total timesteps | 99492     |
| train/             |           |
|    actor_loss      | -63.3     |
|    critic_loss     | 0.508     |
|    learning_rate   | 0.001     |
|    n_updates       | 94490     |
----------------------------------
----------------------------------
| reward             | 0.471     |
| reward_ctrl        | 0.167     |
| reward_motion      | 0.12      |
| reward_orientation | 0.0413    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.132     |
| reward_velocity    | 0.0112    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 281       |
| time/              |           |
|    episodes        | 236       |
|    fps             | 20        |
|    time_elapsed    | 4986      |
|    total timesteps | 101492    |
| train/             |           |
|    actor_loss      | -63.3     |
|    critic_loss     | 0.452     |
|    learning_rate   | 0.001     |
|    n_updates       | 96490     |
----------------------------------
Num timesteps: 102000
Best mean reward: 285.72 - Last mean reward per episode: 281.81
----------------------------------
| reward             | 0.475     |
| reward_ctrl        | 0.167     |
| reward_motion      | 0.126     |
| reward_orientation | 0.0418    |
| reward_position    | 5.76e-130 |
| reward_rotation    | 0.127     |
| reward_velocity    | 0.0126    |
| rollout/           |           |
|    ep_len_mean     | 500       |
|    ep_rew_mean     | 282       |
| time/              |           |
|    episodes        | 240       |
|    fps             | 20        |
|    time_elapsed    | 5088      |
|    total timesteps | 103492    |
| train/             |           |
|    actor_loss      | -63.5     |
|    critic_loss     | 0.52      |
|    learning_rate   | 0.001     |
|    n_updates       | 98490     |
----------------------------------
---------------------------------
| reward             | 0.476    |
| reward_ctrl        | 0.167    |
| reward_motion      | 0.129    |
| reward_orientation | 0.0419   |
| reward_position    | 4.5e-213 |
| reward_rotation    | 0.126    |
| reward_velocity    | 0.0118   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 282      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 20       |
|    time_elapsed    | 5191     |
|    total timesteps | 105492   |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 0.627    |
|    learning_rate   | 0.001    |
|    n_updates       | 100490   |
---------------------------------
---------------------------------
| reward             | 0.472    |
| reward_ctrl        | 0.167    |
| reward_motion      | 0.126    |
| reward_orientation | 0.0418   |
| reward_position    | 4.5e-213 |
| reward_rotation    | 0.125    |
| reward_velocity    | 0.0115   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 283      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 20       |
|    time_elapsed    | 5295     |
|    total timesteps | 107492   |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 0.423    |
|    learning_rate   | 0.001    |
|    n_updates       | 102490   |
---------------------------------
Num timesteps: 108000
Best mean reward: 285.72 - Last mean reward per episode: 282.73
---------------------------------
| reward             | 0.473    |
| reward_ctrl        | 0.167    |
| reward_motion      | 0.124    |
| reward_orientation | 0.0423   |
| reward_position    | 4.5e-213 |
| reward_rotation    | 0.128    |
| reward_velocity    | 0.0119   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 284      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 20       |
|    time_elapsed    | 5399     |
|    total timesteps | 109492   |
| train/             |          |
|    actor_loss      | -64.7    |
|    critic_loss     | 0.522    |
|    learning_rate   | 0.001    |
|    n_updates       | 104490   |
---------------------------------
---------------------------------
| reward             | 0.466    |
| reward_ctrl        | 0.167    |
| reward_motion      | 0.117    |
| reward_orientation | 0.0418   |
| reward_position    | 4.5e-213 |
| reward_rotation    | 0.128    |
| reward_velocity    | 0.0129   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 283      |
| time/              |          |
|    episodes        | 256      |
|    fps             | 20       |
|    time_elapsed    | 5503     |
|    total timesteps | 111492   |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 0.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 106490   |
---------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 194, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 916, in get_code
  File "<frozen importlib._bootstrap_external>", line 846, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 672
    info['reward_velocity'] = -np.square(np.linalg.norm(err[:,:3], axis = -1))) * self.w[0]
                                                                              ^
SyntaxError: unmatched ')'
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 194, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 18:38:37.462313: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 18:38:37.462363: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_41
---------------------------------
| reward             | -3.94    |
| reward_ctrl        | 0.00175  |
| reward_motion      | 0.6      |
| reward_orientation | 0.0702   |
| reward_position    | 8.4e-06  |
| reward_rotation    | 0.00294  |
| reward_velocity    | -4.61    |
| rollout/           |          |
|    ep_len_mean     | 64.5     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 56       |
|    time_elapsed    | 4        |
|    total timesteps | 258      |
---------------------------------
---------------------------------
| reward             | -3.43    |
| reward_ctrl        | 0.00516  |
| reward_motion      | 0.6      |
| reward_orientation | 0.0651   |
| reward_position    | 0.00606  |
| reward_rotation    | 0.00103  |
| reward_velocity    | -4.1     |
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 66       |
|    time_elapsed    | 8        |
|    total timesteps | 556      |
---------------------------------
---------------------------------
| reward             | -3.27    |
| reward_ctrl        | 0.00461  |
| reward_motion      | 0.6      |
| reward_orientation | 0.0661   |
| reward_position    | 0.0033   |
| reward_rotation    | 0.000715 |
| reward_velocity    | -3.95    |
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | -160     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 73       |
|    time_elapsed    | 17       |
|    total timesteps | 1304     |
---------------------------------
---------------------------------
| reward             | -2.86    |
| reward_ctrl        | 0.0078   |
| reward_motion      | 0.574    |
| reward_orientation | 0.0655   |
| reward_position    | 0.0025   |
| reward_rotation    | 0.00695  |
| reward_velocity    | -3.52    |
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | -189     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 73       |
|    time_elapsed    | 24       |
|    total timesteps | 1776     |
---------------------------------
---------------------------------
| reward             | -3       |
| reward_ctrl        | 0.0127   |
| reward_motion      | 0.579    |
| reward_orientation | 0.0672   |
| reward_position    | 0.00197  |
| reward_rotation    | 0.0101   |
| reward_velocity    | -3.67    |
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | -181     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 74       |
|    time_elapsed    | 27       |
|    total timesteps | 2041     |
---------------------------------
---------------------------------
| reward             | -2.6     |
| reward_ctrl        | 0.0147   |
| reward_motion      | 0.582    |
| reward_orientation | 0.0693   |
| reward_position    | 0.0125   |
| reward_rotation    | 0.0112   |
| reward_velocity    | -3.29    |
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | -183     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 75       |
|    time_elapsed    | 33       |
|    total timesteps | 2531     |
---------------------------------
---------------------------------
| reward             | -2.55    |
| reward_ctrl        | 0.0151   |
| reward_motion      | 0.576    |
| reward_orientation | 0.0675   |
| reward_position    | 0.0106   |
| reward_rotation    | 0.0145   |
| reward_velocity    | -3.24    |
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | -195     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 74       |
|    time_elapsed    | 42       |
|    total timesteps | 3169     |
---------------------------------
---------------------------------
| reward             | -2.45    |
| reward_ctrl        | 0.015    |
| reward_motion      | 0.579    |
| reward_orientation | 0.0676   |
| reward_position    | 0.013    |
| reward_rotation    | 0.0126   |
| reward_velocity    | -3.13    |
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 75       |
|    time_elapsed    | 45       |
|    total timesteps | 3448     |
---------------------------------
---------------------------------
| reward             | -2.66    |
| reward_ctrl        | 0.0164   |
| reward_motion      | 0.582    |
| reward_orientation | 0.0674   |
| reward_position    | 0.0137   |
| reward_rotation    | 0.0109   |
| reward_velocity    | -3.35    |
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | -211     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 75       |
|    time_elapsed    | 54       |
|    total timesteps | 4127     |
---------------------------------
---------------------------------
| reward             | -2.48    |
| reward_ctrl        | 0.0161   |
| reward_motion      | 0.581    |
| reward_orientation | 0.0649   |
| reward_position    | 0.0123   |
| reward_rotation    | 0.0101   |
| reward_velocity    | -3.16    |
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 75       |
|    time_elapsed    | 64       |
|    total timesteps | 4845     |
---------------------------------
---------------------------------
| reward             | -2.76    |
| reward_ctrl        | 0.016    |
| reward_motion      | 0.583    |
| reward_orientation | 0.0646   |
| reward_position    | 0.0112   |
| reward_rotation    | 0.00919  |
| reward_velocity    | -3.44    |
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | -209     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 66       |
|    time_elapsed    | 82       |
|    total timesteps | 5492     |
| train/             |          |
|    actor_loss      | 1.97     |
|    critic_loss     | 7.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 490      |
---------------------------------
2021-06-01 18:41:57.996380: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 18:41:57.996431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_42
---------------------------------
| reward             | -1.13    |
| reward_ctrl        | 0.00492  |
| reward_motion      | 0.436    |
| reward_orientation | 0.0469   |
| reward_position    | 6.76e-32 |
| reward_rotation    | 0.00704  |
| reward_velocity    | -1.63    |
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | -451     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 70       |
|    time_elapsed    | 18       |
|    total timesteps | 1298     |
---------------------------------
---------------------------------
| reward             | -1.62    |
| reward_ctrl        | 0.00947  |
| reward_motion      | 0.53     |
| reward_orientation | 0.0547   |
| reward_position    | 2.91e-07 |
| reward_rotation    | 0.0354   |
| reward_velocity    | -2.25    |
| rollout/           |          |
|    ep_len_mean     | 264      |
|    ep_rew_mean     | -396     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 72       |
|    time_elapsed    | 29       |
|    total timesteps | 2115     |
---------------------------------
---------------------------------
| reward             | -2.1     |
| reward_ctrl        | 0.0129   |
| reward_motion      | 0.555    |
| reward_orientation | 0.0602   |
| reward_position    | 1.85e-07 |
| reward_rotation    | 0.0244   |
| reward_velocity    | -2.76    |
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | -323     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 73       |
|    time_elapsed    | 35       |
|    total timesteps | 2589     |
---------------------------------
---------------------------------
| reward             | -1.83    |
| reward_ctrl        | 0.0158   |
| reward_motion      | 0.567    |
| reward_orientation | 0.0615   |
| reward_position    | 0.00103  |
| reward_rotation    | 0.0183   |
| reward_velocity    | -2.49    |
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | -279     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 74       |
|    time_elapsed    | 39       |
|    total timesteps | 2963     |
---------------------------------
---------------------------------
| reward             | -1.91    |
| reward_ctrl        | 0.0169   |
| reward_motion      | 0.574    |
| reward_orientation | 0.0613   |
| reward_position    | 0.00161  |
| reward_rotation    | 0.0147   |
| reward_velocity    | -2.58    |
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 72       |
|    time_elapsed    | 51       |
|    total timesteps | 3723     |
---------------------------------
---------------------------------
| reward             | -1.83    |
| reward_ctrl        | 0.0213   |
| reward_motion      | 0.579    |
| reward_orientation | 0.0595   |
| reward_position    | 0.00133  |
| reward_rotation    | 0.0129   |
| reward_velocity    | -2.5     |
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -305     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 68       |
|    time_elapsed    | 68       |
|    total timesteps | 4657     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -355.71
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | -1.89    |
| reward_ctrl        | 0.0371   |
| reward_motion      | 0.533    |
| reward_orientation | 0.0589   |
| reward_position    | 0.00114  |
| reward_rotation    | 0.0225   |
| reward_velocity    | -2.54    |
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | -412     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 42       |
|    time_elapsed    | 156      |
|    total timesteps | 6657     |
| train/             |          |
|    actor_loss      | 3.34     |
|    critic_loss     | 3.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 1655     |
---------------------------------
---------------------------------
| reward             | -1.84    |
| reward_ctrl        | 0.0545   |
| reward_motion      | 0.473    |
| reward_orientation | 0.0573   |
| reward_position    | 0.000989 |
| reward_rotation    | 0.0406   |
| reward_velocity    | -2.46    |
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | -437     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 34       |
|    time_elapsed    | 253      |
|    total timesteps | 8657     |
| train/             |          |
|    actor_loss      | 5.4      |
|    critic_loss     | 4.51     |
|    learning_rate   | 0.001    |
|    n_updates       | 3655     |
---------------------------------
---------------------------------
| reward             | -1.78    |
| reward_ctrl        | 0.0677   |
| reward_motion      | 0.423    |
| reward_orientation | 0.0559   |
| reward_position    | 0.000876 |
| reward_rotation    | 0.0515   |
| reward_velocity    | -2.38    |
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | -468     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 30       |
|    time_elapsed    | 349      |
|    total timesteps | 10657    |
| train/             |          |
|    actor_loss      | 5.98     |
|    critic_loss     | 4.42     |
|    learning_rate   | 0.001    |
|    n_updates       | 5655     |
---------------------------------
Num timesteps: 12000
Best mean reward: -355.71 - Last mean reward per episode: -473.97
---------------------------------
| reward             | -1.8     |
| reward_ctrl        | 0.0781   |
| reward_motion      | 0.393    |
| reward_orientation | 0.055    |
| reward_position    | 0.000786 |
| reward_rotation    | 0.0566   |
| reward_velocity    | -2.38    |
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | -496     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 28       |
|    time_elapsed    | 447      |
|    total timesteps | 12657    |
| train/             |          |
|    actor_loss      | 7.55     |
|    critic_loss     | 5.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 7655     |
---------------------------------
---------------------------------
| reward             | -1.76    |
| reward_ctrl        | 0.0863   |
| reward_motion      | 0.363    |
| reward_orientation | 0.0553   |
| reward_position    | 0.000713 |
| reward_rotation    | 0.0661   |
| reward_velocity    | -2.33    |
| rollout/           |          |
|    ep_len_mean     | 333      |
|    ep_rew_mean     | -500     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 26       |
|    time_elapsed    | 551      |
|    total timesteps | 14657    |
| train/             |          |
|    actor_loss      | 8.31     |
|    critic_loss     | 4        |
|    learning_rate   | 0.001    |
|    n_updates       | 9655     |
---------------------------------
2021-06-01 18:53:46.842509: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 18:53:46.842561: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
[Actor] Version 5
[Actor] Version 5
Logging to rl/out_dir/models/exp68/TD3_43
---------------------------------
| reward             | -0.13    |
| reward_ctrl        | 0.0283   |
| reward_motion      | 0.6      |
| reward_orientation | 0.0598   |
| reward_position    | 2.28e-14 |
| reward_rotation    | 0.000366 |
| reward_velocity    | -0.818   |
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 28.6     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 48       |
|    time_elapsed    | 14       |
|    total timesteps | 697      |
---------------------------------
---------------------------------
| reward             | 0.252    |
| reward_ctrl        | 0.0212   |
| reward_motion      | 0.6      |
| reward_orientation | 0.0581   |
| reward_position    | 0.00816  |
| reward_rotation    | 0.0256   |
| reward_velocity    | -0.461   |
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 51       |
|    time_elapsed    | 20       |
|    total timesteps | 1068     |
---------------------------------
---------------------------------
| reward             | 0.13     |
| reward_ctrl        | 0.0251   |
| reward_motion      | 0.591    |
| reward_orientation | 0.0525   |
| reward_position    | 0.00519  |
| reward_rotation    | 0.0177   |
| reward_velocity    | -0.562   |
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | -5.61    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 54       |
|    time_elapsed    | 40       |
|    total timesteps | 2238     |
---------------------------------
---------------------------------
| reward             | 0.111    |
| reward_ctrl        | 0.0225   |
| reward_motion      | 0.594    |
| reward_orientation | 0.0581   |
| reward_position    | 0.00381  |
| reward_rotation    | 0.025    |
| reward_velocity    | -0.592   |
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | -3.94    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 55       |
|    time_elapsed    | 43       |
|    total timesteps | 2414     |
---------------------------------
---------------------------------
| reward             | 0.102    |
| reward_ctrl        | 0.0238   |
| reward_motion      | 0.595    |
| reward_orientation | 0.061    |
| reward_position    | 0.00301  |
| reward_rotation    | 0.0262   |
| reward_velocity    | -0.607   |
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | -4.09    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 56       |
|    time_elapsed    | 46       |
|    total timesteps | 2626     |
---------------------------------
---------------------------------
| reward             | 0.0795   |
| reward_ctrl        | 0.0226   |
| reward_motion      | 0.596    |
| reward_orientation | 0.0613   |
| reward_position    | 0.00248  |
| reward_rotation    | 0.0217   |
| reward_velocity    | -0.625   |
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | -4.21    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 57       |
|    time_elapsed    | 54       |
|    total timesteps | 3123     |
---------------------------------
---------------------------------
| reward             | 0.0565   |
| reward_ctrl        | 0.0233   |
| reward_motion      | 0.597    |
| reward_orientation | 0.0623   |
| reward_position    | 0.00224  |
| reward_rotation    | 0.0191   |
| reward_velocity    | -0.647   |
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | -3.97    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 59       |
|    time_elapsed    | 67       |
|    total timesteps | 4019     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -14.17
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.0161   |
| reward_ctrl        | 0.0332   |
| reward_motion      | 0.562    |
| reward_orientation | 0.0615   |
| reward_position    | 0.00195  |
| reward_rotation    | 0.0257   |
| reward_velocity    | -0.668   |
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | -17.4    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 45       |
|    time_elapsed    | 132      |
|    total timesteps | 6019     |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.374    |
|    learning_rate   | 0.001    |
|    n_updates       | 1015     |
---------------------------------
---------------------------------
| reward             | 0.00484  |
| reward_ctrl        | 0.0486   |
| reward_motion      | 0.521    |
| reward_orientation | 0.0586   |
| reward_position    | 0.00173  |
| reward_rotation    | 0.0305   |
| reward_velocity    | -0.655   |
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 35       |
|    time_elapsed    | 228      |
|    total timesteps | 8019     |
| train/             |          |
|    actor_loss      | -3.37    |
|    critic_loss     | 0.355    |
|    learning_rate   | 0.001    |
|    n_updates       | 3015     |
---------------------------------
---------------------------------
| reward             | -0.0138  |
| reward_ctrl        | 0.0613   |
| reward_motion      | 0.47     |
| reward_orientation | 0.0569   |
| reward_position    | 0.00155  |
| reward_rotation    | 0.0417   |
| reward_velocity    | -0.645   |
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -24.2    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 30       |
|    time_elapsed    | 324      |
|    total timesteps | 10019    |
| train/             |          |
|    actor_loss      | -4.86    |
|    critic_loss     | 0.338    |
|    learning_rate   | 0.001    |
|    n_updates       | 5015     |
---------------------------------
Num timesteps: 12000
Best mean reward: -14.17 - Last mean reward per episode: -31.38
---------------------------------
| reward             | -0.0581  |
| reward_ctrl        | 0.0713   |
| reward_motion      | 0.43     |
| reward_orientation | 0.0549   |
| reward_position    | 0.0014   |
| reward_rotation    | 0.0472   |
| reward_velocity    | -0.663   |
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | -31.1    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 28       |
|    time_elapsed    | 422      |
|    total timesteps | 12019    |
| train/             |          |
|    actor_loss      | -6.33    |
|    critic_loss     | 0.359    |
|    learning_rate   | 0.001    |
|    n_updates       | 7015     |
---------------------------------
---------------------------------
| reward             | -0.0581  |
| reward_ctrl        | 0.0797   |
| reward_motion      | 0.415    |
| reward_orientation | 0.0553   |
| reward_position    | 0.00129  |
| reward_rotation    | 0.0528   |
| reward_velocity    | -0.662   |
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | -31.8    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 26       |
|    time_elapsed    | 520      |
|    total timesteps | 14019    |
| train/             |          |
|    actor_loss      | -7.73    |
|    critic_loss     | 0.331    |
|    learning_rate   | 0.001    |
|    n_updates       | 9015     |
---------------------------------
---------------------------------
| reward             | -0.0882  |
| reward_ctrl        | 0.0867   |
| reward_motion      | 0.385    |
| reward_orientation | 0.0545   |
| reward_position    | 0.00118  |
| reward_rotation    | 0.0545   |
| reward_velocity    | -0.671   |
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | -39.5    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 25       |
|    time_elapsed    | 618      |
|    total timesteps | 16019    |
| train/             |          |
|    actor_loss      | -9.28    |
|    critic_loss     | 0.342    |
|    learning_rate   | 0.001    |
|    n_updates       | 11015    |
---------------------------------
Num timesteps: 18000
Best mean reward: -14.17 - Last mean reward per episode: -40.68
---------------------------------
| reward             | -0.0911  |
| reward_ctrl        | 0.0927   |
| reward_motion      | 0.375    |
| reward_orientation | 0.0547   |
| reward_position    | 0.0011   |
| reward_rotation    | 0.059    |
| reward_velocity    | -0.674   |
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | -42.9    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 25       |
|    time_elapsed    | 715      |
|    total timesteps | 18019    |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.408    |
|    learning_rate   | 0.001    |
|    n_updates       | 13015    |
---------------------------------
---------------------------------
| reward             | -0.0898  |
| reward_ctrl        | 0.098    |
| reward_motion      | 0.364    |
| reward_orientation | 0.0549   |
| reward_position    | 0.00102  |
| reward_rotation    | 0.0636   |
| reward_velocity    | -0.671   |
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | -45.3    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 24       |
|    time_elapsed    | 812      |
|    total timesteps | 20019    |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.378    |
|    learning_rate   | 0.001    |
|    n_updates       | 15015    |
---------------------------------
---------------------------------
| reward             | -0.104   |
| reward_ctrl        | 0.103    |
| reward_motion      | 0.347    |
| reward_orientation | 0.0551   |
| reward_position    | 0.000959 |
| reward_rotation    | 0.0672   |
| reward_velocity    | -0.676   |
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | -45.3    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 24       |
|    time_elapsed    | 908      |
|    total timesteps | 22019    |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.364    |
|    learning_rate   | 0.001    |
|    n_updates       | 17015    |
---------------------------------
Num timesteps: 24000
Best mean reward: -14.17 - Last mean reward per episode: -41.81
---------------------------------
| reward             | -0.101   |
| reward_ctrl        | 0.107    |
| reward_motion      | 0.335    |
| reward_orientation | 0.0552   |
| reward_position    | 0.000902 |
| reward_rotation    | 0.071    |
| reward_velocity    | -0.669   |
| rollout/           |          |
|    ep_len_mean     | 353      |
|    ep_rew_mean     | -43.6    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 23       |
|    time_elapsed    | 1004     |
|    total timesteps | 24019    |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.352    |
|    learning_rate   | 0.001    |
|    n_updates       | 19015    |
---------------------------------
---------------------------------
| reward             | -0.102   |
| reward_ctrl        | 0.11     |
| reward_motion      | 0.324    |
| reward_orientation | 0.0541   |
| reward_position    | 0.000851 |
| reward_rotation    | 0.0767   |
| reward_velocity    | -0.669   |
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | -40.2    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 23       |
|    time_elapsed    | 1106     |
|    total timesteps | 26019    |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.352    |
|    learning_rate   | 0.001    |
|    n_updates       | 21015    |
---------------------------------
2021-06-01 20:04:40.872853: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 20:04:40.872922: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
Traceback (most recent call last):
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/compat/__init__.py", line 46, in tf
    from tensorboard.compat import notf  # noqa: F401
ImportError: cannot import name 'notf' from 'tensorboard.compat' (/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/compat/__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "ddpg.py", line 273, in <module>
    model.learn(total_timesteps=int(3e6), callback=callback)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/td3/td3.py", line 203, in learn
    return super(TD3, self).learn(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 339, in learn
    total_timesteps, callback = self._setup_learn(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 315, in _setup_learn
    return super()._setup_learn(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py", line 406, in _setup_learn
    utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/utils.py", line 190, in configure_logger
    logger.configure(save_path, ["stdout", "tensorboard"])
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/logger.py", line 693, in configure
    output_formats = [make_output_format(f, folder, log_suffix) for f in format_strings]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/logger.py", line 693, in <listcomp>
    output_formats = [make_output_format(f, folder, log_suffix) for f in format_strings]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/logger.py", line 396, in make_output_format
    return TensorBoardOutputFormat(log_dir)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/logger.py", line 337, in __init__
    self.writer = SummaryWriter(log_dir=folder)
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py", line 221, in __init__
    self._get_file_writer()
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py", line 251, in _get_file_writer
    self.file_writer = FileWriter(self.log_dir, self.max_queue,
  File "/home/shandilya/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py", line 61, in __init__
    self.event_writer = EventFileWriter(
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py", line 76, in __init__
    if not tf.io.gfile.exists(logdir):
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/lazy.py", line 68, in __getattr__
    return getattr(load_once(self), attr_name)
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/lazy.py", line 100, in wrapper
    cache[arg] = f(arg)
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/lazy.py", line 53, in load_once
    module = load_fn()
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorboard/compat/__init__.py", line 49, in tf
    import tensorflow
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorflow/__init__.py", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File "/home/shandilya/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
KeyboardInterrupt
2021-06-01 20:04:52.530614: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 20:04:52.530692: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
Logging to rl/out_dir/models/exp68/TD3_45
---------------------------------
| reward             | 0.304    |
| reward_ctrl        | 0.0743   |
| reward_motion      | 0        |
| reward_orientation | 0.0351   |
| reward_position    | 0        |
| reward_rotation    | 0.0568   |
| reward_velocity    | 0.0245   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 72       |
|    time_elapsed    | 27       |
|    total timesteps | 2000     |
---------------------------------
2021-06-01 20:08:24.059547: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 20:08:24.059588: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
Logging to rl/out_dir/models/exp68/TD3_46
---------------------------------
| reward             | 0.139    |
| reward_contact     | 5.19e-07 |
| reward_ctrl        | 0.0238   |
| reward_motion      | 0        |
| reward_orientation | 0.057    |
| reward_position    | 0        |
| reward_rotation    | 0.00776  |
| reward_torque      | 0.0432   |
| reward_velocity    | 0.00768  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 71       |
|    time_elapsed    | 27       |
|    total timesteps | 2000     |
---------------------------------
---------------------------------
| reward             | 0.146    |
| reward_contact     | 0.00865  |
| reward_ctrl        | 0.0228   |
| reward_motion      | 0        |
| reward_orientation | 0.0515   |
| reward_position    | 0        |
| reward_rotation    | 0.0164   |
| reward_torque      | 0.0433   |
| reward_velocity    | 0.00362  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 73       |
|    time_elapsed    | 54       |
|    total timesteps | 4000     |
---------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 124.85
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.198    |
| reward_contact     | 0.0113   |
| reward_ctrl        | 0.039    |
| reward_motion      | 0        |
| reward_orientation | 0.0505   |
| reward_position    | 0        |
| reward_rotation    | 0.0423   |
| reward_torque      | 0.0459   |
| reward_velocity    | 0.00874  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 60       |
|    time_elapsed    | 98       |
|    total timesteps | 6000     |
| train/             |          |
|    actor_loss      | -1.16    |
|    critic_loss     | 0.0301   |
|    learning_rate   | 0.001    |
|    n_updates       | 995      |
---------------------------------
---------------------------------
| reward             | 0.257    |
| reward_contact     | 0.00883  |
| reward_ctrl        | 0.0679   |
| reward_motion      | 0        |
| reward_orientation | 0.05     |
| reward_position    | 0        |
| reward_rotation    | 0.074    |
| reward_torque      | 0.0496   |
| reward_velocity    | 0.0067   |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 49       |
|    time_elapsed    | 161      |
|    total timesteps | 8000     |
| train/             |          |
|    actor_loss      | -3.74    |
|    critic_loss     | 0.0984   |
|    learning_rate   | 0.001    |
|    n_updates       | 2995     |
---------------------------------
---------------------------------
| reward             | 0.273    |
| reward_contact     | 0.00726  |
| reward_ctrl        | 0.0846   |
| reward_motion      | 0        |
| reward_orientation | 0.0493   |
| reward_position    | 0        |
| reward_rotation    | 0.0749   |
| reward_torque      | 0.0518   |
| reward_velocity    | 0.00542  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 44       |
|    time_elapsed    | 225      |
|    total timesteps | 10000    |
| train/             |          |
|    actor_loss      | -6.99    |
|    critic_loss     | 0.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 4995     |
---------------------------------
Num timesteps: 12000
Best mean reward: 124.85 - Last mean reward per episode: 152.44
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.291    |
| reward_contact     | 0.00609  |
| reward_ctrl        | 0.0958   |
| reward_motion      | 0        |
| reward_orientation | 0.0477   |
| reward_position    | 0        |
| reward_rotation    | 0.0835   |
| reward_torque      | 0.0532   |
| reward_velocity    | 0.00452  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 41       |
|    time_elapsed    | 288      |
|    total timesteps | 12000    |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.168    |
|    learning_rate   | 0.001    |
|    n_updates       | 6995     |
---------------------------------
---------------------------------
| reward             | 0.303    |
| reward_contact     | 0.0078   |
| reward_ctrl        | 0.101    |
| reward_motion      | 0        |
| reward_orientation | 0.0468   |
| reward_position    | 0        |
| reward_rotation    | 0.088    |
| reward_torque      | 0.054    |
| reward_velocity    | 0.00535  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 39       |
|    time_elapsed    | 351      |
|    total timesteps | 14000    |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.229    |
|    learning_rate   | 0.001    |
|    n_updates       | 8995     |
---------------------------------
---------------------------------
| reward             | 0.314    |
| reward_contact     | 0.0068   |
| reward_ctrl        | 0.107    |
| reward_motion      | 0        |
| reward_orientation | 0.0473   |
| reward_position    | 0        |
| reward_rotation    | 0.0925   |
| reward_torque      | 0.0547   |
| reward_velocity    | 0.00569  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 38       |
|    time_elapsed    | 415      |
|    total timesteps | 16000    |
| train/             |          |
|    actor_loss      | -15.7    |
|    critic_loss     | 0.251    |
|    learning_rate   | 0.001    |
|    n_updates       | 10995    |
---------------------------------
Num timesteps: 18000
Best mean reward: 152.44 - Last mean reward per episode: 170.16
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------
| reward             | 0.333    |
| reward_contact     | 0.00606  |
| reward_ctrl        | 0.112    |
| reward_motion      | 0        |
| reward_orientation | 0.0467   |
| reward_position    | 1.3e-291 |
| reward_rotation    | 0.105    |
| reward_torque      | 0.0553   |
| reward_velocity    | 0.00792  |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 37       |
|    time_elapsed    | 480      |
|    total timesteps | 18000    |
| train/             |          |
|    actor_loss      | -18.2    |
|    critic_loss     | 0.188    |
|    learning_rate   | 0.001    |
|    n_updates       | 12995    |
---------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Creating window glfw
Traceback (most recent call last):
  File "ddpg.py", line 200, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 467, in __init__
    super(AntEnvV4, self).__init__(path)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 84, in __init__
    mujoco_env.MujocoEnv.__init__(self, path, 5)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 64, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 646, in step
    info['reward_contact'] = np.exp(-np.square(np.linalg.norm(np.clip(self.sim.data.cfrc_ext, -1, 1).flat))) * self.w[6]
IndexError: index 6 is out of bounds for axis 0 with size 5
2021-06-01 20:36:10.847650: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-01 20:36:10.847702: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
Using cpu device
Logging to rl/out_dir/models/exp68/PPO_1
---------------------------------
| reward             | 0.208    |
| reward_contact     | 0.045    |
| reward_ctrl        | 0.0145   |
| reward_motion      | 0        |
| reward_orientation | 0.0345   |
| reward_position    | 0        |
| reward_rotation    | 0.0482   |
| reward_torque      | 0.0376   |
| reward_velocity    | 0.0284   |
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 1        |
|    time_elapsed    | 45       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| reward                  | 0.194       |
| reward_contact          | 0.0301      |
| reward_ctrl             | 0.0262      |
| reward_motion           | 0           |
| reward_orientation      | 0.0417      |
| reward_position         | 0           |
| reward_rotation         | 0.0406      |
| reward_torque           | 0.0409      |
| reward_velocity         | 0.0145      |
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 99.8        |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 2           |
|    time_elapsed         | 94          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.052356414 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | -0.398      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0369     |
|    std                  | 1           |
|    value_loss           | 0.532       |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 97.37
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.188      |
| reward_contact          | 0.0401     |
| reward_ctrl             | 0.0216     |
| reward_motion           | 0          |
| reward_orientation      | 0.0419     |
| reward_position         | 0          |
| reward_rotation         | 0.0311     |
| reward_torque           | 0.0399     |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 478        |
|    ep_rew_mean          | 97.4       |
| time/                   |            |
|    fps                  | 43         |
|    iterations           | 3          |
|    time_elapsed         | 140        |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.06653556 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.4      |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.049      |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0364    |
|    std                  | 1          |
|    value_loss           | 0.565      |
----------------------------------------
-----------------------------------------
| reward                  | 0.198       |
| reward_contact          | 0.0413      |
| reward_ctrl             | 0.0196      |
| reward_motion           | 0           |
| reward_orientation      | 0.0426      |
| reward_position         | 0           |
| reward_rotation         | 0.0448      |
| reward_torque           | 0.0389      |
| reward_velocity         | 0.0105      |
| rollout/                |             |
|    ep_len_mean          | 477         |
|    ep_rew_mean          | 96.6        |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 4           |
|    time_elapsed         | 187         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.051808137 |
|    clip_fraction        | 0.43        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0643      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0338     |
|    std                  | 0.995       |
|    value_loss           | 0.599       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.229       |
| reward_contact          | 0.0464      |
| reward_ctrl             | 0.0197      |
| reward_motion           | 0           |
| reward_orientation      | 0.043       |
| reward_position         | 4.7e-51     |
| reward_rotation         | 0.0491      |
| reward_torque           | 0.0398      |
| reward_velocity         | 0.0311      |
| rollout/                |             |
|    ep_len_mean          | 458         |
|    ep_rew_mean          | 93.4        |
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 5           |
|    time_elapsed         | 222         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.042862345 |
|    clip_fraction        | 0.397       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0343     |
|    std                  | 0.991       |
|    value_loss           | 0.752       |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 97.37 - Last mean reward per episode: 86.16
-----------------------------------------
| reward                  | 0.221       |
| reward_contact          | 0.0493      |
| reward_ctrl             | 0.0179      |
| reward_motion           | 0           |
| reward_orientation      | 0.0417      |
| reward_position         | 3.69e-51    |
| reward_rotation         | 0.0481      |
| reward_torque           | 0.039       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 430         |
|    ep_rew_mean          | 86.4        |
| time/                   |             |
|    fps                  | 48          |
|    iterations           | 6           |
|    time_elapsed         | 255         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.051391575 |
|    clip_fraction        | 0.456       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.035      |
|    std                  | 0.99        |
|    value_loss           | 1.03        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.224       |
| reward_contact          | 0.0473      |
| reward_ctrl             | 0.0205      |
| reward_motion           | 0           |
| reward_orientation      | 0.0429      |
| reward_position         | 3.13e-51    |
| reward_rotation         | 0.0452      |
| reward_torque           | 0.0397      |
| reward_velocity         | 0.0285      |
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | 86.3        |
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 7           |
|    time_elapsed         | 288         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.047505833 |
|    clip_fraction        | 0.44        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.37        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0306     |
|    std                  | 0.985       |
|    value_loss           | 1.01        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.223       |
| reward_contact          | 0.0477      |
| reward_ctrl             | 0.0212      |
| reward_motion           | 0           |
| reward_orientation      | 0.0434      |
| reward_position         | 3.66e-15    |
| reward_rotation         | 0.0453      |
| reward_torque           | 0.0403      |
| reward_velocity         | 0.0251      |
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 82.8        |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 8           |
|    time_elapsed         | 321         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.062072456 |
|    clip_fraction        | 0.449       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0869      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0324     |
|    std                  | 0.983       |
|    value_loss           | 0.98        |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 97.37 - Last mean reward per episode: 80.98
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0478     |
| reward_ctrl             | 0.0207     |
| reward_motion           | 0          |
| reward_orientation      | 0.0438     |
| reward_position         | 3.24e-15   |
| reward_rotation         | 0.044      |
| reward_torque           | 0.0401     |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 410        |
|    ep_rew_mean          | 81.5       |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 9          |
|    time_elapsed         | 354        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06317091 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.219      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0359    |
|    std                  | 0.983      |
|    value_loss           | 1.33       |
----------------------------------------
-----------------------------------------
| reward                  | 0.223       |
| reward_contact          | 0.0475      |
| reward_ctrl             | 0.0217      |
| reward_motion           | 0           |
| reward_orientation      | 0.0445      |
| reward_position         | 2.97e-15    |
| reward_rotation         | 0.0414      |
| reward_torque           | 0.0405      |
| reward_velocity         | 0.0274      |
| rollout/                |             |
|    ep_len_mean          | 417         |
|    ep_rew_mean          | 83.4        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 10          |
|    time_elapsed         | 386         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.067820676 |
|    clip_fraction        | 0.488       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0287     |
|    std                  | 0.985       |
|    value_loss           | 0.938       |
-----------------------------------------
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0476     |
| reward_ctrl             | 0.0212     |
| reward_motion           | 0          |
| reward_orientation      | 0.0447     |
| reward_position         | 2.69e-15   |
| reward_rotation         | 0.0416     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.026      |
| rollout/                |            |
|    ep_len_mean          | 417        |
|    ep_rew_mean          | 83.9       |
| time/                   |            |
|    fps                  | 53         |
|    iterations           | 11         |
|    time_elapsed         | 420        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07811628 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.364      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0307    |
|    std                  | 0.982      |
|    value_loss           | 1.2        |
----------------------------------------
Num timesteps: 24000
Best mean reward: 97.37 - Last mean reward per episode: 84.18
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.048      |
| reward_ctrl             | 0.0202     |
| reward_motion           | 0          |
| reward_orientation      | 0.0452     |
| reward_position         | 4.25e-06   |
| reward_rotation         | 0.0371     |
| reward_torque           | 0.0404     |
| reward_velocity         | 0.0282     |
| rollout/                |            |
|    ep_len_mean          | 407        |
|    ep_rew_mean          | 82.2       |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 12         |
|    time_elapsed         | 452        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08893118 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0296     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0311    |
|    std                  | 0.982      |
|    value_loss           | 0.879      |
----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.049      |
| reward_ctrl             | 0.0206     |
| reward_motion           | 0          |
| reward_orientation      | 0.0459     |
| reward_position         | 3.93e-06   |
| reward_rotation         | 0.0345     |
| reward_torque           | 0.0404     |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 405        |
|    ep_rew_mean          | 81.6       |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 13         |
|    time_elapsed         | 485        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.08422646 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.294      |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0348    |
|    std                  | 0.98       |
|    value_loss           | 1.66       |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0479     |
| reward_ctrl             | 0.0212     |
| reward_motion           | 0          |
| reward_orientation      | 0.0455     |
| reward_position         | 3.7e-06    |
| reward_rotation         | 0.0351     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.025      |
| rollout/                |            |
|    ep_len_mean          | 411        |
|    ep_rew_mean          | 82.1       |
| time/                   |            |
|    fps                  | 55         |
|    iterations           | 14         |
|    time_elapsed         | 518        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.10316871 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0284    |
|    std                  | 0.978      |
|    value_loss           | 0.766      |
----------------------------------------
Num timesteps: 30000
Best mean reward: 97.37 - Last mean reward per episode: 81.77
-----------------------------------------
| reward                  | 0.216       |
| reward_contact          | 0.048       |
| reward_ctrl             | 0.0207      |
| reward_motion           | 0           |
| reward_orientation      | 0.0453      |
| reward_position         | 3.45e-06    |
| reward_rotation         | 0.0379      |
| reward_torque           | 0.0406      |
| reward_velocity         | 0.0237      |
| rollout/                |             |
|    ep_len_mean          | 413         |
|    ep_rew_mean          | 82.3        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 15          |
|    time_elapsed         | 550         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.106866926 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0793      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0292     |
|    std                  | 0.978       |
|    value_loss           | 0.571       |
-----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.0486     |
| reward_ctrl             | 0.0203     |
| reward_motion           | 0          |
| reward_orientation      | 0.0458     |
| reward_position         | 3.27e-06   |
| reward_rotation         | 0.0387     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 415        |
|    ep_rew_mean          | 82.9       |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 16         |
|    time_elapsed         | 583        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08524905 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00302    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0363    |
|    std                  | 0.979      |
|    value_loss           | 0.562      |
----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.0484     |
| reward_ctrl             | 0.0208     |
| reward_motion           | 0          |
| reward_orientation      | 0.0457     |
| reward_position         | 3.11e-06   |
| reward_rotation         | 0.0369     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 84.1       |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 17         |
|    time_elapsed         | 616        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.09411088 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00198    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0418    |
|    std                  | 0.975      |
|    value_loss           | 0.543      |
----------------------------------------
Num timesteps: 36000
Best mean reward: 97.37 - Last mean reward per episode: 83.89
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0477     |
| reward_ctrl             | 0.0201     |
| reward_motion           | 0          |
| reward_orientation      | 0.0452     |
| reward_position         | 2.93e-06   |
| reward_rotation         | 0.0366     |
| reward_torque           | 0.0403     |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 84.5       |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 18         |
|    time_elapsed         | 648        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.06915943 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0919     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0356    |
|    std                  | 0.972      |
|    value_loss           | 1.01       |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0479     |
| reward_ctrl             | 0.0199     |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 2.75e-06   |
| reward_rotation         | 0.0378     |
| reward_torque           | 0.0403     |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 83.3       |
| time/                   |            |
|    fps                  | 57         |
|    iterations           | 19         |
|    time_elapsed         | 680        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.10781521 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0338     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0397    |
|    std                  | 0.97       |
|    value_loss           | 0.73       |
----------------------------------------
-----------------------------------------
| reward                  | 0.215       |
| reward_contact          | 0.0473      |
| reward_ctrl             | 0.0203      |
| reward_motion           | 0           |
| reward_orientation      | 0.0449      |
| reward_position         | 2.61e-06    |
| reward_rotation         | 0.0374      |
| reward_torque           | 0.0405      |
| reward_velocity         | 0.025       |
| rollout/                |             |
|    ep_len_mean          | 413         |
|    ep_rew_mean          | 83          |
| time/                   |             |
|    fps                  | 57          |
|    iterations           | 20          |
|    time_elapsed         | 713         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.080441326 |
|    clip_fraction        | 0.533       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0984      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0318     |
|    std                  | 0.966       |
|    value_loss           | 1.45        |
-----------------------------------------
Num timesteps: 42000
Best mean reward: 97.37 - Last mean reward per episode: 83.49
----------------------------------------
| reward                  | 0.216      |
| reward_contact          | 0.0463     |
| reward_ctrl             | 0.0204     |
| reward_motion           | 0          |
| reward_orientation      | 0.0454     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0379     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 415        |
|    ep_rew_mean          | 83.5       |
| time/                   |            |
|    fps                  | 57         |
|    iterations           | 21         |
|    time_elapsed         | 745        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.08174657 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.207      |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0377    |
|    std                  | 0.968      |
|    value_loss           | 0.807      |
----------------------------------------
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0469     |
| reward_ctrl             | 0.0198     |
| reward_motion           | 0          |
| reward_orientation      | 0.045      |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0355     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 412        |
|    ep_rew_mean          | 83.1       |
| time/                   |            |
|    fps                  | 57         |
|    iterations           | 22         |
|    time_elapsed         | 778        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.11251241 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0139     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0401    |
|    std                  | 0.964      |
|    value_loss           | 0.607      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.047      |
| reward_ctrl             | 0.02       |
| reward_motion           | 0          |
| reward_orientation      | 0.0449     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0369     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.026      |
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 83.4       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 23         |
|    time_elapsed         | 811        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.08218044 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0346    |
|    std                  | 0.963      |
|    value_loss           | 0.754      |
----------------------------------------
Num timesteps: 48000
Best mean reward: 97.37 - Last mean reward per episode: 82.22
-----------------------------------------
| reward                  | 0.213       |
| reward_contact          | 0.0462      |
| reward_ctrl             | 0.0213      |
| reward_motion           | 0           |
| reward_orientation      | 0.0447      |
| reward_position         | 2.55e-06    |
| reward_rotation         | 0.035       |
| reward_torque           | 0.0413      |
| reward_velocity         | 0.025       |
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 82.5        |
| time/                   |             |
|    fps                  | 58          |
|    iterations           | 24          |
|    time_elapsed         | 843         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.095074005 |
|    clip_fraction        | 0.574       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0249      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0379     |
|    std                  | 0.966       |
|    value_loss           | 0.533       |
-----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.045      |
| reward_ctrl             | 0.0219     |
| reward_motion           | 0          |
| reward_orientation      | 0.0451     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0329     |
| reward_torque           | 0.0414     |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 409        |
|    ep_rew_mean          | 82.7       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 25         |
|    time_elapsed         | 876        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.08038214 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.588      |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0357    |
|    std                  | 0.962      |
|    value_loss           | 1.44       |
----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0444     |
| reward_ctrl             | 0.0222     |
| reward_motion           | 0          |
| reward_orientation      | 0.0462     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0326     |
| reward_torque           | 0.0413     |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 412        |
|    ep_rew_mean          | 83.4       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 26         |
|    time_elapsed         | 909        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.11471115 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0405     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0437    |
|    std                  | 0.961      |
|    value_loss           | 0.573      |
----------------------------------------
Num timesteps: 54000
Best mean reward: 97.37 - Last mean reward per episode: 83.45
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0438     |
| reward_ctrl             | 0.0211     |
| reward_motion           | 0          |
| reward_orientation      | 0.0458     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0325     |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 414        |
|    ep_rew_mean          | 84.3       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 27         |
|    time_elapsed         | 941        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.09753433 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.266      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.96       |
|    value_loss           | 0.786      |
----------------------------------------
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0444     |
| reward_ctrl             | 0.0215     |
| reward_motion           | 0          |
| reward_orientation      | 0.0458     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0366     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 85.3       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 28         |
|    time_elapsed         | 974        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.09176916 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0555     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0405    |
|    std                  | 0.959      |
|    value_loss           | 0.557      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0426     |
| reward_ctrl             | 0.0214     |
| reward_motion           | 0          |
| reward_orientation      | 0.0447     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 421        |
|    ep_rew_mean          | 85.7       |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 29         |
|    time_elapsed         | 1007       |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.11703261 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0145     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0416    |
|    std                  | 0.959      |
|    value_loss           | 0.306      |
----------------------------------------
Num timesteps: 60000
Best mean reward: 97.37 - Last mean reward per episode: 86.51
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0426     |
| reward_ctrl             | 0.0224     |
| reward_motion           | 0          |
| reward_orientation      | 0.0446     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0413     |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 85.7       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 30         |
|    time_elapsed         | 1039       |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.09097919 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00902    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0324    |
|    std                  | 0.956      |
|    value_loss           | 0.531      |
----------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0416     |
| reward_ctrl             | 0.0219     |
| reward_motion           | 0          |
| reward_orientation      | 0.0445     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.041      |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 418        |
|    ep_rew_mean          | 85         |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 31         |
|    time_elapsed         | 1071       |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.08921928 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.041      |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0371    |
|    std                  | 0.952      |
|    value_loss           | 0.672      |
----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.041      |
| reward_ctrl             | 0.022      |
| reward_motion           | 0          |
| reward_orientation      | 0.0439     |
| reward_position         | 2.55e-06   |
| reward_rotation         | 0.04       |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 414        |
|    ep_rew_mean          | 83.6       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 32         |
|    time_elapsed         | 1104       |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.11949683 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0162    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0434    |
|    std                  | 0.949      |
|    value_loss           | 0.739      |
----------------------------------------
Num timesteps: 66000
Best mean reward: 97.37 - Last mean reward per episode: 83.75
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0398     |
| reward_ctrl             | 0.0229     |
| reward_motion           | 0          |
| reward_orientation      | 0.0436     |
| reward_position         | 9.89e-11   |
| reward_rotation         | 0.0412     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 85.2       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 33         |
|    time_elapsed         | 1137       |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.15314746 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00217    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.045     |
|    std                  | 0.948      |
|    value_loss           | 0.491      |
----------------------------------------
--------------------------------------
| reward                  | 0.213    |
| reward_contact          | 0.0392   |
| reward_ctrl             | 0.0237   |
| reward_motion           | 0        |
| reward_orientation      | 0.0435   |
| reward_position         | 9.89e-11 |
| reward_rotation         | 0.0428   |
| reward_torque           | 0.0417   |
| reward_velocity         | 0.0216   |
| rollout/                |          |
|    ep_len_mean          | 422      |
|    ep_rew_mean          | 86       |
| time/                   |          |
|    fps                  | 59       |
|    iterations           | 34       |
|    time_elapsed         | 1169     |
|    total_timesteps      | 69632    |
| train/                  |          |
|    approx_kl            | 0.114464 |
|    clip_fraction        | 0.57     |
|    clip_range           | 0.2      |
|    entropy_loss         | -10.9    |
|    explained_variance   | 0.832    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0963   |
|    n_updates            | 330      |
|    policy_gradient_loss | -0.0424  |
|    std                  | 0.945    |
|    value_loss           | 0.645    |
--------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0381     |
| reward_ctrl             | 0.0234     |
| reward_motion           | 0          |
| reward_orientation      | 0.0426     |
| reward_position         | 9.89e-11   |
| reward_rotation         | 0.0434     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 425        |
|    ep_rew_mean          | 86.5       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 35         |
|    time_elapsed         | 1203       |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.11214922 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.172      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0404    |
|    std                  | 0.94       |
|    value_loss           | 0.73       |
----------------------------------------
Num timesteps: 72000
Best mean reward: 97.37 - Last mean reward per episode: 86.54
---------------------------------------
| reward                  | 0.206     |
| reward_contact          | 0.0375    |
| reward_ctrl             | 0.0239    |
| reward_motion           | 0         |
| reward_orientation      | 0.0419    |
| reward_position         | 9.89e-11  |
| reward_rotation         | 0.0389    |
| reward_torque           | 0.0418    |
| reward_velocity         | 0.0218    |
| rollout/                |           |
|    ep_len_mean          | 422       |
|    ep_rew_mean          | 85.9      |
| time/                   |           |
|    fps                  | 59        |
|    iterations           | 36        |
|    time_elapsed         | 1235      |
|    total_timesteps      | 73728     |
| train/                  |           |
|    approx_kl            | 0.1458636 |
|    clip_fraction        | 0.631     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.8     |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0253   |
|    n_updates            | 350       |
|    policy_gradient_loss | -0.0457   |
|    std                  | 0.935     |
|    value_loss           | 0.285     |
---------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0358     |
| reward_ctrl             | 0.0238     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 9.89e-11   |
| reward_rotation         | 0.0392     |
| reward_torque           | 0.0418     |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 84         |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 37         |
|    time_elapsed         | 1268       |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.14768627 |
|    clip_fraction        | 0.605      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00702   |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.038     |
|    std                  | 0.933      |
|    value_loss           | 0.243      |
----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0358     |
| reward_ctrl             | 0.0257     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 9.89e-11   |
| reward_rotation         | 0.0402     |
| reward_torque           | 0.0423     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 84.7       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 38         |
|    time_elapsed         | 1300       |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.12285592 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.016     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.93       |
|    value_loss           | 0.382      |
----------------------------------------
Num timesteps: 78000
Best mean reward: 97.37 - Last mean reward per episode: 84.18
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0364     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0396     |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.0193     |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 84.1       |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 39         |
|    time_elapsed         | 1333       |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.17544961 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.07      |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0539    |
|    std                  | 0.925      |
|    value_loss           | 0.253      |
----------------------------------------
-----------------------------------------
| reward                  | 0.205       |
| reward_contact          | 0.0376      |
| reward_ctrl             | 0.0266      |
| reward_motion           | 0           |
| reward_orientation      | 0.0415      |
| reward_position         | 0.000113    |
| reward_rotation         | 0.0379      |
| reward_torque           | 0.0427      |
| reward_velocity         | 0.0182      |
| rollout/                |             |
|    ep_len_mean          | 421         |
|    ep_rew_mean          | 85.2        |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 40          |
|    time_elapsed         | 1365        |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.106710985 |
|    clip_fraction        | 0.58        |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0487     |
|    std                  | 0.922       |
|    value_loss           | 0.506       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.205       |
| reward_contact          | 0.0376      |
| reward_ctrl             | 0.0264      |
| reward_motion           | 0           |
| reward_orientation      | 0.0418      |
| reward_position         | 0.000113    |
| reward_rotation         | 0.0382      |
| reward_torque           | 0.0426      |
| reward_velocity         | 0.0182      |
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 85.8        |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 41          |
|    time_elapsed         | 1398        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.080972455 |
|    clip_fraction        | 0.549       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0895      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0505     |
|    std                  | 0.922       |
|    value_loss           | 0.843       |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 97.37 - Last mean reward per episode: 85.84
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0377     |
| reward_ctrl             | 0.0261     |
| reward_motion           | 0          |
| reward_orientation      | 0.0422     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0387     |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 425        |
|    ep_rew_mean          | 86.5       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 42         |
|    time_elapsed         | 1431       |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.14004725 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0384    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0528    |
|    std                  | 0.918      |
|    value_loss           | 0.544      |
----------------------------------------
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.0382     |
| reward_ctrl             | 0.0263     |
| reward_motion           | 0          |
| reward_orientation      | 0.0418     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0369     |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 84.7       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 43         |
|    time_elapsed         | 1464       |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.13821916 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00387    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0551    |
|    std                  | 0.915      |
|    value_loss           | 0.377      |
----------------------------------------
Num timesteps: 90000
Best mean reward: 97.37 - Last mean reward per episode: 86.03
-----------------------------------------
| reward                  | 0.205       |
| reward_contact          | 0.037       |
| reward_ctrl             | 0.0267      |
| reward_motion           | 0           |
| reward_orientation      | 0.0415      |
| reward_position         | 0.000113    |
| reward_rotation         | 0.0382      |
| reward_torque           | 0.0426      |
| reward_velocity         | 0.0191      |
| rollout/                |             |
|    ep_len_mean          | 421         |
|    ep_rew_mean          | 86          |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 44          |
|    time_elapsed         | 1498        |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.096726075 |
|    clip_fraction        | 0.566       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.218       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0467     |
|    std                  | 0.911       |
|    value_loss           | 1.2         |
-----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0372     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0385     |
| reward_torque           | 0.0423     |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 86.3       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 45         |
|    time_elapsed         | 1531       |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.17431837 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0363    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0385    |
|    std                  | 0.905      |
|    value_loss           | 0.257      |
----------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0365     |
| reward_ctrl             | 0.0259     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0372     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 88.1       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 46         |
|    time_elapsed         | 1563       |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.12581281 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0415    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0558    |
|    std                  | 0.903      |
|    value_loss           | 0.516      |
----------------------------------------
Num timesteps: 96000
Best mean reward: 97.37 - Last mean reward per episode: 88.34
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.0365     |
| reward_ctrl             | 0.0263     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0384     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0189     |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 88.3       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 47         |
|    time_elapsed         | 1596       |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.14646451 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0556    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0481    |
|    std                  | 0.902      |
|    value_loss           | 0.351      |
----------------------------------------
---------------------------------------
| reward                  | 0.198     |
| reward_contact          | 0.0376    |
| reward_ctrl             | 0.0255    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 0.000113  |
| reward_rotation         | 0.0344    |
| reward_torque           | 0.0427    |
| reward_velocity         | 0.0169    |
| rollout/                |           |
|    ep_len_mean          | 423       |
|    ep_rew_mean          | 87        |
| time/                   |           |
|    fps                  | 60        |
|    iterations           | 48        |
|    time_elapsed         | 1629      |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.1592549 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.5     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00286   |
|    n_updates            | 470       |
|    policy_gradient_loss | -0.0486   |
|    std                  | 0.897     |
|    value_loss           | 0.283     |
---------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.037      |
| reward_ctrl             | 0.026      |
| reward_motion           | 0          |
| reward_orientation      | 0.0411     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0298     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 424        |
|    ep_rew_mean          | 87.4       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 49         |
|    time_elapsed         | 1661       |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.19279414 |
|    clip_fraction        | 0.658      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00255   |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0526    |
|    std                  | 0.894      |
|    value_loss           | 0.41       |
----------------------------------------
Num timesteps: 102000
Best mean reward: 97.37 - Last mean reward per episode: 86.90
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0383     |
| reward_ctrl             | 0.0256     |
| reward_motion           | 0          |
| reward_orientation      | 0.0417     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0282     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0195     |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 86.9       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 50         |
|    time_elapsed         | 1694       |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.15083674 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.045     |
|    std                  | 0.893      |
|    value_loss           | 0.509      |
----------------------------------------
----------------------------------------
| reward                  | 0.191      |
| reward_contact          | 0.0376     |
| reward_ctrl             | 0.0255     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0242     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 86         |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 51         |
|    time_elapsed         | 1727       |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.12733456 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.036     |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.048     |
|    std                  | 0.888      |
|    value_loss           | 0.691      |
----------------------------------------
----------------------------------------
| reward                  | 0.189      |
| reward_contact          | 0.0364     |
| reward_ctrl             | 0.0255     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0255     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 86.5       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 52         |
|    time_elapsed         | 1759       |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.15578061 |
|    clip_fraction        | 0.628      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.178      |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0478    |
|    std                  | 0.883      |
|    value_loss           | 1.26       |
----------------------------------------
Num timesteps: 108000
Best mean reward: 97.37 - Last mean reward per episode: 87.73
----------------------------------------
| reward                  | 0.188      |
| reward_contact          | 0.0358     |
| reward_ctrl             | 0.0257     |
| reward_motion           | 0          |
| reward_orientation      | 0.0415     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0258     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 423        |
|    ep_rew_mean          | 87.9       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 53         |
|    time_elapsed         | 1792       |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.15946235 |
|    clip_fraction        | 0.624      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.254      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.877      |
|    value_loss           | 0.672      |
----------------------------------------
----------------------------------------
| reward                  | 0.187      |
| reward_contact          | 0.0347     |
| reward_ctrl             | 0.0264     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0272     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 87.9       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 54         |
|    time_elapsed         | 1824       |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.17524073 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0761    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.872      |
|    value_loss           | 0.301      |
----------------------------------------
----------------------------------------
| reward                  | 0.186      |
| reward_contact          | 0.0364     |
| reward_ctrl             | 0.0252     |
| reward_motion           | 0          |
| reward_orientation      | 0.0422     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0255     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 418        |
|    ep_rew_mean          | 86.4       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 55         |
|    time_elapsed         | 1857       |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.19285138 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0883    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0683    |
|    std                  | 0.867      |
|    value_loss           | 0.277      |
----------------------------------------
Num timesteps: 114000
Best mean reward: 97.37 - Last mean reward per episode: 86.49
---------------------------------------
| reward                  | 0.187     |
| reward_contact          | 0.0352    |
| reward_ctrl             | 0.0255    |
| reward_motion           | 0         |
| reward_orientation      | 0.0431    |
| reward_position         | 0.000113  |
| reward_rotation         | 0.0261    |
| reward_torque           | 0.0426    |
| reward_velocity         | 0.0148    |
| rollout/                |           |
|    ep_len_mean          | 414       |
|    ep_rew_mean          | 85.9      |
| time/                   |           |
|    fps                  | 60        |
|    iterations           | 56        |
|    time_elapsed         | 1889      |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.1368551 |
|    clip_fraction        | 0.634     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.2     |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0196   |
|    n_updates            | 550       |
|    policy_gradient_loss | -0.0566   |
|    std                  | 0.859     |
|    value_loss           | 0.565     |
---------------------------------------
----------------------------------------
| reward                  | 0.187      |
| reward_contact          | 0.0362     |
| reward_ctrl             | 0.0245     |
| reward_motion           | 0          |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0254     |
| reward_torque           | 0.0423     |
| reward_velocity         | 0.0152     |
| rollout/                |            |
|    ep_len_mean          | 415        |
|    ep_rew_mean          | 86.6       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 57         |
|    time_elapsed         | 1922       |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.17609042 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0281    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.057     |
|    std                  | 0.858      |
|    value_loss           | 0.635      |
----------------------------------------
----------------------------------------
| reward                  | 0.186      |
| reward_contact          | 0.0368     |
| reward_ctrl             | 0.0234     |
| reward_motion           | 0          |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0253     |
| reward_torque           | 0.0421     |
| reward_velocity         | 0.0151     |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 87.7       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 58         |
|    time_elapsed         | 1955       |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.22242698 |
|    clip_fraction        | 0.691      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0754    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0668    |
|    std                  | 0.851      |
|    value_loss           | 0.245      |
----------------------------------------
Num timesteps: 120000
Best mean reward: 97.37 - Last mean reward per episode: 87.68
----------------------------------------
| reward                  | 0.185      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0229     |
| reward_motion           | 0          |
| reward_orientation      | 0.0432     |
| reward_position         | 0.000113   |
| reward_rotation         | 0.0256     |
| reward_torque           | 0.0421     |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 422        |
|    ep_rew_mean          | 88         |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 59         |
|    time_elapsed         | 1988       |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.23058623 |
|    clip_fraction        | 0.672      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10        |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.102     |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.843      |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------------
| reward                  | 0.185     |
| reward_contact          | 0.0357    |
| reward_ctrl             | 0.0232    |
| reward_motion           | 0         |
| reward_orientation      | 0.0431    |
| reward_position         | 6.72e-11  |
| reward_rotation         | 0.0274    |
| reward_torque           | 0.0421    |
| reward_velocity         | 0.0137    |
| rollout/                |           |
|    ep_len_mean          | 427       |
|    ep_rew_mean          | 88.9      |
| time/                   |           |
|    fps                  | 60        |
|    iterations           | 60        |
|    time_elapsed         | 2020      |
|    total_timesteps      | 122880    |
| train/                  |           |
|    approx_kl            | 0.2266956 |
|    clip_fraction        | 0.681     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.95     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0945   |
|    n_updates            | 590       |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.837     |
|    value_loss           | 0.188     |
---------------------------------------
----------------------------------------
| reward                  | 0.182      |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.0233     |
| reward_motion           | 0          |
| reward_orientation      | 0.0429     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0277     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 430        |
|    ep_rew_mean          | 89.2       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 61         |
|    time_elapsed         | 2053       |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.19416356 |
|    clip_fraction        | 0.649      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.88      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0856    |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.828      |
|    value_loss           | 0.195      |
----------------------------------------
Num timesteps: 126000
Best mean reward: 97.37 - Last mean reward per episode: 88.78
----------------------------------------
| reward                  | 0.183      |
| reward_contact          | 0.034      |
| reward_ctrl             | 0.0242     |
| reward_motion           | 0          |
| reward_orientation      | 0.0418     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0281     |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | 89.2       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 62         |
|    time_elapsed         | 2086       |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.16780955 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.82      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0395    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.826      |
|    value_loss           | 0.233      |
----------------------------------------
----------------------------------------
| reward                  | 0.181      |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.024      |
| reward_motion           | 0          |
| reward_orientation      | 0.0422     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0274     |
| reward_torque           | 0.0423     |
| reward_velocity         | 0.0113     |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | 89.6       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 63         |
|    time_elapsed         | 2118       |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.18181884 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.81      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0379    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0533    |
|    std                  | 0.827      |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------------
| reward                  | 0.18      |
| reward_contact          | 0.0328    |
| reward_ctrl             | 0.0232    |
| reward_motion           | 0         |
| reward_orientation      | 0.0424    |
| reward_position         | 6.72e-11  |
| reward_rotation         | 0.0287    |
| reward_torque           | 0.0419    |
| reward_velocity         | 0.0112    |
| rollout/                |           |
|    ep_len_mean          | 432       |
|    ep_rew_mean          | 90        |
| time/                   |           |
|    fps                  | 60        |
|    iterations           | 64        |
|    time_elapsed         | 2151      |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.1776425 |
|    clip_fraction        | 0.644     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.81     |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.024    |
|    n_updates            | 630       |
|    policy_gradient_loss | -0.0525   |
|    std                  | 0.827     |
|    value_loss           | 0.516     |
---------------------------------------
Num timesteps: 132000
Best mean reward: 97.37 - Last mean reward per episode: 89.65
----------------------------------------
| reward                  | 0.182      |
| reward_contact          | 0.0333     |
| reward_ctrl             | 0.0218     |
| reward_motion           | 0          |
| reward_orientation      | 0.0426     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0289     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 429        |
|    ep_rew_mean          | 89.2       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 65         |
|    time_elapsed         | 2184       |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.10614641 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.78      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00325   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0568    |
|    std                  | 0.821      |
|    value_loss           | 1.13       |
----------------------------------------
----------------------------------------
| reward                  | 0.186      |
| reward_contact          | 0.0341     |
| reward_ctrl             | 0.0231     |
| reward_motion           | 0          |
| reward_orientation      | 0.0423     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0298     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | 88.1       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 66         |
|    time_elapsed         | 2217       |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.19767444 |
|    clip_fraction        | 0.653      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.74      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00687   |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0512    |
|    std                  | 0.819      |
|    value_loss           | 1.09       |
----------------------------------------
----------------------------------------
| reward                  | 0.19       |
| reward_contact          | 0.0331     |
| reward_ctrl             | 0.0242     |
| reward_motion           | 0          |
| reward_orientation      | 0.0427     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0332     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 88.1       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 67         |
|    time_elapsed         | 2250       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.19418341 |
|    clip_fraction        | 0.661      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.7       |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0254     |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0567    |
|    std                  | 0.811      |
|    value_loss           | 0.761      |
----------------------------------------
Num timesteps: 138000
Best mean reward: 97.37 - Last mean reward per episode: 88.15
---------------------------------------
| reward                  | 0.189     |
| reward_contact          | 0.0319    |
| reward_ctrl             | 0.0243    |
| reward_motion           | 0         |
| reward_orientation      | 0.0419    |
| reward_position         | 6.72e-11  |
| reward_rotation         | 0.0337    |
| reward_torque           | 0.0422    |
| reward_velocity         | 0.0145    |
| rollout/                |           |
|    ep_len_mean          | 427       |
|    ep_rew_mean          | 88.1      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 68        |
|    time_elapsed         | 2282      |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.3156826 |
|    clip_fraction        | 0.724     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.65     |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.106    |
|    n_updates            | 670       |
|    policy_gradient_loss | -0.0591   |
|    std                  | 0.808     |
|    value_loss           | 0.176     |
---------------------------------------
--------------------------------------
| reward                  | 0.19     |
| reward_contact          | 0.0307   |
| reward_ctrl             | 0.0245   |
| reward_motion           | 0        |
| reward_orientation      | 0.0421   |
| reward_position         | 6.72e-11 |
| reward_rotation         | 0.036    |
| reward_torque           | 0.0423   |
| reward_velocity         | 0.0147   |
| rollout/                |          |
|    ep_len_mean          | 430      |
|    ep_rew_mean          | 89.2     |
| time/                   |          |
|    fps                  | 61       |
|    iterations           | 69       |
|    time_elapsed         | 2315     |
|    total_timesteps      | 141312   |
| train/                  |          |
|    approx_kl            | 0.243572 |
|    clip_fraction        | 0.698    |
|    clip_range           | 0.2      |
|    entropy_loss         | -9.61    |
|    explained_variance   | 0.949    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0565  |
|    n_updates            | 680      |
|    policy_gradient_loss | -0.06    |
|    std                  | 0.804    |
|    value_loss           | 0.189    |
--------------------------------------
----------------------------------------
| reward                  | 0.189      |
| reward_contact          | 0.0296     |
| reward_ctrl             | 0.0245     |
| reward_motion           | 0          |
| reward_orientation      | 0.0422     |
| reward_position         | 6.72e-11   |
| reward_rotation         | 0.0356     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 430        |
|    ep_rew_mean          | 88.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 70         |
|    time_elapsed         | 2348       |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.23674548 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.6       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0993    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0618    |
|    std                  | 0.803      |
|    value_loss           | 0.275      |
----------------------------------------
Num timesteps: 144000
Best mean reward: 97.37 - Last mean reward per episode: 89.48
---------------------------------------
| reward                  | 0.194     |
| reward_contact          | 0.0296    |
| reward_ctrl             | 0.0272    |
| reward_motion           | 0         |
| reward_orientation      | 0.0419    |
| reward_position         | 6.72e-11  |
| reward_rotation         | 0.0397    |
| reward_torque           | 0.0429    |
| reward_velocity         | 0.0124    |
| rollout/                |           |
|    ep_len_mean          | 435       |
|    ep_rew_mean          | 89.7      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 71        |
|    time_elapsed         | 2380      |
|    total_timesteps      | 145408    |
| train/                  |           |
|    approx_kl            | 0.2070853 |
|    clip_fraction        | 0.676     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.59     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0826   |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.0549   |
|    std                  | 0.804     |
|    value_loss           | 0.225     |
---------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0302     |
| reward_ctrl             | 0.027      |
| reward_motion           | 0          |
| reward_orientation      | 0.0418     |
| reward_position         | 2.9e-16    |
| reward_rotation         | 0.0407     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 438        |
|    ep_rew_mean          | 90.5       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 72         |
|    time_elapsed         | 2413       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.19510198 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.57      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.203      |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0627    |
|    std                  | 0.8        |
|    value_loss           | 0.462      |
----------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.031      |
| reward_ctrl             | 0.0278     |
| reward_motion           | 0          |
| reward_orientation      | 0.0419     |
| reward_position         | 2.13e-19   |
| reward_rotation         | 0.0424     |
| reward_torque           | 0.0433     |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 91.9       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 73         |
|    time_elapsed         | 2446       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.21822059 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.53      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0485    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.053     |
|    std                  | 0.794      |
|    value_loss           | 0.438      |
----------------------------------------
Num timesteps: 150000
Best mean reward: 97.37 - Last mean reward per episode: 91.81
---------------------------------------
| reward                  | 0.205     |
| reward_contact          | 0.0317    |
| reward_ctrl             | 0.0284    |
| reward_motion           | 0         |
| reward_orientation      | 0.0418    |
| reward_position         | 2.13e-19  |
| reward_rotation         | 0.0423    |
| reward_torque           | 0.0434    |
| reward_velocity         | 0.0169    |
| rollout/                |           |
|    ep_len_mean          | 444       |
|    ep_rew_mean          | 92.1      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 74        |
|    time_elapsed         | 2479      |
|    total_timesteps      | 151552    |
| train/                  |           |
|    approx_kl            | 0.2347588 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.49     |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.108     |
|    n_updates            | 730       |
|    policy_gradient_loss | -0.0547   |
|    std                  | 0.792     |
|    value_loss           | 0.309     |
---------------------------------------
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0327     |
| reward_ctrl             | 0.0275     |
| reward_motion           | 0          |
| reward_orientation      | 0.0424     |
| reward_position         | 2.13e-19   |
| reward_rotation         | 0.0402     |
| reward_torque           | 0.0432     |
| reward_velocity         | 0.0173     |
| rollout/                |            |
|    ep_len_mean          | 443        |
|    ep_rew_mean          | 92.3       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 75         |
|    time_elapsed         | 2512       |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.35367256 |
|    clip_fraction        | 0.728      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.113     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0783    |
|    std                  | 0.787      |
|    value_loss           | 0.216      |
----------------------------------------
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0328     |
| reward_ctrl             | 0.0281     |
| reward_motion           | 0          |
| reward_orientation      | 0.0423     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.0421     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | 93.5       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 76         |
|    time_elapsed         | 2544       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.22588724 |
|    clip_fraction        | 0.705      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.41      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0189    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0468    |
|    std                  | 0.784      |
|    value_loss           | 0.212      |
----------------------------------------
Num timesteps: 156000
Best mean reward: 97.37 - Last mean reward per episode: 93.60
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0282     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.042      |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 451        |
|    ep_rew_mean          | 94.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 77         |
|    time_elapsed         | 2577       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.15910034 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.38      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.04      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.045     |
|    std                  | 0.782      |
|    value_loss           | 0.711      |
----------------------------------------
---------------------------------------
| reward                  | 0.203     |
| reward_contact          | 0.033     |
| reward_ctrl             | 0.0287    |
| reward_motion           | 0         |
| reward_orientation      | 0.0415    |
| reward_position         | 1.47e-19  |
| reward_rotation         | 0.0405    |
| reward_torque           | 0.0431    |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 454       |
|    ep_rew_mean          | 95        |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 78        |
|    time_elapsed         | 2610      |
|    total_timesteps      | 159744    |
| train/                  |           |
|    approx_kl            | 0.3053235 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.34     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0918   |
|    n_updates            | 770       |
|    policy_gradient_loss | -0.0696   |
|    std                  | 0.775     |
|    value_loss           | 0.263     |
---------------------------------------
---------------------------------------
| reward                  | 0.203     |
| reward_contact          | 0.0318    |
| reward_ctrl             | 0.0293    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 1.47e-19  |
| reward_rotation         | 0.0423    |
| reward_torque           | 0.0433    |
| reward_velocity         | 0.0154    |
| rollout/                |           |
|    ep_len_mean          | 457       |
|    ep_rew_mean          | 95.2      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 79        |
|    time_elapsed         | 2643      |
|    total_timesteps      | 161792    |
| train/                  |           |
|    approx_kl            | 0.2939561 |
|    clip_fraction        | 0.725     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.28     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0709   |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.064    |
|    std                  | 0.772     |
|    value_loss           | 0.192     |
---------------------------------------
Num timesteps: 162000
Best mean reward: 97.37 - Last mean reward per episode: 95.25
---------------------------------------
| reward                  | 0.204     |
| reward_contact          | 0.0313    |
| reward_ctrl             | 0.0293    |
| reward_motion           | 0         |
| reward_orientation      | 0.0413    |
| reward_position         | 1.47e-19  |
| reward_rotation         | 0.0434    |
| reward_torque           | 0.0434    |
| reward_velocity         | 0.0152    |
| rollout/                |           |
|    ep_len_mean          | 453       |
|    ep_rew_mean          | 94.6      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 80        |
|    time_elapsed         | 2675      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.2752987 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.26     |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00286   |
|    n_updates            | 790       |
|    policy_gradient_loss | -0.0632   |
|    std                  | 0.77      |
|    value_loss           | 0.315     |
---------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0293     |
| reward_motion           | 0          |
| reward_orientation      | 0.0415     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.0474     |
| reward_torque           | 0.0434     |
| reward_velocity         | 0.0156     |
| rollout/                |            |
|    ep_len_mean          | 449        |
|    ep_rew_mean          | 94.2       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 81         |
|    time_elapsed         | 2709       |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.27518338 |
|    clip_fraction        | 0.706      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.22      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0991    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0667    |
|    std                  | 0.765      |
|    value_loss           | 0.258      |
----------------------------------------
---------------------------------------
| reward                  | 0.208     |
| reward_contact          | 0.0295    |
| reward_ctrl             | 0.0304    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 1.47e-19  |
| reward_rotation         | 0.0476    |
| reward_torque           | 0.0437    |
| reward_velocity         | 0.0154    |
| rollout/                |           |
|    ep_len_mean          | 449       |
|    ep_rew_mean          | 94.4      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 82        |
|    time_elapsed         | 2742      |
|    total_timesteps      | 167936    |
| train/                  |           |
|    approx_kl            | 0.2703854 |
|    clip_fraction        | 0.692     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.18     |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0938   |
|    n_updates            | 810       |
|    policy_gradient_loss | -0.0586   |
|    std                  | 0.762     |
|    value_loss           | 0.401     |
---------------------------------------
Num timesteps: 168000
Best mean reward: 97.37 - Last mean reward per episode: 94.37
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.031      |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | 94.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 83         |
|    time_elapsed         | 2774       |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.31781366 |
|    clip_fraction        | 0.706      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.14      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0257    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0592    |
|    std                  | 0.758      |
|    value_loss           | 0.254      |
----------------------------------------
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0295    |
| reward_ctrl             | 0.0307    |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 1.47e-19  |
| reward_rotation         | 0.0459    |
| reward_torque           | 0.0438    |
| reward_velocity         | 0.0178    |
| rollout/                |           |
|    ep_len_mean          | 447       |
|    ep_rew_mean          | 94.6      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 84        |
|    time_elapsed         | 2807      |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.2423615 |
|    clip_fraction        | 0.697     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.1      |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0041    |
|    n_updates            | 830       |
|    policy_gradient_loss | -0.0609   |
|    std                  | 0.754     |
|    value_loss           | 0.631     |
---------------------------------------
Num timesteps: 174000
Best mean reward: 97.37 - Last mean reward per episode: 93.96
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0295     |
| reward_ctrl             | 0.0312     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.0466     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | 94         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 85         |
|    time_elapsed         | 2840       |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.31308642 |
|    clip_fraction        | 0.716      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.07      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0926    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0636    |
|    std                  | 0.751      |
|    value_loss           | 0.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0291     |
| reward_ctrl             | 0.0338     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 1.47e-19   |
| reward_rotation         | 0.0453     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 450        |
|    ep_rew_mean          | 94.4       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 86         |
|    time_elapsed         | 2873       |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.36967057 |
|    clip_fraction        | 0.735      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.03      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0749    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0691    |
|    std                  | 0.746      |
|    value_loss           | 0.156      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0298     |
| reward_ctrl             | 0.0345     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 1.74e-10   |
| reward_rotation         | 0.0434     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 446        |
|    ep_rew_mean          | 93.8       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 87         |
|    time_elapsed         | 2906       |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.24929723 |
|    clip_fraction        | 0.681      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.98      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0413    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0522    |
|    std                  | 0.743      |
|    value_loss           | 0.864      |
----------------------------------------
Num timesteps: 180000
Best mean reward: 97.37 - Last mean reward per episode: 95.14
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.029      |
| reward_ctrl             | 0.0339     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.74e-10   |
| reward_rotation         | 0.0433     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.0156     |
| rollout/                |            |
|    ep_len_mean          | 448        |
|    ep_rew_mean          | 94.4       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 88         |
|    time_elapsed         | 2938       |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.28109175 |
|    clip_fraction        | 0.697      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.92      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0179    |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0473    |
|    std                  | 0.736      |
|    value_loss           | 0.947      |
----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0304     |
| reward_ctrl             | 0.0351     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.74e-10   |
| reward_rotation         | 0.0421     |
| reward_torque           | 0.0447     |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 445        |
|    ep_rew_mean          | 93.9       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 89         |
|    time_elapsed         | 2972       |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.28088933 |
|    clip_fraction        | 0.697      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.87      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.047     |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0519    |
|    std                  | 0.733      |
|    value_loss           | 0.607      |
----------------------------------------
---------------------------------------
| reward                  | 0.211     |
| reward_contact          | 0.0322    |
| reward_ctrl             | 0.0341    |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 1.74e-10  |
| reward_rotation         | 0.0418    |
| reward_torque           | 0.0444    |
| reward_velocity         | 0.0176    |
| rollout/                |           |
|    ep_len_mean          | 445       |
|    ep_rew_mean          | 94.4      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 90        |
|    time_elapsed         | 3005      |
|    total_timesteps      | 184320    |
| train/                  |           |
|    approx_kl            | 0.3577398 |
|    clip_fraction        | 0.744     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.85     |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.417     |
|    n_updates            | 890       |
|    policy_gradient_loss | -0.0515   |
|    std                  | 0.73      |
|    value_loss           | 0.616     |
---------------------------------------
Num timesteps: 186000
Best mean reward: 97.37 - Last mean reward per episode: 94.39
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.0345     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.74e-10   |
| reward_rotation         | 0.0411     |
| reward_torque           | 0.0445     |
| reward_velocity         | 0.0173     |
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | 94.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 91         |
|    time_elapsed         | 3038       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.49008352 |
|    clip_fraction        | 0.75       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.84      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0625    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.731      |
|    value_loss           | 0.273      |
----------------------------------------
---------------------------------------
| reward                  | 0.215     |
| reward_contact          | 0.0346    |
| reward_ctrl             | 0.0346    |
| reward_motion           | 0         |
| reward_orientation      | 0.0406    |
| reward_position         | 1.74e-10  |
| reward_rotation         | 0.0404    |
| reward_torque           | 0.0445    |
| reward_velocity         | 0.0199    |
| rollout/                |           |
|    ep_len_mean          | 444       |
|    ep_rew_mean          | 94.1      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 92        |
|    time_elapsed         | 3071      |
|    total_timesteps      | 188416    |
| train/                  |           |
|    approx_kl            | 0.3922383 |
|    clip_fraction        | 0.742     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.81     |
|    explained_variance   | 0.95      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.114    |
|    n_updates            | 910       |
|    policy_gradient_loss | -0.0708   |
|    std                  | 0.727     |
|    value_loss           | 0.216     |
---------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0343     |
| reward_ctrl             | 0.0336     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 1.74e-10   |
| reward_rotation         | 0.0412     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 94.2       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 93         |
|    time_elapsed         | 3103       |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.34123278 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.75      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0802    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0554    |
|    std                  | 0.72       |
|    value_loss           | 0.859      |
----------------------------------------
Num timesteps: 192000
Best mean reward: 97.37 - Last mean reward per episode: 94.55
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0342     |
| reward_ctrl             | 0.0352     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0392     |
| reward_torque           | 0.0445     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 93.8       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 94         |
|    time_elapsed         | 3136       |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.28849185 |
|    clip_fraction        | 0.723      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0898    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0483    |
|    std                  | 0.717      |
|    value_loss           | 0.483      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0331     |
| reward_ctrl             | 0.0377     |
| reward_motion           | 0          |
| reward_orientation      | 0.0388     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0428     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 440        |
|    ep_rew_mean          | 94         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 95         |
|    time_elapsed         | 3169       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.30648744 |
|    clip_fraction        | 0.726      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.65      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.042      |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0446    |
|    std                  | 0.714      |
|    value_loss           | 0.644      |
----------------------------------------
---------------------------------------
| reward                  | 0.22      |
| reward_contact          | 0.0338    |
| reward_ctrl             | 0.0381    |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 3.39e-07  |
| reward_rotation         | 0.0447    |
| reward_torque           | 0.0449    |
| reward_velocity         | 0.0195    |
| rollout/                |           |
|    ep_len_mean          | 440       |
|    ep_rew_mean          | 94.4      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 96        |
|    time_elapsed         | 3202      |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.4183956 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.64     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0602   |
|    n_updates            | 950       |
|    policy_gradient_loss | -0.0645   |
|    std                  | 0.711     |
|    value_loss           | 0.378     |
---------------------------------------
Num timesteps: 198000
Best mean reward: 97.37 - Last mean reward per episode: 94.41
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.0331     |
| reward_ctrl             | 0.0383     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0445     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 94.5       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 97         |
|    time_elapsed         | 3235       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.30529296 |
|    clip_fraction        | 0.723      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.61      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0576    |
|    std                  | 0.71       |
|    value_loss           | 0.525      |
----------------------------------------
---------------------------------------
| reward                  | 0.219     |
| reward_contact          | 0.0313    |
| reward_ctrl             | 0.0396    |
| reward_motion           | 0         |
| reward_orientation      | 0.0382    |
| reward_position         | 3.39e-07  |
| reward_rotation         | 0.0442    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.0198    |
| rollout/                |           |
|    ep_len_mean          | 442       |
|    ep_rew_mean          | 94.6      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 98        |
|    time_elapsed         | 3268      |
|    total_timesteps      | 200704    |
| train/                  |           |
|    approx_kl            | 0.4709798 |
|    clip_fraction        | 0.751     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.57     |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0864   |
|    n_updates            | 970       |
|    policy_gradient_loss | -0.0618   |
|    std                  | 0.706     |
|    value_loss           | 0.162     |
---------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0319     |
| reward_ctrl             | 0.0387     |
| reward_motion           | 0          |
| reward_orientation      | 0.0387     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0455     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.0198     |
| rollout/                |            |
|    ep_len_mean          | 438        |
|    ep_rew_mean          | 94         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 99         |
|    time_elapsed         | 3301       |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.38868916 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.55      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0771    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.055     |
|    std                  | 0.705      |
|    value_loss           | 0.364      |
----------------------------------------
Num timesteps: 204000
Best mean reward: 97.37 - Last mean reward per episode: 94.04
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0325     |
| reward_ctrl             | 0.0392     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0464     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 437        |
|    ep_rew_mean          | 94.1       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 100        |
|    time_elapsed         | 3334       |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.36463922 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.51      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0327    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0593    |
|    std                  | 0.701      |
|    value_loss           | 0.324      |
----------------------------------------
----------------------------------------
| reward                  | 0.23       |
| reward_contact          | 0.0337     |
| reward_ctrl             | 0.0405     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.049      |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.0203     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 95.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 101        |
|    time_elapsed         | 3367       |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.38084966 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.45      |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.079     |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0669    |
|    std                  | 0.695      |
|    value_loss           | 0.343      |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0345     |
| reward_ctrl             | 0.0419     |
| reward_motion           | 0          |
| reward_orientation      | 0.0395     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.046      |
| reward_torque           | 0.0462     |
| reward_velocity         | 0.0203     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 95.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 102        |
|    time_elapsed         | 3400       |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.38631493 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.41      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0594    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0614    |
|    std                  | 0.692      |
|    value_loss           | 0.488      |
----------------------------------------
Num timesteps: 210000
Best mean reward: 97.37 - Last mean reward per episode: 96.16
----------------------------------------
| reward                  | 0.23       |
| reward_contact          | 0.0346     |
| reward_ctrl             | 0.043      |
| reward_motion           | 0          |
| reward_orientation      | 0.0395     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.046      |
| reward_torque           | 0.0463     |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 96.1       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 103        |
|    time_elapsed         | 3433       |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.49715722 |
|    clip_fraction        | 0.77       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.38      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0542    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.065     |
|    std                  | 0.69       |
|    value_loss           | 0.151      |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0335     |
| reward_ctrl             | 0.0423     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.046      |
| reward_torque           | 0.0462     |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 441        |
|    ep_rew_mean          | 96.3       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 104        |
|    time_elapsed         | 3465       |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.35824764 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.34      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0764    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0556    |
|    std                  | 0.687      |
|    value_loss           | 0.456      |
----------------------------------------
---------------------------------------
| reward                  | 0.225     |
| reward_contact          | 0.0335    |
| reward_ctrl             | 0.0418    |
| reward_motion           | 0         |
| reward_orientation      | 0.04      |
| reward_position         | 3.4e-07   |
| reward_rotation         | 0.046     |
| reward_torque           | 0.046     |
| reward_velocity         | 0.0174    |
| rollout/                |           |
|    ep_len_mean          | 437       |
|    ep_rew_mean          | 94.9      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 105       |
|    time_elapsed         | 3498      |
|    total_timesteps      | 215040    |
| train/                  |           |
|    approx_kl            | 0.4020558 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.3      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0228   |
|    n_updates            | 1040      |
|    policy_gradient_loss | -0.0735   |
|    std                  | 0.683     |
|    value_loss           | 0.33      |
---------------------------------------
Num timesteps: 216000
Best mean reward: 97.37 - Last mean reward per episode: 94.19
---------------------------------------
| reward                  | 0.225     |
| reward_contact          | 0.0339    |
| reward_ctrl             | 0.0405    |
| reward_motion           | 0         |
| reward_orientation      | 0.0406    |
| reward_position         | 3.4e-07   |
| reward_rotation         | 0.0466    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.0177    |
| rollout/                |           |
|    ep_len_mean          | 424       |
|    ep_rew_mean          | 92.6      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 106       |
|    time_elapsed         | 3531      |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.3155188 |
|    clip_fraction        | 0.729     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.27     |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0101   |
|    n_updates            | 1050      |
|    policy_gradient_loss | -0.0398   |
|    std                  | 0.681     |
|    value_loss           | 0.892     |
---------------------------------------
----------------------------------------
| reward                  | 0.224      |
| reward_contact          | 0.0327     |
| reward_ctrl             | 0.0392     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 3.4e-07    |
| reward_rotation         | 0.048      |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 93.3       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 107        |
|    time_elapsed         | 3564       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.32851598 |
|    clip_fraction        | 0.725      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.25      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0262    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.678      |
|    value_loss           | 1.07       |
----------------------------------------
----------------------------------------
| reward                  | 0.224      |
| reward_contact          | 0.0317     |
| reward_ctrl             | 0.0398     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0487     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 433        |
|    ep_rew_mean          | 94.5       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 108        |
|    time_elapsed         | 3597       |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.45953536 |
|    clip_fraction        | 0.76       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.21      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0525     |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0575    |
|    std                  | 0.676      |
|    value_loss           | 0.354      |
----------------------------------------
Num timesteps: 222000
Best mean reward: 97.37 - Last mean reward per episode: 94.74
---------------------------------------
| reward                  | 0.228     |
| reward_contact          | 0.0317    |
| reward_ctrl             | 0.0417    |
| reward_motion           | 0         |
| reward_orientation      | 0.0402    |
| reward_position         | 3.39e-07  |
| reward_rotation         | 0.0506    |
| reward_torque           | 0.0462    |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 434       |
|    ep_rew_mean          | 94.9      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 109       |
|    time_elapsed         | 3629      |
|    total_timesteps      | 223232    |
| train/                  |           |
|    approx_kl            | 0.4647095 |
|    clip_fraction        | 0.747     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.16     |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00298  |
|    n_updates            | 1080      |
|    policy_gradient_loss | -0.0485   |
|    std                  | 0.671     |
|    value_loss           | 0.437     |
---------------------------------------
---------------------------------------
| reward                  | 0.225     |
| reward_contact          | 0.0303    |
| reward_ctrl             | 0.0406    |
| reward_motion           | 0         |
| reward_orientation      | 0.0402    |
| reward_position         | 3.39e-07  |
| reward_rotation         | 0.0513    |
| reward_torque           | 0.0461    |
| reward_velocity         | 0.0161    |
| rollout/                |           |
|    ep_len_mean          | 438       |
|    ep_rew_mean          | 96.3      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 110       |
|    time_elapsed         | 3662      |
|    total_timesteps      | 225280    |
| train/                  |           |
|    approx_kl            | 0.6348164 |
|    clip_fraction        | 0.772     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.13     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0953   |
|    n_updates            | 1090      |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.669     |
|    value_loss           | 0.199     |
---------------------------------------
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0303     |
| reward_ctrl             | 0.0421     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0511     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | 95.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 111        |
|    time_elapsed         | 3695       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.47482184 |
|    clip_fraction        | 0.756      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.09      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0593    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0611    |
|    std                  | 0.665      |
|    value_loss           | 0.287      |
----------------------------------------
Num timesteps: 228000
Best mean reward: 97.37 - Last mean reward per episode: 94.87
----------------------------------------
| reward                  | 0.226      |
| reward_contact          | 0.0291     |
| reward_ctrl             | 0.0426     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0527     |
| reward_torque           | 0.0466     |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 94.9       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 112        |
|    time_elapsed         | 3728       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.29808697 |
|    clip_fraction        | 0.712      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.04      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.248      |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0556    |
|    std                  | 0.659      |
|    value_loss           | 1.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0284     |
| reward_ctrl             | 0.0433     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0514     |
| reward_torque           | 0.0468     |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | 96         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 113        |
|    time_elapsed         | 3760       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.42179403 |
|    clip_fraction        | 0.739      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.99      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0801    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0483    |
|    std                  | 0.658      |
|    value_loss           | 0.444      |
----------------------------------------
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0288     |
| reward_ctrl             | 0.0433     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0502     |
| reward_torque           | 0.0468     |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 435        |
|    ep_rew_mean          | 96.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 114        |
|    time_elapsed         | 3793       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.49550954 |
|    clip_fraction        | 0.763      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.96      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0309    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.654      |
|    value_loss           | 0.224      |
----------------------------------------
Num timesteps: 234000
Best mean reward: 97.37 - Last mean reward per episode: 96.70
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0282     |
| reward_ctrl             | 0.0429     |
| reward_motion           | 0          |
| reward_orientation      | 0.0415     |
| reward_position         | 3.39e-07   |
| reward_rotation         | 0.0525     |
| reward_torque           | 0.0468     |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 435        |
|    ep_rew_mean          | 97.3       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 115        |
|    time_elapsed         | 3826       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.57175785 |
|    clip_fraction        | 0.777      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.9       |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0913    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0659    |
|    std                  | 0.648      |
|    value_loss           | 0.164      |
----------------------------------------
--------------------------------------
| reward                  | 0.221    |
| reward_contact          | 0.0281   |
| reward_ctrl             | 0.0418   |
| reward_motion           | 0        |
| reward_orientation      | 0.042    |
| reward_position         | 1.56e-10 |
| reward_rotation         | 0.052    |
| reward_torque           | 0.0468   |
| reward_velocity         | 0.0103   |
| rollout/                |          |
|    ep_len_mean          | 440      |
|    ep_rew_mean          | 98.2     |
| time/                   |          |
|    fps                  | 61       |
|    iterations           | 116      |
|    time_elapsed         | 3859     |
|    total_timesteps      | 237568   |
| train/                  |          |
|    approx_kl            | 0.400435 |
|    clip_fraction        | 0.738    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.85    |
|    explained_variance   | 0.933    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0606  |
|    n_updates            | 1150     |
|    policy_gradient_loss | -0.0586  |
|    std                  | 0.647    |
|    value_loss           | 0.506    |
--------------------------------------
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0277     |
| reward_ctrl             | 0.044      |
| reward_motion           | 0          |
| reward_orientation      | 0.0418     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0509     |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 443        |
|    ep_rew_mean          | 98.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 117        |
|    time_elapsed         | 3892       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.45209786 |
|    clip_fraction        | 0.759      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.84      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0739    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.061     |
|    std                  | 0.646      |
|    value_loss           | 0.207      |
----------------------------------------
Num timesteps: 240000
Best mean reward: 97.37 - Last mean reward per episode: 99.06
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.027      |
| reward_ctrl             | 0.0428     |
| reward_motion           | 0          |
| reward_orientation      | 0.0424     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0516     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.0113     |
| rollout/                |            |
|    ep_len_mean          | 443        |
|    ep_rew_mean          | 99.2       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 118        |
|    time_elapsed         | 3924       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.36374617 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.82      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00581    |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0447    |
|    std                  | 0.645      |
|    value_loss           | 0.547      |
----------------------------------------
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0276     |
| reward_ctrl             | 0.0424     |
| reward_motion           | 0          |
| reward_orientation      | 0.0431     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0512     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 443        |
|    ep_rew_mean          | 99.3       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 119        |
|    time_elapsed         | 3957       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.54000485 |
|    clip_fraction        | 0.766      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.8       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0498    |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.063     |
|    std                  | 0.641      |
|    value_loss           | 0.308      |
----------------------------------------
----------------------------------------
| reward                  | 0.226      |
| reward_contact          | 0.0276     |
| reward_ctrl             | 0.0435     |
| reward_motion           | 0          |
| reward_orientation      | 0.0423     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0546     |
| reward_torque           | 0.0471     |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 450        |
|    ep_rew_mean          | 100        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 120        |
|    time_elapsed         | 3990       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.58403856 |
|    clip_fraction        | 0.791      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.75      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0863    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0672    |
|    std                  | 0.638      |
|    value_loss           | 0.168      |
----------------------------------------
Num timesteps: 246000
Best mean reward: 99.06 - Last mean reward per episode: 100.49
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0264     |
| reward_ctrl             | 0.0441     |
| reward_motion           | 0          |
| reward_orientation      | 0.0413     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0523     |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 450        |
|    ep_rew_mean          | 101        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 121        |
|    time_elapsed         | 4023       |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.52685064 |
|    clip_fraction        | 0.774      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.71      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.051     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0561    |
|    std                  | 0.634      |
|    value_loss           | 0.205      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.026      |
| reward_ctrl             | 0.0447     |
| reward_motion           | 0          |
| reward_orientation      | 0.0403     |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0508     |
| reward_torque           | 0.0474     |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | 100        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 122        |
|    time_elapsed         | 4056       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.46710032 |
|    clip_fraction        | 0.761      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.65      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0197    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.629      |
|    value_loss           | 0.198      |
----------------------------------------
---------------------------------------
| reward                  | 0.22      |
| reward_contact          | 0.0247    |
| reward_ctrl             | 0.0456    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 1.56e-10  |
| reward_rotation         | 0.0516    |
| reward_torque           | 0.0477    |
| reward_velocity         | 0.0102    |
| rollout/                |           |
|    ep_len_mean          | 452       |
|    ep_rew_mean          | 100       |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 123       |
|    time_elapsed         | 4089      |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.3827081 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.61     |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0129   |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.0539   |
|    std                  | 0.627     |
|    value_loss           | 0.556     |
---------------------------------------
Num timesteps: 252000
Best mean reward: 100.49 - Last mean reward per episode: 100.28
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.0252     |
| reward_ctrl             | 0.0439     |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 1.56e-10   |
| reward_rotation         | 0.0505     |
| reward_torque           | 0.0473     |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 450        |
|    ep_rew_mean          | 100        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 124        |
|    time_elapsed         | 4121       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.54121464 |
|    clip_fraction        | 0.783      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.56      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0511    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.623      |
|    value_loss           | 0.197      |
----------------------------------------
---------------------------------------
| reward                  | 0.216     |
| reward_contact          | 0.0269    |
| reward_ctrl             | 0.0439    |
| reward_motion           | 0         |
| reward_orientation      | 0.04      |
| reward_position         | 3.21e-09  |
| reward_rotation         | 0.0466    |
| reward_torque           | 0.0473    |
| reward_velocity         | 0.0116    |
| rollout/                |           |
|    ep_len_mean          | 444       |
|    ep_rew_mean          | 98.9      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 125       |
|    time_elapsed         | 4154      |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.5717944 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.53     |
|    explained_variance   | 0.968     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0536   |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.0617   |
|    std                  | 0.621     |
|    value_loss           | 0.177     |
---------------------------------------
Num timesteps: 258000
Best mean reward: 100.49 - Last mean reward per episode: 98.93
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.028      |
| reward_ctrl             | 0.0439     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 3.05e-09   |
| reward_rotation         | 0.0478     |
| reward_torque           | 0.0473     |
| reward_velocity         | 0.0117     |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 98.9       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 126        |
|    time_elapsed         | 4187       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.41764075 |
|    clip_fraction        | 0.737      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.48      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00408    |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0512    |
|    std                  | 0.617      |
|    value_loss           | 1.31       |
----------------------------------------
---------------------------------------
| reward                  | 0.225     |
| reward_contact          | 0.0286    |
| reward_ctrl             | 0.0455    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 3.05e-09  |
| reward_rotation         | 0.0497    |
| reward_torque           | 0.0475    |
| reward_velocity         | 0.0134    |
| rollout/                |           |
|    ep_len_mean          | 448       |
|    ep_rew_mean          | 100       |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 127       |
|    time_elapsed         | 4220      |
|    total_timesteps      | 260096    |
| train/                  |           |
|    approx_kl            | 0.6485662 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.45     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0823   |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0461   |
|    std                  | 0.615     |
|    value_loss           | 0.268     |
---------------------------------------
---------------------------------------
| reward                  | 0.228     |
| reward_contact          | 0.0291    |
| reward_ctrl             | 0.0466    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 3.05e-09  |
| reward_rotation         | 0.0507    |
| reward_torque           | 0.048     |
| reward_velocity         | 0.0136    |
| rollout/                |           |
|    ep_len_mean          | 452       |
|    ep_rew_mean          | 101       |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 128       |
|    time_elapsed         | 4253      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.7022941 |
|    clip_fraction        | 0.801     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.44     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0638   |
|    n_updates            | 1270      |
|    policy_gradient_loss | -0.0534   |
|    std                  | 0.614     |
|    value_loss           | 0.164     |
---------------------------------------
Num timesteps: 264000
Best mean reward: 100.49 - Last mean reward per episode: 101.45
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.23       |
| reward_contact          | 0.0303     |
| reward_ctrl             | 0.0473     |
| reward_motion           | 0          |
| reward_orientation      | 0.0396     |
| reward_position         | 3.05e-09   |
| reward_rotation         | 0.0503     |
| reward_torque           | 0.0481     |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 453        |
|    ep_rew_mean          | 101        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 129        |
|    time_elapsed         | 4287       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.70394117 |
|    clip_fraction        | 0.789      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.4       |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.109     |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.611      |
|    value_loss           | 0.241      |
----------------------------------------
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.0299     |
| reward_ctrl             | 0.0479     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 3.05e-09   |
| reward_rotation         | 0.0482     |
| reward_torque           | 0.0481     |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 453        |
|    ep_rew_mean          | 101        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 130        |
|    time_elapsed         | 4319       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.47835666 |
|    clip_fraction        | 0.778      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.36      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0587    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.608      |
|    value_loss           | 0.839      |
----------------------------------------
---------------------------------------
| reward                  | 0.225     |
| reward_contact          | 0.0299    |
| reward_ctrl             | 0.0467    |
| reward_motion           | 0         |
| reward_orientation      | 0.0393    |
| reward_position         | 9.47e-07  |
| reward_rotation         | 0.047     |
| reward_torque           | 0.0479    |
| reward_velocity         | 0.0143    |
| rollout/                |           |
|    ep_len_mean          | 448       |
|    ep_rew_mean          | 99.6      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 131       |
|    time_elapsed         | 4352      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.6650307 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.33     |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.106    |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.0757   |
|    std                  | 0.605     |
|    value_loss           | 0.222     |
---------------------------------------
Num timesteps: 270000
Best mean reward: 101.45 - Last mean reward per episode: 97.69
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.0466     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 9.47e-07   |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 97.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 132        |
|    time_elapsed         | 4384       |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.62553096 |
|    clip_fraction        | 0.784      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.29      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.029     |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0592    |
|    std                  | 0.603      |
|    value_loss           | 1.03       |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0298     |
| reward_ctrl             | 0.0466     |
| reward_motion           | 0          |
| reward_orientation      | 0.039      |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0502     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 448        |
|    ep_rew_mean          | 99.1       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 133        |
|    time_elapsed         | 4417       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.51641893 |
|    clip_fraction        | 0.762      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.26      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0632     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0481    |
|    std                  | 0.601      |
|    value_loss           | 0.965      |
----------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.0292    |
| reward_ctrl             | 0.048     |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 2.92e-05  |
| reward_rotation         | 0.0511    |
| reward_torque           | 0.0479    |
| reward_velocity         | 0.015     |
| rollout/                |           |
|    ep_len_mean          | 446       |
|    ep_rew_mean          | 98.4      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 134       |
|    time_elapsed         | 4450      |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.6190423 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.22     |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0288   |
|    n_updates            | 1330      |
|    policy_gradient_loss | -0.0639   |
|    std                  | 0.598     |
|    value_loss           | 0.873     |
---------------------------------------
Num timesteps: 276000
Best mean reward: 101.45 - Last mean reward per episode: 97.66
----------------------------------------
| reward                  | 0.233      |
| reward_contact          | 0.0297     |
| reward_ctrl             | 0.0472     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.054      |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 98         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 135        |
|    time_elapsed         | 4483       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.66270566 |
|    clip_fraction        | 0.784      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.2       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0576     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0438    |
|    std                  | 0.595      |
|    value_loss           | 0.319      |
----------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.0285    |
| reward_ctrl             | 0.0468    |
| reward_motion           | 0         |
| reward_orientation      | 0.0387    |
| reward_position         | 2.92e-05  |
| reward_rotation         | 0.0513    |
| reward_torque           | 0.0476    |
| reward_velocity         | 0.0169    |
| rollout/                |           |
|    ep_len_mean          | 439       |
|    ep_rew_mean          | 97.3      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 136       |
|    time_elapsed         | 4516      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.7618979 |
|    clip_fraction        | 0.799     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.16     |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0345   |
|    n_updates            | 1350      |
|    policy_gradient_loss | -0.0745   |
|    std                  | 0.594     |
|    value_loss           | 0.313     |
---------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.0279    |
| reward_ctrl             | 0.0477    |
| reward_motion           | 0         |
| reward_orientation      | 0.0388    |
| reward_position         | 2.92e-05  |
| reward_rotation         | 0.0515    |
| reward_torque           | 0.0478    |
| reward_velocity         | 0.0163    |
| rollout/                |           |
|    ep_len_mean          | 439       |
|    ep_rew_mean          | 96.9      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 137       |
|    time_elapsed         | 4549      |
|    total_timesteps      | 280576    |
| train/                  |           |
|    approx_kl            | 0.5669081 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.15     |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0499    |
|    n_updates            | 1360      |
|    policy_gradient_loss | -0.0499   |
|    std                  | 0.593     |
|    value_loss           | 0.614     |
---------------------------------------
Num timesteps: 282000
Best mean reward: 101.45 - Last mean reward per episode: 97.28
----------------------------------------
| reward                  | 0.23       |
| reward_contact          | 0.0285     |
| reward_ctrl             | 0.0476     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0517     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 439        |
|    ep_rew_mean          | 97.4       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 138        |
|    time_elapsed         | 4581       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.88075614 |
|    clip_fraction        | 0.806      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.13      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0892    |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0579    |
|    std                  | 0.591      |
|    value_loss           | 0.15       |
----------------------------------------
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.029      |
| reward_ctrl             | 0.0476     |
| reward_motion           | 0          |
| reward_orientation      | 0.0384     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.05       |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | 96.1       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 139        |
|    time_elapsed         | 4614       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.59408975 |
|    clip_fraction        | 0.786      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.09      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0601    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0664    |
|    std                  | 0.588      |
|    value_loss           | 0.267      |
----------------------------------------
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0278     |
| reward_ctrl             | 0.0474     |
| reward_motion           | 0          |
| reward_orientation      | 0.0386     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.049      |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0144     |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | 96         |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 140        |
|    time_elapsed         | 4647       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.55519307 |
|    clip_fraction        | 0.787      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.06      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0401    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0557    |
|    std                  | 0.586      |
|    value_loss           | 0.353      |
----------------------------------------
Num timesteps: 288000
Best mean reward: 101.45 - Last mean reward per episode: 95.75
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0282     |
| reward_ctrl             | 0.0462     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0457     |
| reward_torque           | 0.0474     |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 434        |
|    ep_rew_mean          | 95.7       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 141        |
|    time_elapsed         | 4679       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.98107666 |
|    clip_fraction        | 0.825      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.02      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.114     |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0677    |
|    std                  | 0.583      |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------------
| reward                  | 0.222     |
| reward_contact          | 0.0288    |
| reward_ctrl             | 0.0451    |
| reward_motion           | 0         |
| reward_orientation      | 0.0388    |
| reward_position         | 2.92e-05  |
| reward_rotation         | 0.0465    |
| reward_torque           | 0.0472    |
| reward_velocity         | 0.0153    |
| rollout/                |           |
|    ep_len_mean          | 434       |
|    ep_rew_mean          | 96.1      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 142       |
|    time_elapsed         | 4712      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.5437559 |
|    clip_fraction        | 0.776     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.98     |
|    explained_variance   | 0.934     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0431   |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0582   |
|    std                  | 0.579     |
|    value_loss           | 0.29      |
---------------------------------------
---------------------------------------
| reward                  | 0.224     |
| reward_contact          | 0.0277    |
| reward_ctrl             | 0.0463    |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 2.92e-05  |
| reward_rotation         | 0.0484    |
| reward_torque           | 0.0473    |
| reward_velocity         | 0.0153    |
| rollout/                |           |
|    ep_len_mean          | 437       |
|    ep_rew_mean          | 96.2      |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 143       |
|    time_elapsed         | 4745      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.7089893 |
|    clip_fraction        | 0.793     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.93     |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0784   |
|    n_updates            | 1420      |
|    policy_gradient_loss | -0.0638   |
|    std                  | 0.575     |
|    value_loss           | 0.297     |
---------------------------------------
Num timesteps: 294000
Best mean reward: 101.45 - Last mean reward per episode: 96.22
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0276     |
| reward_ctrl             | 0.0462     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0484     |
| reward_torque           | 0.0473     |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 433        |
|    ep_rew_mean          | 95.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 144        |
|    time_elapsed         | 4778       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.70182943 |
|    clip_fraction        | 0.793      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.87      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0945    |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.572      |
|    value_loss           | 0.22       |
----------------------------------------
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0277     |
| reward_ctrl             | 0.0487     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0536     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 94.6       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 145        |
|    time_elapsed         | 4811       |
|    total_timesteps      | 296960     |
| train/                  |            |
|    approx_kl            | 0.66405475 |
|    clip_fraction        | 0.791      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.81      |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.066     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.055     |
|    std                  | 0.567      |
|    value_loss           | 0.381      |
----------------------------------------
----------------------------------------
| reward                  | 0.237      |
| reward_contact          | 0.0265     |
| reward_ctrl             | 0.0493     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 2.92e-05   |
| reward_rotation         | 0.0567     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.0185     |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 95.1       |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 146        |
|    time_elapsed         | 4844       |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.82771075 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.77      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0394     |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0375    |
|    std                  | 0.566      |
|    value_loss           | 0.721      |
----------------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 205, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 205, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 205, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 06:10:39.456843: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 06:10:39.456893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
Using cpu device
Logging to rl/out_dir/models/exp68/PPO_4
---------------------------------
| reward             | 0.244    |
| reward_contact     | 0.0451   |
| reward_ctrl        | 0.0149   |
| reward_motion      | 0        |
| reward_orientation | 0.0394   |
| reward_position    | 1.05e-35 |
| reward_rotation    | 0.0494   |
| reward_torque      | 0.0401   |
| reward_velocity    | 0.0555   |
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 94.8     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 1        |
|    time_elapsed    | 30       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| reward                  | 0.217       |
| reward_contact          | 0.0486      |
| reward_ctrl             | 0.0192      |
| reward_motion           | 0           |
| reward_orientation      | 0.044       |
| reward_position         | 6.9e-11     |
| reward_rotation         | 0.0295      |
| reward_torque           | 0.0401      |
| reward_velocity         | 0.0358      |
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 87.6        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 2           |
|    time_elapsed         | 63          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.047354676 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | -0.17       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0298     |
|    std                  | 0.993       |
|    value_loss           | 0.834       |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 94.54
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.211       |
| reward_contact          | 0.0395      |
| reward_ctrl             | 0.017       |
| reward_motion           | 0           |
| reward_orientation      | 0.0435      |
| reward_position         | 4.93e-11    |
| reward_rotation         | 0.024       |
| reward_torque           | 0.0398      |
| reward_velocity         | 0.0475      |
| rollout/                |             |
|    ep_len_mean          | 407         |
|    ep_rew_mean          | 94.5        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 3           |
|    time_elapsed         | 95          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.045837514 |
|    clip_fraction        | 0.381       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0335     |
|    std                  | 0.99        |
|    value_loss           | 1.27        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.235       |
| reward_contact          | 0.0407      |
| reward_ctrl             | 0.0196      |
| reward_motion           | 0           |
| reward_orientation      | 0.0432      |
| reward_position         | 3.84e-11    |
| reward_rotation         | 0.0209      |
| reward_torque           | 0.0403      |
| reward_velocity         | 0.0704      |
| rollout/                |             |
|    ep_len_mean          | 428         |
|    ep_rew_mean          | 102         |
| time/                   |             |
|    fps                  | 63          |
|    iterations           | 4           |
|    time_elapsed         | 128         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.047763713 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0299     |
|    std                  | 0.989       |
|    value_loss           | 0.893       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.23        |
| reward_contact          | 0.0423      |
| reward_ctrl             | 0.02        |
| reward_motion           | 0           |
| reward_orientation      | 0.0402      |
| reward_position         | 3e-11       |
| reward_rotation         | 0.0232      |
| reward_torque           | 0.0411      |
| reward_velocity         | 0.0628      |
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 63          |
|    iterations           | 5           |
|    time_elapsed         | 161         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.047077913 |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.489       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.031      |
|    std                  | 0.99        |
|    value_loss           | 1.33        |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 94.54 - Last mean reward per episode: 103.44
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.233       |
| reward_contact          | 0.039       |
| reward_ctrl             | 0.0233      |
| reward_motion           | 0           |
| reward_orientation      | 0.0428      |
| reward_position         | 2.68e-11    |
| reward_rotation         | 0.0256      |
| reward_torque           | 0.042       |
| reward_velocity         | 0.0602      |
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | 104         |
| time/                   |             |
|    fps                  | 63          |
|    iterations           | 6           |
|    time_elapsed         | 194         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.054442685 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.346       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0386     |
|    std                  | 0.988       |
|    value_loss           | 1.7         |
-----------------------------------------
---------------------------------------
| reward                  | 0.226     |
| reward_contact          | 0.0374    |
| reward_ctrl             | 0.0214    |
| reward_motion           | 0         |
| reward_orientation      | 0.0443    |
| reward_position         | 2.21e-11  |
| reward_rotation         | 0.0218    |
| reward_torque           | 0.041     |
| reward_velocity         | 0.0604    |
| rollout/                |           |
|    ep_len_mean          | 420       |
|    ep_rew_mean          | 101       |
| time/                   |           |
|    fps                  | 63        |
|    iterations           | 7         |
|    time_elapsed         | 227       |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.0488578 |
|    clip_fraction        | 0.415     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.3     |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.58      |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0323   |
|    std                  | 0.988     |
|    value_loss           | 1.81      |
---------------------------------------
-----------------------------------------
| reward                  | 0.23        |
| reward_contact          | 0.0382      |
| reward_ctrl             | 0.0213      |
| reward_motion           | 0           |
| reward_orientation      | 0.045       |
| reward_position         | 1.98e-11    |
| reward_rotation         | 0.0261      |
| reward_torque           | 0.041       |
| reward_velocity         | 0.0587      |
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 8           |
|    time_elapsed         | 260         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.055703368 |
|    clip_fraction        | 0.44        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.863       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0393     |
|    std                  | 0.987       |
|    value_loss           | 2.36        |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 103.44 - Last mean reward per episode: 101.39
-----------------------------------------
| reward                  | 0.236       |
| reward_contact          | 0.0383      |
| reward_ctrl             | 0.0204      |
| reward_motion           | 0           |
| reward_orientation      | 0.0446      |
| reward_position         | 3.01e-06    |
| reward_rotation         | 0.0299      |
| reward_torque           | 0.0407      |
| reward_velocity         | 0.0619      |
| rollout/                |             |
|    ep_len_mean          | 421         |
|    ep_rew_mean          | 101         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 9           |
|    time_elapsed         | 293         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.089271866 |
|    clip_fraction        | 0.494       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0253     |
|    std                  | 0.987       |
|    value_loss           | 1.02        |
-----------------------------------------
-----------------------------------------
| reward                  | 0.236       |
| reward_contact          | 0.0405      |
| reward_ctrl             | 0.0199      |
| reward_motion           | 0           |
| reward_orientation      | 0.0442      |
| reward_position         | 2.7e-06     |
| reward_rotation         | 0.0297      |
| reward_torque           | 0.0407      |
| reward_velocity         | 0.0608      |
| rollout/                |             |
|    ep_len_mean          | 418         |
|    ep_rew_mean          | 102         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 10          |
|    time_elapsed         | 326         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.071096644 |
|    clip_fraction        | 0.455       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.482       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0319     |
|    std                  | 0.982       |
|    value_loss           | 1.73        |
-----------------------------------------
--------------------------------------
| reward                  | 0.231    |
| reward_contact          | 0.0412   |
| reward_ctrl             | 0.0198   |
| reward_motion           | 0        |
| reward_orientation      | 0.0439   |
| reward_position         | 2.44e-06 |
| reward_rotation         | 0.0277   |
| reward_torque           | 0.0405   |
| reward_velocity         | 0.0575   |
| rollout/                |          |
|    ep_len_mean          | 416      |
|    ep_rew_mean          | 102      |
| time/                   |          |
|    fps                  | 62       |
|    iterations           | 11       |
|    time_elapsed         | 358      |
|    total_timesteps      | 22528    |
| train/                  |          |
|    approx_kl            | 0.081966 |
|    clip_fraction        | 0.469    |
|    clip_range           | 0.2      |
|    entropy_loss         | -11.2    |
|    explained_variance   | 0.705    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.444    |
|    n_updates            | 100      |
|    policy_gradient_loss | -0.0291  |
|    std                  | 0.983    |
|    value_loss           | 1.69     |
--------------------------------------
Num timesteps: 24000
Best mean reward: 103.44 - Last mean reward per episode: 101.90
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0405     |
| reward_ctrl             | 0.0197     |
| reward_motion           | 0          |
| reward_orientation      | 0.0449     |
| reward_position         | 2.27e-06   |
| reward_rotation         | 0.0305     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.058      |
| rollout/                |            |
|    ep_len_mean          | 423        |
|    ep_rew_mean          | 103        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 12         |
|    time_elapsed         | 391        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08119593 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0791     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0239    |
|    std                  | 0.985      |
|    value_loss           | 1.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.232      |
| reward_contact          | 0.0411     |
| reward_ctrl             | 0.02       |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 2.09e-06   |
| reward_rotation         | 0.0295     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0562     |
| rollout/                |            |
|    ep_len_mean          | 422        |
|    ep_rew_mean          | 103        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 13         |
|    time_elapsed         | 423        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.08456555 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0895     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0371    |
|    std                  | 0.986      |
|    value_loss           | 1.17       |
----------------------------------------
----------------------------------------
| reward                  | 0.237      |
| reward_contact          | 0.0416     |
| reward_ctrl             | 0.0194     |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 3.9e-06    |
| reward_rotation         | 0.0312     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0596     |
| rollout/                |            |
|    ep_len_mean          | 421        |
|    ep_rew_mean          | 103        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 14         |
|    time_elapsed         | 456        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06218866 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.42       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.032     |
|    std                  | 0.986      |
|    value_loss           | 2.09       |
----------------------------------------
Num timesteps: 30000
Best mean reward: 103.44 - Last mean reward per episode: 103.19
----------------------------------------
| reward                  | 0.241      |
| reward_contact          | 0.0397     |
| reward_ctrl             | 0.0216     |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 3.63e-06   |
| reward_rotation         | 0.029      |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.0651     |
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | 104        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 15         |
|    time_elapsed         | 489        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.07710705 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.698      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0267    |
|    std                  | 0.986      |
|    value_loss           | 1.76       |
----------------------------------------
-----------------------------------------
| reward                  | 0.238       |
| reward_contact          | 0.0402      |
| reward_ctrl             | 0.0211      |
| reward_motion           | 0           |
| reward_orientation      | 0.0447      |
| reward_position         | 3.4e-06     |
| reward_rotation         | 0.0293      |
| reward_torque           | 0.0409      |
| reward_velocity         | 0.0617      |
| rollout/                |             |
|    ep_len_mean          | 423         |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 16          |
|    time_elapsed         | 522         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.098554924 |
|    clip_fraction        | 0.55        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.334       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0257     |
|    std                  | 0.987       |
|    value_loss           | 0.849       |
-----------------------------------------
---------------------------------------
| reward                  | 0.237     |
| reward_contact          | 0.0397    |
| reward_ctrl             | 0.0208    |
| reward_motion           | 0         |
| reward_orientation      | 0.0452    |
| reward_position         | 3.23e-06  |
| reward_rotation         | 0.0281    |
| reward_torque           | 0.0409    |
| reward_velocity         | 0.062     |
| rollout/                |           |
|    ep_len_mean          | 426       |
|    ep_rew_mean          | 104       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 17        |
|    time_elapsed         | 555       |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0949329 |
|    clip_fraction        | 0.521     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.3     |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.134     |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0248   |
|    std                  | 0.988     |
|    value_loss           | 1.35      |
---------------------------------------
Num timesteps: 36000
Best mean reward: 103.44 - Last mean reward per episode: 103.71
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.239      |
| reward_contact          | 0.0388     |
| reward_ctrl             | 0.0205     |
| reward_motion           | 0          |
| reward_orientation      | 0.0454     |
| reward_position         | 3.04e-06   |
| reward_rotation         | 0.0288     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0652     |
| rollout/                |            |
|    ep_len_mean          | 427        |
|    ep_rew_mean          | 104        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 18         |
|    time_elapsed         | 588        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.09857404 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.3      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0303    |
|    std                  | 0.987      |
|    value_loss           | 1.12       |
----------------------------------------
---------------------------------------
| reward                  | 0.242     |
| reward_contact          | 0.0385    |
| reward_ctrl             | 0.0206    |
| reward_motion           | 0         |
| reward_orientation      | 0.0454    |
| reward_position         | 2.91e-06  |
| reward_rotation         | 0.0302    |
| reward_torque           | 0.0407    |
| reward_velocity         | 0.0661    |
| rollout/                |           |
|    ep_len_mean          | 428       |
|    ep_rew_mean          | 104       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 19        |
|    time_elapsed         | 621       |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0879322 |
|    clip_fraction        | 0.527     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.2     |
|    explained_variance   | 0.83      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.329     |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0328   |
|    std                  | 0.982     |
|    value_loss           | 1.63      |
---------------------------------------
--------------------------------------
| reward                  | 0.242    |
| reward_contact          | 0.0384   |
| reward_ctrl             | 0.0205   |
| reward_motion           | 0        |
| reward_orientation      | 0.0455   |
| reward_position         | 2.75e-06 |
| reward_rotation         | 0.0301   |
| reward_torque           | 0.0407   |
| reward_velocity         | 0.0664   |
| rollout/                |          |
|    ep_len_mean          | 428      |
|    ep_rew_mean          | 104      |
| time/                   |          |
|    fps                  | 62       |
|    iterations           | 20       |
|    time_elapsed         | 653      |
|    total_timesteps      | 40960    |
| train/                  |          |
|    approx_kl            | 0.126239 |
|    clip_fraction        | 0.542    |
|    clip_range           | 0.2      |
|    entropy_loss         | -11.2    |
|    explained_variance   | 0.846    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.201    |
|    n_updates            | 190      |
|    policy_gradient_loss | -0.0324  |
|    std                  | 0.98     |
|    value_loss           | 0.641    |
--------------------------------------
Num timesteps: 42000
Best mean reward: 103.71 - Last mean reward per episode: 104.12
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.246     |
| reward_contact          | 0.0386    |
| reward_ctrl             | 0.0209    |
| reward_motion           | 0         |
| reward_orientation      | 0.046     |
| reward_position         | 2.64e-06  |
| reward_rotation         | 0.0313    |
| reward_torque           | 0.0408    |
| reward_velocity         | 0.0686    |
| rollout/                |           |
|    ep_len_mean          | 431       |
|    ep_rew_mean          | 105       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 21        |
|    time_elapsed         | 686       |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0983631 |
|    clip_fraction        | 0.551     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.2     |
|    explained_variance   | 0.839     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0647    |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0294   |
|    std                  | 0.978     |
|    value_loss           | 0.757     |
---------------------------------------
-----------------------------------------
| reward                  | 0.245       |
| reward_contact          | 0.0376      |
| reward_ctrl             | 0.0209      |
| reward_motion           | 0           |
| reward_orientation      | 0.0461      |
| reward_position         | 2.62e-06    |
| reward_rotation         | 0.0298      |
| reward_torque           | 0.0408      |
| reward_velocity         | 0.0694      |
| rollout/                |             |
|    ep_len_mean          | 434         |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 22          |
|    time_elapsed         | 719         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.115527585 |
|    clip_fraction        | 0.597       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.204       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0212     |
|    std                  | 0.973       |
|    value_loss           | 0.731       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.251       |
| reward_contact          | 0.037       |
| reward_ctrl             | 0.0213      |
| reward_motion           | 0           |
| reward_orientation      | 0.0465      |
| reward_position         | 2.62e-06    |
| reward_rotation         | 0.0332      |
| reward_torque           | 0.041       |
| reward_velocity         | 0.0724      |
| rollout/                |             |
|    ep_len_mean          | 439         |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 23          |
|    time_elapsed         | 753         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.082958326 |
|    clip_fraction        | 0.529       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0346     |
|    std                  | 0.97        |
|    value_loss           | 1.38        |
-----------------------------------------
Num timesteps: 48000
Best mean reward: 104.12 - Last mean reward per episode: 109.42
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0375     |
| reward_ctrl             | 0.021      |
| reward_motion           | 0          |
| reward_orientation      | 0.0462     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0347     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0749     |
| rollout/                |            |
|    ep_len_mean          | 440        |
|    ep_rew_mean          | 109        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 24         |
|    time_elapsed         | 786        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.13697526 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0956     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.964      |
|    value_loss           | 0.728      |
----------------------------------------
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0381     |
| reward_ctrl             | 0.0217     |
| reward_motion           | 0          |
| reward_orientation      | 0.0463     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0349     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0728     |
| rollout/                |            |
|    ep_len_mean          | 440        |
|    ep_rew_mean          | 109        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 25         |
|    time_elapsed         | 819        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.11058487 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.171      |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0408    |
|    std                  | 0.957      |
|    value_loss           | 0.962      |
----------------------------------------
----------------------------------------
| reward                  | 0.253      |
| reward_contact          | 0.0369     |
| reward_ctrl             | 0.0217     |
| reward_motion           | 0          |
| reward_orientation      | 0.0469     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0342     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0722     |
| rollout/                |            |
|    ep_len_mean          | 437        |
|    ep_rew_mean          | 109        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 26         |
|    time_elapsed         | 852        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.09378196 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0751     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0362    |
|    std                  | 0.956      |
|    value_loss           | 0.994      |
----------------------------------------
Num timesteps: 54000
Best mean reward: 109.42 - Last mean reward per episode: 108.57
----------------------------------------
| reward                  | 0.258      |
| reward_contact          | 0.0376     |
| reward_ctrl             | 0.0211     |
| reward_motion           | 0          |
| reward_orientation      | 0.0466     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0756     |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | 109        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 27         |
|    time_elapsed         | 885        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.07509361 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0118    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0368    |
|    std                  | 0.952      |
|    value_loss           | 1.27       |
----------------------------------------
----------------------------------------
| reward                  | 0.258      |
| reward_contact          | 0.0382     |
| reward_ctrl             | 0.0221     |
| reward_motion           | 0          |
| reward_orientation      | 0.0463     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0353     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0757     |
| rollout/                |            |
|    ep_len_mean          | 437        |
|    ep_rew_mean          | 109        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 28         |
|    time_elapsed         | 919        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.09711979 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0423     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0414    |
|    std                  | 0.952      |
|    value_loss           | 1.09       |
----------------------------------------
----------------------------------------
| reward                  | 0.262      |
| reward_contact          | 0.04       |
| reward_ctrl             | 0.0229     |
| reward_motion           | 0          |
| reward_orientation      | 0.0462     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0382     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0735     |
| rollout/                |            |
|    ep_len_mean          | 438        |
|    ep_rew_mean          | 110        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 29         |
|    time_elapsed         | 952        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.10318427 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0413    |
|    std                  | 0.954      |
|    value_loss           | 1.7        |
----------------------------------------
Num timesteps: 60000
Best mean reward: 109.42 - Last mean reward per episode: 110.57
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.259      |
| reward_contact          | 0.0382     |
| reward_ctrl             | 0.0228     |
| reward_motion           | 0          |
| reward_orientation      | 0.0454     |
| reward_position         | 2.62e-06   |
| reward_rotation         | 0.0378     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0738     |
| rollout/                |            |
|    ep_len_mean          | 440        |
|    ep_rew_mean          | 111        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 30         |
|    time_elapsed         | 984        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.15440166 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.138      |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0436    |
|    std                  | 0.952      |
|    value_loss           | 0.69       |
----------------------------------------
----------------------------------------
| reward                  | 0.258      |
| reward_contact          | 0.0375     |
| reward_ctrl             | 0.0229     |
| reward_motion           | 0          |
| reward_orientation      | 0.0453     |
| reward_position         | 1.32e-06   |
| reward_rotation         | 0.0374     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0735     |
| rollout/                |            |
|    ep_len_mean          | 446        |
|    ep_rew_mean          | 114        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 31         |
|    time_elapsed         | 1017       |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.14023364 |
|    clip_fraction        | 0.597      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.016      |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0408    |
|    std                  | 0.951      |
|    value_loss           | 0.703      |
----------------------------------------
-----------------------------------------
| reward                  | 0.261       |
| reward_contact          | 0.0369      |
| reward_ctrl             | 0.0237      |
| reward_motion           | 0           |
| reward_orientation      | 0.0449      |
| reward_position         | 4.38e-05    |
| reward_rotation         | 0.0398      |
| reward_torque           | 0.0412      |
| reward_velocity         | 0.0742      |
| rollout/                |             |
|    ep_len_mean          | 447         |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 32          |
|    time_elapsed         | 1050        |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.122464344 |
|    clip_fraction        | 0.575       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0407     |
|    std                  | 0.947       |
|    value_loss           | 0.877       |
-----------------------------------------
Num timesteps: 66000
Best mean reward: 110.57 - Last mean reward per episode: 114.22
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.261       |
| reward_contact          | 0.0357      |
| reward_ctrl             | 0.0238      |
| reward_motion           | 0           |
| reward_orientation      | 0.0452      |
| reward_position         | 4.38e-05    |
| reward_rotation         | 0.041       |
| reward_torque           | 0.0413      |
| reward_velocity         | 0.074       |
| rollout/                |             |
|    ep_len_mean          | 451         |
|    ep_rew_mean          | 116         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 33          |
|    time_elapsed         | 1083        |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.089913145 |
|    clip_fraction        | 0.552       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0377     |
|    std                  | 0.944       |
|    value_loss           | 1.65        |
-----------------------------------------
----------------------------------------
| reward                  | 0.262      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0243     |
| reward_motion           | 0          |
| reward_orientation      | 0.0446     |
| reward_position         | 4.38e-05   |
| reward_rotation         | 0.0389     |
| reward_torque           | 0.0414     |
| reward_velocity         | 0.0767     |
| rollout/                |            |
|    ep_len_mean          | 451        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 34         |
|    time_elapsed         | 1115       |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.08838236 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00734   |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0356    |
|    std                  | 0.942      |
|    value_loss           | 0.787      |
----------------------------------------
----------------------------------------
| reward                  | 0.267      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.025      |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 4.38e-05   |
| reward_rotation         | 0.0397     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.08       |
| rollout/                |            |
|    ep_len_mean          | 451        |
|    ep_rew_mean          | 117        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 35         |
|    time_elapsed         | 1148       |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.14385743 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0407    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0416    |
|    std                  | 0.943      |
|    value_loss           | 0.609      |
----------------------------------------
Num timesteps: 72000
Best mean reward: 114.22 - Last mean reward per episode: 117.05
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.272      |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.0261     |
| reward_motion           | 0          |
| reward_orientation      | 0.0446     |
| reward_position         | 4.25e-05   |
| reward_rotation         | 0.0434     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.0825     |
| rollout/                |            |
|    ep_len_mean          | 461        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 36         |
|    time_elapsed         | 1181       |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.11257778 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0254     |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0471    |
|    std                  | 0.937      |
|    value_loss           | 0.788      |
----------------------------------------
----------------------------------------
| reward                  | 0.266      |
| reward_contact          | 0.0339     |
| reward_ctrl             | 0.0253     |
| reward_motion           | 0          |
| reward_orientation      | 0.0447     |
| reward_position         | 4.25e-05   |
| reward_rotation         | 0.0418     |
| reward_torque           | 0.0418     |
| reward_velocity         | 0.0786     |
| rollout/                |            |
|    ep_len_mean          | 461        |
|    ep_rew_mean          | 120        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 37         |
|    time_elapsed         | 1214       |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.10574281 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0273     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0332    |
|    std                  | 0.937      |
|    value_loss           | 1.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.269      |
| reward_contact          | 0.0339     |
| reward_ctrl             | 0.0249     |
| reward_motion           | 0          |
| reward_orientation      | 0.044      |
| reward_position         | 4.25e-05   |
| reward_rotation         | 0.0437     |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.0805     |
| rollout/                |            |
|    ep_len_mean          | 463        |
|    ep_rew_mean          | 121        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 38         |
|    time_elapsed         | 1247       |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.12396939 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0358    |
|    std                  | 0.936      |
|    value_loss           | 0.758      |
----------------------------------------
Num timesteps: 78000
Best mean reward: 117.05 - Last mean reward per episode: 121.44
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.275      |
| reward_contact          | 0.0339     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0444     |
| reward_position         | 4.25e-05   |
| reward_rotation         | 0.0461     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.0823     |
| rollout/                |            |
|    ep_len_mean          | 467        |
|    ep_rew_mean          | 123        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 39         |
|    time_elapsed         | 1280       |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.10775594 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0396     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0437    |
|    std                  | 0.933      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.274      |
| reward_contact          | 0.0346     |
| reward_ctrl             | 0.0263     |
| reward_motion           | 0          |
| reward_orientation      | 0.0437     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0471     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0806     |
| rollout/                |            |
|    ep_len_mean          | 463        |
|    ep_rew_mean          | 122        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 40         |
|    time_elapsed         | 1313       |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.18225138 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0633    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.931      |
|    value_loss           | 0.507      |
----------------------------------------
-----------------------------------------
| reward                  | 0.276       |
| reward_contact          | 0.0358      |
| reward_ctrl             | 0.0271      |
| reward_motion           | 0           |
| reward_orientation      | 0.0429      |
| reward_position         | 4.32e-05    |
| reward_rotation         | 0.0472      |
| reward_torque           | 0.0424      |
| reward_velocity         | 0.0804      |
| rollout/                |             |
|    ep_len_mean          | 466         |
|    ep_rew_mean          | 123         |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 41          |
|    time_elapsed         | 1346        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.111881435 |
|    clip_fraction        | 0.571       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0836      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0373     |
|    std                  | 0.928       |
|    value_loss           | 1.11        |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 121.44 - Last mean reward per episode: 123.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.283      |
| reward_contact          | 0.0352     |
| reward_ctrl             | 0.0276     |
| reward_motion           | 0          |
| reward_orientation      | 0.0429     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0498     |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.0852     |
| rollout/                |            |
|    ep_len_mean          | 467        |
|    ep_rew_mean          | 124        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 42         |
|    time_elapsed         | 1379       |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.15152773 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0296     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0499    |
|    std                  | 0.927      |
|    value_loss           | 0.46       |
----------------------------------------
----------------------------------------
| reward                  | 0.283      |
| reward_contact          | 0.0342     |
| reward_ctrl             | 0.0275     |
| reward_motion           | 0          |
| reward_orientation      | 0.043      |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0494     |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.0859     |
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 126        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 43         |
|    time_elapsed         | 1412       |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.14988603 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0185    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0569    |
|    std                  | 0.921      |
|    value_loss           | 0.371      |
----------------------------------------
Num timesteps: 90000
Best mean reward: 123.18 - Last mean reward per episode: 127.34
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.28       |
| reward_contact          | 0.0342     |
| reward_ctrl             | 0.0276     |
| reward_motion           | 0          |
| reward_orientation      | 0.0428     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.049      |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0834     |
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 44         |
|    time_elapsed         | 1445       |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.11414934 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0134     |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.054     |
|    std                  | 0.914      |
|    value_loss           | 1.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.282      |
| reward_contact          | 0.0348     |
| reward_ctrl             | 0.0291     |
| reward_motion           | 0          |
| reward_orientation      | 0.0424     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0505     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0824     |
| rollout/                |            |
|    ep_len_mean          | 470        |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 45         |
|    time_elapsed         | 1478       |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.13798185 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0248    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.055     |
|    std                  | 0.915      |
|    value_loss           | 0.647      |
----------------------------------------
----------------------------------------
| reward                  | 0.284      |
| reward_contact          | 0.0354     |
| reward_ctrl             | 0.0305     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0498     |
| reward_torque           | 0.0434     |
| reward_velocity         | 0.0832     |
| rollout/                |            |
|    ep_len_mean          | 470        |
|    ep_rew_mean          | 129        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 46         |
|    time_elapsed         | 1512       |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.14092422 |
|    clip_fraction        | 0.604      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0154    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0539    |
|    std                  | 0.914      |
|    value_loss           | 0.687      |
----------------------------------------
Num timesteps: 96000
Best mean reward: 127.34 - Last mean reward per episode: 129.86
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.282      |
| reward_contact          | 0.0349     |
| reward_ctrl             | 0.0313     |
| reward_motion           | 0          |
| reward_orientation      | 0.042      |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0488     |
| reward_torque           | 0.0436     |
| reward_velocity         | 0.0815     |
| rollout/                |            |
|    ep_len_mean          | 472        |
|    ep_rew_mean          | 130        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 47         |
|    time_elapsed         | 1544       |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.15531191 |
|    clip_fraction        | 0.605      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0452     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0574    |
|    std                  | 0.911      |
|    value_loss           | 0.708      |
----------------------------------------
----------------------------------------
| reward                  | 0.285      |
| reward_contact          | 0.0355     |
| reward_ctrl             | 0.0309     |
| reward_motion           | 0          |
| reward_orientation      | 0.0422     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0515     |
| reward_torque           | 0.0435     |
| reward_velocity         | 0.0812     |
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 48         |
|    time_elapsed         | 1577       |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.14469373 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0267    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.909      |
|    value_loss           | 0.615      |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0354     |
| reward_ctrl             | 0.0303     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 4.32e-05   |
| reward_rotation         | 0.0536     |
| reward_torque           | 0.0433     |
| reward_velocity         | 0.0859     |
| rollout/                |            |
|    ep_len_mean          | 475        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 49         |
|    time_elapsed         | 1610       |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.14356127 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.014     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0512    |
|    std                  | 0.91       |
|    value_loss           | 0.927      |
----------------------------------------
Num timesteps: 102000
Best mean reward: 129.86 - Last mean reward per episode: 131.55
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.288      |
| reward_contact          | 0.0365     |
| reward_ctrl             | 0.0305     |
| reward_motion           | 0          |
| reward_orientation      | 0.0427     |
| reward_position         | 6.92e-05   |
| reward_rotation         | 0.053      |
| reward_torque           | 0.0433     |
| reward_velocity         | 0.0824     |
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 50         |
|    time_elapsed         | 1643       |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.10917675 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.152      |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.908      |
|    value_loss           | 1.31       |
----------------------------------------
---------------------------------------
| reward                  | 0.289     |
| reward_contact          | 0.0359    |
| reward_ctrl             | 0.0306    |
| reward_motion           | 0         |
| reward_orientation      | 0.0427    |
| reward_position         | 6.92e-05  |
| reward_rotation         | 0.0541    |
| reward_torque           | 0.0434    |
| reward_velocity         | 0.0821    |
| rollout/                |           |
|    ep_len_mean          | 474       |
|    ep_rew_mean          | 132       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 51        |
|    time_elapsed         | 1677      |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.1019634 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.5     |
|    explained_variance   | 0.719     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.66      |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0447   |
|    std                  | 0.902     |
|    value_loss           | 3         |
---------------------------------------
----------------------------------------
| reward                  | 0.288      |
| reward_contact          | 0.0347     |
| reward_ctrl             | 0.0291     |
| reward_motion           | 0          |
| reward_orientation      | 0.0426     |
| reward_position         | 6.92e-05   |
| reward_rotation         | 0.0522     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.0865     |
| rollout/                |            |
|    ep_len_mean          | 480        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 52         |
|    time_elapsed         | 1709       |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.13830064 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.2        |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0484    |
|    std                  | 0.898      |
|    value_loss           | 1.2        |
----------------------------------------
Num timesteps: 108000
Best mean reward: 131.55 - Last mean reward per episode: 132.25
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.286      |
| reward_contact          | 0.0345     |
| reward_ctrl             | 0.0294     |
| reward_motion           | 0          |
| reward_orientation      | 0.0427     |
| reward_position         | 6.92e-05   |
| reward_rotation         | 0.05       |
| reward_torque           | 0.0433     |
| reward_velocity         | 0.0861     |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 53         |
|    time_elapsed         | 1743       |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.12893672 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.212      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0349    |
|    std                  | 0.891      |
|    value_loss           | 1.34       |
----------------------------------------
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0          |
| reward_orientation      | 0.0431     |
| reward_position         | 6.92e-05   |
| reward_rotation         | 0.0488     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0881     |
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 54         |
|    time_elapsed         | 1775       |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.15457204 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0619     |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0431    |
|    std                  | 0.886      |
|    value_loss           | 1.57       |
----------------------------------------
----------------------------------------
| reward                  | 0.289      |
| reward_contact          | 0.0346     |
| reward_ctrl             | 0.0307     |
| reward_motion           | 0          |
| reward_orientation      | 0.0439     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0485     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0877     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 55         |
|    time_elapsed         | 1808       |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.11738634 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0782     |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0411    |
|    std                  | 0.882      |
|    value_loss           | 2.13       |
----------------------------------------
Num timesteps: 114000
Best mean reward: 132.25 - Last mean reward per episode: 130.96
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0352     |
| reward_ctrl             | 0.0305     |
| reward_motion           | 0          |
| reward_orientation      | 0.0439     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0477     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0888     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 56         |
|    time_elapsed         | 1840       |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.17287624 |
|    clip_fraction        | 0.634      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00636   |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0425    |
|    std                  | 0.88       |
|    value_loss           | 1.27       |
----------------------------------------
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.0316     |
| reward_motion           | 0          |
| reward_orientation      | 0.0429     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0502     |
| reward_torque           | 0.0439     |
| reward_velocity         | 0.0876     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 57         |
|    time_elapsed         | 1873       |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.22298896 |
|    clip_fraction        | 0.671      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0414    |
|    std                  | 0.877      |
|    value_loss           | 0.522      |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0309     |
| reward_motion           | 0          |
| reward_orientation      | 0.0424     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0518     |
| reward_torque           | 0.0436     |
| reward_velocity         | 0.0891     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 58         |
|    time_elapsed         | 1906       |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.19369099 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0132    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0574    |
|    std                  | 0.872      |
|    value_loss           | 0.828      |
----------------------------------------
Num timesteps: 120000
Best mean reward: 132.25 - Last mean reward per episode: 131.43
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0494     |
| reward_torque           | 0.0435     |
| reward_velocity         | 0.0911     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 59         |
|    time_elapsed         | 1940       |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.21874987 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0257    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0466    |
|    std                  | 0.868      |
|    value_loss           | 0.941      |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0335     |
| reward_ctrl             | 0.031      |
| reward_motion           | 0          |
| reward_orientation      | 0.0419     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0507     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0904     |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 60         |
|    time_elapsed         | 1973       |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.24173212 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0171    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.047     |
|    std                  | 0.863      |
|    value_loss           | 0.478      |
----------------------------------------
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0335     |
| reward_ctrl             | 0.0313     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0492     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0905     |
| rollout/                |            |
|    ep_len_mean          | 473        |
|    ep_rew_mean          | 131        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 61         |
|    time_elapsed         | 2006       |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.20626432 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00616   |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0499    |
|    std                  | 0.862      |
|    value_loss           | 0.475      |
----------------------------------------
Num timesteps: 126000
Best mean reward: 132.25 - Last mean reward per episode: 131.68
----------------------------------------
| reward                  | 0.293      |
| reward_contact          | 0.0336     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0          |
| reward_orientation      | 0.0413     |
| reward_position         | 2.67e-05   |
| reward_rotation         | 0.0492     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0945     |
| rollout/                |            |
|    ep_len_mean          | 473        |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 62         |
|    time_elapsed         | 2038       |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.19846044 |
|    clip_fraction        | 0.649      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.28       |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0579    |
|    std                  | 0.858      |
|    value_loss           | 0.931      |
----------------------------------------
----------------------------------------
| reward                  | 0.292      |
| reward_contact          | 0.033      |
| reward_ctrl             | 0.0308     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0488     |
| reward_torque           | 0.0436     |
| reward_velocity         | 0.094      |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 63         |
|    time_elapsed         | 2071       |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.21115121 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0411     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.062     |
|    std                  | 0.856      |
|    value_loss           | 1.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0326     |
| reward_ctrl             | 0.0322     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0468     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.0935     |
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 64         |
|    time_elapsed         | 2104       |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.20969419 |
|    clip_fraction        | 0.651      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.236      |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0572    |
|    std                  | 0.85       |
|    value_loss           | 1.01       |
----------------------------------------
Num timesteps: 132000
Best mean reward: 132.25 - Last mean reward per episode: 134.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.292      |
| reward_contact          | 0.0326     |
| reward_ctrl             | 0.0324     |
| reward_motion           | 0          |
| reward_orientation      | 0.0411     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0491     |
| reward_torque           | 0.0442     |
| reward_velocity         | 0.0928     |
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 135        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 65         |
|    time_elapsed         | 2138       |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.17700289 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10        |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0602    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0712    |
|    std                  | 0.843      |
|    value_loss           | 0.701      |
----------------------------------------
----------------------------------------
| reward                  | 0.292      |
| reward_contact          | 0.032      |
| reward_ctrl             | 0.0335     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0486     |
| reward_torque           | 0.0445     |
| reward_velocity         | 0.0939     |
| rollout/                |            |
|    ep_len_mean          | 480        |
|    ep_rew_mean          | 136        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 66         |
|    time_elapsed         | 2171       |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.20187372 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.97      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0136    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0749    |
|    std                  | 0.84       |
|    value_loss           | 0.693      |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0324     |
| reward_ctrl             | 0.0331     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0489     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.0925     |
| rollout/                |            |
|    ep_len_mean          | 480        |
|    ep_rew_mean          | 135        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 67         |
|    time_elapsed         | 2204       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.21397643 |
|    clip_fraction        | 0.671      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.91      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.041     |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0677    |
|    std                  | 0.834      |
|    value_loss           | 0.603      |
----------------------------------------
Num timesteps: 138000
Best mean reward: 134.99 - Last mean reward per episode: 134.93
----------------------------------------
| reward                  | 0.29       |
| reward_contact          | 0.0319     |
| reward_ctrl             | 0.0322     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0492     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.0927     |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 68         |
|    time_elapsed         | 2237       |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.21516302 |
|    clip_fraction        | 0.658      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.87      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0206    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0664    |
|    std                  | 0.83       |
|    value_loss           | 0.511      |
----------------------------------------
----------------------------------------
| reward                  | 0.287      |
| reward_contact          | 0.0312     |
| reward_ctrl             | 0.0307     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0476     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.094      |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 69         |
|    time_elapsed         | 2270       |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.11922102 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.84      |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0905     |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.056     |
|    std                  | 0.827      |
|    value_loss           | 1.29       |
----------------------------------------
---------------------------------------
| reward                  | 0.29      |
| reward_contact          | 0.0318    |
| reward_ctrl             | 0.032     |
| reward_motion           | 0         |
| reward_orientation      | 0.0395    |
| reward_position         | 2.6e-05   |
| reward_rotation         | 0.0468    |
| reward_torque           | 0.0441    |
| reward_velocity         | 0.0952    |
| rollout/                |           |
|    ep_len_mean          | 477       |
|    ep_rew_mean          | 133       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 70        |
|    time_elapsed         | 2303      |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.1379964 |
|    clip_fraction        | 0.612     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.81     |
|    explained_variance   | 0.714     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.125     |
|    n_updates            | 690       |
|    policy_gradient_loss | -0.05     |
|    std                  | 0.824     |
|    value_loss           | 1.86      |
---------------------------------------
Num timesteps: 144000
Best mean reward: 134.99 - Last mean reward per episode: 133.62
----------------------------------------
| reward                  | 0.285      |
| reward_contact          | 0.0312     |
| reward_ctrl             | 0.0315     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0445     |
| reward_torque           | 0.0441     |
| reward_velocity         | 0.0935     |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 133        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 71         |
|    time_elapsed         | 2336       |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.23614341 |
|    clip_fraction        | 0.665      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.76      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0586    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0599    |
|    std                  | 0.817      |
|    value_loss           | 0.442      |
----------------------------------------
----------------------------------------
| reward                  | 0.283      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.0317     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0443     |
| reward_torque           | 0.0442     |
| reward_velocity         | 0.0931     |
| rollout/                |            |
|    ep_len_mean          | 478        |
|    ep_rew_mean          | 133        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 72         |
|    time_elapsed         | 2369       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.26220712 |
|    clip_fraction        | 0.697      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.73      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0529    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0552    |
|    std                  | 0.816      |
|    value_loss           | 0.514      |
----------------------------------------
----------------------------------------
| reward                  | 0.291      |
| reward_contact          | 0.0307     |
| reward_ctrl             | 0.0332     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 2.6e-05    |
| reward_rotation         | 0.0453     |
| reward_torque           | 0.0447     |
| reward_velocity         | 0.0978     |
| rollout/                |            |
|    ep_len_mean          | 480        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 73         |
|    time_elapsed         | 2402       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.22032963 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.71      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0635    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0591    |
|    std                  | 0.814      |
|    value_loss           | 0.671      |
----------------------------------------
Num timesteps: 150000
Best mean reward: 134.99 - Last mean reward per episode: 133.69
----------------------------------------
| reward                  | 0.295      |
| reward_contact          | 0.0301     |
| reward_ctrl             | 0.0339     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 1.34e-26   |
| reward_rotation         | 0.0456     |
| reward_torque           | 0.0448     |
| reward_velocity         | 0.103      |
| rollout/                |            |
|    ep_len_mean          | 485        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 74         |
|    time_elapsed         | 2435       |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.20221242 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.69      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.253      |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0445    |
|    std                  | 0.812      |
|    value_loss           | 1.01       |
----------------------------------------
----------------------------------------
| reward                  | 0.297      |
| reward_contact          | 0.0291     |
| reward_ctrl             | 0.0353     |
| reward_motion           | 0          |
| reward_orientation      | 0.0386     |
| reward_position         | 1.34e-26   |
| reward_rotation         | 0.0451     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.104      |
| rollout/                |            |
|    ep_len_mean          | 485        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 75         |
|    time_elapsed         | 2467       |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.23026735 |
|    clip_fraction        | 0.681      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.66      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0451     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0485    |
|    std                  | 0.809      |
|    value_loss           | 0.512      |
----------------------------------------
----------------------------------------
| reward                  | 0.301      |
| reward_contact          | 0.0291     |
| reward_ctrl             | 0.0357     |
| reward_motion           | 0          |
| reward_orientation      | 0.0391     |
| reward_position         | 1.34e-26   |
| reward_rotation         | 0.0454     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.106      |
| rollout/                |            |
|    ep_len_mean          | 485        |
|    ep_rew_mean          | 134        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 76         |
|    time_elapsed         | 2501       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.26305294 |
|    clip_fraction        | 0.693      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.65      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0515    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0488    |
|    std                  | 0.808      |
|    value_loss           | 0.492      |
----------------------------------------
Num timesteps: 156000
Best mean reward: 134.99 - Last mean reward per episode: 134.26
----------------------------------------
| reward                  | 0.302      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0357     |
| reward_motion           | 0          |
| reward_orientation      | 0.0389     |
| reward_position         | 1.34e-26   |
| reward_rotation         | 0.0473     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.106      |
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | 136        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 77         |
|    time_elapsed         | 2534       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.23641284 |
|    clip_fraction        | 0.683      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.6       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.048     |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.054     |
|    std                  | 0.801      |
|    value_loss           | 0.535      |
----------------------------------------
----------------------------------------
| reward                  | 0.303      |
| reward_contact          | 0.0288     |
| reward_ctrl             | 0.0359     |
| reward_motion           | 0          |
| reward_orientation      | 0.0386     |
| reward_position         | 1.34e-26   |
| reward_rotation         | 0.0485     |
| reward_torque           | 0.0451     |
| reward_velocity         | 0.106      |
| rollout/                |            |
|    ep_len_mean          | 490        |
|    ep_rew_mean          | 136        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 78         |
|    time_elapsed         | 2567       |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.19839388 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.56      |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00135    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0479    |
|    std                  | 0.798      |
|    value_loss           | 0.858      |
----------------------------------------
----------------------------------------
| reward                  | 0.306      |
| reward_contact          | 0.0281     |
| reward_ctrl             | 0.0359     |
| reward_motion           | 0          |
| reward_orientation      | 0.0388     |
| reward_position         | 2.25e-41   |
| reward_rotation         | 0.0487     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 79         |
|    time_elapsed         | 2600       |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.39997488 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.54      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0957    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0717    |
|    std                  | 0.797      |
|    value_loss           | 0.313      |
----------------------------------------
Num timesteps: 162000
Best mean reward: 134.99 - Last mean reward per episode: 137.08
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.31      |
| reward_contact          | 0.0288    |
| reward_ctrl             | 0.0371    |
| reward_motion           | 0         |
| reward_orientation      | 0.039     |
| reward_position         | 2.25e-41  |
| reward_rotation         | 0.049     |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.111     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 136       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 80        |
|    time_elapsed         | 2634      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.3072641 |
|    clip_fraction        | 0.681     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.54     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0857   |
|    n_updates            | 790       |
|    policy_gradient_loss | -0.0661   |
|    std                  | 0.797     |
|    value_loss           | 0.307     |
---------------------------------------
---------------------------------------
| reward                  | 0.31      |
| reward_contact          | 0.0301    |
| reward_ctrl             | 0.0369    |
| reward_motion           | 0         |
| reward_orientation      | 0.0394    |
| reward_position         | 2.25e-41  |
| reward_rotation         | 0.0486    |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.109     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 137       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 81        |
|    time_elapsed         | 2667      |
|    total_timesteps      | 165888    |
| train/                  |           |
|    approx_kl            | 0.2417561 |
|    clip_fraction        | 0.672     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.5      |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.626     |
|    n_updates            | 800       |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.791     |
|    value_loss           | 1.11      |
---------------------------------------
----------------------------------------
| reward                  | 0.309      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0381     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 2.25e-41   |
| reward_rotation         | 0.0483     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.108      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 139        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 82         |
|    time_elapsed         | 2699       |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.26953882 |
|    clip_fraction        | 0.684      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.46      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00353    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.789      |
|    value_loss           | 0.465      |
----------------------------------------
Num timesteps: 168000
Best mean reward: 137.08 - Last mean reward per episode: 138.94
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.314      |
| reward_contact          | 0.0311     |
| reward_ctrl             | 0.039      |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 2.25e-41   |
| reward_rotation         | 0.0509     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.107      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 140        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 83         |
|    time_elapsed         | 2733       |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.22101435 |
|    clip_fraction        | 0.678      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.44      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0377     |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0644    |
|    std                  | 0.787      |
|    value_loss           | 0.645      |
----------------------------------------
---------------------------------------
| reward                  | 0.314     |
| reward_contact          | 0.0289    |
| reward_ctrl             | 0.0393    |
| reward_motion           | 0         |
| reward_orientation      | 0.0394    |
| reward_position         | 2e-40     |
| reward_rotation         | 0.0512    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.109     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 142       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 84        |
|    time_elapsed         | 2765      |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.2686171 |
|    clip_fraction        | 0.689     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.4      |
|    explained_variance   | 0.939     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00161  |
|    n_updates            | 830       |
|    policy_gradient_loss | -0.0788   |
|    std                  | 0.781     |
|    value_loss           | 0.591     |
---------------------------------------
Num timesteps: 174000
Best mean reward: 138.94 - Last mean reward per episode: 142.28
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.313      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0389     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.052      |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.107      |
| rollout/                |            |
|    ep_len_mean          | 490        |
|    ep_rew_mean          | 142        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 85         |
|    time_elapsed         | 2799       |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.29776263 |
|    clip_fraction        | 0.711      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0267    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.775      |
|    value_loss           | 0.52       |
----------------------------------------
---------------------------------------
| reward                  | 0.318     |
| reward_contact          | 0.0294    |
| reward_ctrl             | 0.0392    |
| reward_motion           | 0         |
| reward_orientation      | 0.0396    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0555    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.108     |
| rollout/                |           |
|    ep_len_mean          | 490       |
|    ep_rew_mean          | 142       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 86        |
|    time_elapsed         | 2831      |
|    total_timesteps      | 176128    |
| train/                  |           |
|    approx_kl            | 0.2183997 |
|    clip_fraction        | 0.651     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.29     |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.867     |
|    n_updates            | 850       |
|    policy_gradient_loss | -0.0663   |
|    std                  | 0.773     |
|    value_loss           | 1.19      |
---------------------------------------
---------------------------------------
| reward                  | 0.32      |
| reward_contact          | 0.0294    |
| reward_ctrl             | 0.0395    |
| reward_motion           | 0         |
| reward_orientation      | 0.0393    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.057     |
| reward_torque           | 0.0461    |
| reward_velocity         | 0.108     |
| rollout/                |           |
|    ep_len_mean          | 490       |
|    ep_rew_mean          | 142       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 87        |
|    time_elapsed         | 2864      |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.2948178 |
|    clip_fraction        | 0.699     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.26     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0269   |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.0757   |
|    std                  | 0.768     |
|    value_loss           | 0.7       |
---------------------------------------
Num timesteps: 180000
Best mean reward: 142.28 - Last mean reward per episode: 142.95
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.322     |
| reward_contact          | 0.0293    |
| reward_ctrl             | 0.0391    |
| reward_motion           | 0         |
| reward_orientation      | 0.0402    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0576    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.11      |
| rollout/                |           |
|    ep_len_mean          | 491       |
|    ep_rew_mean          | 143       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 88        |
|    time_elapsed         | 2898      |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.2254737 |
|    clip_fraction        | 0.679     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.23     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0396    |
|    n_updates            | 870       |
|    policy_gradient_loss | -0.0622   |
|    std                  | 0.768     |
|    value_loss           | 0.853     |
---------------------------------------
---------------------------------------
| reward                  | 0.321     |
| reward_contact          | 0.0292    |
| reward_ctrl             | 0.0392    |
| reward_motion           | 0         |
| reward_orientation      | 0.0404    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0572    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.109     |
| rollout/                |           |
|    ep_len_mean          | 491       |
|    ep_rew_mean          | 143       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 89        |
|    time_elapsed         | 2931      |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.2848048 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.23     |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0301   |
|    n_updates            | 880       |
|    policy_gradient_loss | -0.0677   |
|    std                  | 0.768     |
|    value_loss           | 0.639     |
---------------------------------------
----------------------------------------
| reward                  | 0.318      |
| reward_contact          | 0.0298     |
| reward_ctrl             | 0.0383     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0559     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.108      |
| rollout/                |            |
|    ep_len_mean          | 491        |
|    ep_rew_mean          | 144        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 90         |
|    time_elapsed         | 2964       |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.24424872 |
|    clip_fraction        | 0.684      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.21      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0323     |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.053     |
|    std                  | 0.765      |
|    value_loss           | 1.01       |
----------------------------------------
Num timesteps: 186000
Best mean reward: 142.95 - Last mean reward per episode: 144.90
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.322      |
| reward_contact          | 0.0304     |
| reward_ctrl             | 0.0382     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0579     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 144        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 91         |
|    time_elapsed         | 2997       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.24176294 |
|    clip_fraction        | 0.685      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.17      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0477     |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.761      |
|    value_loss           | 1.25       |
----------------------------------------
----------------------------------------
| reward                  | 0.325      |
| reward_contact          | 0.0298     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0606     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.111      |
| rollout/                |            |
|    ep_len_mean          | 491        |
|    ep_rew_mean          | 146        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 92         |
|    time_elapsed         | 3030       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.21532328 |
|    clip_fraction        | 0.677      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.15      |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.761      |
|    value_loss           | 1.25       |
----------------------------------------
----------------------------------------
| reward                  | 0.325      |
| reward_contact          | 0.0293     |
| reward_ctrl             | 0.0384     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0611     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 491        |
|    ep_rew_mean          | 146        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 93         |
|    time_elapsed         | 3063       |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.27729803 |
|    clip_fraction        | 0.705      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.13      |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0467     |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.757      |
|    value_loss           | 0.906      |
----------------------------------------
Num timesteps: 192000
Best mean reward: 144.90 - Last mean reward per episode: 145.17
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.321      |
| reward_contact          | 0.0271     |
| reward_ctrl             | 0.0373     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.062      |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.108      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 145        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 94         |
|    time_elapsed         | 3096       |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.28316766 |
|    clip_fraction        | 0.689      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.1       |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.756      |
|    value_loss           | 0.871      |
----------------------------------------
----------------------------------------
| reward                  | 0.319      |
| reward_contact          | 0.0271     |
| reward_ctrl             | 0.0368     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0627     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.106      |
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | 146        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 95         |
|    time_elapsed         | 3130       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.41282982 |
|    clip_fraction        | 0.726      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.07      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00177   |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.75       |
|    value_loss           | 0.824      |
----------------------------------------
----------------------------------------
| reward                  | 0.324      |
| reward_contact          | 0.0272     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0651     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.108      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 147        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 96         |
|    time_elapsed         | 3163       |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.39800292 |
|    clip_fraction        | 0.72       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.02      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0226     |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.746      |
|    value_loss           | 0.47       |
----------------------------------------
Num timesteps: 198000
Best mean reward: 145.17 - Last mean reward per episode: 147.52
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.321      |
| reward_contact          | 0.0261     |
| reward_ctrl             | 0.0369     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0636     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.108      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 148        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 97         |
|    time_elapsed         | 3196       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.27915064 |
|    clip_fraction        | 0.686      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.99      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0401    |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0642    |
|    std                  | 0.744      |
|    value_loss           | 0.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.32       |
| reward_contact          | 0.025      |
| reward_ctrl             | 0.0364     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0637     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 98         |
|    time_elapsed         | 3229       |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.38704687 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.98      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0636    |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.744      |
|    value_loss           | 0.517      |
----------------------------------------
---------------------------------------
| reward                  | 0.322     |
| reward_contact          | 0.0248    |
| reward_ctrl             | 0.0366    |
| reward_motion           | 0         |
| reward_orientation      | 0.0406    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0637    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.111     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 151       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 99        |
|    time_elapsed         | 3262      |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.3396734 |
|    clip_fraction        | 0.718     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.98     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0182   |
|    n_updates            | 980       |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.744     |
|    value_loss           | 0.691     |
---------------------------------------
Num timesteps: 204000
Best mean reward: 147.52 - Last mean reward per episode: 152.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.32      |
| reward_contact          | 0.0248    |
| reward_ctrl             | 0.0369    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0637    |
| reward_torque           | 0.0452    |
| reward_velocity         | 0.109     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 152       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 100       |
|    time_elapsed         | 3295      |
|    total_timesteps      | 204800    |
| train/                  |           |
|    approx_kl            | 0.3006016 |
|    clip_fraction        | 0.706     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.96     |
|    explained_variance   | 0.901     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0663   |
|    n_updates            | 990       |
|    policy_gradient_loss | -0.0654   |
|    std                  | 0.741     |
|    value_loss           | 0.802     |
---------------------------------------
---------------------------------------
| reward                  | 0.32      |
| reward_contact          | 0.0236    |
| reward_ctrl             | 0.0393    |
| reward_motion           | 0         |
| reward_orientation      | 0.0404    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0631    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.108     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 153       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 101       |
|    time_elapsed         | 3328      |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.3286903 |
|    clip_fraction        | 0.696     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.94     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0114    |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0608   |
|    std                  | 0.74      |
|    value_loss           | 0.756     |
---------------------------------------
---------------------------------------
| reward                  | 0.324     |
| reward_contact          | 0.0244    |
| reward_ctrl             | 0.0394    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0647    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.11      |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 102       |
|    time_elapsed         | 3361      |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.4468869 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.92     |
|    explained_variance   | 0.932     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0162   |
|    n_updates            | 1010      |
|    policy_gradient_loss | -0.0762   |
|    std                  | 0.737     |
|    value_loss           | 0.717     |
---------------------------------------
Num timesteps: 210000
Best mean reward: 152.18 - Last mean reward per episode: 153.93
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.33      |
| reward_contact          | 0.0251    |
| reward_ctrl             | 0.0405    |
| reward_motion           | 0         |
| reward_orientation      | 0.04      |
| reward_position         | 1.93e-21  |
| reward_rotation         | 0.0667    |
| reward_torque           | 0.0462    |
| reward_velocity         | 0.112     |
| rollout/                |           |
|    ep_len_mean          | 488       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 62        |
|    iterations           | 103       |
|    time_elapsed         | 3394      |
|    total_timesteps      | 210944    |
| train/                  |           |
|    approx_kl            | 0.4192575 |
|    clip_fraction        | 0.736     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.89     |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0214    |
|    n_updates            | 1020      |
|    policy_gradient_loss | -0.0637   |
|    std                  | 0.735     |
|    value_loss           | 0.396     |
---------------------------------------
----------------------------------------
| reward                  | 0.329      |
| reward_contact          | 0.025      |
| reward_ctrl             | 0.0392     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0683     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.111      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 104        |
|    time_elapsed         | 3427       |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.35662755 |
|    clip_fraction        | 0.722      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.85      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0683     |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.731      |
|    value_loss           | 0.716      |
----------------------------------------
----------------------------------------
| reward                  | 0.333      |
| reward_contact          | 0.0243     |
| reward_ctrl             | 0.0398     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0702     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 156        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 105        |
|    time_elapsed         | 3461       |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.41244105 |
|    clip_fraction        | 0.74       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.83      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0855    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.729      |
|    value_loss           | 0.47       |
----------------------------------------
Num timesteps: 216000
Best mean reward: 153.93 - Last mean reward per episode: 156.70
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.333      |
| reward_contact          | 0.0243     |
| reward_ctrl             | 0.0386     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0704     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.114      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 156        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 106        |
|    time_elapsed         | 3494       |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.27902126 |
|    clip_fraction        | 0.699      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.79      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0323    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.065     |
|    std                  | 0.726      |
|    value_loss           | 0.987      |
----------------------------------------
----------------------------------------
| reward                  | 0.33       |
| reward_contact          | 0.0248     |
| reward_ctrl             | 0.0378     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0682     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 155        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 107        |
|    time_elapsed         | 3528       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.29737064 |
|    clip_fraction        | 0.699      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.78      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0106     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0622    |
|    std                  | 0.726      |
|    value_loss           | 0.876      |
----------------------------------------
----------------------------------------
| reward                  | 0.332      |
| reward_contact          | 0.0259     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 1.93e-21   |
| reward_rotation         | 0.0691     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 155        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 108        |
|    time_elapsed         | 3561       |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.56720567 |
|    clip_fraction        | 0.775      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.77      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.089     |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.724      |
|    value_loss           | 0.293      |
----------------------------------------
Num timesteps: 222000
Best mean reward: 156.70 - Last mean reward per episode: 154.80
----------------------------------------
| reward                  | 0.327      |
| reward_contact          | 0.0258     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0          |
| reward_orientation      | 0.0417     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0669     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.11       |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 155        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 109        |
|    time_elapsed         | 3594       |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.46231925 |
|    clip_fraction        | 0.749      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.73      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0684    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0664    |
|    std                  | 0.719      |
|    value_loss           | 0.358      |
----------------------------------------
----------------------------------------
| reward                  | 0.324      |
| reward_contact          | 0.0241     |
| reward_ctrl             | 0.0385     |
| reward_motion           | 0          |
| reward_orientation      | 0.0418     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.065      |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 110        |
|    time_elapsed         | 3627       |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.52962226 |
|    clip_fraction        | 0.763      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.7       |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.101     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.718      |
|    value_loss           | 0.264      |
----------------------------------------
----------------------------------------
| reward                  | 0.323      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0381     |
| reward_motion           | 0          |
| reward_orientation      | 0.0423     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0628     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.111      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 111        |
|    time_elapsed         | 3660       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.40711188 |
|    clip_fraction        | 0.735      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.67      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0113    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0684    |
|    std                  | 0.715      |
|    value_loss           | 0.418      |
----------------------------------------
Num timesteps: 228000
Best mean reward: 156.70 - Last mean reward per episode: 154.67
----------------------------------------
| reward                  | 0.327      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0374     |
| reward_motion           | 0          |
| reward_orientation      | 0.0425     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0648     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 112        |
|    time_elapsed         | 3693       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.26628727 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.65      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.66       |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0654    |
|    std                  | 0.714      |
|    value_loss           | 1.05       |
----------------------------------------
----------------------------------------
| reward                  | 0.327      |
| reward_contact          | 0.0238     |
| reward_ctrl             | 0.0384     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0634     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 113        |
|    time_elapsed         | 3726       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.33069026 |
|    clip_fraction        | 0.721      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.61      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0666    |
|    std                  | 0.709      |
|    value_loss           | 0.938      |
----------------------------------------
----------------------------------------
| reward                  | 0.322      |
| reward_contact          | 0.0239     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.043      |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0628     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 152        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 114        |
|    time_elapsed         | 3759       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.55419976 |
|    clip_fraction        | 0.76       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.57      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0709    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.706      |
|    value_loss           | 0.408      |
----------------------------------------
Num timesteps: 234000
Best mean reward: 156.70 - Last mean reward per episode: 152.54
----------------------------------------
| reward                  | 0.322      |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0          |
| reward_orientation      | 0.0432     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0621     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.111      |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | 152        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 115        |
|    time_elapsed         | 3794       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.38788766 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.53      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0276    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0633    |
|    std                  | 0.702      |
|    value_loss           | 0.685      |
----------------------------------------
----------------------------------------
| reward                  | 0.325      |
| reward_contact          | 0.0233     |
| reward_ctrl             | 0.0378     |
| reward_motion           | 0          |
| reward_orientation      | 0.0425     |
| reward_position         | 6.2e-38    |
| reward_rotation         | 0.0614     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | 153        |
| time/                   |            |
|    fps                  | 62         |
|    iterations           | 116        |
|    time_elapsed         | 3830       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.51348096 |
|    clip_fraction        | 0.739      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.51      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0507    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0534    |
|    std                  | 0.703      |
|    value_loss           | 0.886      |
----------------------------------------
---------------------------------------
| reward                  | 0.329     |
| reward_contact          | 0.0233    |
| reward_ctrl             | 0.0383    |
| reward_motion           | 0         |
| reward_orientation      | 0.0426    |
| reward_position         | 6.2e-38   |
| reward_rotation         | 0.0627    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 61        |
|    iterations           | 117       |
|    time_elapsed         | 3879      |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.3871271 |
|    clip_fraction        | 0.724     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.52     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00737   |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.0615   |
|    std                  | 0.702     |
|    value_loss           | 0.697     |
---------------------------------------
Num timesteps: 240000
Best mean reward: 156.70 - Last mean reward per episode: 153.87
----------------------------------------
| reward                  | 0.335      |
| reward_contact          | 0.0233     |
| reward_ctrl             | 0.038      |
| reward_motion           | 0          |
| reward_orientation      | 0.0429     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0642     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 155        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 118        |
|    time_elapsed         | 3936       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.37407672 |
|    clip_fraction        | 0.739      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.47      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00338   |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.062     |
|    std                  | 0.697      |
|    value_loss           | 0.753      |
----------------------------------------
----------------------------------------
| reward                  | 0.336      |
| reward_contact          | 0.0231     |
| reward_ctrl             | 0.0378     |
| reward_motion           | 0          |
| reward_orientation      | 0.0434     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0646     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 155        |
| time/                   |            |
|    fps                  | 61         |
|    iterations           | 119        |
|    time_elapsed         | 3988       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.26332453 |
|    clip_fraction        | 0.688      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.43      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.225      |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0434    |
|    std                  | 0.694      |
|    value_loss           | 2.88       |
----------------------------------------
----------------------------------------
| reward                  | 0.335      |
| reward_contact          | 0.0231     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.0434     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0655     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.12       |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 120        |
|    time_elapsed         | 4039       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.39808422 |
|    clip_fraction        | 0.733      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.41      |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.694      |
|    value_loss           | 2.79       |
----------------------------------------
Num timesteps: 246000
Best mean reward: 156.70 - Last mean reward per episode: 154.44
---------------------------------------
| reward                  | 0.333     |
| reward_contact          | 0.023     |
| reward_ctrl             | 0.0374    |
| reward_motion           | 0         |
| reward_orientation      | 0.0436    |
| reward_position         | 7.12e-54  |
| reward_rotation         | 0.0659    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 60        |
|    iterations           | 121       |
|    time_elapsed         | 4091      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.5161275 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.38     |
|    explained_variance   | 0.9       |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0735   |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.0504   |
|    std                  | 0.69      |
|    value_loss           | 0.488     |
---------------------------------------
----------------------------------------
| reward                  | 0.334      |
| reward_contact          | 0.0228     |
| reward_ctrl             | 0.0381     |
| reward_motion           | 0          |
| reward_orientation      | 0.0437     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0665     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.117      |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 122        |
|    time_elapsed         | 4143       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.48868045 |
|    clip_fraction        | 0.759      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.34      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.687      |
|    value_loss           | 0.828      |
----------------------------------------
----------------------------------------
| reward                  | 0.332      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.037      |
| reward_motion           | 0          |
| reward_orientation      | 0.0445     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0667     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 154        |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 123        |
|    time_elapsed         | 4195       |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.40372267 |
|    clip_fraction        | 0.746      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.31      |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0267    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0555    |
|    std                  | 0.684      |
|    value_loss           | 0.761      |
----------------------------------------
Num timesteps: 252000
Best mean reward: 156.70 - Last mean reward per episode: 153.69
----------------------------------------
| reward                  | 0.334      |
| reward_contact          | 0.0228     |
| reward_ctrl             | 0.0369     |
| reward_motion           | 0          |
| reward_orientation      | 0.0444     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0689     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.116      |
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | 153        |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 124        |
|    time_elapsed         | 4248       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.56833196 |
|    clip_fraction        | 0.757      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.26      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0143    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0542    |
|    std                  | 0.678      |
|    value_loss           | 0.469      |
----------------------------------------
---------------------------------------
| reward                  | 0.336     |
| reward_contact          | 0.0229    |
| reward_ctrl             | 0.0349    |
| reward_motion           | 0         |
| reward_orientation      | 0.0441    |
| reward_position         | 7.12e-54  |
| reward_rotation         | 0.0687    |
| reward_torque           | 0.045     |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | 153       |
| time/                   |           |
|    fps                  | 59        |
|    iterations           | 125       |
|    time_elapsed         | 4301      |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.6132307 |
|    clip_fraction        | 0.762     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.21     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0239    |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.0663   |
|    std                  | 0.674     |
|    value_loss           | 0.522     |
---------------------------------------
Num timesteps: 258000
Best mean reward: 156.70 - Last mean reward per episode: 152.00
----------------------------------------
| reward                  | 0.332      |
| reward_contact          | 0.0239     |
| reward_ctrl             | 0.033      |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0654     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.12       |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 152        |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 126        |
|    time_elapsed         | 4353       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.54852957 |
|    clip_fraction        | 0.755      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.17      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0744     |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0637    |
|    std                  | 0.673      |
|    value_loss           | 0.453      |
----------------------------------------
----------------------------------------
| reward                  | 0.33       |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0336     |
| reward_motion           | 0          |
| reward_orientation      | 0.0452     |
| reward_position         | 7.12e-54   |
| reward_rotation         | 0.0646     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.119      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 151        |
| time/                   |            |
|    fps                  | 59         |
|    iterations           | 127        |
|    time_elapsed         | 4405       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.46678892 |
|    clip_fraction        | 0.74       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.16      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0609    |
|    std                  | 0.673      |
|    value_loss           | 0.878      |
----------------------------------------
----------------------------------------
| reward                  | 0.326      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0329     |
| reward_motion           | 0          |
| reward_orientation      | 0.0454     |
| reward_position         | 6.36e-67   |
| reward_rotation         | 0.0637     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.117      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 150        |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 128        |
|    time_elapsed         | 4459       |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.50285935 |
|    clip_fraction        | 0.76       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.18      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.585      |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0473    |
|    std                  | 0.673      |
|    value_loss           | 0.682      |
----------------------------------------
Num timesteps: 264000
Best mean reward: 156.70 - Last mean reward per episode: 149.91
---------------------------------------
| reward                  | 0.327     |
| reward_contact          | 0.0231    |
| reward_ctrl             | 0.0316    |
| reward_motion           | 0         |
| reward_orientation      | 0.0457    |
| reward_position         | 6.36e-67  |
| reward_rotation         | 0.0639    |
| reward_torque           | 0.0442    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 129       |
|    time_elapsed         | 4513      |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.3117149 |
|    clip_fraction        | 0.727     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.16     |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0116   |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.051    |
|    std                  | 0.67      |
|    value_loss           | 0.791     |
---------------------------------------
----------------------------------------
| reward                  | 0.33       |
| reward_contact          | 0.0221     |
| reward_ctrl             | 0.0323     |
| reward_motion           | 0          |
| reward_orientation      | 0.0449     |
| reward_position         | 6.36e-67   |
| reward_rotation         | 0.0667     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.12       |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 130        |
|    time_elapsed         | 4566       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.42711997 |
|    clip_fraction        | 0.773      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.14      |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.412      |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0377    |
|    std                  | 0.671      |
|    value_loss           | 0.752      |
----------------------------------------
----------------------------------------
| reward                  | 0.33       |
| reward_contact          | 0.0215     |
| reward_ctrl             | 0.0314     |
| reward_motion           | 0          |
| reward_orientation      | 0.045      |
| reward_position         | 6.36e-67   |
| reward_rotation         | 0.0688     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.119      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 58         |
|    iterations           | 131        |
|    time_elapsed         | 4619       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.46936536 |
|    clip_fraction        | 0.759      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.13      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0575    |
|    std                  | 0.668      |
|    value_loss           | 0.804      |
----------------------------------------
Num timesteps: 270000
Best mean reward: 156.70 - Last mean reward per episode: 148.44
---------------------------------------
| reward                  | 0.33      |
| reward_contact          | 0.0221    |
| reward_ctrl             | 0.0318    |
| reward_motion           | 0         |
| reward_orientation      | 0.0448    |
| reward_position         | 6.36e-67  |
| reward_rotation         | 0.0662    |
| reward_torque           | 0.0442    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 148       |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 132       |
|    time_elapsed         | 4673      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.4792691 |
|    clip_fraction        | 0.747     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.09     |
|    explained_variance   | 0.897     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0488   |
|    n_updates            | 1310      |
|    policy_gradient_loss | -0.0537   |
|    std                  | 0.665     |
|    value_loss           | 0.682     |
---------------------------------------
---------------------------------------
| reward                  | 0.332     |
| reward_contact          | 0.0221    |
| reward_ctrl             | 0.0325    |
| reward_motion           | 0         |
| reward_orientation      | 0.0453    |
| reward_position         | 6.36e-67  |
| reward_rotation         | 0.0649    |
| reward_torque           | 0.0444    |
| reward_velocity         | 0.123     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 149       |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 133       |
|    time_elapsed         | 4725      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.5930246 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.09     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.056    |
|    n_updates            | 1320      |
|    policy_gradient_loss | -0.0516   |
|    std                  | 0.667     |
|    value_loss           | 0.5       |
---------------------------------------
---------------------------------------
| reward                  | 0.34      |
| reward_contact          | 0.0239    |
| reward_ctrl             | 0.0319    |
| reward_motion           | 0         |
| reward_orientation      | 0.0448    |
| reward_position         | 2.65e-47  |
| reward_rotation         | 0.0682    |
| reward_torque           | 0.0443    |
| reward_velocity         | 0.127     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 134       |
|    time_elapsed         | 4779      |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.7256707 |
|    clip_fraction        | 0.796     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.11     |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0677   |
|    n_updates            | 1330      |
|    policy_gradient_loss | -0.0677   |
|    std                  | 0.667     |
|    value_loss           | 0.518     |
---------------------------------------
Num timesteps: 276000
Best mean reward: 156.70 - Last mean reward per episode: 150.00
---------------------------------------
| reward                  | 0.344     |
| reward_contact          | 0.0234    |
| reward_ctrl             | 0.0333    |
| reward_motion           | 0         |
| reward_orientation      | 0.0442    |
| reward_position         | 2.65e-47  |
| reward_rotation         | 0.0716    |
| reward_torque           | 0.0445    |
| reward_velocity         | 0.127     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 135       |
|    time_elapsed         | 4832      |
|    total_timesteps      | 276480    |
| train/                  |           |
|    approx_kl            | 0.6411005 |
|    clip_fraction        | 0.782     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.09     |
|    explained_variance   | 0.939     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0887   |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0632   |
|    std                  | 0.664     |
|    value_loss           | 0.5       |
---------------------------------------
---------------------------------------
| reward                  | 0.339     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0336    |
| reward_motion           | 0         |
| reward_orientation      | 0.044     |
| reward_position         | 2.65e-47  |
| reward_rotation         | 0.0714    |
| reward_torque           | 0.0445    |
| reward_velocity         | 0.122     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 151       |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 136       |
|    time_elapsed         | 4885      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.7209057 |
|    clip_fraction        | 0.793     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.04     |
|    explained_variance   | 0.896     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0938   |
|    n_updates            | 1350      |
|    policy_gradient_loss | -0.0846   |
|    std                  | 0.659     |
|    value_loss           | 0.602     |
---------------------------------------
---------------------------------------
| reward                  | 0.337     |
| reward_contact          | 0.0236    |
| reward_ctrl             | 0.032     |
| reward_motion           | 0         |
| reward_orientation      | 0.0448    |
| reward_position         | 2.65e-47  |
| reward_rotation         | 0.0728    |
| reward_torque           | 0.044     |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 152       |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 137       |
|    time_elapsed         | 4934      |
|    total_timesteps      | 280576    |
| train/                  |           |
|    approx_kl            | 0.5300832 |
|    clip_fraction        | 0.768     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.99     |
|    explained_variance   | 0.939     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0741   |
|    n_updates            | 1360      |
|    policy_gradient_loss | -0.0869   |
|    std                  | 0.655     |
|    value_loss           | 0.537     |
---------------------------------------
2021-06-02 07:33:18.407776: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 07:33:18.407943: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
[DDPG] MultiInputPolicy
[DDPG] Using HER
Using cpu device
Logging to rl/out_dir/models/exp69/SAC_3
Num timesteps: 282000
Best mean reward: 156.70 - Last mean reward per episode: 153.17
---------------------------------------
| reward                  | 0.346     |
| reward_contact          | 0.0231    |
| reward_ctrl             | 0.0342    |
| reward_motion           | 0         |
| reward_orientation      | 0.0448    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0738    |
| reward_torque           | 0.0444    |
| reward_velocity         | 0.126     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 153       |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 138       |
|    time_elapsed         | 4976      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.5499701 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.95     |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0253   |
|    n_updates            | 1370      |
|    policy_gradient_loss | -0.0768   |
|    std                  | 0.653     |
|    value_loss           | 0.414     |
---------------------------------------
---------------------------------
| reward             | 0.336    |
| reward_contact     | 0.02     |
| reward_ctrl        | 0.0483   |
| reward_motion      | 0        |
| reward_orientation | 0.0376   |
| reward_position    | 8.1e-117 |
| reward_rotation    | 0.0639   |
| reward_torque      | 0.05     |
| reward_velocity    | 0.116    |
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 52       |
|    time_elapsed    | 37       |
|    total timesteps | 2000     |
---------------------------------
----------------------------------------
| reward                  | 0.351      |
| reward_contact          | 0.0237     |
| reward_ctrl             | 0.0369     |
| reward_motion           | 0          |
| reward_orientation      | 0.0442     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0747     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.127      |
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | 153        |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 139        |
|    time_elapsed         | 5024       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.42350262 |
|    clip_fraction        | 0.745      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.91      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0391    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.65       |
|    value_loss           | 1.26       |
----------------------------------------
---------------------------------------
| reward                  | 0.356     |
| reward_contact          | 0.0228    |
| reward_ctrl             | 0.0387    |
| reward_motion           | 0         |
| reward_orientation      | 0.0439    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0778    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.127     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 140       |
|    time_elapsed         | 5080      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.5716263 |
|    clip_fraction        | 0.764     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.89     |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0226   |
|    n_updates            | 1390      |
|    policy_gradient_loss | -0.0784   |
|    std                  | 0.649     |
|    value_loss           | 0.452     |
---------------------------------------
Num timesteps: 288000
Best mean reward: 156.70 - Last mean reward per episode: 154.56
---------------------------------------
| reward                  | 0.35      |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0389    |
| reward_motion           | 0         |
| reward_orientation      | 0.044     |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0749    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.123     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 141       |
|    time_elapsed         | 5135      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.5390379 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.87     |
|    explained_variance   | 0.948     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0599   |
|    n_updates            | 1400      |
|    policy_gradient_loss | -0.0742   |
|    std                  | 0.647     |
|    value_loss           | 0.616     |
---------------------------------------
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0252    |
| reward_ctrl             | 0.0395    |
| reward_motion           | 0         |
| reward_orientation      | 0.0445    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.079     |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.122     |
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | 156       |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 142       |
|    time_elapsed         | 5188      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.4168242 |
|    clip_fraction        | 0.745     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.85     |
|    explained_variance   | 0.88      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.132     |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0718   |
|    std                  | 0.646     |
|    value_loss           | 1.38      |
---------------------------------------
---------------------------------------
| reward                  | 0.353     |
| reward_contact          | 0.0247    |
| reward_ctrl             | 0.0394    |
| reward_motion           | 0         |
| reward_orientation      | 0.0442    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0776    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.122     |
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | 157       |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 143       |
|    time_elapsed         | 5241      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.5355887 |
|    clip_fraction        | 0.757     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.82     |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0343    |
|    n_updates            | 1420      |
|    policy_gradient_loss | -0.0617   |
|    std                  | 0.641     |
|    value_loss           | 0.887     |
---------------------------------------
Num timesteps: 294000
Best mean reward: 156.70 - Last mean reward per episode: 157.57
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0253    |
| reward_ctrl             | 0.0396    |
| reward_motion           | 0         |
| reward_orientation      | 0.0438    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0803    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 495       |
|    ep_rew_mean          | 157       |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 144       |
|    time_elapsed         | 5297      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.5208075 |
|    clip_fraction        | 0.764     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.75     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0566   |
|    n_updates            | 1430      |
|    policy_gradient_loss | -0.0646   |
|    std                  | 0.637     |
|    value_loss           | 0.835     |
---------------------------------------
---------------------------------------
| reward                  | 0.35      |
| reward_contact          | 0.0241    |
| reward_ctrl             | 0.0393    |
| reward_motion           | 0         |
| reward_orientation      | 0.044     |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0794    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 495       |
|    ep_rew_mean          | 157       |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 145       |
|    time_elapsed         | 5352      |
|    total_timesteps      | 296960    |
| train/                  |           |
|    approx_kl            | 0.5416095 |
|    clip_fraction        | 0.763     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.69     |
|    explained_variance   | 0.876     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0335   |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.0525   |
|    std                  | 0.631     |
|    value_loss           | 1.04      |
---------------------------------------
----------------------------------------
| reward                  | 0.355      |
| reward_contact          | 0.0247     |
| reward_ctrl             | 0.0397     |
| reward_motion           | 0          |
| reward_orientation      | 0.0444     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0794     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 55         |
|    iterations           | 146        |
|    time_elapsed         | 5405       |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.76000917 |
|    clip_fraction        | 0.782      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.64      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.054     |
|    std                  | 0.629      |
|    value_loss           | 1.24       |
----------------------------------------
Num timesteps: 300000
Best mean reward: 157.57 - Last mean reward per episode: 157.28
---------------------------------------
| reward                  | 0.353     |
| reward_contact          | 0.0247    |
| reward_ctrl             | 0.0389    |
| reward_motion           | 0         |
| reward_orientation      | 0.0442    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0789    |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 495       |
|    ep_rew_mean          | 158       |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 147       |
|    time_elapsed         | 5459      |
|    total_timesteps      | 301056    |
| train/                  |           |
|    approx_kl            | 0.3739534 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.62     |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.341     |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0446   |
|    std                  | 0.627     |
|    value_loss           | 0.925     |
---------------------------------------
----------------------------------------
| reward                  | 0.357      |
| reward_contact          | 0.0265     |
| reward_ctrl             | 0.0395     |
| reward_motion           | 0          |
| reward_orientation      | 0.0438     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0806     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | 158        |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 148        |
|    time_elapsed         | 5512       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.34522843 |
|    clip_fraction        | 0.727      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.239      |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.626      |
|    value_loss           | 3.67       |
----------------------------------------
----------------------------------------
| reward                  | 0.357      |
| reward_contact          | 0.0264     |
| reward_ctrl             | 0.0398     |
| reward_motion           | 0          |
| reward_orientation      | 0.0443     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0845     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.116      |
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | 159        |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 149        |
|    time_elapsed         | 5566       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.71068037 |
|    clip_fraction        | 0.795      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0608    |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0549    |
|    std                  | 0.625      |
|    value_loss           | 0.628      |
----------------------------------------
Num timesteps: 306000
Best mean reward: 157.57 - Last mean reward per episode: 159.48
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.366      |
| reward_contact          | 0.0264     |
| reward_ctrl             | 0.0412     |
| reward_motion           | 0          |
| reward_orientation      | 0.0448     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0893     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.118      |
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 150        |
|    time_elapsed         | 5619       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.32516694 |
|    clip_fraction        | 0.745      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0413    |
|    std                  | 0.626      |
|    value_loss           | 1.51       |
----------------------------------------
----------------------------------------
| reward                  | 0.364      |
| reward_contact          | 0.027      |
| reward_ctrl             | 0.0407     |
| reward_motion           | 0          |
| reward_orientation      | 0.0446     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0889     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.117      |
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | 162        |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 151        |
|    time_elapsed         | 5672       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.77842456 |
|    clip_fraction        | 0.786      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.56      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0612     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0667    |
|    std                  | 0.62       |
|    value_loss           | 0.527      |
----------------------------------------
--------------------------------------
| reward                  | 0.363    |
| reward_contact          | 0.027    |
| reward_ctrl             | 0.0401   |
| reward_motion           | 0        |
| reward_orientation      | 0.0447   |
| reward_position         | 6.68e-36 |
| reward_rotation         | 0.0887   |
| reward_torque           | 0.0455   |
| reward_velocity         | 0.117    |
| rollout/                |          |
|    ep_len_mean          | 496      |
|    ep_rew_mean          | 162      |
| time/                   |          |
|    fps                  | 54       |
|    iterations           | 152      |
|    time_elapsed         | 5726     |
|    total_timesteps      | 311296   |
| train/                  |          |
|    approx_kl            | 0.920592 |
|    clip_fraction        | 0.811    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.5     |
|    explained_variance   | 0.942    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0268  |
|    n_updates            | 1510     |
|    policy_gradient_loss | -0.0545  |
|    std                  | 0.618    |
|    value_loss           | 0.364    |
--------------------------------------
Num timesteps: 312000
Best mean reward: 159.48 - Last mean reward per episode: 162.63
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.359     |
| reward_contact          | 0.0265    |
| reward_ctrl             | 0.0406    |
| reward_motion           | 0         |
| reward_orientation      | 0.0448    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0871    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.114     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 54        |
|    iterations           | 153       |
|    time_elapsed         | 5788      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.6139384 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.49     |
|    explained_variance   | 0.9       |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00821  |
|    n_updates            | 1520      |
|    policy_gradient_loss | -0.0483   |
|    std                  | 0.617     |
|    value_loss           | 1.15      |
---------------------------------------
--------------------------------------
| reward                  | 0.355    |
| reward_contact          | 0.0268   |
| reward_ctrl             | 0.0402   |
| reward_motion           | 0        |
| reward_orientation      | 0.0448   |
| reward_position         | 6.68e-36 |
| reward_rotation         | 0.0855   |
| reward_torque           | 0.0456   |
| reward_velocity         | 0.112    |
| rollout/                |          |
|    ep_len_mean          | 496      |
|    ep_rew_mean          | 163      |
| time/                   |          |
|    fps                  | 53       |
|    iterations           | 154      |
|    time_elapsed         | 5855     |
|    total_timesteps      | 315392   |
| train/                  |          |
|    approx_kl            | 0.548831 |
|    clip_fraction        | 0.766    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.46    |
|    explained_variance   | 0.87     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.142    |
|    n_updates            | 1530     |
|    policy_gradient_loss | -0.0562  |
|    std                  | 0.615    |
|    value_loss           | 1.11     |
--------------------------------------
----------------------------------------
| reward                  | 0.357      |
| reward_contact          | 0.0274     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0          |
| reward_orientation      | 0.0449     |
| reward_position         | 6.68e-36   |
| reward_rotation         | 0.0827     |
| reward_torque           | 0.0461     |
| reward_velocity         | 0.114      |
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 53         |
|    iterations           | 155        |
|    time_elapsed         | 5915       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.81767327 |
|    clip_fraction        | 0.805      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.46      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0578    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.616      |
|    value_loss           | 0.581      |
----------------------------------------
Num timesteps: 318000
Best mean reward: 162.63 - Last mean reward per episode: 163.33
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0274    |
| reward_ctrl             | 0.0415    |
| reward_motion           | 0         |
| reward_orientation      | 0.0446    |
| reward_position         | 6.68e-36  |
| reward_rotation         | 0.0821    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 53        |
|    iterations           | 156       |
|    time_elapsed         | 5970      |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.6859976 |
|    clip_fraction        | 0.782     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.46     |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0387   |
|    n_updates            | 1550      |
|    policy_gradient_loss | -0.0574   |
|    std                  | 0.616     |
|    value_loss           | 0.637     |
---------------------------------------
---------------------------------------
| reward                  | 0.353     |
| reward_contact          | 0.0257    |
| reward_ctrl             | 0.0428    |
| reward_motion           | 0         |
| reward_orientation      | 0.0449    |
| reward_position         | 8.62e-36  |
| reward_rotation         | 0.0814    |
| reward_torque           | 0.0464    |
| reward_velocity         | 0.112     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 53        |
|    iterations           | 157       |
|    time_elapsed         | 6022      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.8257688 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.47     |
|    explained_variance   | 0.903     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0601    |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.0513   |
|    std                  | 0.616     |
|    value_loss           | 0.592     |
---------------------------------------
----------------------------------------
| reward                  | 0.351      |
| reward_contact          | 0.0245     |
| reward_ctrl             | 0.0434     |
| reward_motion           | 0          |
| reward_orientation      | 0.0452     |
| reward_position         | 8.62e-36   |
| reward_rotation         | 0.0797     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.112      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 53         |
|    iterations           | 158        |
|    time_elapsed         | 6075       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.58854586 |
|    clip_fraction        | 0.781      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.45      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0245    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0614    |
|    std                  | 0.612      |
|    value_loss           | 0.515      |
----------------------------------------
Num timesteps: 324000
Best mean reward: 163.33 - Last mean reward per episode: 164.30
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.348     |
| reward_contact          | 0.0244    |
| reward_ctrl             | 0.0418    |
| reward_motion           | 0         |
| reward_orientation      | 0.0449    |
| reward_position         | 8.62e-36  |
| reward_rotation         | 0.0782    |
| reward_torque           | 0.0461    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 53        |
|    iterations           | 159       |
|    time_elapsed         | 6128      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 1.2015607 |
|    clip_fraction        | 0.797     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.43     |
|    explained_variance   | 0.89      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0348    |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0671   |
|    std                  | 0.614     |
|    value_loss           | 0.527     |
---------------------------------------
---------------------------------------
| reward                  | 0.347     |
| reward_contact          | 0.0244    |
| reward_ctrl             | 0.0404    |
| reward_motion           | 0         |
| reward_orientation      | 0.0453    |
| reward_position         | 8.62e-36  |
| reward_rotation         | 0.0763    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.115     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 53        |
|    iterations           | 160       |
|    time_elapsed         | 6180      |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.3913614 |
|    clip_fraction        | 0.734     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.44     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.157     |
|    n_updates            | 1590      |
|    policy_gradient_loss | -0.0558   |
|    std                  | 0.613     |
|    value_loss           | 1.75      |
---------------------------------------
---------------------------------------
| reward                  | 0.352     |
| reward_contact          | 0.025     |
| reward_ctrl             | 0.0412    |
| reward_motion           | 0         |
| reward_orientation      | 0.0457    |
| reward_position         | 8.62e-36  |
| reward_rotation         | 0.076     |
| reward_torque           | 0.0463    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 52        |
|    iterations           | 161       |
|    time_elapsed         | 6233      |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.9099343 |
|    clip_fraction        | 0.799     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.44     |
|    explained_variance   | 0.945     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0961   |
|    n_updates            | 1600      |
|    policy_gradient_loss | -0.0537   |
|    std                  | 0.615     |
|    value_loss           | 0.318     |
---------------------------------------
Num timesteps: 330000
Best mean reward: 164.30 - Last mean reward per episode: 163.09
----------------------------------------
| reward                  | 0.35       |
| reward_contact          | 0.0254     |
| reward_ctrl             | 0.0416     |
| reward_motion           | 0          |
| reward_orientation      | 0.0456     |
| reward_position         | 8.62e-36   |
| reward_rotation         | 0.0751     |
| reward_torque           | 0.0464     |
| reward_velocity         | 0.116      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 162        |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 162        |
|    time_elapsed         | 6284       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.55826014 |
|    clip_fraction        | 0.801      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.45      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.68       |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0296    |
|    std                  | 0.614      |
|    value_loss           | 1.14       |
----------------------------------------
--------------------------------------
| reward                  | 0.341    |
| reward_contact          | 0.0241   |
| reward_ctrl             | 0.0393   |
| reward_motion           | 0        |
| reward_orientation      | 0.0459   |
| reward_position         | 1.95e-36 |
| reward_rotation         | 0.0743   |
| reward_torque           | 0.046    |
| reward_velocity         | 0.112    |
| rollout/                |          |
|    ep_len_mean          | 497      |
|    ep_rew_mean          | 162      |
| time/                   |          |
|    fps                  | 52       |
|    iterations           | 163      |
|    time_elapsed         | 6337     |
|    total_timesteps      | 333824   |
| train/                  |          |
|    approx_kl            | 0.668139 |
|    clip_fraction        | 0.768    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.43    |
|    explained_variance   | 0.891    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0331  |
|    n_updates            | 1620     |
|    policy_gradient_loss | -0.0611  |
|    std                  | 0.612    |
|    value_loss           | 0.511    |
--------------------------------------
---------------------------------------
| reward                  | 0.341     |
| reward_contact          | 0.0256    |
| reward_ctrl             | 0.0369    |
| reward_motion           | 0         |
| reward_orientation      | 0.0468    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0753    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.111     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 52        |
|    iterations           | 164       |
|    time_elapsed         | 6390      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.8843018 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.42     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0907   |
|    n_updates            | 1630      |
|    policy_gradient_loss | -0.0671   |
|    std                  | 0.612     |
|    value_loss           | 0.563     |
---------------------------------------
Num timesteps: 336000
Best mean reward: 164.30 - Last mean reward per episode: 161.55
----------------------------------------
| reward                  | 0.344      |
| reward_contact          | 0.0244     |
| reward_ctrl             | 0.0374     |
| reward_motion           | 0          |
| reward_orientation      | 0.047      |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.075      |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 165        |
|    time_elapsed         | 6442       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.66664195 |
|    clip_fraction        | 0.788      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.4       |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.101     |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.61       |
|    value_loss           | 0.428      |
----------------------------------------
----------------------------------------
| reward                  | 0.347      |
| reward_contact          | 0.0232     |
| reward_ctrl             | 0.0391     |
| reward_motion           | 0          |
| reward_orientation      | 0.0466     |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0746     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.117      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 160        |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 166        |
|    time_elapsed         | 6494       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.73318243 |
|    clip_fraction        | 0.794      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.37      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0683     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0539    |
|    std                  | 0.608      |
|    value_loss           | 0.709      |
----------------------------------------
Num timesteps: 342000
Best mean reward: 164.30 - Last mean reward per episode: 160.43
----------------------------------------
| reward                  | 0.346      |
| reward_contact          | 0.0224     |
| reward_ctrl             | 0.039      |
| reward_motion           | 0          |
| reward_orientation      | 0.0473     |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0715     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.12       |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 160        |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 167        |
|    time_elapsed         | 6546       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.63380116 |
|    clip_fraction        | 0.788      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.34      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.103     |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0665    |
|    std                  | 0.606      |
|    value_loss           | 0.517      |
----------------------------------------
----------------------------------------
| reward                  | 0.346      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0394     |
| reward_motion           | 0          |
| reward_orientation      | 0.0476     |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0721     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.118      |
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 159        |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 168        |
|    time_elapsed         | 6599       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.67774314 |
|    clip_fraction        | 0.789      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.32      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0654    |
|    std                  | 0.604      |
|    value_loss           | 0.99       |
----------------------------------------
---------------------------------------
| reward                  | 0.352     |
| reward_contact          | 0.0231    |
| reward_ctrl             | 0.0398    |
| reward_motion           | 0         |
| reward_orientation      | 0.0475    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0751    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 52        |
|    iterations           | 169       |
|    time_elapsed         | 6652      |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.6884912 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.3      |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0015    |
|    n_updates            | 1680      |
|    policy_gradient_loss | -0.0491   |
|    std                  | 0.603     |
|    value_loss           | 0.882     |
---------------------------------------
Num timesteps: 348000
Best mean reward: 164.30 - Last mean reward per episode: 161.01
----------------------------------------
| reward                  | 0.358      |
| reward_contact          | 0.0243     |
| reward_ctrl             | 0.0395     |
| reward_motion           | 0          |
| reward_orientation      | 0.0471     |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0786     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.123      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 51         |
|    iterations           | 170        |
|    time_elapsed         | 6704       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.96044475 |
|    clip_fraction        | 0.795      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.29      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0356    |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0604    |
|    std                  | 0.603      |
|    value_loss           | 0.759      |
----------------------------------------
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0237    |
| reward_ctrl             | 0.0385    |
| reward_motion           | 0         |
| reward_orientation      | 0.0473    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.078     |
| reward_torque           | 0.0458    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 171       |
|    time_elapsed         | 6763      |
|    total_timesteps      | 350208    |
| train/                  |           |
|    approx_kl            | 0.8592705 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.29     |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0919    |
|    n_updates            | 1700      |
|    policy_gradient_loss | -0.0563   |
|    std                  | 0.603     |
|    value_loss           | 0.673     |
---------------------------------------
---------------------------------------
| reward                  | 0.352     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.039     |
| reward_motion           | 0         |
| reward_orientation      | 0.0478    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0778    |
| reward_torque           | 0.0458    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 172       |
|    time_elapsed         | 6818      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.7344313 |
|    clip_fraction        | 0.781     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.27     |
|    explained_variance   | 0.817     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0362   |
|    n_updates            | 1710      |
|    policy_gradient_loss | -0.0606   |
|    std                  | 0.6       |
|    value_loss           | 1.12      |
---------------------------------------
Num timesteps: 354000
Best mean reward: 164.30 - Last mean reward per episode: 160.40
---------------------------------------
| reward                  | 0.35      |
| reward_contact          | 0.0219    |
| reward_ctrl             | 0.0381    |
| reward_motion           | 0         |
| reward_orientation      | 0.0479    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0757    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 173       |
|    time_elapsed         | 6870      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 1.0552285 |
|    clip_fraction        | 0.808     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.25     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0582   |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.0593   |
|    std                  | 0.599     |
|    value_loss           | 0.328     |
---------------------------------------
---------------------------------------
| reward                  | 0.343     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0383    |
| reward_motion           | 0         |
| reward_orientation      | 0.0471    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.072     |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 159       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 174       |
|    time_elapsed         | 6922      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.6101898 |
|    clip_fraction        | 0.791     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.23     |
|    explained_variance   | 0.931     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.119     |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.0544   |
|    std                  | 0.598     |
|    value_loss           | 0.559     |
---------------------------------------
---------------------------------------
| reward                  | 0.345     |
| reward_contact          | 0.0231    |
| reward_ctrl             | 0.0366    |
| reward_motion           | 0         |
| reward_orientation      | 0.0474    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0712    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 175       |
|    time_elapsed         | 6975      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.6057915 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.22     |
|    explained_variance   | 0.909     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0311   |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0731   |
|    std                  | 0.597     |
|    value_loss           | 0.553     |
---------------------------------------
Num timesteps: 360000
Best mean reward: 164.30 - Last mean reward per episode: 159.50
---------------------------------------
| reward                  | 0.346     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0361    |
| reward_motion           | 0         |
| reward_orientation      | 0.0479    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0698    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.125     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 176       |
|    time_elapsed         | 7028      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.7025964 |
|    clip_fraction        | 0.785     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.21     |
|    explained_variance   | 0.891     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0186    |
|    n_updates            | 1750      |
|    policy_gradient_loss | -0.0522   |
|    std                  | 0.596     |
|    value_loss           | 0.803     |
---------------------------------------
----------------------------------------
| reward                  | 0.35       |
| reward_contact          | 0.0231     |
| reward_ctrl             | 0.0368     |
| reward_motion           | 0          |
| reward_orientation      | 0.0479     |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0706     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.126      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 160        |
| time/                   |            |
|    fps                  | 51         |
|    iterations           | 177        |
|    time_elapsed         | 7080       |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.98970056 |
|    clip_fraction        | 0.804      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.2       |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0586    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.052     |
|    std                  | 0.596      |
|    value_loss           | 0.447      |
----------------------------------------
---------------------------------------
| reward                  | 0.35      |
| reward_contact          | 0.0237    |
| reward_ctrl             | 0.037     |
| reward_motion           | 0         |
| reward_orientation      | 0.0477    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0728    |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.123     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 51        |
|    iterations           | 178       |
|    time_elapsed         | 7132      |
|    total_timesteps      | 364544    |
| train/                  |           |
|    approx_kl            | 0.8197305 |
|    clip_fraction        | 0.816     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.18     |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0209   |
|    n_updates            | 1770      |
|    policy_gradient_loss | -0.0526   |
|    std                  | 0.594     |
|    value_loss           | 0.676     |
---------------------------------------
Num timesteps: 366000
Best mean reward: 164.30 - Last mean reward per episode: 160.88
----------------------------------------
| reward                  | 0.351      |
| reward_contact          | 0.0237     |
| reward_ctrl             | 0.0359     |
| reward_motion           | 0          |
| reward_orientation      | 0.048      |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0747     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 51         |
|    iterations           | 179        |
|    time_elapsed         | 7183       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.59339124 |
|    clip_fraction        | 0.779      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.16      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0695    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.592      |
|    value_loss           | 0.747      |
----------------------------------------
---------------------------------------
| reward                  | 0.351     |
| reward_contact          | 0.0237    |
| reward_ctrl             | 0.0362    |
| reward_motion           | 0         |
| reward_orientation      | 0.0484    |
| reward_position         | 1.94e-36  |
| reward_rotation         | 0.0759    |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 180       |
|    time_elapsed         | 7235      |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.8013679 |
|    clip_fraction        | 0.788     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.15     |
|    explained_variance   | 0.888     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.155     |
|    n_updates            | 1790      |
|    policy_gradient_loss | -0.05     |
|    std                  | 0.592     |
|    value_loss           | 1.41      |
---------------------------------------
----------------------------------------
| reward                  | 0.352      |
| reward_contact          | 0.0243     |
| reward_ctrl             | 0.0359     |
| reward_motion           | 0          |
| reward_orientation      | 0.049      |
| reward_position         | 1.94e-36   |
| reward_rotation         | 0.0802     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.118      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 160        |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 181        |
|    time_elapsed         | 7288       |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.86163044 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.14      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.218      |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0563    |
|    std                  | 0.591      |
|    value_loss           | 1.06       |
----------------------------------------
Num timesteps: 372000
Best mean reward: 164.30 - Last mean reward per episode: 159.70
---------------------------------------
| reward                  | 0.353     |
| reward_contact          | 0.0249    |
| reward_ctrl             | 0.0348    |
| reward_motion           | 0         |
| reward_orientation      | 0.0493    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.081     |
| reward_torque           | 0.0449    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 182       |
|    time_elapsed         | 7339      |
|    total_timesteps      | 372736    |
| train/                  |           |
|    approx_kl            | 0.9307882 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.12     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0664   |
|    n_updates            | 1810      |
|    policy_gradient_loss | -0.0632   |
|    std                  | 0.588     |
|    value_loss           | 0.356     |
---------------------------------------
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0249    |
| reward_ctrl             | 0.0368    |
| reward_motion           | 0         |
| reward_orientation      | 0.0494    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0813    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 183       |
|    time_elapsed         | 7391      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.6988598 |
|    clip_fraction        | 0.792     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.09     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0935    |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.0561   |
|    std                  | 0.586     |
|    value_loss           | 0.519     |
---------------------------------------
----------------------------------------
| reward                  | 0.354      |
| reward_contact          | 0.0255     |
| reward_ctrl             | 0.0368     |
| reward_motion           | 0          |
| reward_orientation      | 0.0495     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.084      |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 160        |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 184        |
|    time_elapsed         | 7443       |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.90484244 |
|    clip_fraction        | 0.808      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.06      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.299      |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0394    |
|    std                  | 0.585      |
|    value_loss           | 0.979      |
----------------------------------------
Num timesteps: 378000
Best mean reward: 164.30 - Last mean reward per episode: 160.82
---------------------------------------
| reward                  | 0.356     |
| reward_contact          | 0.0261    |
| reward_ctrl             | 0.0368    |
| reward_motion           | 0         |
| reward_orientation      | 0.0494    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0853    |
| reward_torque           | 0.0452    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 185       |
|    time_elapsed         | 7497      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.9518174 |
|    clip_fraction        | 0.799     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.04     |
|    explained_variance   | 0.915     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.075    |
|    n_updates            | 1840      |
|    policy_gradient_loss | -0.059    |
|    std                  | 0.583     |
|    value_loss           | 0.459     |
---------------------------------------
--------------------------------------
| reward                  | 0.355    |
| reward_contact          | 0.0243   |
| reward_ctrl             | 0.0374   |
| reward_motion           | 0        |
| reward_orientation      | 0.0494   |
| reward_position         | 4.16e-46 |
| reward_rotation         | 0.0855   |
| reward_torque           | 0.0453   |
| reward_velocity         | 0.113    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 161      |
| time/                   |          |
|    fps                  | 50       |
|    iterations           | 186      |
|    time_elapsed         | 7549     |
|    total_timesteps      | 380928   |
| train/                  |          |
|    approx_kl            | 1.178536 |
|    clip_fraction        | 0.816    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.02    |
|    explained_variance   | 0.937    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0444  |
|    n_updates            | 1850     |
|    policy_gradient_loss | -0.0598  |
|    std                  | 0.582    |
|    value_loss           | 0.452    |
--------------------------------------
----------------------------------------
| reward                  | 0.358      |
| reward_contact          | 0.0266     |
| reward_ctrl             | 0.0361     |
| reward_motion           | 0          |
| reward_orientation      | 0.0496     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.0877     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.113      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 161        |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 187        |
|    time_elapsed         | 7602       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.90282357 |
|    clip_fraction        | 0.82       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.99      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.065     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.58       |
|    value_loss           | 0.705      |
----------------------------------------
Num timesteps: 384000
Best mean reward: 164.30 - Last mean reward per episode: 161.79
---------------------------------------
| reward                  | 0.359     |
| reward_contact          | 0.0266    |
| reward_ctrl             | 0.0379    |
| reward_motion           | 0         |
| reward_orientation      | 0.0497    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0847    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.115     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 188       |
|    time_elapsed         | 7654      |
|    total_timesteps      | 385024    |
| train/                  |           |
|    approx_kl            | 0.9372035 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.96     |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0039   |
|    n_updates            | 1870      |
|    policy_gradient_loss | -0.0474   |
|    std                  | 0.578     |
|    value_loss           | 0.703     |
---------------------------------------
----------------------------------------
| reward                  | 0.355      |
| reward_contact          | 0.0272     |
| reward_ctrl             | 0.0397     |
| reward_motion           | 0          |
| reward_orientation      | 0.0494     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.0832     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 162        |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 189        |
|    time_elapsed         | 7707       |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.69687814 |
|    clip_fraction        | 0.796      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.93      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0277     |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0402    |
|    std                  | 0.576      |
|    value_loss           | 1.07       |
----------------------------------------
---------------------------------------
| reward                  | 0.351     |
| reward_contact          | 0.0272    |
| reward_ctrl             | 0.0363    |
| reward_motion           | 0         |
| reward_orientation      | 0.049     |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0823    |
| reward_torque           | 0.0452    |
| reward_velocity         | 0.112     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 190       |
|    time_elapsed         | 7759      |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 1.4027393 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.92     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0597   |
|    n_updates            | 1890      |
|    policy_gradient_loss | -0.0409   |
|    std                  | 0.575     |
|    value_loss           | 0.601     |
---------------------------------------
Num timesteps: 390000
Best mean reward: 164.30 - Last mean reward per episode: 162.14
---------------------------------------
| reward                  | 0.352     |
| reward_contact          | 0.0278    |
| reward_ctrl             | 0.0368    |
| reward_motion           | 0         |
| reward_orientation      | 0.0491    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.083     |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.11      |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 191       |
|    time_elapsed         | 7812      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.7545305 |
|    clip_fraction        | 0.792     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.89     |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.041    |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.0617   |
|    std                  | 0.572     |
|    value_loss           | 0.662     |
---------------------------------------
---------------------------------------
| reward                  | 0.348     |
| reward_contact          | 0.029     |
| reward_ctrl             | 0.0371    |
| reward_motion           | 0         |
| reward_orientation      | 0.0487    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0787    |
| reward_torque           | 0.0454    |
| reward_velocity         | 0.109     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 192       |
|    time_elapsed         | 7865      |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.6339227 |
|    clip_fraction        | 0.772     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.85     |
|    explained_variance   | 0.836     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0302    |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.0437   |
|    std                  | 0.571     |
|    value_loss           | 2.14      |
---------------------------------------
----------------------------------------
| reward                  | 0.349      |
| reward_contact          | 0.0284     |
| reward_ctrl             | 0.0379     |
| reward_motion           | 0          |
| reward_orientation      | 0.0489     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.0791     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 49         |
|    iterations           | 193        |
|    time_elapsed         | 7917       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.78192616 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.85      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0517    |
|    std                  | 0.57       |
|    value_loss           | 1.35       |
----------------------------------------
Num timesteps: 396000
Best mean reward: 164.30 - Last mean reward per episode: 162.56
---------------------------------------
| reward                  | 0.346     |
| reward_contact          | 0.0278    |
| reward_ctrl             | 0.0379    |
| reward_motion           | 0         |
| reward_orientation      | 0.0496    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0772    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.108     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 194       |
|    time_elapsed         | 7970      |
|    total_timesteps      | 397312    |
| train/                  |           |
|    approx_kl            | 0.9802203 |
|    clip_fraction        | 0.811     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.83     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00325  |
|    n_updates            | 1930      |
|    policy_gradient_loss | -0.0419   |
|    std                  | 0.569     |
|    value_loss           | 0.642     |
---------------------------------------
----------------------------------------
| reward                  | 0.348      |
| reward_contact          | 0.029      |
| reward_ctrl             | 0.0393     |
| reward_motion           | 0          |
| reward_orientation      | 0.0496     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.0749     |
| reward_torque           | 0.0459     |
| reward_velocity         | 0.109      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 49         |
|    iterations           | 195        |
|    time_elapsed         | 8023       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.96499395 |
|    clip_fraction        | 0.815      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.82      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0254    |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0476    |
|    std                  | 0.567      |
|    value_loss           | 0.456      |
----------------------------------------
---------------------------------------
| reward                  | 0.347     |
| reward_contact          | 0.0296    |
| reward_ctrl             | 0.0396    |
| reward_motion           | 0         |
| reward_orientation      | 0.0495    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0758    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.107     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 196       |
|    time_elapsed         | 8076      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 1.2538772 |
|    clip_fraction        | 0.837     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.78     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0974    |
|    n_updates            | 1950      |
|    policy_gradient_loss | -0.0716   |
|    std                  | 0.564     |
|    value_loss           | 0.676     |
---------------------------------------
Num timesteps: 402000
Best mean reward: 164.30 - Last mean reward per episode: 162.11
---------------------------------------
| reward                  | 0.344     |
| reward_contact          | 0.029     |
| reward_ctrl             | 0.04      |
| reward_motion           | 0         |
| reward_orientation      | 0.0497    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0746    |
| reward_torque           | 0.046     |
| reward_velocity         | 0.104     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 197       |
|    time_elapsed         | 8129      |
|    total_timesteps      | 403456    |
| train/                  |           |
|    approx_kl            | 1.1266762 |
|    clip_fraction        | 0.821     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.76     |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0284   |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0622   |
|    std                  | 0.564     |
|    value_loss           | 0.447     |
---------------------------------------
----------------------------------------
| reward                  | 0.343      |
| reward_contact          | 0.029      |
| reward_ctrl             | 0.0402     |
| reward_motion           | 0          |
| reward_orientation      | 0.0495     |
| reward_position         | 4.16e-46   |
| reward_rotation         | 0.0774     |
| reward_torque           | 0.0461     |
| reward_velocity         | 0.101      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 162        |
| time/                   |            |
|    fps                  | 49         |
|    iterations           | 198        |
|    time_elapsed         | 8182       |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.83046365 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.73      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0491    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0488    |
|    std                  | 0.561      |
|    value_loss           | 0.502      |
----------------------------------------
---------------------------------------
| reward                  | 0.346     |
| reward_contact          | 0.0278    |
| reward_ctrl             | 0.0399    |
| reward_motion           | 0         |
| reward_orientation      | 0.0497    |
| reward_position         | 4.16e-46  |
| reward_rotation         | 0.0785    |
| reward_torque           | 0.0461    |
| reward_velocity         | 0.104     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 199       |
|    time_elapsed         | 8236      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 1.0139807 |
|    clip_fraction        | 0.807     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.69     |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0361   |
|    n_updates            | 1980      |
|    policy_gradient_loss | -0.0528   |
|    std                  | 0.558     |
|    value_loss           | 0.423     |
---------------------------------------
Num timesteps: 408000
Best mean reward: 164.30 - Last mean reward per episode: 163.56
---------------------------------------
| reward                  | 0.348     |
| reward_contact          | 0.0278    |
| reward_ctrl             | 0.0406    |
| reward_motion           | 0         |
| reward_orientation      | 0.0498    |
| reward_position         | 4.59e-46  |
| reward_rotation         | 0.08      |
| reward_torque           | 0.0463    |
| reward_velocity         | 0.104     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 200       |
|    time_elapsed         | 8288      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.8125481 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.67     |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00157   |
|    n_updates            | 1990      |
|    policy_gradient_loss | -0.0469   |
|    std                  | 0.557     |
|    value_loss           | 0.9       |
---------------------------------------
---------------------------------------
| reward                  | 0.35      |
| reward_contact          | 0.0272    |
| reward_ctrl             | 0.0412    |
| reward_motion           | 0         |
| reward_orientation      | 0.05      |
| reward_position         | 4.59e-46  |
| reward_rotation         | 0.0813    |
| reward_torque           | 0.0465    |
| reward_velocity         | 0.103     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 201       |
|    time_elapsed         | 8340      |
|    total_timesteps      | 411648    |
| train/                  |           |
|    approx_kl            | 0.9102854 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.65     |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0786   |
|    n_updates            | 2000      |
|    policy_gradient_loss | -0.0742   |
|    std                  | 0.555     |
|    value_loss           | 0.609     |
---------------------------------------
---------------------------------------
| reward                  | 0.352     |
| reward_contact          | 0.0272    |
| reward_ctrl             | 0.0412    |
| reward_motion           | 0         |
| reward_orientation      | 0.0504    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0813    |
| reward_torque           | 0.0465    |
| reward_velocity         | 0.105     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 202       |
|    time_elapsed         | 8393      |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 1.2210457 |
|    clip_fraction        | 0.822     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.62     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.117    |
|    n_updates            | 2010      |
|    policy_gradient_loss | -0.0747   |
|    std                  | 0.553     |
|    value_loss           | 0.338     |
---------------------------------------
Num timesteps: 414000
Best mean reward: 164.30 - Last mean reward per episode: 162.97
---------------------------------------
| reward                  | 0.357     |
| reward_contact          | 0.0268    |
| reward_ctrl             | 0.0416    |
| reward_motion           | 0         |
| reward_orientation      | 0.0496    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0844    |
| reward_torque           | 0.0467    |
| reward_velocity         | 0.108     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 203       |
|    time_elapsed         | 8445      |
|    total_timesteps      | 415744    |
| train/                  |           |
|    approx_kl            | 1.1164527 |
|    clip_fraction        | 0.814     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.59     |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.009    |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0542   |
|    std                  | 0.552     |
|    value_loss           | 1.12      |
---------------------------------------
--------------------------------------
| reward                  | 0.359    |
| reward_contact          | 0.0262   |
| reward_ctrl             | 0.0416   |
| reward_motion           | 0        |
| reward_orientation      | 0.0492   |
| reward_position         | 4.28e-47 |
| reward_rotation         | 0.0828   |
| reward_torque           | 0.0466   |
| reward_velocity         | 0.113    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 164      |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 204      |
|    time_elapsed         | 8498     |
|    total_timesteps      | 417792   |
| train/                  |          |
|    approx_kl            | 1.129552 |
|    clip_fraction        | 0.815    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.57    |
|    explained_variance   | 0.891    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0547  |
|    n_updates            | 2030     |
|    policy_gradient_loss | -0.0623  |
|    std                  | 0.551    |
|    value_loss           | 0.962    |
--------------------------------------
---------------------------------------
| reward                  | 0.358     |
| reward_contact          | 0.0256    |
| reward_ctrl             | 0.0418    |
| reward_motion           | 0         |
| reward_orientation      | 0.0489    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0819    |
| reward_torque           | 0.0465    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 205       |
|    time_elapsed         | 8550      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 1.0965601 |
|    clip_fraction        | 0.806     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.58     |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.000437 |
|    n_updates            | 2040      |
|    policy_gradient_loss | -0.0597   |
|    std                  | 0.552     |
|    value_loss           | 0.926     |
---------------------------------------
Num timesteps: 420000
Best mean reward: 164.30 - Last mean reward per episode: 164.11
----------------------------------------
| reward                  | 0.359      |
| reward_contact          | 0.025      |
| reward_ctrl             | 0.041      |
| reward_motion           | 0          |
| reward_orientation      | 0.0488     |
| reward_position         | 4.28e-47   |
| reward_rotation         | 0.0814     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.117      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 49         |
|    iterations           | 206        |
|    time_elapsed         | 8603       |
|    total_timesteps      | 421888     |
| train/                  |            |
|    approx_kl            | 0.75677645 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.57      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0251     |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0561    |
|    std                  | 0.55       |
|    value_loss           | 1.32       |
----------------------------------------
---------------------------------------
| reward                  | 0.357     |
| reward_contact          | 0.0244    |
| reward_ctrl             | 0.0407    |
| reward_motion           | 0         |
| reward_orientation      | 0.0485    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0817    |
| reward_torque           | 0.0464    |
| reward_velocity         | 0.115     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 207       |
|    time_elapsed         | 8656      |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 1.1961589 |
|    clip_fraction        | 0.826     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.55     |
|    explained_variance   | 0.905     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0123   |
|    n_updates            | 2060      |
|    policy_gradient_loss | -0.0553   |
|    std                  | 0.549     |
|    value_loss           | 0.423     |
---------------------------------------
---------------------------------------
| reward                  | 0.356     |
| reward_contact          | 0.0238    |
| reward_ctrl             | 0.0404    |
| reward_motion           | 0         |
| reward_orientation      | 0.0486    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0815    |
| reward_torque           | 0.0464    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 208       |
|    time_elapsed         | 8709      |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.9744648 |
|    clip_fraction        | 0.819     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.54     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0211   |
|    n_updates            | 2070      |
|    policy_gradient_loss | -0.0584   |
|    std                  | 0.549     |
|    value_loss           | 0.301     |
---------------------------------------
Num timesteps: 426000
Best mean reward: 164.30 - Last mean reward per episode: 164.46
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.359     |
| reward_contact          | 0.0243    |
| reward_ctrl             | 0.0417    |
| reward_motion           | 0         |
| reward_orientation      | 0.0489    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0806    |
| reward_torque           | 0.0466    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 209       |
|    time_elapsed         | 8762      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.7562277 |
|    clip_fraction        | 0.793     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.53     |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.234     |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.0404   |
|    std                  | 0.547     |
|    value_loss           | 1.34      |
---------------------------------------
---------------------------------------
| reward                  | 0.363     |
| reward_contact          | 0.0251    |
| reward_ctrl             | 0.0406    |
| reward_motion           | 0         |
| reward_orientation      | 0.0477    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0846    |
| reward_torque           | 0.0464    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 210       |
|    time_elapsed         | 8814      |
|    total_timesteps      | 430080    |
| train/                  |           |
|    approx_kl            | 0.9851403 |
|    clip_fraction        | 0.806     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.5      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0782    |
|    n_updates            | 2090      |
|    policy_gradient_loss | -0.0507   |
|    std                  | 0.545     |
|    value_loss           | 0.737     |
---------------------------------------
Num timesteps: 432000
Best mean reward: 164.46 - Last mean reward per episode: 165.20
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.361     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.042     |
| reward_motion           | 0         |
| reward_orientation      | 0.0479    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0832    |
| reward_torque           | 0.0467    |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 211       |
|    time_elapsed         | 8867      |
|    total_timesteps      | 432128    |
| train/                  |           |
|    approx_kl            | 1.0605912 |
|    clip_fraction        | 0.808     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.46     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.02     |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.0577   |
|    std                  | 0.542     |
|    value_loss           | 0.557     |
---------------------------------------
---------------------------------------
| reward                  | 0.36      |
| reward_contact          | 0.0228    |
| reward_ctrl             | 0.0408    |
| reward_motion           | 0         |
| reward_orientation      | 0.0476    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0839    |
| reward_torque           | 0.0464    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 164       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 212       |
|    time_elapsed         | 8920      |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 1.1274893 |
|    clip_fraction        | 0.824     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.43     |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0148   |
|    n_updates            | 2110      |
|    policy_gradient_loss | -0.0588   |
|    std                  | 0.541     |
|    value_loss           | 0.654     |
---------------------------------------
----------------------------------------
| reward                  | 0.367      |
| reward_contact          | 0.0222     |
| reward_ctrl             | 0.0414     |
| reward_motion           | 0          |
| reward_orientation      | 0.0478     |
| reward_position         | 4.28e-47   |
| reward_rotation         | 0.0859     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.123      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 48         |
|    iterations           | 213        |
|    time_elapsed         | 8972       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.97260517 |
|    clip_fraction        | 0.837      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.42      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0209    |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0359    |
|    std                  | 0.54       |
|    value_loss           | 0.614      |
----------------------------------------
Num timesteps: 438000
Best mean reward: 165.20 - Last mean reward per episode: 165.65
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.37      |
| reward_contact          | 0.0217    |
| reward_ctrl             | 0.0413    |
| reward_motion           | 0         |
| reward_orientation      | 0.0479    |
| reward_position         | 4.28e-47  |
| reward_rotation         | 0.0876    |
| reward_torque           | 0.0465    |
| reward_velocity         | 0.125     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 214       |
|    time_elapsed         | 9027      |
|    total_timesteps      | 438272    |
| train/                  |           |
|    approx_kl            | 1.0064325 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.41     |
|    explained_variance   | 0.809     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0682    |
|    n_updates            | 2130      |
|    policy_gradient_loss | -0.0681   |
|    std                  | 0.54      |
|    value_loss           | 1.61      |
---------------------------------------
----------------------------------------
| reward                  | 0.376      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0424     |
| reward_motion           | 0          |
| reward_orientation      | 0.0479     |
| reward_position         | 4.28e-47   |
| reward_rotation         | 0.0919     |
| reward_torque           | 0.0468     |
| reward_velocity         | 0.125      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 166        |
| time/                   |            |
|    fps                  | 48         |
|    iterations           | 215        |
|    time_elapsed         | 9079       |
|    total_timesteps      | 440320     |
| train/                  |            |
|    approx_kl            | 0.90021646 |
|    clip_fraction        | 0.796      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.39      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00497   |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0564    |
|    std                  | 0.537      |
|    value_loss           | 0.556      |
----------------------------------------
----------------------------------------
| reward                  | 0.378      |
| reward_contact          | 0.0211     |
| reward_ctrl             | 0.0438     |
| reward_motion           | 0          |
| reward_orientation      | 0.0474     |
| reward_position         | 4.28e-47   |
| reward_rotation         | 0.0944     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.124      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 165        |
| time/                   |            |
|    fps                  | 48         |
|    iterations           | 216        |
|    time_elapsed         | 9131       |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.79072803 |
|    clip_fraction        | 0.794      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.35      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.279      |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0581    |
|    std                  | 0.536      |
|    value_loss           | 1.52       |
----------------------------------------
Num timesteps: 444000
Best mean reward: 165.65 - Last mean reward per episode: 165.87
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.373     |
| reward_contact          | 0.0208    |
| reward_ctrl             | 0.043     |
| reward_motion           | 0         |
| reward_orientation      | 0.0474    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0945    |
| reward_torque           | 0.0467    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 217       |
|    time_elapsed         | 9184      |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 1.2628323 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0511   |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.0408   |
|    std                  | 0.536     |
|    value_loss           | 0.838     |
---------------------------------------
---------------------------------------
| reward                  | 0.377     |
| reward_contact          | 0.0208    |
| reward_ctrl             | 0.0443    |
| reward_motion           | 0         |
| reward_orientation      | 0.0479    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0952    |
| reward_torque           | 0.047     |
| reward_velocity         | 0.122     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 218       |
|    time_elapsed         | 9237      |
|    total_timesteps      | 446464    |
| train/                  |           |
|    approx_kl            | 0.8549657 |
|    clip_fraction        | 0.806     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.32     |
|    explained_variance   | 0.795     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0503    |
|    n_updates            | 2170      |
|    policy_gradient_loss | -0.0436   |
|    std                  | 0.533     |
|    value_loss           | 2.98      |
---------------------------------------
---------------------------------------
| reward                  | 0.379     |
| reward_contact          | 0.0203    |
| reward_ctrl             | 0.0442    |
| reward_motion           | 0         |
| reward_orientation      | 0.048     |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0957    |
| reward_torque           | 0.047     |
| reward_velocity         | 0.124     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 219       |
|    time_elapsed         | 9289      |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 1.2213972 |
|    clip_fraction        | 0.815     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.27     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0467   |
|    n_updates            | 2180      |
|    policy_gradient_loss | -0.0531   |
|    std                  | 0.53      |
|    value_loss           | 0.464     |
---------------------------------------
Num timesteps: 450000
Best mean reward: 165.87 - Last mean reward per episode: 166.71
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.382     |
| reward_contact          | 0.0197    |
| reward_ctrl             | 0.0444    |
| reward_motion           | 0         |
| reward_orientation      | 0.0478    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0981    |
| reward_torque           | 0.0471    |
| reward_velocity         | 0.125     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 220       |
|    time_elapsed         | 9343      |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 1.2493484 |
|    clip_fraction        | 0.843     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.24     |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.269     |
|    n_updates            | 2190      |
|    policy_gradient_loss | -0.0574   |
|    std                  | 0.528     |
|    value_loss           | 0.51      |
---------------------------------------
---------------------------------------
| reward                  | 0.381     |
| reward_contact          | 0.0193    |
| reward_ctrl             | 0.0432    |
| reward_motion           | 0         |
| reward_orientation      | 0.047     |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0942    |
| reward_torque           | 0.0469    |
| reward_velocity         | 0.131     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 221       |
|    time_elapsed         | 9396      |
|    total_timesteps      | 452608    |
| train/                  |           |
|    approx_kl            | 1.0180477 |
|    clip_fraction        | 0.824     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.22     |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0116   |
|    n_updates            | 2200      |
|    policy_gradient_loss | -0.0457   |
|    std                  | 0.527     |
|    value_loss           | 0.744     |
---------------------------------------
---------------------------------------
| reward                  | 0.384     |
| reward_contact          | 0.0203    |
| reward_ctrl             | 0.044     |
| reward_motion           | 0         |
| reward_orientation      | 0.0467    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0942    |
| reward_torque           | 0.0471    |
| reward_velocity         | 0.131     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 222       |
|    time_elapsed         | 9448      |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 1.3430686 |
|    clip_fraction        | 0.837     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.19     |
|    explained_variance   | 0.95      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0593   |
|    n_updates            | 2210      |
|    policy_gradient_loss | -0.0553   |
|    std                  | 0.525     |
|    value_loss           | 0.349     |
---------------------------------------
Num timesteps: 456000
Best mean reward: 166.71 - Last mean reward per episode: 167.46
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.386     |
| reward_contact          | 0.0215    |
| reward_ctrl             | 0.0456    |
| reward_motion           | 0         |
| reward_orientation      | 0.0465    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0952    |
| reward_torque           | 0.0474    |
| reward_velocity         | 0.129     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 48        |
|    iterations           | 223       |
|    time_elapsed         | 9500      |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 1.2049628 |
|    clip_fraction        | 0.836     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.18     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00508  |
|    n_updates            | 2220      |
|    policy_gradient_loss | -0.0568   |
|    std                  | 0.525     |
|    value_loss           | 0.473     |
---------------------------------------
--------------------------------------
| reward                  | 0.38     |
| reward_contact          | 0.0203   |
| reward_ctrl             | 0.0458   |
| reward_motion           | 0        |
| reward_orientation      | 0.0465   |
| reward_position         | 3.56e-41 |
| reward_rotation         | 0.0951   |
| reward_torque           | 0.0475   |
| reward_velocity         | 0.125    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 166      |
| time/                   |          |
|    fps                  | 48       |
|    iterations           | 224      |
|    time_elapsed         | 9553     |
|    total_timesteps      | 458752   |
| train/                  |          |
|    approx_kl            | 1.020261 |
|    clip_fraction        | 0.821    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.17    |
|    explained_variance   | 0.821    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0166   |
|    n_updates            | 2230     |
|    policy_gradient_loss | -0.0232  |
|    std                  | 0.524    |
|    value_loss           | 1.33     |
--------------------------------------
---------------------------------------
| reward                  | 0.379     |
| reward_contact          | 0.0197    |
| reward_ctrl             | 0.0459    |
| reward_motion           | 0         |
| reward_orientation      | 0.0455    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0943    |
| reward_torque           | 0.0475    |
| reward_velocity         | 0.126     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 225       |
|    time_elapsed         | 9606      |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.9892478 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.14     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0305   |
|    n_updates            | 2240      |
|    policy_gradient_loss | -0.037    |
|    std                  | 0.521     |
|    value_loss           | 0.627     |
---------------------------------------
Num timesteps: 462000
Best mean reward: 167.46 - Last mean reward per episode: 166.91
----------------------------------------
| reward                  | 0.38       |
| reward_contact          | 0.0192     |
| reward_ctrl             | 0.0466     |
| reward_motion           | 0          |
| reward_orientation      | 0.0449     |
| reward_position         | 3.56e-41   |
| reward_rotation         | 0.0967     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.125      |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 167        |
| time/                   |            |
|    fps                  | 47         |
|    iterations           | 226        |
|    time_elapsed         | 9659       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.90606207 |
|    clip_fraction        | 0.81       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.11      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0458    |
|    std                  | 0.521      |
|    value_loss           | 1.36       |
----------------------------------------
---------------------------------------
| reward                  | 0.375     |
| reward_contact          | 0.0187    |
| reward_ctrl             | 0.0464    |
| reward_motion           | 0         |
| reward_orientation      | 0.0449    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0934    |
| reward_torque           | 0.0475    |
| reward_velocity         | 0.124     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 227       |
|    time_elapsed         | 9712      |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 1.4166595 |
|    clip_fraction        | 0.832     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.12     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0646   |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.0576   |
|    std                  | 0.52      |
|    value_loss           | 0.44      |
---------------------------------------
---------------------------------------
| reward                  | 0.367     |
| reward_contact          | 0.0179    |
| reward_ctrl             | 0.0473    |
| reward_motion           | 0         |
| reward_orientation      | 0.0451    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0903    |
| reward_torque           | 0.0477    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 228       |
|    time_elapsed         | 9765      |
|    total_timesteps      | 466944    |
| train/                  |           |
|    approx_kl            | 1.3845475 |
|    clip_fraction        | 0.848     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.11     |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00131   |
|    n_updates            | 2270      |
|    policy_gradient_loss | -0.0529   |
|    std                  | 0.521     |
|    value_loss           | 0.424     |
---------------------------------------
Num timesteps: 468000
Best mean reward: 167.46 - Last mean reward per episode: 166.54
---------------------------------------
| reward                  | 0.37      |
| reward_contact          | 0.0191    |
| reward_ctrl             | 0.0478    |
| reward_motion           | 0         |
| reward_orientation      | 0.0451    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0912    |
| reward_torque           | 0.0481    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 229       |
|    time_elapsed         | 9816      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 1.9553158 |
|    clip_fraction        | 0.847     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.13     |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0308   |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.0524   |
|    std                  | 0.523     |
|    value_loss           | 0.497     |
---------------------------------------
---------------------------------------
| reward                  | 0.371     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.0469    |
| reward_motion           | 0         |
| reward_orientation      | 0.0458    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0925    |
| reward_torque           | 0.0479    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 230       |
|    time_elapsed         | 9870      |
|    total_timesteps      | 471040    |
| train/                  |           |
|    approx_kl            | 1.2218698 |
|    clip_fraction        | 0.816     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.14     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0144   |
|    n_updates            | 2290      |
|    policy_gradient_loss | -0.049    |
|    std                  | 0.522     |
|    value_loss           | 0.545     |
---------------------------------------
---------------------------------------
| reward                  | 0.378     |
| reward_contact          | 0.0185    |
| reward_ctrl             | 0.0484    |
| reward_motion           | 0         |
| reward_orientation      | 0.0458    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.096     |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 231       |
|    time_elapsed         | 9923      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 1.2602311 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.13     |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0877   |
|    n_updates            | 2300      |
|    policy_gradient_loss | -0.0642   |
|    std                  | 0.52      |
|    value_loss           | 0.298     |
---------------------------------------
Num timesteps: 474000
Best mean reward: 167.46 - Last mean reward per episode: 167.67
Saving new best model to rl/out_dir/models/exp68/best_model.zip
--------------------------------------
| reward                  | 0.382    |
| reward_contact          | 0.0185   |
| reward_ctrl             | 0.0473   |
| reward_motion           | 0        |
| reward_orientation      | 0.0462   |
| reward_position         | 3.56e-41 |
| reward_rotation         | 0.0981   |
| reward_torque           | 0.0481   |
| reward_velocity         | 0.124    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 167      |
| time/                   |          |
|    fps                  | 47       |
|    iterations           | 232      |
|    time_elapsed         | 9976     |
|    total_timesteps      | 475136   |
| train/                  |          |
|    approx_kl            | 0.985363 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.1     |
|    explained_variance   | 0.866    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.682    |
|    n_updates            | 2310     |
|    policy_gradient_loss | -0.0467  |
|    std                  | 0.52     |
|    value_loss           | 1.65     |
--------------------------------------
---------------------------------------
| reward                  | 0.377     |
| reward_contact          | 0.0179    |
| reward_ctrl             | 0.0494    |
| reward_motion           | 0         |
| reward_orientation      | 0.0455    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0957    |
| reward_torque           | 0.0484    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 233       |
|    time_elapsed         | 10030     |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 1.0552555 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.09     |
|    explained_variance   | 0.878     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.417     |
|    n_updates            | 2320      |
|    policy_gradient_loss | -0.0595   |
|    std                  | 0.518     |
|    value_loss           | 1.35      |
---------------------------------------
---------------------------------------
| reward                  | 0.369     |
| reward_contact          | 0.0172    |
| reward_ctrl             | 0.0487    |
| reward_motion           | 0         |
| reward_orientation      | 0.0457    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0926    |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 234       |
|    time_elapsed         | 10082     |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 1.4546833 |
|    clip_fraction        | 0.855     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.05     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0203   |
|    n_updates            | 2330      |
|    policy_gradient_loss | -0.0319   |
|    std                  | 0.516     |
|    value_loss           | 0.463     |
---------------------------------------
Num timesteps: 480000
Best mean reward: 167.67 - Last mean reward per episode: 167.03
--------------------------------------
| reward                  | 0.371    |
| reward_contact          | 0.0172   |
| reward_ctrl             | 0.0485   |
| reward_motion           | 0        |
| reward_orientation      | 0.0454   |
| reward_position         | 3.56e-41 |
| reward_rotation         | 0.0923   |
| reward_torque           | 0.0482   |
| reward_velocity         | 0.119    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 167      |
| time/                   |          |
|    fps                  | 47       |
|    iterations           | 235      |
|    time_elapsed         | 10134    |
|    total_timesteps      | 481280   |
| train/                  |          |
|    approx_kl            | 0.917264 |
|    clip_fraction        | 0.792    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.02    |
|    explained_variance   | 0.887    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.00191  |
|    n_updates            | 2340     |
|    policy_gradient_loss | -0.0512  |
|    std                  | 0.514    |
|    value_loss           | 0.937    |
--------------------------------------
---------------------------------------
| reward                  | 0.373     |
| reward_contact          | 0.0183    |
| reward_ctrl             | 0.0484    |
| reward_motion           | 0         |
| reward_orientation      | 0.0455    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0932    |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.119     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 168       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 236       |
|    time_elapsed         | 10186     |
|    total_timesteps      | 483328    |
| train/                  |           |
|    approx_kl            | 1.3617308 |
|    clip_fraction        | 0.844     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6        |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0465   |
|    n_updates            | 2350      |
|    policy_gradient_loss | -0.0511   |
|    std                  | 0.51      |
|    value_loss           | 0.49      |
---------------------------------------
---------------------------------------
| reward                  | 0.369     |
| reward_contact          | 0.0184    |
| reward_ctrl             | 0.0488    |
| reward_motion           | 0         |
| reward_orientation      | 0.0455    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0923    |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 237       |
|    time_elapsed         | 10239     |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 1.3741965 |
|    clip_fraction        | 0.834     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.95     |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0169   |
|    n_updates            | 2360      |
|    policy_gradient_loss | -0.0398   |
|    std                  | 0.509     |
|    value_loss           | 0.504     |
---------------------------------------
Num timesteps: 486000
Best mean reward: 167.67 - Last mean reward per episode: 166.86
---------------------------------------
| reward                  | 0.368     |
| reward_contact          | 0.0184    |
| reward_ctrl             | 0.0478    |
| reward_motion           | 0         |
| reward_orientation      | 0.045     |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0932    |
| reward_torque           | 0.0482    |
| reward_velocity         | 0.115     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 238       |
|    time_elapsed         | 10291     |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 1.4736633 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.91     |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0372   |
|    n_updates            | 2370      |
|    policy_gradient_loss | -0.0485   |
|    std                  | 0.507     |
|    value_loss           | 0.513     |
---------------------------------------
---------------------------------------
| reward                  | 0.358     |
| reward_contact          | 0.0179    |
| reward_ctrl             | 0.0476    |
| reward_motion           | 0         |
| reward_orientation      | 0.0444    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0868    |
| reward_torque           | 0.0481    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 239       |
|    time_elapsed         | 10343     |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 1.8180039 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.88     |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0416   |
|    n_updates            | 2380      |
|    policy_gradient_loss | -0.0486   |
|    std                  | 0.504     |
|    value_loss           | 0.354     |
---------------------------------------
---------------------------------------
| reward                  | 0.354     |
| reward_contact          | 0.0173    |
| reward_ctrl             | 0.047     |
| reward_motion           | 0         |
| reward_orientation      | 0.0444    |
| reward_position         | 3.56e-41  |
| reward_rotation         | 0.0857    |
| reward_torque           | 0.0479    |
| reward_velocity         | 0.112     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 240       |
|    time_elapsed         | 10395     |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 1.2944536 |
|    clip_fraction        | 0.822     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.84     |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0512   |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.0536   |
|    std                  | 0.503     |
|    value_loss           | 0.92      |
---------------------------------------
Num timesteps: 492000
Best mean reward: 167.67 - Last mean reward per episode: 166.39
---------------------------------------
| reward                  | 0.355     |
| reward_contact          | 0.0182    |
| reward_ctrl             | 0.0475    |
| reward_motion           | 0         |
| reward_orientation      | 0.0446    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.0835    |
| reward_torque           | 0.0481    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 241       |
|    time_elapsed         | 10447     |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 1.1610041 |
|    clip_fraction        | 0.832     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.84     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0855    |
|    n_updates            | 2400      |
|    policy_gradient_loss | -0.0439   |
|    std                  | 0.503     |
|    value_loss           | 1.04      |
---------------------------------------
---------------------------------------
| reward                  | 0.365     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0487    |
| reward_motion           | 0         |
| reward_orientation      | 0.0444    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.086     |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 242       |
|    time_elapsed         | 10500     |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 1.7462623 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.83     |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0433    |
|    n_updates            | 2410      |
|    policy_gradient_loss | -0.0476   |
|    std                  | 0.503     |
|    value_loss           | 0.527     |
---------------------------------------
---------------------------------------
| reward                  | 0.361     |
| reward_contact          | 0.0194    |
| reward_ctrl             | 0.0488    |
| reward_motion           | 0         |
| reward_orientation      | 0.0447    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.0829    |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 243       |
|    time_elapsed         | 10552     |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 1.0566416 |
|    clip_fraction        | 0.819     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.83     |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0896   |
|    n_updates            | 2420      |
|    policy_gradient_loss | -0.0534   |
|    std                  | 0.501     |
|    value_loss           | 0.717     |
---------------------------------------
Num timesteps: 498000
Best mean reward: 167.67 - Last mean reward per episode: 164.92
---------------------------------------
| reward                  | 0.357     |
| reward_contact          | 0.0192    |
| reward_ctrl             | 0.0476    |
| reward_motion           | 0         |
| reward_orientation      | 0.0447    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.0824    |
| reward_torque           | 0.048     |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 244       |
|    time_elapsed         | 10604     |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 1.2825376 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.79     |
|    explained_variance   | 0.89      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0543   |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.0494   |
|    std                  | 0.498     |
|    value_loss           | 0.513     |
---------------------------------------
---------------------------------------
| reward                  | 0.358     |
| reward_contact          | 0.0197    |
| reward_ctrl             | 0.0482    |
| reward_motion           | 0         |
| reward_orientation      | 0.0453    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.0834    |
| reward_torque           | 0.0481    |
| reward_velocity         | 0.114     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 47        |
|    iterations           | 245       |
|    time_elapsed         | 10658     |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 1.5678065 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.76     |
|    explained_variance   | 0.905     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0492   |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.0413   |
|    std                  | 0.497     |
|    value_loss           | 0.576     |
---------------------------------------
--------------------------------------
| reward                  | 0.359    |
| reward_contact          | 0.0198   |
| reward_ctrl             | 0.0491   |
| reward_motion           | 0        |
| reward_orientation      | 0.046    |
| reward_position         | 1.15e-52 |
| reward_rotation         | 0.0846   |
| reward_torque           | 0.0483   |
| reward_velocity         | 0.111    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 166      |
| time/                   |          |
|    fps                  | 47       |
|    iterations           | 246      |
|    time_elapsed         | 10710    |
|    total_timesteps      | 503808   |
| train/                  |          |
|    approx_kl            | 1.796508 |
|    clip_fraction        | 0.853    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.74    |
|    explained_variance   | 0.943    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0161  |
|    n_updates            | 2450     |
|    policy_gradient_loss | -0.0474  |
|    std                  | 0.496    |
|    value_loss           | 0.436    |
--------------------------------------
Num timesteps: 504000
Best mean reward: 167.67 - Last mean reward per episode: 165.58
---------------------------------------
| reward                  | 0.363     |
| reward_contact          | 0.0199    |
| reward_ctrl             | 0.0492    |
| reward_motion           | 0         |
| reward_orientation      | 0.0455    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.087     |
| reward_torque           | 0.0482    |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 247       |
|    time_elapsed         | 10763     |
|    total_timesteps      | 505856    |
| train/                  |           |
|    approx_kl            | 1.5489596 |
|    clip_fraction        | 0.849     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.76     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0321   |
|    n_updates            | 2460      |
|    policy_gradient_loss | -0.0332   |
|    std                  | 0.499     |
|    value_loss           | 0.641     |
---------------------------------------
---------------------------------------
| reward                  | 0.364     |
| reward_contact          | 0.0199    |
| reward_ctrl             | 0.0497    |
| reward_motion           | 0         |
| reward_orientation      | 0.0456    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.0857    |
| reward_torque           | 0.0482    |
| reward_velocity         | 0.115     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 248       |
|    time_elapsed         | 10807     |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 1.3211471 |
|    clip_fraction        | 0.833     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.77     |
|    explained_variance   | 0.957     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0152   |
|    n_updates            | 2470      |
|    policy_gradient_loss | -0.0281   |
|    std                  | 0.499     |
|    value_loss           | 0.423     |
---------------------------------------
---------------------------------------
| reward                  | 0.364     |
| reward_contact          | 0.0211    |
| reward_ctrl             | 0.0495    |
| reward_motion           | 0         |
| reward_orientation      | 0.0456    |
| reward_position         | 1.15e-52  |
| reward_rotation         | 0.084     |
| reward_torque           | 0.0482    |
| reward_velocity         | 0.116     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 169       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 249       |
|    time_elapsed         | 10855     |
|    total_timesteps      | 509952    |
| train/                  |           |
|    approx_kl            | 0.9254541 |
|    clip_fraction        | 0.804     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.75     |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.129     |
|    n_updates            | 2480      |
|    policy_gradient_loss | -0.0478   |
|    std                  | 0.497     |
|    value_loss           | 0.712     |
---------------------------------------
Num timesteps: 510000
Best mean reward: 167.67 - Last mean reward per episode: 168.77
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.364     |
| reward_contact          | 0.0207    |
| reward_ctrl             | 0.0498    |
| reward_motion           | 0         |
| reward_orientation      | 0.0456    |
| reward_position         | 3.73e-60  |
| reward_rotation         | 0.0823    |
| reward_torque           | 0.0482    |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 169       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 250       |
|    time_elapsed         | 10911     |
|    total_timesteps      | 512000    |
| train/                  |           |
|    approx_kl            | 1.1224558 |
|    clip_fraction        | 0.821     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.71     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0141    |
|    n_updates            | 2490      |
|    policy_gradient_loss | -0.0356   |
|    std                  | 0.494     |
|    value_loss           | 1.58      |
---------------------------------------
--------------------------------------
| reward                  | 0.368    |
| reward_contact          | 0.0217   |
| reward_ctrl             | 0.05     |
| reward_motion           | 0        |
| reward_orientation      | 0.0458   |
| reward_position         | 3.73e-60 |
| reward_rotation         | 0.0843   |
| reward_torque           | 0.0483   |
| reward_velocity         | 0.118    |
| rollout/                |          |
|    ep_len_mean          | 500      |
|    ep_rew_mean          | 170      |
| time/                   |          |
|    fps                  | 46       |
|    iterations           | 251      |
|    time_elapsed         | 10966    |
|    total_timesteps      | 514048   |
| train/                  |          |
|    approx_kl            | 1.718149 |
|    clip_fraction        | 0.868    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.67    |
|    explained_variance   | 0.934    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0769  |
|    n_updates            | 2500     |
|    policy_gradient_loss | -0.0396  |
|    std                  | 0.49     |
|    value_loss           | 0.328    |
--------------------------------------
Num timesteps: 516000
Best mean reward: 168.77 - Last mean reward per episode: 170.30
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.373     |
| reward_contact          | 0.0218    |
| reward_ctrl             | 0.0504    |
| reward_motion           | 0         |
| reward_orientation      | 0.0456    |
| reward_position         | 3.49e-49  |
| reward_rotation         | 0.087     |
| reward_torque           | 0.0483    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 170       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 252       |
|    time_elapsed         | 11020     |
|    total_timesteps      | 516096    |
| train/                  |           |
|    approx_kl            | 1.2476234 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.61     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0416   |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.0343   |
|    std                  | 0.488     |
|    value_loss           | 0.544     |
---------------------------------------
---------------------------------------
| reward                  | 0.375     |
| reward_contact          | 0.0219    |
| reward_ctrl             | 0.0502    |
| reward_motion           | 0         |
| reward_orientation      | 0.046     |
| reward_position         | 3.49e-49  |
| reward_rotation         | 0.0851    |
| reward_torque           | 0.0484    |
| reward_velocity         | 0.123     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 171       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 253       |
|    time_elapsed         | 11073     |
|    total_timesteps      | 518144    |
| train/                  |           |
|    approx_kl            | 2.3223805 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.59     |
|    explained_variance   | 0.876     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00222  |
|    n_updates            | 2520      |
|    policy_gradient_loss | -0.0591   |
|    std                  | 0.487     |
|    value_loss           | 0.912     |
---------------------------------------
---------------------------------------
| reward                  | 0.378     |
| reward_contact          | 0.0219    |
| reward_ctrl             | 0.0508    |
| reward_motion           | 0         |
| reward_orientation      | 0.0459    |
| reward_position         | 3.49e-49  |
| reward_rotation         | 0.0871    |
| reward_torque           | 0.0484    |
| reward_velocity         | 0.124     |
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 254       |
|    time_elapsed         | 11124     |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 1.0436975 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.58     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.048    |
|    n_updates            | 2530      |
|    policy_gradient_loss | -0.0474   |
|    std                  | 0.487     |
|    value_loss           | 0.807     |
---------------------------------------
Num timesteps: 522000
Best mean reward: 170.30 - Last mean reward per episode: 170.88
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.375     |
| reward_contact          | 0.0234    |
| reward_ctrl             | 0.0516    |
| reward_motion           | 0         |
| reward_orientation      | 0.0457    |
| reward_position         | 1.04e-39  |
| reward_rotation         | 0.086     |
| reward_torque           | 0.0485    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 171       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 255       |
|    time_elapsed         | 11178     |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 1.0958402 |
|    clip_fraction        | 0.817     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.55     |
|    explained_variance   | 0.868     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0237    |
|    n_updates            | 2540      |
|    policy_gradient_loss | -0.057    |
|    std                  | 0.485     |
|    value_loss           | 1.02      |
---------------------------------------
---------------------------------------
| reward                  | 0.376     |
| reward_contact          | 0.0251    |
| reward_ctrl             | 0.0527    |
| reward_motion           | 0         |
| reward_orientation      | 0.0449    |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0867    |
| reward_torque           | 0.0486    |
| reward_velocity         | 0.118     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 256       |
|    time_elapsed         | 11231     |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 1.8778743 |
|    clip_fraction        | 0.852     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.52     |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0667    |
|    n_updates            | 2550      |
|    policy_gradient_loss | -0.0434   |
|    std                  | 0.483     |
|    value_loss           | 1.76      |
---------------------------------------
---------------------------------------
| reward                  | 0.383     |
| reward_contact          | 0.0253    |
| reward_ctrl             | 0.0529    |
| reward_motion           | 0         |
| reward_orientation      | 0.0457    |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0905    |
| reward_torque           | 0.0486    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 257       |
|    time_elapsed         | 11283     |
|    total_timesteps      | 526336    |
| train/                  |           |
|    approx_kl            | 1.7756929 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.48     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.114    |
|    n_updates            | 2560      |
|    policy_gradient_loss | -0.0426   |
|    std                  | 0.479     |
|    value_loss           | 0.581     |
---------------------------------------
Num timesteps: 528000
Best mean reward: 170.88 - Last mean reward per episode: 172.45
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.392     |
| reward_contact          | 0.0247    |
| reward_ctrl             | 0.0531    |
| reward_motion           | 0         |
| reward_orientation      | 0.0456    |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0948    |
| reward_torque           | 0.0487    |
| reward_velocity         | 0.125     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 173       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 258       |
|    time_elapsed         | 11335     |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 1.5596068 |
|    clip_fraction        | 0.845     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.44     |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0931   |
|    n_updates            | 2570      |
|    policy_gradient_loss | -0.0541   |
|    std                  | 0.479     |
|    value_loss           | 0.556     |
---------------------------------------
---------------------------------------
| reward                  | 0.388     |
| reward_contact          | 0.0251    |
| reward_ctrl             | 0.0537    |
| reward_motion           | 0         |
| reward_orientation      | 0.046     |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0924    |
| reward_torque           | 0.0489    |
| reward_velocity         | 0.122     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 174       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 259       |
|    time_elapsed         | 11388     |
|    total_timesteps      | 530432    |
| train/                  |           |
|    approx_kl            | 1.9965045 |
|    clip_fraction        | 0.844     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.41     |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000175  |
|    n_updates            | 2580      |
|    policy_gradient_loss | -0.0729   |
|    std                  | 0.476     |
|    value_loss           | 0.55      |
---------------------------------------
---------------------------------------
| reward                  | 0.387     |
| reward_contact          | 0.0251    |
| reward_ctrl             | 0.0535    |
| reward_motion           | 0         |
| reward_orientation      | 0.0459    |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0942    |
| reward_torque           | 0.0489    |
| reward_velocity         | 0.12      |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 174       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 260       |
|    time_elapsed         | 11440     |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 1.0599504 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.38     |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0945   |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.0705   |
|    std                  | 0.475     |
|    value_loss           | 0.557     |
---------------------------------------
Num timesteps: 534000
Best mean reward: 172.45 - Last mean reward per episode: 173.97
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.391     |
| reward_contact          | 0.025     |
| reward_ctrl             | 0.0531    |
| reward_motion           | 0         |
| reward_orientation      | 0.0463    |
| reward_position         | 1.2e-14   |
| reward_rotation         | 0.0972    |
| reward_torque           | 0.0489    |
| reward_velocity         | 0.121     |
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | 175       |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 261       |
|    time_elapsed         | 11493     |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 2.4591079 |
|    clip_fraction        | 0.861     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.34     |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0695   |
|    n_updates            | 2600      |
|    policy_gradient_loss | -0.0554   |
|    std                  | 0.472     |
|    value_loss           | 0.47      |
---------------------------------------
--------------------------------------
| reward                  | 0.393    |
| reward_contact          | 0.025    |
| reward_ctrl             | 0.053    |
| reward_motion           | 0        |
| reward_orientation      | 0.0465   |
| reward_position         | 1.2e-14  |
| reward_rotation         | 0.0965   |
| reward_torque           | 0.0488   |
| reward_velocity         | 0.123    |
| rollout/                |          |
|    ep_len_mean          | 497      |
|    ep_rew_mean          | 175      |
| time/                   |          |
|    fps                  | 46       |
|    iterations           | 262      |
|    time_elapsed         | 11545    |
|    total_timesteps      | 536576   |
| train/                  |          |
|    approx_kl            | 1.555888 |
|    clip_fraction        | 0.836    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.3     |
|    explained_variance   | 0.901    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0461  |
|    n_updates            | 2610     |
|    policy_gradient_loss | -0.0535  |
|    std                  | 0.47     |
|    value_loss           | 0.629    |
--------------------------------------
2021-06-02 09:23:49.802786: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 09:23:49.802843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
Using cpu device
Logging to rl/out_dir/models/exp68/PPO_5
---------------------------------
| reward             | 0.215    |
| reward_contact     | 0.06     |
| reward_ctrl        | 0.0145   |
| reward_motion      | 0        |
| reward_orientation | 0.0579   |
| reward_position    | 0        |
| reward_rotation    | 0.0409   |
| reward_torque      | 0.0413   |
| reward_velocity    | 0.000576 |
| rollout/           |          |
|    ep_len_mean     | 1.82e+03 |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 1        |
|    time_elapsed    | 48       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| reward                  | 0.272       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0379      |
| reward_motion           | 0           |
| reward_orientation      | 0.049       |
| reward_position         | 3.68e-136   |
| reward_rotation         | 0.0803      |
| reward_torque           | 0.0448      |
| reward_velocity         | 0.000344    |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 240         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 2           |
|    time_elapsed         | 100         |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.053106353 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.0708      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0682      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0344     |
|    std                  | 1           |
|    value_loss           | 0.702       |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 239.75
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.243       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0278      |
| reward_motion           | 0           |
| reward_orientation      | 0.0534      |
| reward_position         | 1.84e-136   |
| reward_rotation         | 0.0469      |
| reward_torque           | 0.0434      |
| reward_velocity         | 0.0118      |
| rollout/                |             |
|    ep_len_mean          | 985         |
|    ep_rew_mean          | 240         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 3           |
|    time_elapsed         | 153         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.054424874 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0934      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0359     |
|    std                  | 0.999       |
|    value_loss           | 0.791       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.269       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0229      |
| reward_motion           | 0           |
| reward_orientation      | 0.0531      |
| reward_position         | 1.4e-61     |
| reward_rotation         | 0.0476      |
| reward_torque           | 0.0421      |
| reward_velocity         | 0.0432      |
| rollout/                |             |
|    ep_len_mean          | 872         |
|    ep_rew_mean          | 213         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 4           |
|    time_elapsed         | 205         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.050213568 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0343     |
|    std                  | 0.996       |
|    value_loss           | 0.721       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.264       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.021       |
| reward_motion           | 0           |
| reward_orientation      | 0.0508      |
| reward_position         | 1.26e-61    |
| reward_rotation         | 0.0478      |
| reward_torque           | 0.0412      |
| reward_velocity         | 0.0433      |
| rollout/                |             |
|    ep_len_mean          | 892         |
|    ep_rew_mean          | 220         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 5           |
|    time_elapsed         | 256         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.058231235 |
|    clip_fraction        | 0.416       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0337     |
|    std                  | 0.99        |
|    value_loss           | 0.777       |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 239.75 - Last mean reward per episode: 224.78
---------------------------------------
| reward                  | 0.252     |
| reward_contact          | 0.06      |
| reward_ctrl             | 0.0195    |
| reward_motion           | 0         |
| reward_orientation      | 0.0478    |
| reward_position         | 5.34e-55  |
| reward_rotation         | 0.0473    |
| reward_torque           | 0.0409    |
| reward_velocity         | 0.0361    |
| rollout/                |           |
|    ep_len_mean          | 948       |
|    ep_rew_mean          | 225       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 6         |
|    time_elapsed         | 308       |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0893649 |
|    clip_fraction        | 0.464     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.3     |
|    explained_variance   | 0.385     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0208    |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0318   |
|    std                  | 0.989     |
|    value_loss           | 0.568     |
---------------------------------------
----------------------------------------
| reward                  | 0.249      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0181     |
| reward_motion           | 0          |
| reward_orientation      | 0.046      |
| reward_position         | 4.93e-55   |
| reward_rotation         | 0.0516     |
| reward_torque           | 0.0401     |
| reward_velocity         | 0.0333     |
| rollout/                |            |
|    ep_len_mean          | 1.03e+03   |
|    ep_rew_mean          | 237        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 7          |
|    time_elapsed         | 360        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07688603 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.3      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.246      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0223    |
|    std                  | 0.989      |
|    value_loss           | 0.476      |
----------------------------------------
-----------------------------------------
| reward                  | 0.242       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0173      |
| reward_motion           | 0           |
| reward_orientation      | 0.0443      |
| reward_position         | 4.58e-55    |
| reward_rotation         | 0.0498      |
| reward_torque           | 0.0398      |
| reward_velocity         | 0.0309      |
| rollout/                |             |
|    ep_len_mean          | 1.04e+03    |
|    ep_rew_mean          | 240         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 8           |
|    time_elapsed         | 411         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.061783824 |
|    clip_fraction        | 0.502       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0459      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0115     |
|    std                  | 0.988       |
|    value_loss           | 0.593       |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 239.75 - Last mean reward per episode: 207.69
----------------------------------------
| reward                  | 0.229      |
| reward_contact          | 0.057      |
| reward_ctrl             | 0.019      |
| reward_motion           | 0          |
| reward_orientation      | 0.044      |
| reward_position         | 0.000798   |
| reward_rotation         | 0.0446     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0229     |
| rollout/                |            |
|    ep_len_mean          | 899        |
|    ep_rew_mean          | 208        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 9          |
|    time_elapsed         | 464        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06699768 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0316    |
|    std                  | 0.984      |
|    value_loss           | 0.715      |
----------------------------------------
-----------------------------------------
| reward                  | 0.225       |
| reward_contact          | 0.0571      |
| reward_ctrl             | 0.0186      |
| reward_motion           | 0           |
| reward_orientation      | 0.044       |
| reward_position         | 0.00076     |
| reward_rotation         | 0.0425      |
| reward_torque           | 0.0405      |
| reward_velocity         | 0.0218      |
| rollout/                |             |
|    ep_len_mean          | 901         |
|    ep_rew_mean          | 209         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 10          |
|    time_elapsed         | 516         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.038838953 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.623       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0374     |
|    std                  | 0.979       |
|    value_loss           | 2.37        |
-----------------------------------------
----------------------------------------
| reward                  | 0.226      |
| reward_contact          | 0.0573     |
| reward_ctrl             | 0.0179     |
| reward_motion           | 0          |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000725   |
| reward_rotation         | 0.0406     |
| reward_torque           | 0.0403     |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 934        |
|    ep_rew_mean          | 219        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 11         |
|    time_elapsed         | 568        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.08458613 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0343     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0279    |
|    std                  | 0.977      |
|    value_loss           | 0.503      |
----------------------------------------
Num timesteps: 24000
Best mean reward: 239.75 - Last mean reward per episode: 233.88
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0548     |
| reward_ctrl             | 0.0182     |
| reward_motion           | 0          |
| reward_orientation      | 0.0439     |
| reward_position         | 0.000694   |
| reward_rotation         | 0.0392     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 994        |
|    ep_rew_mean          | 234        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 12         |
|    time_elapsed         | 620        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.12852493 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0302     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0327    |
|    std                  | 0.972      |
|    value_loss           | 0.662      |
----------------------------------------
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.0527     |
| reward_ctrl             | 0.0188     |
| reward_motion           | 0          |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000665   |
| reward_rotation         | 0.0376     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.025      |
| rollout/                |            |
|    ep_len_mean          | 1.05e+03   |
|    ep_rew_mean          | 244        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 13         |
|    time_elapsed         | 671        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.06629027 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0609     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0189    |
|    std                  | 0.972      |
|    value_loss           | 0.736      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.053      |
| reward_ctrl             | 0.0184     |
| reward_motion           | 0          |
| reward_orientation      | 0.0437     |
| reward_position         | 0.000638   |
| reward_rotation         | 0.0394     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 1.07e+03   |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 14         |
|    time_elapsed         | 721        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.05958663 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0224    |
|    std                  | 0.968      |
|    value_loss           | 0.52       |
----------------------------------------
Num timesteps: 30000
Best mean reward: 239.75 - Last mean reward per episode: 257.37
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.0509     |
| reward_ctrl             | 0.0191     |
| reward_motion           | 0          |
| reward_orientation      | 0.0443     |
| reward_position         | 0.000614   |
| reward_rotation         | 0.0456     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.026      |
| rollout/                |            |
|    ep_len_mean          | 1.12e+03   |
|    ep_rew_mean          | 257        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 15         |
|    time_elapsed         | 774        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06080155 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.02       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0306    |
|    std                  | 0.968      |
|    value_loss           | 0.686      |
----------------------------------------
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0491     |
| reward_ctrl             | 0.0189     |
| reward_motion           | 0          |
| reward_orientation      | 0.0435     |
| reward_position         | 0.000591   |
| reward_rotation         | 0.0439     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0281     |
| rollout/                |            |
|    ep_len_mean          | 1.16e+03   |
|    ep_rew_mean          | 270        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 16         |
|    time_elapsed         | 824        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08265419 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0309    |
|    std                  | 0.97       |
|    value_loss           | 0.711      |
----------------------------------------
----------------------------------------
| reward                  | 0.226      |
| reward_contact          | 0.0494     |
| reward_ctrl             | 0.0184     |
| reward_motion           | 0          |
| reward_orientation      | 0.0428     |
| reward_position         | 0.00057    |
| reward_rotation         | 0.0469     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0271     |
| rollout/                |            |
|    ep_len_mean          | 1.17e+03   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 17         |
|    time_elapsed         | 876        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.08142573 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0494     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0278    |
|    std                  | 0.972      |
|    value_loss           | 0.572      |
----------------------------------------
Num timesteps: 36000
Best mean reward: 257.37 - Last mean reward per episode: 278.08
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.048      |
| reward_ctrl             | 0.0196     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 0.00055    |
| reward_rotation         | 0.046      |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0262     |
| rollout/                |            |
|    ep_len_mean          | 1.21e+03   |
|    ep_rew_mean          | 278        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 18         |
|    time_elapsed         | 928        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.07431942 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00732   |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0198    |
|    std                  | 0.97       |
|    value_loss           | 0.377      |
----------------------------------------
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0484     |
| reward_ctrl             | 0.019      |
| reward_motion           | 0          |
| reward_orientation      | 0.0425     |
| reward_position         | 0.000532   |
| reward_rotation         | 0.0445     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 1.25e+03   |
|    ep_rew_mean          | 286        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 19         |
|    time_elapsed         | 980        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.05933201 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0752     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0253    |
|    std                  | 0.968      |
|    value_loss           | 0.678      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0488     |
| reward_ctrl             | 0.0193     |
| reward_motion           | 0          |
| reward_orientation      | 0.0431     |
| reward_position         | 0.000515   |
| reward_rotation         | 0.0431     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0248     |
| rollout/                |            |
|    ep_len_mean          | 1.28e+03   |
|    ep_rew_mean          | 292        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 20         |
|    time_elapsed         | 1030       |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.07139835 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0321     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0346    |
|    std                  | 0.967      |
|    value_loss           | 0.612      |
----------------------------------------
Num timesteps: 42000
Best mean reward: 278.08 - Last mean reward per episode: 291.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.0472     |
| reward_ctrl             | 0.0193     |
| reward_motion           | 0          |
| reward_orientation      | 0.0434     |
| reward_position         | 0.000499   |
| reward_rotation         | 0.0418     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0244     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 298        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 21         |
|    time_elapsed         | 1082       |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.09940413 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0311    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0276    |
|    std                  | 0.969      |
|    value_loss           | 0.326      |
----------------------------------------
---------------------------------------
| reward                  | 0.213     |
| reward_contact          | 0.0464    |
| reward_ctrl             | 0.0184    |
| reward_motion           | 0         |
| reward_orientation      | 0.0433    |
| reward_position         | 0.000469  |
| reward_rotation         | 0.0396    |
| reward_torque           | 0.0404    |
| reward_velocity         | 0.0247    |
| rollout/                |           |
|    ep_len_mean          | 1.32e+03  |
|    ep_rew_mean          | 300       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 22        |
|    time_elapsed         | 1134      |
|    total_timesteps      | 45056     |
| train/                  |           |
|    approx_kl            | 0.1004859 |
|    clip_fraction        | 0.562     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.1     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00458   |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0281   |
|    std                  | 0.966     |
|    value_loss           | 0.442     |
---------------------------------------
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0471     |
| reward_ctrl             | 0.0187     |
| reward_motion           | 0          |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000443   |
| reward_rotation         | 0.0385     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 297        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 23         |
|    time_elapsed         | 1186       |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.07707362 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0353     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0365    |
|    std                  | 0.962      |
|    value_loss           | 0.656      |
----------------------------------------
Num timesteps: 48000
Best mean reward: 291.99 - Last mean reward per episode: 296.87
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0471     |
| reward_ctrl             | 0.0187     |
| reward_motion           | 0          |
| reward_orientation      | 0.0428     |
| reward_position         | 0.000443   |
| reward_rotation         | 0.0385     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 297        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 24         |
|    time_elapsed         | 1239       |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07425946 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.082      |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0183    |
|    std                  | 0.961      |
|    value_loss           | 0.564      |
----------------------------------------
-----------------------------------------
| reward                  | 0.208       |
| reward_contact          | 0.0463      |
| reward_ctrl             | 0.0191      |
| reward_motion           | 0           |
| reward_orientation      | 0.0423      |
| reward_position         | 0.00042     |
| reward_rotation         | 0.0367      |
| reward_torque           | 0.0408      |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 295         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 25          |
|    time_elapsed         | 1292        |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.074163035 |
|    clip_fraction        | 0.501       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.025       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0293     |
|    std                  | 0.958       |
|    value_loss           | 0.366       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.203       |
| reward_contact          | 0.0463      |
| reward_ctrl             | 0.0197      |
| reward_motion           | 0           |
| reward_orientation      | 0.0416      |
| reward_position         | 0.00038     |
| reward_rotation         | 0.0335      |
| reward_torque           | 0.0407      |
| reward_velocity         | 0.0209      |
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 285         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 26          |
|    time_elapsed         | 1344        |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.079693325 |
|    clip_fraction        | 0.494       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0293      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0305     |
|    std                  | 0.953       |
|    value_loss           | 0.448       |
-----------------------------------------
Num timesteps: 54000
Best mean reward: 296.87 - Last mean reward per episode: 283.07
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.047      |
| reward_ctrl             | 0.0194     |
| reward_motion           | 0          |
| reward_orientation      | 0.0419     |
| reward_position         | 0.000363   |
| reward_rotation         | 0.0322     |
| reward_torque           | 0.0404     |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 1.23e+03   |
|    ep_rew_mean          | 278        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 27         |
|    time_elapsed         | 1396       |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.08014804 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0896     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0384    |
|    std                  | 0.952      |
|    value_loss           | 1.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0472     |
| reward_ctrl             | 0.0191     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 0.000355   |
| reward_rotation         | 0.0314     |
| reward_torque           | 0.0402     |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 1.25e+03   |
|    ep_rew_mean          | 283        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 28         |
|    time_elapsed         | 1448       |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08477342 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0116     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0275    |
|    std                  | 0.952      |
|    value_loss           | 0.572      |
----------------------------------------
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0465     |
| reward_ctrl             | 0.0212     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 0.000347   |
| reward_rotation         | 0.0308     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 1.28e+03   |
|    ep_rew_mean          | 286        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 29         |
|    time_elapsed         | 1499       |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.06262391 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0336    |
|    std                  | 0.948      |
|    value_loss           | 0.736      |
----------------------------------------
Num timesteps: 60000
Best mean reward: 296.87 - Last mean reward per episode: 286.16
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0455     |
| reward_ctrl             | 0.0213     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 0.000339   |
| reward_rotation         | 0.0313     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 1.3e+03    |
|    ep_rew_mean          | 292        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 30         |
|    time_elapsed         | 1550       |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.10694437 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00485    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.015     |
|    std                  | 0.949      |
|    value_loss           | 0.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.201      |
| reward_contact          | 0.0458     |
| reward_ctrl             | 0.021      |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 0.000332   |
| reward_rotation         | 0.0306     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 296        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 31         |
|    time_elapsed         | 1601       |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.06603754 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0821     |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0227    |
|    std                  | 0.952      |
|    value_loss           | 0.405      |
----------------------------------------
----------------------------------------
| reward                  | 0.201      |
| reward_contact          | 0.0458     |
| reward_ctrl             | 0.021      |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 0.000332   |
| reward_rotation         | 0.0306     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 296        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 32         |
|    time_elapsed         | 1652       |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.09766097 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0335    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.95       |
|    value_loss           | 0.364      |
----------------------------------------
Num timesteps: 66000
Best mean reward: 296.87 - Last mean reward per episode: 302.78
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0449     |
| reward_ctrl             | 0.0207     |
| reward_motion           | 0          |
| reward_orientation      | 0.042      |
| reward_position         | 0.000326   |
| reward_rotation         | 0.0303     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 306        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 33         |
|    time_elapsed         | 1704       |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.11079663 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0412     |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.947      |
|    value_loss           | 0.489      |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0443     |
| reward_ctrl             | 0.0205     |
| reward_motion           | 0          |
| reward_orientation      | 0.0413     |
| reward_position         | 0.000313   |
| reward_rotation         | 0.0294     |
| reward_torque           | 0.0405     |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 309        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 34         |
|    time_elapsed         | 1755       |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.07956002 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.307      |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0328    |
|    std                  | 0.946      |
|    value_loss           | 0.885      |
----------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0446     |
| reward_ctrl             | 0.0201     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 0.000307   |
| reward_rotation         | 0.0289     |
| reward_torque           | 0.0403     |
| reward_velocity         | 0.0229     |
| rollout/                |            |
|    ep_len_mean          | 1.34e+03   |
|    ep_rew_mean          | 304        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 35         |
|    time_elapsed         | 1808       |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.07394264 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.134      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0327    |
|    std                  | 0.946      |
|    value_loss           | 0.425      |
----------------------------------------
Num timesteps: 72000
Best mean reward: 302.78 - Last mean reward per episode: 304.29
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.197      |
| reward_contact          | 0.0441     |
| reward_ctrl             | 0.0205     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 0.000301   |
| reward_rotation         | 0.0289     |
| reward_torque           | 0.0404     |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 308        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 36         |
|    time_elapsed         | 1859       |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.10052471 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0225     |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0348    |
|    std                  | 0.939      |
|    value_loss           | 0.7        |
----------------------------------------
---------------------------------------
| reward                  | 0.196     |
| reward_contact          | 0.0425    |
| reward_ctrl             | 0.0208    |
| reward_motion           | 0         |
| reward_orientation      | 0.0405    |
| reward_position         | 0.00029   |
| reward_rotation         | 0.0278    |
| reward_torque           | 0.0406    |
| reward_velocity         | 0.0235    |
| rollout/                |           |
|    ep_len_mean          | 1.37e+03  |
|    ep_rew_mean          | 311       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 37        |
|    time_elapsed         | 1911      |
|    total_timesteps      | 75776     |
| train/                  |           |
|    approx_kl            | 0.0692039 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.8     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0279    |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0162   |
|    std                  | 0.939     |
|    value_loss           | 0.442     |
---------------------------------------
----------------------------------------
| reward                  | 0.197      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0221     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 0.000285   |
| reward_rotation         | 0.0285     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 315        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 38         |
|    time_elapsed         | 1962       |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.13216266 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.29       |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0276    |
|    std                  | 0.939      |
|    value_loss           | 0.78       |
----------------------------------------
Num timesteps: 78000
Best mean reward: 304.29 - Last mean reward per episode: 314.71
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.197       |
| reward_contact          | 0.0418      |
| reward_ctrl             | 0.0221      |
| reward_motion           | 0           |
| reward_orientation      | 0.0402      |
| reward_position         | 0.000285    |
| reward_rotation         | 0.0285      |
| reward_torque           | 0.0409      |
| reward_velocity         | 0.0231      |
| rollout/                |             |
|    ep_len_mean          | 1.39e+03    |
|    ep_rew_mean          | 315         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 39          |
|    time_elapsed         | 2013        |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.110044636 |
|    clip_fraction        | 0.554       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.058      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0374     |
|    std                  | 0.931       |
|    value_loss           | 0.265       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.196       |
| reward_contact          | 0.0412      |
| reward_ctrl             | 0.0223      |
| reward_motion           | 0           |
| reward_orientation      | 0.0399      |
| reward_position         | 0.00028     |
| reward_rotation         | 0.0284      |
| reward_torque           | 0.041       |
| reward_velocity         | 0.0227      |
| rollout/                |             |
|    ep_len_mean          | 1.4e+03     |
|    ep_rew_mean          | 318         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 40          |
|    time_elapsed         | 2065        |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.085955806 |
|    clip_fraction        | 0.517       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0862      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0314     |
|    std                  | 0.927       |
|    value_loss           | 0.419       |
-----------------------------------------
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0411     |
| reward_ctrl             | 0.0234     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 0.000279   |
| reward_rotation         | 0.0281     |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 1.38e+03   |
|    ep_rew_mean          | 313        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 41         |
|    time_elapsed         | 2117       |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.10580662 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0138     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0356    |
|    std                  | 0.923      |
|    value_loss           | 0.482      |
----------------------------------------
Num timesteps: 84000
Best mean reward: 314.71 - Last mean reward per episode: 312.81
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0405     |
| reward_ctrl             | 0.0231     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 0.000275   |
| reward_rotation         | 0.028      |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 42         |
|    time_elapsed         | 2169       |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.09598254 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0953     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0382    |
|    std                  | 0.922      |
|    value_loss           | 1.43       |
----------------------------------------
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0404     |
| reward_ctrl             | 0.0231     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 0.000266   |
| reward_rotation         | 0.0283     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 43         |
|    time_elapsed         | 2221       |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10583873 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0429     |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0419    |
|    std                  | 0.92       |
|    value_loss           | 0.498      |
----------------------------------------
Num timesteps: 90000
Best mean reward: 314.71 - Last mean reward per episode: 309.54
-----------------------------------------
| reward                  | 0.2         |
| reward_contact          | 0.041       |
| reward_ctrl             | 0.0228      |
| reward_motion           | 0           |
| reward_orientation      | 0.0395      |
| reward_position         | 0.000258    |
| reward_rotation         | 0.0344      |
| reward_torque           | 0.0408      |
| reward_velocity         | 0.0216      |
| rollout/                |             |
|    ep_len_mean          | 1.36e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 44          |
|    time_elapsed         | 2272        |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.091806054 |
|    clip_fraction        | 0.561       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0363     |
|    std                  | 0.92        |
|    value_loss           | 0.8         |
-----------------------------------------
----------------------------------------
| reward                  | 0.201      |
| reward_contact          | 0.0416     |
| reward_ctrl             | 0.0233     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 0.00025    |
| reward_rotation         | 0.0337     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 309        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 45         |
|    time_elapsed         | 2324       |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.10448655 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0431     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0391    |
|    std                  | 0.912      |
|    value_loss           | 1.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0423     |
| reward_ctrl             | 0.0228     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 0.000239   |
| reward_rotation         | 0.0336     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 1.33e+03   |
|    ep_rew_mean          | 305        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 46         |
|    time_elapsed         | 2377       |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.12113105 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0476     |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0421    |
|    std                  | 0.908      |
|    value_loss           | 0.524      |
----------------------------------------
Num timesteps: 96000
Best mean reward: 314.71 - Last mean reward per episode: 299.17
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0431     |
| reward_ctrl             | 0.0231     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 0.000234   |
| reward_rotation         | 0.0347     |
| reward_torque           | 0.041      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 299        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 47         |
|    time_elapsed         | 2430       |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.08150421 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0292     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0455    |
|    std                  | 0.904      |
|    value_loss           | 0.809      |
----------------------------------------
----------------------------------------
| reward                  | 0.201      |
| reward_contact          | 0.0433     |
| reward_ctrl             | 0.0228     |
| reward_motion           | 0          |
| reward_orientation      | 0.0403     |
| reward_position         | 0.000231   |
| reward_rotation         | 0.0342     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0198     |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 301        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 48         |
|    time_elapsed         | 2482       |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.09888671 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.112      |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0284    |
|    std                  | 0.897      |
|    value_loss           | 0.816      |
----------------------------------------
----------------------------------------
| reward                  | 0.201      |
| reward_contact          | 0.0429     |
| reward_ctrl             | 0.023      |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 0.000228   |
| reward_rotation         | 0.0338     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 304        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 49         |
|    time_elapsed         | 2535       |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.11807829 |
|    clip_fraction        | 0.597      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0084     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0383    |
|    std                  | 0.896      |
|    value_loss           | 0.567      |
----------------------------------------
Num timesteps: 102000
Best mean reward: 314.71 - Last mean reward per episode: 302.58
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0426     |
| reward_ctrl             | 0.0227     |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 0.000225   |
| reward_rotation         | 0.0332     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 303        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 50         |
|    time_elapsed         | 2588       |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.11765671 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.284      |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.00374   |
|    std                  | 0.893      |
|    value_loss           | 0.412      |
----------------------------------------
---------------------------------------
| reward                  | 0.198     |
| reward_contact          | 0.0421    |
| reward_ctrl             | 0.0224    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 0.000222  |
| reward_rotation         | 0.0331    |
| reward_torque           | 0.0408    |
| reward_velocity         | 0.0193    |
| rollout/                |           |
|    ep_len_mean          | 1.33e+03  |
|    ep_rew_mean          | 307       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 51        |
|    time_elapsed         | 2640      |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.1323409 |
|    clip_fraction        | 0.601     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.4     |
|    explained_variance   | 0.161     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0288    |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0124   |
|    std                  | 0.891     |
|    value_loss           | 0.821     |
---------------------------------------
-----------------------------------------
| reward                  | 0.197       |
| reward_contact          | 0.0415      |
| reward_ctrl             | 0.0227      |
| reward_motion           | 0           |
| reward_orientation      | 0.0401      |
| reward_position         | 0.00022     |
| reward_rotation         | 0.033       |
| reward_torque           | 0.0409      |
| reward_velocity         | 0.0191      |
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 52          |
|    time_elapsed         | 2691        |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.110439226 |
|    clip_fraction        | 0.568       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0364     |
|    std                  | 0.888       |
|    value_loss           | 0.373       |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 314.71 - Last mean reward per episode: 309.67
-----------------------------------------
| reward                  | 0.197       |
| reward_contact          | 0.0415      |
| reward_ctrl             | 0.0227      |
| reward_motion           | 0           |
| reward_orientation      | 0.0401      |
| reward_position         | 0.00022     |
| reward_rotation         | 0.033       |
| reward_torque           | 0.0409      |
| reward_velocity         | 0.0191      |
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 53          |
|    time_elapsed         | 2742        |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.089094505 |
|    clip_fraction        | 0.541       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0513      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0418     |
|    std                  | 0.885       |
|    value_loss           | 0.453       |
-----------------------------------------
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0415     |
| reward_ctrl             | 0.022      |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 0.000212   |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0196     |
| rollout/                |            |
|    ep_len_mean          | 1.33e+03   |
|    ep_rew_mean          | 308        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 54         |
|    time_elapsed         | 2794       |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.08922215 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0709     |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0217    |
|    std                  | 0.886      |
|    value_loss           | 0.452      |
----------------------------------------
----------------------------------------
| reward                  | 0.197      |
| reward_contact          | 0.042      |
| reward_ctrl             | 0.0222     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 0.000208   |
| reward_rotation         | 0.0322     |
| reward_torque           | 0.0406     |
| reward_velocity         | 0.0193     |
| rollout/                |            |
|    ep_len_mean          | 1.33e+03   |
|    ep_rew_mean          | 308        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 55         |
|    time_elapsed         | 2846       |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.11638948 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0934     |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0503    |
|    std                  | 0.882      |
|    value_loss           | 0.621      |
----------------------------------------
Num timesteps: 114000
Best mean reward: 314.71 - Last mean reward per episode: 310.40
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0422     |
| reward_ctrl             | 0.0224     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 0.000205   |
| reward_rotation         | 0.0337     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 1.34e+03   |
|    ep_rew_mean          | 310        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 56         |
|    time_elapsed         | 2898       |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.10270414 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0254     |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0415    |
|    std                  | 0.879      |
|    value_loss           | 1.06       |
----------------------------------------
----------------------------------------
| reward                  | 0.197      |
| reward_contact          | 0.0419     |
| reward_ctrl             | 0.0222     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 0.000201   |
| reward_rotation         | 0.0333     |
| reward_torque           | 0.0407     |
| reward_velocity         | 0.0187     |
| rollout/                |            |
|    ep_len_mean          | 1.33e+03   |
|    ep_rew_mean          | 309        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 57         |
|    time_elapsed         | 2950       |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.13708921 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0446    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0491    |
|    std                  | 0.876      |
|    value_loss           | 0.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0425     |
| reward_ctrl             | 0.0224     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 0.000194   |
| reward_rotation         | 0.0326     |
| reward_torque           | 0.0408     |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 307        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 58         |
|    time_elapsed         | 3003       |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.09937041 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0129    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.051     |
|    std                  | 0.87       |
|    value_loss           | 1.02       |
----------------------------------------
Num timesteps: 120000
Best mean reward: 314.71 - Last mean reward per episode: 306.53
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.023      |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 0.000192   |
| reward_rotation         | 0.0327     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0186     |
| rollout/                |            |
|    ep_len_mean          | 1.32e+03   |
|    ep_rew_mean          | 310        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 59         |
|    time_elapsed         | 3055       |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.16370176 |
|    clip_fraction        | 0.614      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0578    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.871      |
|    value_loss           | 0.483      |
----------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0427     |
| reward_ctrl             | 0.023      |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 0.000192   |
| reward_rotation         | 0.0327     |
| reward_torque           | 0.0409     |
| reward_velocity         | 0.0186     |
| rollout/                |            |
|    ep_len_mean          | 1.34e+03   |
|    ep_rew_mean          | 313        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 60         |
|    time_elapsed         | 3106       |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.08079601 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.99       |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0243    |
|    std                  | 0.869      |
|    value_loss           | 0.855      |
----------------------------------------
-----------------------------------------
| reward                  | 0.198       |
| reward_contact          | 0.0422      |
| reward_ctrl             | 0.0233      |
| reward_motion           | 0           |
| reward_orientation      | 0.0401      |
| reward_position         | 0.00019     |
| reward_rotation         | 0.0328      |
| reward_torque           | 0.041       |
| reward_velocity         | 0.0184      |
| rollout/                |             |
|    ep_len_mean          | 1.34e+03    |
|    ep_rew_mean          | 313         |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 61          |
|    time_elapsed         | 3159        |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.106587164 |
|    clip_fraction        | 0.588       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0332      |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0382     |
|    std                  | 0.867       |
|    value_loss           | 0.42        |
-----------------------------------------
Num timesteps: 126000
Best mean reward: 314.71 - Last mean reward per episode: 315.27
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.0235     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 0.000188   |
| reward_rotation         | 0.0347     |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 315        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 62         |
|    time_elapsed         | 3212       |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.11700399 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.128      |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0202    |
|    std                  | 0.859      |
|    value_loss           | 0.616      |
----------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0413     |
| reward_ctrl             | 0.0234     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 0.000186   |
| reward_rotation         | 0.0344     |
| reward_torque           | 0.0411     |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 319        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 63         |
|    time_elapsed         | 3264       |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.12352018 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0358    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0469    |
|    std                  | 0.853      |
|    value_loss           | 0.362      |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0417     |
| reward_ctrl             | 0.0237     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 0.000182   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 319        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 64         |
|    time_elapsed         | 3318       |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.09820181 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10        |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00256    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0252    |
|    std                  | 0.849      |
|    value_loss           | 0.357      |
----------------------------------------
Num timesteps: 132000
Best mean reward: 315.27 - Last mean reward per episode: 318.76
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0415     |
| reward_ctrl             | 0.0237     |
| reward_motion           | 0          |
| reward_orientation      | 0.0396     |
| reward_position         | 0.00018    |
| reward_rotation         | 0.0357     |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 1.37e+03   |
|    ep_rew_mean          | 321        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 65         |
|    time_elapsed         | 3371       |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.08982553 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10        |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.344      |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0455    |
|    std                  | 0.846      |
|    value_loss           | 1.33       |
----------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.041      |
| reward_ctrl             | 0.0246     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 0.000178   |
| reward_rotation         | 0.0377     |
| reward_torque           | 0.0414     |
| reward_velocity         | 0.0173     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 321        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 66         |
|    time_elapsed         | 3424       |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.09582333 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.98      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00913    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.033     |
|    std                  | 0.844      |
|    value_loss           | 0.312      |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0414     |
| reward_ctrl             | 0.0246     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0365     |
| reward_torque           | 0.0414     |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 1.34e+03   |
|    ep_rew_mean          | 313        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 67         |
|    time_elapsed         | 3476       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.12334019 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.98      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.288      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0226    |
|    std                  | 0.845      |
|    value_loss           | 1.34       |
----------------------------------------
Num timesteps: 138000
Best mean reward: 318.76 - Last mean reward per episode: 316.45
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0409     |
| reward_ctrl             | 0.0239     |
| reward_motion           | 0          |
| reward_orientation      | 0.0391     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0357     |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 68         |
|    time_elapsed         | 3529       |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.16681969 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.98      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0389     |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0365    |
|    std                  | 0.841      |
|    value_loss           | 1.01       |
----------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0409     |
| reward_ctrl             | 0.0238     |
| reward_motion           | 0          |
| reward_orientation      | 0.0388     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0371     |
| reward_torque           | 0.0412     |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 322        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 69         |
|    time_elapsed         | 3581       |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.13813782 |
|    clip_fraction        | 0.605      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.92      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.377      |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0382    |
|    std                  | 0.836      |
|    value_loss           | 0.774      |
----------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0404     |
| reward_ctrl             | 0.0245     |
| reward_motion           | 0          |
| reward_orientation      | 0.0384     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0371     |
| reward_torque           | 0.0413     |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 1.38e+03   |
|    ep_rew_mean          | 325        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 70         |
|    time_elapsed         | 3632       |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.14710975 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.89      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0428     |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0361    |
|    std                  | 0.835      |
|    value_loss           | 0.506      |
----------------------------------------
Num timesteps: 144000
Best mean reward: 318.76 - Last mean reward per episode: 324.70
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0398     |
| reward_ctrl             | 0.025      |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0371     |
| reward_torque           | 0.0415     |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 327        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 71         |
|    time_elapsed         | 3684       |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.15510531 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.88      |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0278     |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0282    |
|    std                  | 0.832      |
|    value_loss           | 0.339      |
----------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0398     |
| reward_ctrl             | 0.025      |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0371     |
| reward_torque           | 0.0415     |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 327        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 72         |
|    time_elapsed         | 3736       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.12097831 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.84      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.058      |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.827      |
|    value_loss           | 0.503      |
----------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0398     |
| reward_ctrl             | 0.0254     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0376     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 329        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 73         |
|    time_elapsed         | 3788       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.15798284 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.8       |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0249    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.036     |
|    std                  | 0.825      |
|    value_loss           | 0.246      |
----------------------------------------
Num timesteps: 150000
Best mean reward: 324.70 - Last mean reward per episode: 329.39
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.195     |
| reward_contact          | 0.0392    |
| reward_ctrl             | 0.0259    |
| reward_motion           | 0         |
| reward_orientation      | 0.0377    |
| reward_position         | 0.000175  |
| reward_rotation         | 0.0371    |
| reward_torque           | 0.0418    |
| reward_velocity         | 0.0131    |
| rollout/                |           |
|    ep_len_mean          | 1.41e+03  |
|    ep_rew_mean          | 333       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 74        |
|    time_elapsed         | 3840      |
|    total_timesteps      | 151552    |
| train/                  |           |
|    approx_kl            | 0.1557506 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.79     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0573    |
|    n_updates            | 730       |
|    policy_gradient_loss | -0.047    |
|    std                  | 0.822     |
|    value_loss           | 1.09      |
---------------------------------------
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0387     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.037      |
| reward_torque           | 0.0419     |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 1.41e+03   |
|    ep_rew_mean          | 337        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 75         |
|    time_elapsed         | 3893       |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.12185997 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0793     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0303    |
|    std                  | 0.819      |
|    value_loss           | 0.664      |
----------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0381     |
| reward_ctrl             | 0.0264     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0366     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 1.42e+03   |
|    ep_rew_mean          | 339        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 76         |
|    time_elapsed         | 3944       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.13678586 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.078      |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.821      |
|    value_loss           | 1.05       |
----------------------------------------
Num timesteps: 156000
Best mean reward: 329.39 - Last mean reward per episode: 339.48
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0375     |
| reward_ctrl             | 0.0267     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0365     |
| reward_torque           | 0.0421     |
| reward_velocity         | 0.0144     |
| rollout/                |            |
|    ep_len_mean          | 1.43e+03   |
|    ep_rew_mean          | 344        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 77         |
|    time_elapsed         | 3996       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.16845348 |
|    clip_fraction        | 0.626      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.02      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0336    |
|    std                  | 0.819      |
|    value_loss           | 0.451      |
----------------------------------------
---------------------------------------
| reward                  | 0.195     |
| reward_contact          | 0.0376    |
| reward_ctrl             | 0.0269    |
| reward_motion           | 0         |
| reward_orientation      | 0.0376    |
| reward_position         | 0.000175  |
| reward_rotation         | 0.0358    |
| reward_torque           | 0.0422    |
| reward_velocity         | 0.0144    |
| rollout/                |           |
|    ep_len_mean          | 1.43e+03  |
|    ep_rew_mean          | 344       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 78        |
|    time_elapsed         | 4048      |
|    total_timesteps      | 159744    |
| train/                  |           |
|    approx_kl            | 0.1345028 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.75     |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0419    |
|    n_updates            | 770       |
|    policy_gradient_loss | -0.0391   |
|    std                  | 0.821     |
|    value_loss           | 0.921     |
---------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0376     |
| reward_ctrl             | 0.0269     |
| reward_motion           | 0          |
| reward_orientation      | 0.0376     |
| reward_position         | 0.000175   |
| reward_rotation         | 0.0358     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0144     |
| rollout/                |            |
|    ep_len_mean          | 1.43e+03   |
|    ep_rew_mean          | 344        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 79         |
|    time_elapsed         | 4099       |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.07873431 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0535     |
|    n_updates            | 780        |
|    policy_gradient_loss | 0.0139     |
|    std                  | 0.82       |
|    value_loss           | 0.433      |
----------------------------------------
Num timesteps: 162000
Best mean reward: 339.48 - Last mean reward per episode: 351.48
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.037      |
| reward_ctrl             | 0.0273     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 1.5e-05    |
| reward_rotation         | 0.0357     |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 1.47e+03   |
|    ep_rew_mean          | 356        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 80         |
|    time_elapsed         | 4151       |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.20182681 |
|    clip_fraction        | 0.691      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.79      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0109     |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0325    |
|    std                  | 0.826      |
|    value_loss           | 0.313      |
----------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.037      |
| reward_ctrl             | 0.0273     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 1.5e-05    |
| reward_rotation         | 0.0357     |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 1.47e+03   |
|    ep_rew_mean          | 356        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 81         |
|    time_elapsed         | 4203       |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.12962252 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.79      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0409     |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.824      |
|    value_loss           | 0.825      |
----------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0368     |
| reward_ctrl             | 0.0272     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0356     |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 1.47e+03   |
|    ep_rew_mean          | 357        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 82         |
|    time_elapsed         | 4255       |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.20728564 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.78      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0362    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0345    |
|    std                  | 0.822      |
|    value_loss           | 0.251      |
----------------------------------------
Num timesteps: 168000
Best mean reward: 351.48 - Last mean reward per episode: 356.55
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0362     |
| reward_ctrl             | 0.0273     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0356     |
| reward_torque           | 0.0423     |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 358        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 83         |
|    time_elapsed         | 4308       |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.13221022 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0348    |
|    std                  | 0.817      |
|    value_loss           | 2.22       |
----------------------------------------
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0277     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0144     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 358        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 84         |
|    time_elapsed         | 4359       |
|    total_timesteps      | 172032     |
| train/                  |            |
|    approx_kl            | 0.12750939 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.72      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.128      |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0256    |
|    std                  | 0.817      |
|    value_loss           | 0.645      |
----------------------------------------
Num timesteps: 174000
Best mean reward: 356.55 - Last mean reward per episode: 359.38
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.195      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0284     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0425     |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 359        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 85         |
|    time_elapsed         | 4410       |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.14632533 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.71      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.052      |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0378    |
|    std                  | 0.817      |
|    value_loss           | 0.518      |
----------------------------------------
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0288     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0375     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 361        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 86         |
|    time_elapsed         | 4462       |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.20006955 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.71      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0232    |
|    std                  | 0.817      |
|    value_loss           | 0.864      |
----------------------------------------
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0284     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 361        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 87         |
|    time_elapsed         | 4513       |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.20399755 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.73      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.125      |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0247    |
|    std                  | 0.82       |
|    value_loss           | 0.282      |
----------------------------------------
Num timesteps: 180000
Best mean reward: 359.38 - Last mean reward per episode: 361.31
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0284     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0362     |
| reward_torque           | 0.0424     |
| reward_velocity         | 0.0127     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 361        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 88         |
|    time_elapsed         | 4564       |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.24987799 |
|    clip_fraction        | 0.688      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.72      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0656     |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0495    |
|    std                  | 0.814      |
|    value_loss           | 0.29       |
----------------------------------------
----------------------------------------
| reward                  | 0.194      |
| reward_contact          | 0.0358     |
| reward_ctrl             | 0.0298     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0354     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 364        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 89         |
|    time_elapsed         | 4615       |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.17269808 |
|    clip_fraction        | 0.623      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.66      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.104      |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0406    |
|    std                  | 0.809      |
|    value_loss           | 0.375      |
----------------------------------------
---------------------------------------
| reward                  | 0.194     |
| reward_contact          | 0.0358    |
| reward_ctrl             | 0.0298    |
| reward_motion           | 0         |
| reward_orientation      | 0.0374    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0354    |
| reward_torque           | 0.0427    |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 1.49e+03  |
|    ep_rew_mean          | 364       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 90        |
|    time_elapsed         | 4667      |
|    total_timesteps      | 184320    |
| train/                  |           |
|    approx_kl            | 0.1654163 |
|    clip_fraction        | 0.641     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.64     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.01     |
|    n_updates            | 890       |
|    policy_gradient_loss | -0.0259   |
|    std                  | 0.809     |
|    value_loss           | 0.333     |
---------------------------------------
Num timesteps: 186000
Best mean reward: 361.31 - Last mean reward per episode: 366.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.196      |
| reward_contact          | 0.0358     |
| reward_ctrl             | 0.0299     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0372     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 91         |
|    time_elapsed         | 4719       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.12708591 |
|    clip_fraction        | 0.634      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.61      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0297    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0478    |
|    std                  | 0.804      |
|    value_loss           | 0.304      |
----------------------------------------
----------------------------------------
| reward                  | 0.197      |
| reward_contact          | 0.0352     |
| reward_ctrl             | 0.0306     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0381     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 368        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 92         |
|    time_elapsed         | 4771       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.19589159 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.54      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.203      |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0441    |
|    std                  | 0.797      |
|    value_loss           | 0.411      |
----------------------------------------
----------------------------------------
| reward                  | 0.198      |
| reward_contact          | 0.0347     |
| reward_ctrl             | 0.0313     |
| reward_motion           | 0          |
| reward_orientation      | 0.0371     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0381     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 369        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 93         |
|    time_elapsed         | 4823       |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.18966928 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.49      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.512      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0333    |
|    std                  | 0.793      |
|    value_loss           | 1.34       |
----------------------------------------
Num timesteps: 192000
Best mean reward: 366.18 - Last mean reward per episode: 371.10
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.198     |
| reward_contact          | 0.0347    |
| reward_ctrl             | 0.0319    |
| reward_motion           | 0         |
| reward_orientation      | 0.0368    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0385    |
| reward_torque           | 0.0431    |
| reward_velocity         | 0.0131    |
| rollout/                |           |
|    ep_len_mean          | 1.49e+03  |
|    ep_rew_mean          | 371       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 94        |
|    time_elapsed         | 4875      |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.1520213 |
|    clip_fraction        | 0.654     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.48     |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.235     |
|    n_updates            | 930       |
|    policy_gradient_loss | -0.0223   |
|    std                  | 0.793     |
|    value_loss           | 0.507     |
---------------------------------------
----------------------------------------
| reward                  | 0.199      |
| reward_contact          | 0.0347     |
| reward_ctrl             | 0.0325     |
| reward_motion           | 0          |
| reward_orientation      | 0.0368     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0394     |
| reward_torque           | 0.0432     |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 371        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 95         |
|    time_elapsed         | 4928       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.13244873 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.46      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0179     |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0402    |
|    std                  | 0.79       |
|    value_loss           | 0.53       |
----------------------------------------
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0341     |
| reward_ctrl             | 0.0335     |
| reward_motion           | 0          |
| reward_orientation      | 0.0364     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0395     |
| reward_torque           | 0.0435     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 1.51e+03   |
|    ep_rew_mean          | 377        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 96         |
|    time_elapsed         | 4979       |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.22377485 |
|    clip_fraction        | 0.674      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.42      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00954   |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0348    |
|    std                  | 0.786      |
|    value_loss           | 0.353      |
----------------------------------------
Num timesteps: 198000
Best mean reward: 371.10 - Last mean reward per episode: 377.09
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0335     |
| reward_ctrl             | 0.0346     |
| reward_motion           | 0          |
| reward_orientation      | 0.0365     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0402     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 1.51e+03   |
|    ep_rew_mean          | 379        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 97         |
|    time_elapsed         | 5031       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.24962363 |
|    clip_fraction        | 0.668      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.4       |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0198     |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0551    |
|    std                  | 0.783      |
|    value_loss           | 0.439      |
----------------------------------------
----------------------------------------
| reward                  | 0.202      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0355     |
| reward_motion           | 0          |
| reward_orientation      | 0.0366     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0402     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 1.54e+03   |
|    ep_rew_mean          | 386        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 98         |
|    time_elapsed         | 5082       |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.12921196 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.36      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0127     |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0264    |
|    std                  | 0.781      |
|    value_loss           | 0.34       |
----------------------------------------
--------------------------------------
| reward                  | 0.202    |
| reward_contact          | 0.0329   |
| reward_ctrl             | 0.0355   |
| reward_motion           | 0        |
| reward_orientation      | 0.0366   |
| reward_position         | 0.000254 |
| reward_rotation         | 0.0402   |
| reward_torque           | 0.0438   |
| reward_velocity         | 0.0131   |
| rollout/                |          |
|    ep_len_mean          | 1.54e+03 |
|    ep_rew_mean          | 386      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 99       |
|    time_elapsed         | 5133     |
|    total_timesteps      | 202752   |
| train/                  |          |
|    approx_kl            | 0.137933 |
|    clip_fraction        | 0.589    |
|    clip_range           | 0.2      |
|    entropy_loss         | -9.34    |
|    explained_variance   | 0.966    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0087   |
|    n_updates            | 980      |
|    policy_gradient_loss | -0.0394  |
|    std                  | 0.779    |
|    value_loss           | 0.421    |
--------------------------------------
Num timesteps: 204000
Best mean reward: 377.09 - Last mean reward per episode: 386.24
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.204     |
| reward_contact          | 0.0335    |
| reward_ctrl             | 0.036     |
| reward_motion           | 0         |
| reward_orientation      | 0.0367    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0405    |
| reward_torque           | 0.044     |
| reward_velocity         | 0.0131    |
| rollout/                |           |
|    ep_len_mean          | 1.54e+03  |
|    ep_rew_mean          | 386       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 100       |
|    time_elapsed         | 5185      |
|    total_timesteps      | 204800    |
| train/                  |           |
|    approx_kl            | 0.2265077 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.33     |
|    explained_variance   | 0.969     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0205   |
|    n_updates            | 990       |
|    policy_gradient_loss | -0.0401   |
|    std                  | 0.777     |
|    value_loss           | 0.152     |
---------------------------------------
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0357     |
| reward_motion           | 0          |
| reward_orientation      | 0.0365     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0407     |
| reward_torque           | 0.0439     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 1.56e+03   |
|    ep_rew_mean          | 391        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 101        |
|    time_elapsed         | 5238       |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.12434891 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.3       |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0722     |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0443    |
|    std                  | 0.776      |
|    value_loss           | 0.382      |
----------------------------------------
---------------------------------------
| reward                  | 0.205     |
| reward_contact          | 0.0335    |
| reward_ctrl             | 0.0352    |
| reward_motion           | 0         |
| reward_orientation      | 0.0368    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0425    |
| reward_torque           | 0.0437    |
| reward_velocity         | 0.0133    |
| rollout/                |           |
|    ep_len_mean          | 1.56e+03  |
|    ep_rew_mean          | 394       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 102       |
|    time_elapsed         | 5289      |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.1920696 |
|    clip_fraction        | 0.677     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.29     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0383   |
|    n_updates            | 1010      |
|    policy_gradient_loss | -0.0398   |
|    std                  | 0.773     |
|    value_loss           | 0.235     |
---------------------------------------
Num timesteps: 210000
Best mean reward: 386.24 - Last mean reward per episode: 400.87
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0335     |
| reward_ctrl             | 0.036      |
| reward_motion           | 0          |
| reward_orientation      | 0.0366     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0428     |
| reward_torque           | 0.0439     |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 1.58e+03   |
|    ep_rew_mean          | 401        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 103        |
|    time_elapsed         | 5341       |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.19448107 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.26      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0333    |
|    std                  | 0.771      |
|    value_loss           | 0.753      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0367     |
| reward_motion           | 0          |
| reward_orientation      | 0.0364     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0436     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 1.59e+03   |
|    ep_rew_mean          | 404        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 104        |
|    time_elapsed         | 5392       |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.13741264 |
|    clip_fraction        | 0.625      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.24      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.293      |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0308    |
|    std                  | 0.77       |
|    value_loss           | 0.718      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0375     |
| reward_motion           | 0          |
| reward_orientation      | 0.0364     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0436     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 1.61e+03   |
|    ep_rew_mean          | 411        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 105        |
|    time_elapsed         | 5443       |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.15993753 |
|    clip_fraction        | 0.665      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.24      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00618   |
|    std                  | 0.77       |
|    value_loss           | 0.438      |
----------------------------------------
Num timesteps: 216000
Best mean reward: 400.87 - Last mean reward per episode: 410.90
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0329     |
| reward_ctrl             | 0.0378     |
| reward_motion           | 0          |
| reward_orientation      | 0.0362     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0436     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 1.63e+03   |
|    ep_rew_mean          | 415        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 106        |
|    time_elapsed         | 5494       |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.19415589 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.23      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.43       |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0279    |
|    std                  | 0.768      |
|    value_loss           | 0.981      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0325     |
| reward_ctrl             | 0.0384     |
| reward_motion           | 0          |
| reward_orientation      | 0.0359     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0438     |
| reward_torque           | 0.0447     |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 422        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 107        |
|    time_elapsed         | 5547       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.21755087 |
|    clip_fraction        | 0.69       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.2       |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00372   |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0349    |
|    std                  | 0.764      |
|    value_loss           | 0.3        |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0325     |
| reward_ctrl             | 0.0384     |
| reward_motion           | 0          |
| reward_orientation      | 0.0359     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0438     |
| reward_torque           | 0.0447     |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 422        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 108        |
|    time_elapsed         | 5597       |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.20945235 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.13      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0235     |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0227    |
|    std                  | 0.757      |
|    value_loss           | 0.475      |
----------------------------------------
Num timesteps: 222000
Best mean reward: 410.90 - Last mean reward per episode: 422.65
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0319     |
| reward_ctrl             | 0.0391     |
| reward_motion           | 0          |
| reward_orientation      | 0.0359     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0439     |
| reward_torque           | 0.0449     |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 423        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 109        |
|    time_elapsed         | 5649       |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.22999483 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.1       |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0441     |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.755      |
|    value_loss           | 0.317      |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0392     |
| reward_motion           | 0          |
| reward_orientation      | 0.0362     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0454     |
| reward_torque           | 0.0449     |
| reward_velocity         | 0.0127     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 425        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 110        |
|    time_elapsed         | 5700       |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.26053166 |
|    clip_fraction        | 0.723      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.09      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0509    |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0338    |
|    std                  | 0.755      |
|    value_loss           | 0.212      |
----------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0399     |
| reward_motion           | 0          |
| reward_orientation      | 0.0362     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0449     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 426        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 111        |
|    time_elapsed         | 5751       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.15827957 |
|    clip_fraction        | 0.659      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.08      |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.32       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0223    |
|    std                  | 0.756      |
|    value_loss           | 1.7        |
----------------------------------------
Num timesteps: 228000
Best mean reward: 422.65 - Last mean reward per episode: 425.52
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0408     |
| reward_motion           | 0          |
| reward_orientation      | 0.0359     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0448     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 426        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 112        |
|    time_elapsed         | 5803       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.23427147 |
|    clip_fraction        | 0.696      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.09      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0409    |
|    std                  | 0.754      |
|    value_loss           | 0.494      |
----------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0413     |
| reward_motion           | 0          |
| reward_orientation      | 0.0356     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.046      |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 427        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 113        |
|    time_elapsed         | 5855       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.10944781 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.07      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0294    |
|    std                  | 0.753      |
|    value_loss           | 0.809      |
----------------------------------------
---------------------------------------
| reward                  | 0.212     |
| reward_contact          | 0.0312    |
| reward_ctrl             | 0.0417    |
| reward_motion           | 0         |
| reward_orientation      | 0.0356    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0467    |
| reward_torque           | 0.0455    |
| reward_velocity         | 0.011     |
| rollout/                |           |
|    ep_len_mean          | 1.65e+03  |
|    ep_rew_mean          | 430       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 114       |
|    time_elapsed         | 5905      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 1.0694835 |
|    clip_fraction        | 0.774     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.08     |
|    explained_variance   | 0.243     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0609    |
|    n_updates            | 1130      |
|    policy_gradient_loss | -0.00124  |
|    std                  | 0.754     |
|    value_loss           | 1.66      |
---------------------------------------
Num timesteps: 234000
Best mean reward: 425.52 - Last mean reward per episode: 429.76
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0313     |
| reward_ctrl             | 0.0418     |
| reward_motion           | 0          |
| reward_orientation      | 0.0355     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0465     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 1.66e+03   |
|    ep_rew_mean          | 431        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 115        |
|    time_elapsed         | 5957       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.19754541 |
|    clip_fraction        | 0.659      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.07      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.216      |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.00672   |
|    std                  | 0.753      |
|    value_loss           | 0.595      |
----------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0308     |
| reward_ctrl             | 0.0428     |
| reward_motion           | 0          |
| reward_orientation      | 0.0355     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 1.68e+03   |
|    ep_rew_mean          | 436        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 116        |
|    time_elapsed         | 6008       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.19195582 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.06      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0156    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0145    |
|    std                  | 0.752      |
|    value_loss           | 0.35       |
----------------------------------------
---------------------------------------
| reward                  | 0.213     |
| reward_contact          | 0.0308    |
| reward_ctrl             | 0.0428    |
| reward_motion           | 0         |
| reward_orientation      | 0.0355    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.0475    |
| reward_torque           | 0.0458    |
| reward_velocity         | 0.0109    |
| rollout/                |           |
|    ep_len_mean          | 1.68e+03  |
|    ep_rew_mean          | 436       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 117       |
|    time_elapsed         | 6060      |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.1421188 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06     |
|    explained_variance   | 0.965     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.196     |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.011    |
|    std                  | 0.753     |
|    value_loss           | 0.476     |
---------------------------------------
Num timesteps: 240000
Best mean reward: 429.76 - Last mean reward per episode: 437.23
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0312     |
| reward_ctrl             | 0.0425     |
| reward_motion           | 0          |
| reward_orientation      | 0.0355     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0475     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 1.68e+03   |
|    ep_rew_mean          | 437        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 118        |
|    time_elapsed         | 6111       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.30142617 |
|    clip_fraction        | 0.714      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.08      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0106     |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0225    |
|    std                  | 0.755      |
|    value_loss           | 0.267      |
----------------------------------------
---------------------------------------
| reward                  | 0.214     |
| reward_contact          | 0.0313    |
| reward_ctrl             | 0.0424    |
| reward_motion           | 0         |
| reward_orientation      | 0.0355    |
| reward_position         | 0.000254  |
| reward_rotation         | 0.048     |
| reward_torque           | 0.0457    |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 1.68e+03  |
|    ep_rew_mean          | 437       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 119       |
|    time_elapsed         | 6163      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.2394833 |
|    clip_fraction        | 0.716     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.07     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00953  |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.0345   |
|    std                  | 0.752     |
|    value_loss           | 0.167     |
---------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0312     |
| reward_ctrl             | 0.0425     |
| reward_motion           | 0          |
| reward_orientation      | 0.0356     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0499     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.00987    |
| rollout/                |            |
|    ep_len_mean          | 1.69e+03   |
|    ep_rew_mean          | 440        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 120        |
|    time_elapsed         | 6215       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.21514705 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.04      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0808    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0478    |
|    std                  | 0.75       |
|    value_loss           | 0.255      |
----------------------------------------
Num timesteps: 246000
Best mean reward: 437.23 - Last mean reward per episode: 440.12
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0313     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0          |
| reward_orientation      | 0.0356     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0494     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.00987    |
| rollout/                |            |
|    ep_len_mean          | 1.69e+03   |
|    ep_rew_mean          | 440        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 121        |
|    time_elapsed         | 6267       |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.24180377 |
|    clip_fraction        | 0.709      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.01      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0504    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.034     |
|    std                  | 0.746      |
|    value_loss           | 0.317      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0318     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0          |
| reward_orientation      | 0.0356     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0507     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.00985    |
| rollout/                |            |
|    ep_len_mean          | 1.69e+03   |
|    ep_rew_mean          | 442        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 122        |
|    time_elapsed         | 6318       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.21888226 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.96      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0781    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0366    |
|    std                  | 0.743      |
|    value_loss           | 0.282      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0319     |
| reward_ctrl             | 0.0412     |
| reward_motion           | 0          |
| reward_orientation      | 0.0356     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0506     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.0099     |
| rollout/                |            |
|    ep_len_mean          | 1.69e+03   |
|    ep_rew_mean          | 442        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 123        |
|    time_elapsed         | 6370       |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.15719822 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.95      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0262    |
|    std                  | 0.742      |
|    value_loss           | 0.563      |
----------------------------------------
Num timesteps: 252000
Best mean reward: 440.12 - Last mean reward per episode: 442.37
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0315     |
| reward_ctrl             | 0.0415     |
| reward_motion           | 0          |
| reward_orientation      | 0.0354     |
| reward_position         | 0.000254   |
| reward_rotation         | 0.0504     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.0099     |
| rollout/                |            |
|    ep_len_mean          | 1.71e+03   |
|    ep_rew_mean          | 446        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 124        |
|    time_elapsed         | 6421       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.21932998 |
|    clip_fraction        | 0.684      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.93      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.145      |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0139    |
|    std                  | 0.739      |
|    value_loss           | 0.396      |
----------------------------------------
--------------------------------------
| reward                  | 0.215    |
| reward_contact          | 0.0309   |
| reward_ctrl             | 0.0419   |
| reward_motion           | 0        |
| reward_orientation      | 0.035    |
| reward_position         | 0.000246 |
| reward_rotation         | 0.0509   |
| reward_torque           | 0.0456   |
| reward_velocity         | 0.00997  |
| rollout/                |          |
|    ep_len_mean          | 1.73e+03 |
|    ep_rew_mean          | 453      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 125      |
|    time_elapsed         | 6473     |
|    total_timesteps      | 256000   |
| train/                  |          |
|    approx_kl            | 0.257547 |
|    clip_fraction        | 0.674    |
|    clip_range           | 0.2      |
|    entropy_loss         | -8.91    |
|    explained_variance   | 0.986    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0592   |
|    n_updates            | 1240     |
|    policy_gradient_loss | -0.0293  |
|    std                  | 0.739    |
|    value_loss           | 0.347    |
--------------------------------------
Num timesteps: 258000
Best mean reward: 442.37 - Last mean reward per episode: 452.95
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0309     |
| reward_ctrl             | 0.0419     |
| reward_motion           | 0          |
| reward_orientation      | 0.035      |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0509     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.00997    |
| rollout/                |            |
|    ep_len_mean          | 1.73e+03   |
|    ep_rew_mean          | 453        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 126        |
|    time_elapsed         | 6524       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.17045909 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.9       |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0721     |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0309    |
|    std                  | 0.737      |
|    value_loss           | 0.416      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0309     |
| reward_ctrl             | 0.0421     |
| reward_motion           | 0          |
| reward_orientation      | 0.0353     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0509     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.00994    |
| rollout/                |            |
|    ep_len_mean          | 1.73e+03   |
|    ep_rew_mean          | 453        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 127        |
|    time_elapsed         | 6576       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.33169693 |
|    clip_fraction        | 0.769      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0457    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0262    |
|    std                  | 0.737      |
|    value_loss           | 0.172      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0313     |
| reward_ctrl             | 0.0418     |
| reward_motion           | 0          |
| reward_orientation      | 0.0354     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0506     |
| reward_torque           | 0.0457     |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 1.73e+03   |
|    ep_rew_mean          | 455        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 128        |
|    time_elapsed         | 6628       |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.38119686 |
|    clip_fraction        | 0.74       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0587    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0488    |
|    std                  | 0.737      |
|    value_loss           | 0.353      |
----------------------------------------
Num timesteps: 264000
Best mean reward: 452.95 - Last mean reward per episode: 460.20
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.215     |
| reward_contact          | 0.0307    |
| reward_ctrl             | 0.0422    |
| reward_motion           | 0         |
| reward_orientation      | 0.035     |
| reward_position         | 0.000246  |
| reward_rotation         | 0.0504    |
| reward_torque           | 0.0459    |
| reward_velocity         | 0.0102    |
| rollout/                |           |
|    ep_len_mean          | 1.75e+03  |
|    ep_rew_mean          | 460       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 129       |
|    time_elapsed         | 6680      |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.1811291 |
|    clip_fraction        | 0.671     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.89     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.033    |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.0341   |
|    std                  | 0.738     |
|    value_loss           | 0.755     |
---------------------------------------
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0302     |
| reward_ctrl             | 0.0423     |
| reward_motion           | 0          |
| reward_orientation      | 0.035      |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0497     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 1.77e+03   |
|    ep_rew_mean          | 466        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 130        |
|    time_elapsed         | 6732       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.18558984 |
|    clip_fraction        | 0.625      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.87      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0124     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.735      |
|    value_loss           | 0.486      |
----------------------------------------
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0302     |
| reward_ctrl             | 0.0423     |
| reward_motion           | 0          |
| reward_orientation      | 0.0352     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.049      |
| reward_torque           | 0.046      |
| reward_velocity         | 0.00927    |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 471        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 131        |
|    time_elapsed         | 6783       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.18204874 |
|    clip_fraction        | 0.662      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.85      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.202      |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0295    |
|    std                  | 0.733      |
|    value_loss           | 0.331      |
----------------------------------------
Num timesteps: 270000
Best mean reward: 460.20 - Last mean reward per episode: 471.56
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0302     |
| reward_ctrl             | 0.0427     |
| reward_motion           | 0          |
| reward_orientation      | 0.0352     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0496     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.00926    |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 473        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 132        |
|    time_elapsed         | 6834       |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.15639636 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.82      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00628   |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0379    |
|    std                  | 0.73       |
|    value_loss           | 0.387      |
----------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0302     |
| reward_ctrl             | 0.0427     |
| reward_motion           | 0          |
| reward_orientation      | 0.0352     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0496     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.00926    |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 473        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 133        |
|    time_elapsed         | 6886       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.18370733 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.8       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0311    |
|    std                  | 0.729      |
|    value_loss           | 2.4        |
----------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0297     |
| reward_ctrl             | 0.0431     |
| reward_motion           | 0          |
| reward_orientation      | 0.0349     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0499     |
| reward_torque           | 0.0461     |
| reward_velocity         | 0.00944    |
| rollout/                |            |
|    ep_len_mean          | 1.8e+03    |
|    ep_rew_mean          | 476        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 134        |
|    time_elapsed         | 6937       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.37152505 |
|    clip_fraction        | 0.762      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.82      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0764    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0144    |
|    std                  | 0.731      |
|    value_loss           | 0.172      |
----------------------------------------
Num timesteps: 276000
Best mean reward: 471.56 - Last mean reward per episode: 481.41
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.216      |
| reward_contact          | 0.0297     |
| reward_ctrl             | 0.0438     |
| reward_motion           | 0          |
| reward_orientation      | 0.0351     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0517     |
| reward_torque           | 0.0463     |
| reward_velocity         | 0.00941    |
| rollout/                |            |
|    ep_len_mean          | 1.82e+03   |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 135        |
|    time_elapsed         | 6990       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.25910956 |
|    clip_fraction        | 0.662      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.81      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0123     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0513    |
|    std                  | 0.731      |
|    value_loss           | 0.709      |
----------------------------------------
----------------------------------------
| reward                  | 0.216      |
| reward_contact          | 0.0292     |
| reward_ctrl             | 0.044      |
| reward_motion           | 0          |
| reward_orientation      | 0.0351     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0514     |
| reward_torque           | 0.0463     |
| reward_velocity         | 0.00937    |
| rollout/                |            |
|    ep_len_mean          | 1.82e+03   |
|    ep_rew_mean          | 483        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 136        |
|    time_elapsed         | 7041       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.23202598 |
|    clip_fraction        | 0.654      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.78      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.459      |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0336    |
|    std                  | 0.726      |
|    value_loss           | 1.26       |
----------------------------------------
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0286     |
| reward_ctrl             | 0.0443     |
| reward_motion           | 0          |
| reward_orientation      | 0.0347     |
| reward_position         | 0.000246   |
| reward_rotation         | 0.0503     |
| reward_torque           | 0.0464     |
| reward_velocity         | 0.0094     |
| rollout/                |            |
|    ep_len_mean          | 1.84e+03   |
|    ep_rew_mean          | 489        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 137        |
|    time_elapsed         | 7092       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.14151141 |
|    clip_fraction        | 0.636      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.75      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.032      |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.00992   |
|    std                  | 0.726      |
|    value_loss           | 0.354      |
----------------------------------------
Num timesteps: 282000
Best mean reward: 481.41 - Last mean reward per episode: 494.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.214     |
| reward_contact          | 0.028     |
| reward_ctrl             | 0.0449    |
| reward_motion           | 0         |
| reward_orientation      | 0.0344    |
| reward_position         | 0.000242  |
| reward_rotation         | 0.0503    |
| reward_torque           | 0.0465    |
| reward_velocity         | 0.00944   |
| rollout/                |           |
|    ep_len_mean          | 1.86e+03  |
|    ep_rew_mean          | 495       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 138       |
|    time_elapsed         | 7144      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 5.1154366 |
|    clip_fraction        | 0.801     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.75     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.411     |
|    n_updates            | 1370      |
|    policy_gradient_loss | -0.0305   |
|    std                  | 0.724     |
|    value_loss           | 0.79      |
---------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0274     |
| reward_ctrl             | 0.0452     |
| reward_motion           | 0          |
| reward_orientation      | 0.0341     |
| reward_position         | 0.000242   |
| reward_rotation         | 0.0505     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.00909    |
| rollout/                |            |
|    ep_len_mean          | 1.87e+03   |
|    ep_rew_mean          | 497        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 139        |
|    time_elapsed         | 7196       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.28001767 |
|    clip_fraction        | 0.693      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.72      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0107    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0243    |
|    std                  | 0.722      |
|    value_loss           | 1.09       |
----------------------------------------
/home/shandilya/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File "ddpg.py", line 215, in <module>
    env = stable_baselines3.common.env_util.make_vec_env(
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 102, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/.local/lib/python3.8/site-packages/stable_baselines3/common/env_util.py", line 77, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/shandilya/Desktop/CNS/AntController/src/simulations/gym/ant.py", line 4, in <module>
    from gym.envs.mujoco import mujoco_env
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py", line 1, in <module>
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File "/home/shandilya/.local/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py", line 12, in <module>
    import mujoco_py
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/__init__.py", line 3, in <module>
    from mujoco_py.builder import cymj, ignore_mujoco_warnings, functions, MujocoException
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 503, in <module>
    cymj = load_cython_ext(mjpro_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 69, in load_cython_ext
    _ensure_set_env_var("LD_LIBRARY_PATH", lib_path)
  File "/home/shandilya/.local/lib/python3.8/site-packages/mujoco_py/builder.py", line 114, in _ensure_set_env_var
    raise Exception("\nMissing path to your environment variable. \n"
Exception: 
Missing path to your environment variable. 
Current values LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu
Please add following line to .bashrc:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 11:25:33.537442: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/shandilya/.mujoco/mjpro150/bin
2021-06-02 11:25:33.537501: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Creating window glfw
Using cpu device
Logging to rl/out_dir/models/exp68/PPO_6
---------------------------------
| reward             | 0.198    |
| reward_contact     | 0.06     |
| reward_ctrl        | 0.0317   |
| reward_motion      | 0        |
| reward_orientation | 0.0449   |
| reward_position    | 0        |
| reward_rotation    | 0.0108   |
| reward_torque      | 0.0466   |
| reward_velocity    | 0.00429  |
| rollout/           |          |
|    ep_len_mean     | 888      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 1        |
|    time_elapsed    | 30       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| reward                  | 0.172      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0223     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 0          |
| reward_rotation         | 0.00731    |
| reward_torque           | 0.042      |
| reward_velocity         | 0.00286    |
| rollout/                |            |
|    ep_len_mean          | 1.31e+03   |
|    ep_rew_mean          | 306        |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 2          |
|    time_elapsed         | 63         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.04508073 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.3      |
|    explained_variance   | -0.022     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.08       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0275    |
|    std                  | 0.997      |
|    value_loss           | 0.68       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 306.41
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.172       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0223      |
| reward_motion           | 0           |
| reward_orientation      | 0.0378      |
| reward_position         | 0           |
| reward_rotation         | 0.00731     |
| reward_torque           | 0.042       |
| reward_velocity         | 0.00286     |
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 306         |
| time/                   |             |
|    fps                  | 63          |
|    iterations           | 3           |
|    time_elapsed         | 96          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.042814773 |
|    clip_fraction        | 0.419       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.078       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0262     |
|    std                  | 0.994       |
|    value_loss           | 0.578       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.215       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0152      |
| reward_motion           | 0           |
| reward_orientation      | 0.0423      |
| reward_position         | 0           |
| reward_rotation         | 0.00461     |
| reward_torque           | 0.0389      |
| reward_velocity         | 0.0535      |
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | 346         |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 4           |
|    time_elapsed         | 138         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.044913888 |
|    clip_fraction        | 0.413       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0931      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0257     |
|    std                  | 0.993       |
|    value_loss           | 0.471       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.206       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0133      |
| reward_motion           | 0           |
| reward_orientation      | 0.0393      |
| reward_position         | 0           |
| reward_rotation         | 0.0112      |
| reward_torque           | 0.038       |
| reward_velocity         | 0.0446      |
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 5           |
|    time_elapsed         | 193         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.049197286 |
|    clip_fraction        | 0.389       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0253     |
|    std                  | 0.989       |
|    value_loss           | 1.03        |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 306.41 - Last mean reward per episode: 317.13
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.21        |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0129      |
| reward_motion           | 0           |
| reward_orientation      | 0.0414      |
| reward_position         | 0           |
| reward_rotation         | 0.0215      |
| reward_torque           | 0.0378      |
| reward_velocity         | 0.0368      |
| rollout/                |             |
|    ep_len_mean          | 1.32e+03    |
|    ep_rew_mean          | 317         |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 6           |
|    time_elapsed         | 244         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.061640833 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0791      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0361     |
|    std                  | 0.988       |
|    value_loss           | 1.23        |
-----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0171     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 0          |
| reward_rotation         | 0.0197     |
| reward_torque           | 0.0391     |
| reward_velocity         | 0.0331     |
| rollout/                |            |
|    ep_len_mean          | 1.34e+03   |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 48         |
|    iterations           | 7          |
|    time_elapsed         | 297        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.05138331 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.3      |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.035     |
|    std                  | 0.989      |
|    value_loss           | 1.04       |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0189     |
| reward_motion           | 0          |
| reward_orientation      | 0.0424     |
| reward_position         | 0          |
| reward_rotation         | 0.0181     |
| reward_torque           | 0.0399     |
| reward_velocity         | 0.0304     |
| rollout/                |            |
|    ep_len_mean          | 1.36e+03   |
|    ep_rew_mean          | 325        |
| time/                   |            |
|    fps                  | 47         |
|    iterations           | 8          |
|    time_elapsed         | 348        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.08814068 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0493     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0418    |
|    std                  | 0.984      |
|    value_loss           | 0.596      |
----------------------------------------
Num timesteps: 18000
Best mean reward: 317.13 - Last mean reward per episode: 320.61
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0197     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 2.08e-30   |
| reward_rotation         | 0.0278     |
| reward_torque           | 0.0404     |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 321        |
| time/                   |            |
|    fps                  | 45         |
|    iterations           | 9          |
|    time_elapsed         | 401        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08216295 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0168     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0314    |
|    std                  | 0.983      |
|    value_loss           | 0.36       |
----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0223     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.8e-30    |
| reward_rotation         | 0.0274     |
| reward_torque           | 0.0414     |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 1.28e+03   |
|    ep_rew_mean          | 304        |
| time/                   |            |
|    fps                  | 45         |
|    iterations           | 10         |
|    time_elapsed         | 452        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.06693913 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.16       |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0249    |
|    std                  | 0.98       |
|    value_loss           | 0.688      |
----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0238     |
| reward_motion           | 0          |
| reward_orientation      | 0.0417     |
| reward_position         | 1.69e-30   |
| reward_rotation         | 0.0257     |
| reward_torque           | 0.0419     |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 1.28e+03   |
|    ep_rew_mean          | 304        |
| time/                   |            |
|    fps                  | 44         |
|    iterations           | 11         |
|    time_elapsed         | 504        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05515586 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.691      |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0288    |
|    std                  | 0.978      |
|    value_loss           | 0.949      |
----------------------------------------
Num timesteps: 24000
Best mean reward: 320.61 - Last mean reward per episode: 312.22
-----------------------------------------
| reward                  | 0.214       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0244      |
| reward_motion           | 0           |
| reward_orientation      | 0.0405      |
| reward_position         | 1.59e-30    |
| reward_rotation         | 0.0242      |
| reward_torque           | 0.0423      |
| reward_velocity         | 0.023       |
| rollout/                |             |
|    ep_len_mean          | 1.34e+03    |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 44          |
|    iterations           | 12          |
|    time_elapsed         | 555         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.077141754 |
|    clip_fraction        | 0.475       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0194     |
|    std                  | 0.973       |
|    value_loss           | 0.681       |
-----------------------------------------
---------------------------------------
| reward                  | 0.21      |
| reward_contact          | 0.06      |
| reward_ctrl             | 0.0233    |
| reward_motion           | 0         |
| reward_orientation      | 0.0399    |
| reward_position         | 1.5e-30   |
| reward_rotation         | 0.0228    |
| reward_torque           | 0.0418    |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 1.37e+03  |
|    ep_rew_mean          | 320       |
| time/                   |           |
|    fps                  | 43        |
|    iterations           | 13        |
|    time_elapsed         | 607       |
|    total_timesteps      | 26624     |
| train/                  |           |
|    approx_kl            | 0.0688879 |
|    clip_fraction        | 0.499     |
|    clip_range           | 0.2       |
|    entropy_loss         | -11.1     |
|    explained_variance   | 0.888     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0128    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0334   |
|    std                  | 0.973     |
|    value_loss           | 0.419     |
---------------------------------------
-----------------------------------------
| reward                  | 0.202       |
| reward_contact          | 0.0571      |
| reward_ctrl             | 0.0223      |
| reward_motion           | 0           |
| reward_orientation      | 0.039       |
| reward_position         | 1.42e-30    |
| reward_rotation         | 0.0217      |
| reward_torque           | 0.0414      |
| reward_velocity         | 0.0207      |
| rollout/                |             |
|    ep_len_mean          | 1.42e+03    |
|    ep_rew_mean          | 327         |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 14          |
|    time_elapsed         | 658         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.050110154 |
|    clip_fraction        | 0.424       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.301       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.032      |
|    std                  | 0.97        |
|    value_loss           | 0.473       |
-----------------------------------------
Num timesteps: 30000
Best mean reward: 320.61 - Last mean reward per episode: 335.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.2        |
| reward_contact          | 0.0543     |
| reward_ctrl             | 0.0227     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 1.35e-30   |
| reward_rotation         | 0.0223     |
| reward_torque           | 0.0416     |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 1.47e+03   |
|    ep_rew_mean          | 336        |
| time/                   |            |
|    fps                  | 43         |
|    iterations           | 15         |
|    time_elapsed         | 710        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06920323 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0472     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0175    |
|    std                  | 0.967      |
|    value_loss           | 0.602      |
----------------------------------------
-----------------------------------------
| reward                  | 0.204       |
| reward_contact          | 0.0548      |
| reward_ctrl             | 0.023       |
| reward_motion           | 0           |
| reward_orientation      | 0.0402      |
| reward_position         | 1.23e-30    |
| reward_rotation         | 0.026       |
| reward_torque           | 0.0417      |
| reward_velocity         | 0.0179      |
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 336         |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 16          |
|    time_elapsed         | 762         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.075676665 |
|    clip_fraction        | 0.465       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0279      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0319     |
|    std                  | 0.966       |
|    value_loss           | 0.499       |
-----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0552     |
| reward_ctrl             | 0.0237     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 1.12e-30   |
| reward_rotation         | 0.0313     |
| reward_torque           | 0.042      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 1.44e+03   |
|    ep_rew_mean          | 333        |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 17         |
|    time_elapsed         | 813        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07997927 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0244     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0437    |
|    std                  | 0.962      |
|    value_loss           | 0.444      |
----------------------------------------
Num timesteps: 36000
Best mean reward: 335.99 - Last mean reward per episode: 333.05
-----------------------------------------
| reward                  | 0.208       |
| reward_contact          | 0.0552      |
| reward_ctrl             | 0.0237      |
| reward_motion           | 0           |
| reward_orientation      | 0.0394      |
| reward_position         | 1.12e-30    |
| reward_rotation         | 0.0313      |
| reward_torque           | 0.042       |
| reward_velocity         | 0.0165      |
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 333         |
| time/                   |             |
|    fps                  | 42          |
|    iterations           | 18          |
|    time_elapsed         | 864         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.090675764 |
|    clip_fraction        | 0.506       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0415      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0423     |
|    std                  | 0.956       |
|    value_loss           | 0.453       |
-----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0554     |
| reward_ctrl             | 0.023      |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 1.08e-30   |
| reward_rotation         | 0.032      |
| reward_torque           | 0.0417     |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 340        |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 19         |
|    time_elapsed         | 916        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.08225684 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.057      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0194    |
|    std                  | 0.956      |
|    value_loss           | 0.306      |
----------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0558     |
| reward_ctrl             | 0.0239     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 8.91e-21   |
| reward_rotation         | 0.03       |
| reward_torque           | 0.0422     |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 1.46e+03   |
|    ep_rew_mean          | 339        |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 20         |
|    time_elapsed         | 968        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.07595988 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0797     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0258    |
|    std                  | 0.953      |
|    value_loss           | 0.634      |
----------------------------------------
Num timesteps: 42000
Best mean reward: 335.99 - Last mean reward per episode: 348.33
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.056      |
| reward_ctrl             | 0.0269     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 8.29e-21   |
| reward_rotation         | 0.0279     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0286     |
| rollout/                |            |
|    ep_len_mean          | 1.45e+03   |
|    ep_rew_mean          | 340        |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 21         |
|    time_elapsed         | 1019       |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.07521415 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0788     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.032     |
|    std                  | 0.954      |
|    value_loss           | 0.669      |
----------------------------------------
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0562     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 8.02e-21   |
| reward_rotation         | 0.0271     |
| reward_torque           | 0.0426     |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 1.48e+03   |
|    ep_rew_mean          | 346        |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 22         |
|    time_elapsed         | 1070       |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.09366803 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0626     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0351    |
|    std                  | 0.95       |
|    value_loss           | 0.578      |
----------------------------------------
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0563     |
| reward_ctrl             | 0.0262     |
| reward_motion           | 0          |
| reward_orientation      | 0.0421     |
| reward_position         | 7.76e-21   |
| reward_rotation         | 0.027      |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0268     |
| rollout/                |            |
|    ep_len_mean          | 1.51e+03   |
|    ep_rew_mean          | 352        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 23         |
|    time_elapsed         | 1122       |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.07098341 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00911    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0311    |
|    std                  | 0.947      |
|    value_loss           | 0.528      |
----------------------------------------
Num timesteps: 48000
Best mean reward: 348.33 - Last mean reward per episode: 351.70
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.22      |
| reward_contact          | 0.0564    |
| reward_ctrl             | 0.0261    |
| reward_motion           | 0         |
| reward_orientation      | 0.0426    |
| reward_position         | 7.52e-21  |
| reward_rotation         | 0.0264    |
| reward_torque           | 0.0428    |
| reward_velocity         | 0.0261    |
| rollout/                |           |
|    ep_len_mean          | 1.53e+03  |
|    ep_rew_mean          | 357       |
| time/                   |           |
|    fps                  | 41        |
|    iterations           | 24        |
|    time_elapsed         | 1172      |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.1247171 |
|    clip_fraction        | 0.575     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.9     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0286   |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0373   |
|    std                  | 0.943     |
|    value_loss           | 0.311     |
---------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0564     |
| reward_ctrl             | 0.0261     |
| reward_motion           | 0          |
| reward_orientation      | 0.0426     |
| reward_position         | 7.52e-21   |
| reward_rotation         | 0.0264     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 1.53e+03   |
|    ep_rew_mean          | 357        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 25         |
|    time_elapsed         | 1224       |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.07439926 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.159      |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0275    |
|    std                  | 0.939      |
|    value_loss           | 0.343      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0565     |
| reward_ctrl             | 0.0264     |
| reward_motion           | 0          |
| reward_orientation      | 0.0432     |
| reward_position         | 7.29e-21   |
| reward_rotation         | 0.0256     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.0254     |
| rollout/                |            |
|    ep_len_mean          | 1.55e+03   |
|    ep_rew_mean          | 361        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 26         |
|    time_elapsed         | 1275       |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.09579822 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0146    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0317    |
|    std                  | 0.933      |
|    value_loss           | 0.286      |
----------------------------------------
Num timesteps: 54000
Best mean reward: 351.70 - Last mean reward per episode: 364.39
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.225       |
| reward_contact          | 0.0567      |
| reward_ctrl             | 0.0274      |
| reward_motion           | 0           |
| reward_orientation      | 0.0426      |
| reward_position         | 6.87e-21    |
| reward_rotation         | 0.0256      |
| reward_torque           | 0.043       |
| reward_velocity         | 0.0295      |
| rollout/                |             |
|    ep_len_mean          | 1.53e+03    |
|    ep_rew_mean          | 355         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 27          |
|    time_elapsed         | 1326        |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.092619136 |
|    clip_fraction        | 0.537       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0335      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0329     |
|    std                  | 0.93        |
|    value_loss           | 0.399       |
-----------------------------------------
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0568     |
| reward_ctrl             | 0.0272     |
| reward_motion           | 0          |
| reward_orientation      | 0.0426     |
| reward_position         | 6.68e-21   |
| reward_rotation         | 0.0249     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0289     |
| rollout/                |            |
|    ep_len_mean          | 1.55e+03   |
|    ep_rew_mean          | 358        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 28         |
|    time_elapsed         | 1377       |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08760015 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.211      |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0245    |
|    std                  | 0.924      |
|    value_loss           | 1.17       |
----------------------------------------
---------------------------------------
| reward                  | 0.221     |
| reward_contact          | 0.0569    |
| reward_ctrl             | 0.0265    |
| reward_motion           | 0         |
| reward_orientation      | 0.0425    |
| reward_position         | 6.5e-21   |
| reward_rotation         | 0.0243    |
| reward_torque           | 0.0426    |
| reward_velocity         | 0.0282    |
| rollout/                |           |
|    ep_len_mean          | 1.55e+03  |
|    ep_rew_mean          | 358       |
| time/                   |           |
|    fps                  | 41        |
|    iterations           | 29        |
|    time_elapsed         | 1429      |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0855409 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.7     |
|    explained_variance   | 0.822     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.199     |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0301   |
|    std                  | 0.92      |
|    value_loss           | 0.445     |
---------------------------------------
Num timesteps: 60000
Best mean reward: 364.39 - Last mean reward per episode: 363.13
-----------------------------------------
| reward                  | 0.22        |
| reward_contact          | 0.057       |
| reward_ctrl             | 0.0269      |
| reward_motion           | 0           |
| reward_orientation      | 0.042       |
| reward_position         | 6.33e-21    |
| reward_rotation         | 0.024       |
| reward_torque           | 0.0428      |
| reward_velocity         | 0.0275      |
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 363         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 30          |
|    time_elapsed         | 1480        |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.086472556 |
|    clip_fraction        | 0.554       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0332      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0173     |
|    std                  | 0.918       |
|    value_loss           | 0.29        |
-----------------------------------------
---------------------------------------
| reward                  | 0.218     |
| reward_contact          | 0.0557    |
| reward_ctrl             | 0.0272    |
| reward_motion           | 0         |
| reward_orientation      | 0.0418    |
| reward_position         | 6.01e-21  |
| reward_rotation         | 0.0241    |
| reward_torque           | 0.0428    |
| reward_velocity         | 0.0262    |
| rollout/                |           |
|    ep_len_mean          | 1.57e+03  |
|    ep_rew_mean          | 362       |
| time/                   |           |
|    fps                  | 41        |
|    iterations           | 31        |
|    time_elapsed         | 1531      |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.0754963 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.6     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0122   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0242   |
|    std                  | 0.915     |
|    value_loss           | 0.341     |
---------------------------------------
-----------------------------------------
| reward                  | 0.214       |
| reward_contact          | 0.0544      |
| reward_ctrl             | 0.0264      |
| reward_motion           | 0           |
| reward_orientation      | 0.0421      |
| reward_position         | 5.73e-21    |
| reward_rotation         | 0.0235      |
| reward_torque           | 0.0426      |
| reward_velocity         | 0.0249      |
| rollout/                |             |
|    ep_len_mean          | 1.55e+03    |
|    ep_rew_mean          | 358         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 32          |
|    time_elapsed         | 1582        |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.059360657 |
|    clip_fraction        | 0.481       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.305       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0353     |
|    std                  | 0.915       |
|    value_loss           | 0.658       |
-----------------------------------------
Num timesteps: 66000
Best mean reward: 364.39 - Last mean reward per episode: 357.78
-----------------------------------------
| reward                  | 0.214       |
| reward_contact          | 0.0532      |
| reward_ctrl             | 0.0265      |
| reward_motion           | 0           |
| reward_orientation      | 0.0425      |
| reward_position         | 5.59e-21    |
| reward_rotation         | 0.025       |
| reward_torque           | 0.0427      |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 361         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 33          |
|    time_elapsed         | 1633        |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.090225965 |
|    clip_fraction        | 0.487       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0639      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0345     |
|    std                  | 0.912       |
|    value_loss           | 0.649       |
-----------------------------------------
-----------------------------------------
| reward                  | 0.214       |
| reward_contact          | 0.0532      |
| reward_ctrl             | 0.0265      |
| reward_motion           | 0           |
| reward_orientation      | 0.0425      |
| reward_position         | 5.59e-21    |
| reward_rotation         | 0.025       |
| reward_torque           | 0.0427      |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 361         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 34          |
|    time_elapsed         | 1685        |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.111768335 |
|    clip_fraction        | 0.538       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0578      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.031      |
|    std                  | 0.904       |
|    value_loss           | 0.55        |
-----------------------------------------
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.052      |
| reward_ctrl             | 0.0285     |
| reward_motion           | 0          |
| reward_orientation      | 0.042      |
| reward_position         | 5.47e-21   |
| reward_rotation         | 0.0244     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.0238     |
| rollout/                |            |
|    ep_len_mean          | 1.59e+03   |
|    ep_rew_mean          | 364        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 35         |
|    time_elapsed         | 1736       |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.13455233 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00733    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.031     |
|    std                  | 0.897      |
|    value_loss           | 0.468      |
----------------------------------------
Num timesteps: 72000
Best mean reward: 364.39 - Last mean reward per episode: 364.19
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0509     |
| reward_ctrl             | 0.0279     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 5.35e-21   |
| reward_rotation         | 0.0239     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 1.6e+03    |
|    ep_rew_mean          | 367        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 36         |
|    time_elapsed         | 1787       |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.10662981 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0121    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.026     |
|    std                  | 0.897      |
|    value_loss           | 0.331      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0503     |
| reward_ctrl             | 0.0275     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 3.85e-10   |
| reward_rotation         | 0.0231     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 1.59e+03   |
|    ep_rew_mean          | 365        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 37         |
|    time_elapsed         | 1839       |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.10804332 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0304    |
|    std                  | 0.896      |
|    value_loss           | 0.416      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0505     |
| reward_ctrl             | 0.0272     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 3.77e-10   |
| reward_rotation         | 0.0227     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0232     |
| rollout/                |            |
|    ep_len_mean          | 1.6e+03    |
|    ep_rew_mean          | 370        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 38         |
|    time_elapsed         | 1890       |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.10768244 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0495     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.037     |
|    std                  | 0.898      |
|    value_loss           | 1.02       |
----------------------------------------
Num timesteps: 78000
Best mean reward: 364.39 - Last mean reward per episode: 369.56
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0499     |
| reward_ctrl             | 0.0271     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 3.7e-10    |
| reward_rotation         | 0.0223     |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 1.62e+03   |
|    ep_rew_mean          | 374        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 39         |
|    time_elapsed         | 1941       |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.13734362 |
|    clip_fraction        | 0.619      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0436     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.02      |
|    std                  | 0.897      |
|    value_loss           | 0.522      |
----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0492     |
| reward_ctrl             | 0.0269     |
| reward_motion           | 0          |
| reward_orientation      | 0.0403     |
| reward_position         | 3.62e-10   |
| reward_rotation         | 0.022      |
| reward_torque           | 0.0428     |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 1.63e+03   |
|    ep_rew_mean          | 378        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 40         |
|    time_elapsed         | 1991       |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.07483014 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0639     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0364    |
|    std                  | 0.897      |
|    value_loss           | 0.769      |
----------------------------------------
-----------------------------------------
| reward                  | 0.203       |
| reward_contact          | 0.0494      |
| reward_ctrl             | 0.0265      |
| reward_motion           | 0           |
| reward_orientation      | 0.04        |
| reward_position         | 3.55e-10    |
| reward_rotation         | 0.0216      |
| reward_torque           | 0.0427      |
| reward_velocity         | 0.0232      |
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 381         |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 41          |
|    time_elapsed         | 2043        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.115370855 |
|    clip_fraction        | 0.55        |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0756      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0358     |
|    std                  | 0.895       |
|    value_loss           | 0.427       |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 369.56 - Last mean reward per episode: 381.31
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0494     |
| reward_ctrl             | 0.0265     |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 3.55e-10   |
| reward_rotation         | 0.0216     |
| reward_torque           | 0.0427     |
| reward_velocity         | 0.0232     |
| rollout/                |            |
|    ep_len_mean          | 1.64e+03   |
|    ep_rew_mean          | 381        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 42         |
|    time_elapsed         | 2094       |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.08578929 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0583     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0376    |
|    std                  | 0.892      |
|    value_loss           | 0.441      |
----------------------------------------
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0487     |
| reward_ctrl             | 0.0269     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 3.42e-10   |
| reward_rotation         | 0.0242     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 385        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 43         |
|    time_elapsed         | 2146       |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10396766 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00436   |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.05      |
|    std                  | 0.885      |
|    value_loss           | 0.322      |
----------------------------------------
Num timesteps: 90000
Best mean reward: 381.31 - Last mean reward per episode: 389.09
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0489     |
| reward_ctrl             | 0.027      |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 3.35e-10   |
| reward_rotation         | 0.0238     |
| reward_torque           | 0.0429     |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 1.66e+03   |
|    ep_rew_mean          | 389        |
| time/                   |            |
|    fps                  | 41         |
|    iterations           | 44         |
|    time_elapsed         | 2197       |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.07825066 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.134      |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0439    |
|    std                  | 0.883      |
|    value_loss           | 0.681      |
----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.0491     |
| reward_ctrl             | 0.0271     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 3.29e-10   |
| reward_rotation         | 0.0233     |
| reward_torque           | 0.043      |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 1.67e+03   |
|    ep_rew_mean          | 393        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 45         |
|    time_elapsed         | 2249       |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.12501378 |
|    clip_fraction        | 0.603      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0365     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0229    |
|    std                  | 0.881      |
|    value_loss           | 0.874      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0493     |
| reward_ctrl             | 0.0275     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 3.23e-10   |
| reward_rotation         | 0.0244     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 1.68e+03   |
|    ep_rew_mean          | 397        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 46         |
|    time_elapsed         | 2301       |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.07242291 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0295    |
|    std                  | 0.879      |
|    value_loss           | 0.523      |
----------------------------------------
Num timesteps: 96000
Best mean reward: 389.09 - Last mean reward per episode: 396.70
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0493     |
| reward_ctrl             | 0.0275     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 3.23e-10   |
| reward_rotation         | 0.0244     |
| reward_torque           | 0.0431     |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 1.68e+03   |
|    ep_rew_mean          | 397        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 47         |
|    time_elapsed         | 2353       |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.09397465 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.104      |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0194    |
|    std                  | 0.876      |
|    value_loss           | 1.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0486     |
| reward_ctrl             | 0.0274     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 3.12e-10   |
| reward_rotation         | 0.0242     |
| reward_torque           | 0.0432     |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 1.69e+03   |
|    ep_rew_mean          | 402        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 48         |
|    time_elapsed         | 2405       |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.18187541 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.07       |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.051     |
|    std                  | 0.869      |
|    value_loss           | 0.233      |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0488     |
| reward_ctrl             | 0.0282     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 3.07e-10   |
| reward_rotation         | 0.0262     |
| reward_torque           | 0.0434     |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 1.67e+03   |
|    ep_rew_mean          | 397        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 49         |
|    time_elapsed         | 2457       |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.08992925 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0998     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.047     |
|    std                  | 0.867      |
|    value_loss           | 1.33       |
----------------------------------------
Num timesteps: 102000
Best mean reward: 396.70 - Last mean reward per episode: 399.91
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.049      |
| reward_ctrl             | 0.0281     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 3.02e-10   |
| reward_rotation         | 0.0258     |
| reward_torque           | 0.0434     |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 1.68e+03   |
|    ep_rew_mean          | 400        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 50         |
|    time_elapsed         | 2509       |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.12192725 |
|    clip_fraction        | 0.598      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0315    |
|    std                  | 0.865      |
|    value_loss           | 1.63       |
----------------------------------------
-----------------------------------------
| reward                  | 0.209       |
| reward_contact          | 0.0483      |
| reward_ctrl             | 0.0285      |
| reward_motion           | 0           |
| reward_orientation      | 0.0399      |
| reward_position         | 2.97e-10    |
| reward_rotation         | 0.0273      |
| reward_torque           | 0.0435      |
| reward_velocity         | 0.0219      |
| rollout/                |             |
|    ep_len_mean          | 1.69e+03    |
|    ep_rew_mean          | 403         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 51          |
|    time_elapsed         | 2561        |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.102424316 |
|    clip_fraction        | 0.566       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0282     |
|    std                  | 0.862       |
|    value_loss           | 0.36        |
-----------------------------------------
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0479     |
| reward_ctrl             | 0.0294     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 2.92e-10   |
| reward_rotation         | 0.0286     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 1.7e+03    |
|    ep_rew_mean          | 406        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 52         |
|    time_elapsed         | 2613       |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.10370564 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.059      |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0221    |
|    std                  | 0.86       |
|    value_loss           | 0.303      |
----------------------------------------
Num timesteps: 108000
Best mean reward: 399.91 - Last mean reward per episode: 408.65
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0472     |
| reward_ctrl             | 0.0299     |
| reward_motion           | 0          |
| reward_orientation      | 0.0394     |
| reward_position         | 2.87e-10   |
| reward_rotation         | 0.0282     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0213     |
| rollout/                |            |
|    ep_len_mean          | 1.71e+03   |
|    ep_rew_mean          | 409        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 53         |
|    time_elapsed         | 2665       |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.08019071 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0223     |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0365    |
|    std                  | 0.859      |
|    value_loss           | 0.325      |
----------------------------------------
---------------------------------------
| reward                  | 0.211     |
| reward_contact          | 0.0464    |
| reward_ctrl             | 0.0305    |
| reward_motion           | 0         |
| reward_orientation      | 0.0391    |
| reward_position         | 2.83e-10  |
| reward_rotation         | 0.0277    |
| reward_torque           | 0.0439    |
| reward_velocity         | 0.0233    |
| rollout/                |           |
|    ep_len_mean          | 1.72e+03  |
|    ep_rew_mean          | 413       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 54        |
|    time_elapsed         | 2717      |
|    total_timesteps      | 110592    |
| train/                  |           |
|    approx_kl            | 2.6474006 |
|    clip_fraction        | 0.737     |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.1     |
|    explained_variance   | 0.688     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.222     |
|    n_updates            | 530       |
|    policy_gradient_loss | -0.00755  |
|    std                  | 0.856     |
|    value_loss           | 0.881     |
---------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0467     |
| reward_ctrl             | 0.0303     |
| reward_motion           | 0          |
| reward_orientation      | 0.039      |
| reward_position         | 2.79e-10   |
| reward_rotation         | 0.0273     |
| reward_torque           | 0.0439     |
| reward_velocity         | 0.0229     |
| rollout/                |            |
|    ep_len_mean          | 1.72e+03   |
|    ep_rew_mean          | 413        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 55         |
|    time_elapsed         | 2768       |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.20237866 |
|    clip_fraction        | 0.612      |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0111     |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.052     |
|    std                  | 0.848      |
|    value_loss           | 0.478      |
----------------------------------------
Num timesteps: 114000
Best mean reward: 408.65 - Last mean reward per episode: 413.09
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0462    |
| reward_ctrl             | 0.03      |
| reward_motion           | 0         |
| reward_orientation      | 0.0388    |
| reward_position         | 2.74e-10  |
| reward_rotation         | 0.0271    |
| reward_torque           | 0.0439    |
| reward_velocity         | 0.0226    |
| rollout/                |           |
|    ep_len_mean          | 1.73e+03  |
|    ep_rew_mean          | 416       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 56        |
|    time_elapsed         | 2820      |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.1913923 |
|    clip_fraction        | 0.624     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.99     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0132    |
|    n_updates            | 550       |
|    policy_gradient_loss | -0.0482   |
|    std                  | 0.839     |
|    value_loss           | 0.337     |
---------------------------------------
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0464    |
| reward_ctrl             | 0.0297    |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 2.7e-10   |
| reward_rotation         | 0.0281    |
| reward_torque           | 0.0438    |
| reward_velocity         | 0.0223    |
| rollout/                |           |
|    ep_len_mean          | 1.74e+03  |
|    ep_rew_mean          | 418       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 57        |
|    time_elapsed         | 2872      |
|    total_timesteps      | 116736    |
| train/                  |           |
|    approx_kl            | 0.1389508 |
|    clip_fraction        | 0.586     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.91     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.143     |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.0304   |
|    std                  | 0.833     |
|    value_loss           | 0.364     |
---------------------------------------
-----------------------------------------
| reward                  | 0.21        |
| reward_contact          | 0.0466      |
| reward_ctrl             | 0.031       |
| reward_motion           | 0           |
| reward_orientation      | 0.0383      |
| reward_position         | 2.66e-10    |
| reward_rotation         | 0.0277      |
| reward_torque           | 0.044       |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 1.72e+03    |
|    ep_rew_mean          | 414         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 58          |
|    time_elapsed         | 2925        |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.112464756 |
|    clip_fraction        | 0.586       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.87       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.096       |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0359     |
|    std                  | 0.829       |
|    value_loss           | 0.416       |
-----------------------------------------
Num timesteps: 120000
Best mean reward: 413.09 - Last mean reward per episode: 417.92
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0459    |
| reward_ctrl             | 0.0307    |
| reward_motion           | 0         |
| reward_orientation      | 0.0381    |
| reward_position         | 2.62e-10  |
| reward_rotation         | 0.0274    |
| reward_torque           | 0.0439    |
| reward_velocity         | 0.0233    |
| rollout/                |           |
|    ep_len_mean          | 1.73e+03  |
|    ep_rew_mean          | 418       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 59        |
|    time_elapsed         | 2976      |
|    total_timesteps      | 120832    |
| train/                  |           |
|    approx_kl            | 0.1494875 |
|    clip_fraction        | 0.611     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.81     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.068     |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.038    |
|    std                  | 0.822     |
|    value_loss           | 0.417     |
---------------------------------------
-----------------------------------------
| reward                  | 0.206       |
| reward_contact          | 0.0455      |
| reward_ctrl             | 0.03        |
| reward_motion           | 0           |
| reward_orientation      | 0.038       |
| reward_position         | 2.55e-10    |
| reward_rotation         | 0.0267      |
| reward_torque           | 0.0436      |
| reward_velocity         | 0.0227      |
| rollout/                |             |
|    ep_len_mean          | 1.72e+03    |
|    ep_rew_mean          | 418         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 60          |
|    time_elapsed         | 3029        |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.122857355 |
|    clip_fraction        | 0.617       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.75       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0203     |
|    std                  | 0.818       |
|    value_loss           | 0.499       |
-----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0448     |
| reward_ctrl             | 0.0307     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 2.51e-10   |
| reward_rotation         | 0.0277     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 1.73e+03   |
|    ep_rew_mean          | 422        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 61         |
|    time_elapsed         | 3079       |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.14839168 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.74      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0182    |
|    std                  | 0.82       |
|    value_loss           | 2.04       |
----------------------------------------
Num timesteps: 126000
Best mean reward: 417.92 - Last mean reward per episode: 421.67
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0448     |
| reward_ctrl             | 0.0307     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 2.51e-10   |
| reward_rotation         | 0.0277     |
| reward_torque           | 0.0437     |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 1.74e+03   |
|    ep_rew_mean          | 425        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 62         |
|    time_elapsed         | 3130       |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.13780713 |
|    clip_fraction        | 0.647      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.76      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.43       |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0112    |
|    std                  | 0.819      |
|    value_loss           | 0.864      |
----------------------------------------
-----------------------------------------
| reward                  | 0.207       |
| reward_contact          | 0.0442      |
| reward_ctrl             | 0.0307      |
| reward_motion           | 0           |
| reward_orientation      | 0.0386      |
| reward_position         | 2.48e-10    |
| reward_rotation         | 0.0279      |
| reward_torque           | 0.0438      |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 1.74e+03    |
|    ep_rew_mean          | 425         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 63          |
|    time_elapsed         | 3181        |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.104607545 |
|    clip_fraction        | 0.533       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.72       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0451     |
|    std                  | 0.815       |
|    value_loss           | 1.41        |
-----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0436     |
| reward_ctrl             | 0.0308     |
| reward_motion           | 0          |
| reward_orientation      | 0.0387     |
| reward_position         | 2.45e-10   |
| reward_rotation         | 0.0276     |
| reward_torque           | 0.0438     |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 1.75e+03   |
|    ep_rew_mean          | 427        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 64         |
|    time_elapsed         | 3233       |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.17329407 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.71      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0372     |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0409    |
|    std                  | 0.814      |
|    value_loss           | 0.617      |
----------------------------------------
Num timesteps: 132000
Best mean reward: 421.67 - Last mean reward per episode: 430.95
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0433     |
| reward_ctrl             | 0.0309     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 2.38e-10   |
| reward_rotation         | 0.0285     |
| reward_torque           | 0.0439     |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 1.74e+03   |
|    ep_rew_mean          | 427        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 65         |
|    time_elapsed         | 3285       |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.20899165 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.68      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00947   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.031     |
|    std                  | 0.811      |
|    value_loss           | 0.286      |
----------------------------------------
---------------------------------------
| reward                  | 0.207     |
| reward_contact          | 0.0427    |
| reward_ctrl             | 0.0314    |
| reward_motion           | 0         |
| reward_orientation      | 0.0388    |
| reward_position         | 2.35e-10  |
| reward_rotation         | 0.0283    |
| reward_torque           | 0.044     |
| reward_velocity         | 0.0223    |
| rollout/                |           |
|    ep_len_mean          | 1.74e+03  |
|    ep_rew_mean          | 430       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 66        |
|    time_elapsed         | 3337      |
|    total_timesteps      | 135168    |
| train/                  |           |
|    approx_kl            | 0.0789882 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.65     |
|    explained_variance   | 0.853     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.144     |
|    n_updates            | 650       |
|    policy_gradient_loss | -0.045    |
|    std                  | 0.808     |
|    value_loss           | 1.07      |
---------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0423     |
| reward_ctrl             | 0.0315     |
| reward_motion           | 0          |
| reward_orientation      | 0.039      |
| reward_position         | 2.32e-10   |
| reward_rotation         | 0.028      |
| reward_torque           | 0.0441     |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 1.75e+03   |
|    ep_rew_mean          | 433        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 67         |
|    time_elapsed         | 3388       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.28044066 |
|    clip_fraction        | 0.677      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.63      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00671   |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0354    |
|    std                  | 0.807      |
|    value_loss           | 0.401      |
----------------------------------------
Num timesteps: 138000
Best mean reward: 430.95 - Last mean reward per episode: 432.71
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0418     |
| reward_ctrl             | 0.032      |
| reward_motion           | 0          |
| reward_orientation      | 0.0388     |
| reward_position         | 2.29e-10   |
| reward_rotation         | 0.0291     |
| reward_torque           | 0.0442     |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 1.76e+03   |
|    ep_rew_mean          | 435        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 68         |
|    time_elapsed         | 3440       |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.09170762 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.62      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.7        |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.026     |
|    std                  | 0.806      |
|    value_loss           | 0.709      |
----------------------------------------
---------------------------------------
| reward                  | 0.206     |
| reward_contact          | 0.0413    |
| reward_ctrl             | 0.0316    |
| reward_motion           | 0         |
| reward_orientation      | 0.0387    |
| reward_position         | 2.26e-10  |
| reward_rotation         | 0.0288    |
| reward_torque           | 0.044     |
| reward_velocity         | 0.0215    |
| rollout/                |           |
|    ep_len_mean          | 1.77e+03  |
|    ep_rew_mean          | 436       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 69        |
|    time_elapsed         | 3492      |
|    total_timesteps      | 141312    |
| train/                  |           |
|    approx_kl            | 0.1491859 |
|    clip_fraction        | 0.627     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.61     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0173    |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0329   |
|    std                  | 0.804     |
|    value_loss           | 0.309     |
---------------------------------------
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0413     |
| reward_ctrl             | 0.0316     |
| reward_motion           | 0          |
| reward_orientation      | 0.0387     |
| reward_position         | 2.26e-10   |
| reward_rotation         | 0.0288     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 1.77e+03   |
|    ep_rew_mean          | 436        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 70         |
|    time_elapsed         | 3544       |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.18738979 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.56      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0278     |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0336    |
|    std                  | 0.799      |
|    value_loss           | 0.373      |
----------------------------------------
Num timesteps: 144000
Best mean reward: 432.71 - Last mean reward per episode: 433.97
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.0411     |
| reward_ctrl             | 0.0316     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 2.21e-10   |
| reward_rotation         | 0.0281     |
| reward_torque           | 0.0441     |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 1.75e+03   |
|    ep_rew_mean          | 434        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 71         |
|    time_elapsed         | 3596       |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.14248566 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.55      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0421     |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0339    |
|    std                  | 0.799      |
|    value_loss           | 0.299      |
----------------------------------------
----------------------------------------
| reward                  | 0.203      |
| reward_contact          | 0.0406     |
| reward_ctrl             | 0.0314     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 2.18e-10   |
| reward_rotation         | 0.0279     |
| reward_torque           | 0.044      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.76e+03   |
|    ep_rew_mean          | 437        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 72         |
|    time_elapsed         | 3647       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.11735408 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.52      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.162      |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.795      |
|    value_loss           | 0.853      |
----------------------------------------
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.0408     |
| reward_ctrl             | 0.0326     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 2.16e-10   |
| reward_rotation         | 0.0276     |
| reward_torque           | 0.0442     |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.77e+03   |
|    ep_rew_mean          | 440        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 73         |
|    time_elapsed         | 3700       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.14771518 |
|    clip_fraction        | 0.621      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.47      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000923   |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0365    |
|    std                  | 0.79       |
|    value_loss           | 0.448      |
----------------------------------------
Num timesteps: 150000
Best mean reward: 433.97 - Last mean reward per episode: 439.69
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.204      |
| reward_contact          | 0.0404     |
| reward_ctrl             | 0.0326     |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 2.13e-10   |
| reward_rotation         | 0.0279     |
| reward_torque           | 0.0443     |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 1.77e+03   |
|    ep_rew_mean          | 442        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 74         |
|    time_elapsed         | 3752       |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.11673694 |
|    clip_fraction        | 0.628      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.266      |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0268    |
|    std                  | 0.79       |
|    value_loss           | 0.702      |
----------------------------------------
----------------------------------------
| reward                  | 0.205      |
| reward_contact          | 0.04       |
| reward_ctrl             | 0.0336     |
| reward_motion           | 0          |
| reward_orientation      | 0.0376     |
| reward_position         | 2.11e-10   |
| reward_rotation         | 0.0276     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 1.78e+03   |
|    ep_rew_mean          | 445        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 75         |
|    time_elapsed         | 3803       |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.18169129 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0251    |
|    std                  | 0.789      |
|    value_loss           | 0.304      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0402     |
| reward_ctrl             | 0.0336     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 2.08e-10   |
| reward_rotation         | 0.0292     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.0219     |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 449        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 76         |
|    time_elapsed         | 3854       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.13661128 |
|    clip_fraction        | 0.628      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.44      |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.051      |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0115    |
|    std                  | 0.788      |
|    value_loss           | 1.27       |
----------------------------------------
Num timesteps: 156000
Best mean reward: 439.69 - Last mean reward per episode: 448.64
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.0402     |
| reward_ctrl             | 0.0336     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 2.08e-10   |
| reward_rotation         | 0.0292     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.0219     |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 450        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 77         |
|    time_elapsed         | 3905       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.11488572 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.43      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0352    |
|    std                  | 0.785      |
|    value_loss           | 0.538      |
----------------------------------------
----------------------------------------
| reward                  | 0.206      |
| reward_contact          | 0.0397     |
| reward_ctrl             | 0.0335     |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 2.06e-10   |
| reward_rotation         | 0.0289     |
| reward_torque           | 0.0444     |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 1.79e+03   |
|    ep_rew_mean          | 450        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 78         |
|    time_elapsed         | 3956       |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.20727798 |
|    clip_fraction        | 0.674      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.39      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.056      |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0193    |
|    std                  | 0.779      |
|    value_loss           | 0.533      |
----------------------------------------
----------------------------------------
| reward                  | 0.207      |
| reward_contact          | 0.04       |
| reward_ctrl             | 0.0345     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 2.03e-10   |
| reward_rotation         | 0.0287     |
| reward_torque           | 0.0446     |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 1.8e+03    |
|    ep_rew_mean          | 454        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 79         |
|    time_elapsed         | 4007       |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.21501562 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.35      |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0228    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.024     |
|    std                  | 0.778      |
|    value_loss           | 0.245      |
----------------------------------------
Num timesteps: 162000
Best mean reward: 448.64 - Last mean reward per episode: 453.99
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0402     |
| reward_ctrl             | 0.0349     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 2.01e-10   |
| reward_rotation         | 0.0291     |
| reward_torque           | 0.0447     |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 1.8e+03    |
|    ep_rew_mean          | 455        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 80         |
|    time_elapsed         | 4059       |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.10772067 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0303     |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0298    |
|    std                  | 0.778      |
|    value_loss           | 2.26       |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0397     |
| reward_ctrl             | 0.0351     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.99e-10   |
| reward_rotation         | 0.0288     |
| reward_torque           | 0.0448     |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 1.81e+03   |
|    ep_rew_mean          | 457        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 81         |
|    time_elapsed         | 4111       |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.19115596 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.253      |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0285    |
|    std                  | 0.776      |
|    value_loss           | 0.404      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0393     |
| reward_ctrl             | 0.036      |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.97e-10   |
| reward_rotation         | 0.0287     |
| reward_torque           | 0.0449     |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 1.81e+03   |
|    ep_rew_mean          | 459        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 82         |
|    time_elapsed         | 4163       |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.16114552 |
|    clip_fraction        | 0.644      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.31      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0153    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0443    |
|    std                  | 0.774      |
|    value_loss           | 0.476      |
----------------------------------------
Num timesteps: 168000
Best mean reward: 453.99 - Last mean reward per episode: 458.77
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0389     |
| reward_ctrl             | 0.0364     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.95e-10   |
| reward_rotation         | 0.0286     |
| reward_torque           | 0.045      |
| reward_velocity         | 0.0205     |
| rollout/                |            |
|    ep_len_mean          | 1.82e+03   |
|    ep_rew_mean          | 461        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 83         |
|    time_elapsed         | 4215       |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.17248046 |
|    clip_fraction        | 0.676      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.27      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0386    |
|    std                  | 0.768      |
|    value_loss           | 0.723      |
----------------------------------------
-----------------------------------------
| reward                  | 0.209       |
| reward_contact          | 0.0385      |
| reward_ctrl             | 0.0366      |
| reward_motion           | 0           |
| reward_orientation      | 0.0384      |
| reward_position         | 1.93e-10    |
| reward_rotation         | 0.0303      |
| reward_torque           | 0.045       |
| reward_velocity         | 0.0203      |
| rollout/                |             |
|    ep_len_mean          | 1.82e+03    |
|    ep_rew_mean          | 463         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 84          |
|    time_elapsed         | 4268        |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.106168404 |
|    clip_fraction        | 0.576       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.23       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0304     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0336     |
|    std                  | 0.768       |
|    value_loss           | 0.371       |
-----------------------------------------
Num timesteps: 174000
Best mean reward: 458.77 - Last mean reward per episode: 464.24
Saving new best model to rl/out_dir/models/exp68/best_model.zip
-----------------------------------------
| reward                  | 0.21        |
| reward_contact          | 0.0381      |
| reward_ctrl             | 0.0368      |
| reward_motion           | 0           |
| reward_orientation      | 0.0386      |
| reward_position         | 1.91e-10    |
| reward_rotation         | 0.0309      |
| reward_torque           | 0.0451      |
| reward_velocity         | 0.0201      |
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 464         |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 85          |
|    time_elapsed         | 4320        |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.123214364 |
|    clip_fraction        | 0.592       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.21       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0723      |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0337     |
|    std                  | 0.765       |
|    value_loss           | 0.432       |
-----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0383     |
| reward_ctrl             | 0.0369     |
| reward_motion           | 0          |
| reward_orientation      | 0.0386     |
| reward_position         | 1.89e-10   |
| reward_rotation         | 0.0307     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 1.83e+03   |
|    ep_rew_mean          | 467        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 86         |
|    time_elapsed         | 4371       |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.13831761 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.18      |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.41       |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.00245   |
|    std                  | 0.762      |
|    value_loss           | 1.62       |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0383     |
| reward_ctrl             | 0.0369     |
| reward_motion           | 0          |
| reward_orientation      | 0.0386     |
| reward_position         | 1.89e-10   |
| reward_rotation         | 0.0307     |
| reward_torque           | 0.0452     |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 1.83e+03   |
|    ep_rew_mean          | 467        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 87         |
|    time_elapsed         | 4423       |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.21971326 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.17      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.603      |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.024     |
|    std                  | 0.761      |
|    value_loss           | 0.827      |
----------------------------------------
Num timesteps: 180000
Best mean reward: 464.24 - Last mean reward per episode: 469.17
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0379    |
| reward_ctrl             | 0.0373    |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 1.87e-10  |
| reward_rotation         | 0.0304    |
| reward_torque           | 0.0453    |
| reward_velocity         | 0.0197    |
| rollout/                |           |
|    ep_len_mean          | 1.84e+03  |
|    ep_rew_mean          | 469       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 88        |
|    time_elapsed         | 4475      |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.2417622 |
|    clip_fraction        | 0.709     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.18     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0185   |
|    n_updates            | 870       |
|    policy_gradient_loss | -0.0459   |
|    std                  | 0.76      |
|    value_loss           | 0.197     |
---------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0375     |
| reward_ctrl             | 0.0374     |
| reward_motion           | 0          |
| reward_orientation      | 0.0384     |
| reward_position         | 1.85e-10   |
| reward_rotation         | 0.0302     |
| reward_torque           | 0.0453     |
| reward_velocity         | 0.0195     |
| rollout/                |            |
|    ep_len_mean          | 1.84e+03   |
|    ep_rew_mean          | 472        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 89         |
|    time_elapsed         | 4528       |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.38657528 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.13      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.048      |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0215    |
|    std                  | 0.758      |
|    value_loss           | 0.618      |
----------------------------------------
----------------------------------------
| reward                  | 0.208      |
| reward_contact          | 0.0372     |
| reward_ctrl             | 0.0377     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.83e-10   |
| reward_rotation         | 0.0305     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 1.85e+03   |
|    ep_rew_mean          | 474        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 90         |
|    time_elapsed         | 4580       |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.16045797 |
|    clip_fraction        | 0.638      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.13      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0938    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0446    |
|    std                  | 0.757      |
|    value_loss           | 0.772      |
----------------------------------------
Num timesteps: 186000
Best mean reward: 469.17 - Last mean reward per episode: 475.51
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0369     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0315     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 1.85e+03   |
|    ep_rew_mean          | 476        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 91         |
|    time_elapsed         | 4633       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.23802997 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.11      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.159      |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0164    |
|    std                  | 0.756      |
|    value_loss           | 0.56       |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0369     |
| reward_ctrl             | 0.0376     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0322     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 1.87e+03   |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 92         |
|    time_elapsed         | 4685       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.26639843 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.12      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0174     |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0282    |
|    std                  | 0.757      |
|    value_loss           | 0.298      |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0369     |
| reward_ctrl             | 0.0377     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0323     |
| reward_torque           | 0.0454     |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 1.88e+03   |
|    ep_rew_mean          | 485        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 93         |
|    time_elapsed         | 4737       |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.15364237 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.1       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.45       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.044     |
|    std                  | 0.755      |
|    value_loss           | 0.927      |
----------------------------------------
Num timesteps: 192000
Best mean reward: 475.51 - Last mean reward per episode: 485.36
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0377     |
| reward_motion           | 0          |
| reward_orientation      | 0.0383     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0329     |
| reward_torque           | 0.0455     |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 1.88e+03   |
|    ep_rew_mean          | 487        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 94         |
|    time_elapsed         | 4789       |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.13389328 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.08      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.228      |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0206    |
|    std                  | 0.755      |
|    value_loss           | 0.964      |
----------------------------------------
---------------------------------------
| reward                  | 0.209     |
| reward_contact          | 0.0363    |
| reward_ctrl             | 0.0381    |
| reward_motion           | 0         |
| reward_orientation      | 0.038     |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0332    |
| reward_torque           | 0.0456    |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 1.88e+03  |
|    ep_rew_mean          | 488       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 95        |
|    time_elapsed         | 4840      |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.1985935 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.08     |
|    explained_variance   | 0.969     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0385    |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.0063   |
|    std                  | 0.754     |
|    value_loss           | 0.304     |
---------------------------------------
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0363     |
| reward_ctrl             | 0.0381     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0332     |
| reward_torque           | 0.0456     |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 1.88e+03   |
|    ep_rew_mean          | 488        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 96         |
|    time_elapsed         | 4892       |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.15943775 |
|    clip_fraction        | 0.653      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.07      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0789     |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0298    |
|    std                  | 0.752      |
|    value_loss           | 0.947      |
----------------------------------------
Num timesteps: 198000
Best mean reward: 485.36 - Last mean reward per episode: 491.37
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.209      |
| reward_contact          | 0.0357     |
| reward_ctrl             | 0.0389     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0333     |
| reward_torque           | 0.0458     |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 1.89e+03   |
|    ep_rew_mean          | 491        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 97         |
|    time_elapsed         | 4944       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.17565684 |
|    clip_fraction        | 0.675      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.08      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0366     |
|    n_updates            | 960        |
|    policy_gradient_loss | 0.00362    |
|    std                  | 0.757      |
|    value_loss           | 0.329      |
----------------------------------------
----------------------------------------
| reward                  | 0.21       |
| reward_contact          | 0.0351     |
| reward_ctrl             | 0.0397     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0337     |
| reward_torque           | 0.046      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 1.9e+03    |
|    ep_rew_mean          | 496        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 98         |
|    time_elapsed         | 4995       |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.25652635 |
|    clip_fraction        | 0.701      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.11      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.198      |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0482    |
|    std                  | 0.754      |
|    value_loss           | 0.386      |
----------------------------------------
----------------------------------------
| reward                  | 0.212      |
| reward_contact          | 0.0345     |
| reward_ctrl             | 0.0404     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0349     |
| reward_torque           | 0.0462     |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 1.91e+03   |
|    ep_rew_mean          | 499        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 99         |
|    time_elapsed         | 5048       |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.13737094 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.06      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0372     |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0384    |
|    std                  | 0.751      |
|    value_loss           | 0.515      |
----------------------------------------
Num timesteps: 204000
Best mean reward: 491.37 - Last mean reward per episode: 504.58
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.211      |
| reward_contact          | 0.0339     |
| reward_ctrl             | 0.0413     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0341     |
| reward_torque           | 0.0464     |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 1.93e+03   |
|    ep_rew_mean          | 505        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 100        |
|    time_elapsed         | 5100       |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.18076971 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.05      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0127     |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.032     |
|    std                  | 0.751      |
|    value_loss           | 1.02       |
----------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0339     |
| reward_ctrl             | 0.0417     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0357     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.0173     |
| rollout/                |            |
|    ep_len_mean          | 1.94e+03   |
|    ep_rew_mean          | 510        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 101        |
|    time_elapsed         | 5151       |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.17713839 |
|    clip_fraction        | 0.651      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.03      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0369    |
|    std                  | 0.748      |
|    value_loss           | 0.758      |
----------------------------------------
----------------------------------------
| reward                  | 0.213      |
| reward_contact          | 0.0334     |
| reward_ctrl             | 0.0412     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.036      |
| reward_torque           | 0.0463     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 514        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 102        |
|    time_elapsed         | 5203       |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.14086843 |
|    clip_fraction        | 0.634      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.03      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0476     |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0254    |
|    std                  | 0.75       |
|    value_loss           | 0.606      |
----------------------------------------
Num timesteps: 210000
Best mean reward: 504.58 - Last mean reward per episode: 514.40
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0328     |
| reward_ctrl             | 0.0424     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0365     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.93e+03   |
|    ep_rew_mean          | 513        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 103        |
|    time_elapsed         | 5254       |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.21096276 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.2        |
|    entropy_loss         | -9         |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.141      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 0.746      |
|    value_loss           | 0.672      |
----------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0328     |
| reward_ctrl             | 0.0424     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0365     |
| reward_torque           | 0.0465     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.93e+03   |
|    ep_rew_mean          | 513        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 104        |
|    time_elapsed         | 5308       |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.10819122 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.99      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.285      |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0214    |
|    std                  | 0.746      |
|    value_loss           | 2.27       |
----------------------------------------
----------------------------------------
| reward                  | 0.214      |
| reward_contact          | 0.0322     |
| reward_ctrl             | 0.0429     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0362     |
| reward_torque           | 0.0466     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 519        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 105        |
|    time_elapsed         | 5360       |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.23069471 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.03      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0203     |
|    n_updates            | 1040       |
|    policy_gradient_loss | 0.0307     |
|    std                  | 0.752      |
|    value_loss           | 0.247      |
----------------------------------------
Num timesteps: 216000
Best mean reward: 514.40 - Last mean reward per episode: 523.07
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0322     |
| reward_ctrl             | 0.0437     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0367     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 1.97e+03   |
|    ep_rew_mean          | 523        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 106        |
|    time_elapsed         | 5412       |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.27740878 |
|    clip_fraction        | 0.678      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.02      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0255     |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0433    |
|    std                  | 0.746      |
|    value_loss           | 0.312      |
----------------------------------------
----------------------------------------
| reward                  | 0.217      |
| reward_contact          | 0.0322     |
| reward_ctrl             | 0.0434     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0386     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 528        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 107        |
|    time_elapsed         | 5464       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.16456144 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.96      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0303     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.044     |
|    std                  | 0.741      |
|    value_loss           | 0.468      |
----------------------------------------
----------------------------------------
| reward                  | 0.216      |
| reward_contact          | 0.0316     |
| reward_ctrl             | 0.0438     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0387     |
| reward_torque           | 0.0467     |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 1.99e+03   |
|    ep_rew_mean          | 532        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 108        |
|    time_elapsed         | 5516       |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.20739585 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.92      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0209     |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0346    |
|    std                  | 0.738      |
|    value_loss           | 0.422      |
----------------------------------------
Num timesteps: 222000
Best mean reward: 523.07 - Last mean reward per episode: 532.29
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.217     |
| reward_contact          | 0.031     |
| reward_ctrl             | 0.0444    |
| reward_motion           | 0         |
| reward_orientation      | 0.0378    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.039     |
| reward_torque           | 0.0468    |
| reward_velocity         | 0.0183    |
| rollout/                |           |
|    ep_len_mean          | 1.99e+03  |
|    ep_rew_mean          | 536       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 109       |
|    time_elapsed         | 5569      |
|    total_timesteps      | 223232    |
| train/                  |           |
|    approx_kl            | 0.1570481 |
|    clip_fraction        | 0.677     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.89     |
|    explained_variance   | 0.968     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.224     |
|    n_updates            | 1080      |
|    policy_gradient_loss | -0.0117   |
|    std                  | 0.738     |
|    value_loss           | 0.747     |
---------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0309     |
| reward_ctrl             | 0.045      |
| reward_motion           | 0          |
| reward_orientation      | 0.0384     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0399     |
| reward_torque           | 0.047      |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 534        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 110        |
|    time_elapsed         | 5621       |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.16233377 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.164      |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0328    |
|    std                  | 0.738      |
|    value_loss           | 0.815      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0311     |
| reward_ctrl             | 0.0455     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0396     |
| reward_torque           | 0.0471     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 534        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 111        |
|    time_elapsed         | 5673       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.13458142 |
|    clip_fraction        | 0.626      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.88      |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0872     |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 0.735      |
|    value_loss           | 1.92       |
----------------------------------------
Num timesteps: 228000
Best mean reward: 532.29 - Last mean reward per episode: 534.21
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0306     |
| reward_ctrl             | 0.046      |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.04       |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 538        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 112        |
|    time_elapsed         | 5725       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.28611872 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.87      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.303      |
|    n_updates            | 1110       |
|    policy_gradient_loss | 0.0158     |
|    std                  | 0.736      |
|    value_loss           | 0.492      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0306     |
| reward_ctrl             | 0.046      |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.04       |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 538        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 113        |
|    time_elapsed         | 5776       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.16762781 |
|    clip_fraction        | 0.654      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0422     |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.014     |
|    std                  | 0.739      |
|    value_loss           | 0.539      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0306     |
| reward_ctrl             | 0.0461     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0395     |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 2e+03      |
|    ep_rew_mean          | 542        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 114        |
|    time_elapsed         | 5828       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.30915272 |
|    clip_fraction        | 0.764      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.92      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00742    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0245    |
|    std                  | 0.74       |
|    value_loss           | 0.23       |
----------------------------------------
Num timesteps: 234000
Best mean reward: 534.21 - Last mean reward per episode: 546.13
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.0306     |
| reward_ctrl             | 0.0458     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0381     |
| reward_torque           | 0.0472     |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 2.01e+03   |
|    ep_rew_mean          | 546        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 115        |
|    time_elapsed         | 5880       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.22008918 |
|    clip_fraction        | 0.639      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.91      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.193      |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.037     |
|    std                  | 0.738      |
|    value_loss           | 0.975      |
----------------------------------------
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0464     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0382     |
| reward_torque           | 0.0473     |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 549        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 116        |
|    time_elapsed         | 5933       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.23293161 |
|    clip_fraction        | 0.697      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0852     |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0275    |
|    std                  | 0.736      |
|    value_loss           | 0.238      |
----------------------------------------
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.03       |
| reward_ctrl             | 0.0467     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0377     |
| reward_torque           | 0.0474     |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 550        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 117        |
|    time_elapsed         | 5985       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.19131191 |
|    clip_fraction        | 0.672      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.86      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00565    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0393    |
|    std                  | 0.733      |
|    value_loss           | 0.325      |
----------------------------------------
Num timesteps: 240000
Best mean reward: 546.13 - Last mean reward per episode: 550.49
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.0294     |
| reward_ctrl             | 0.0475     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0381     |
| reward_torque           | 0.0475     |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 552        |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 118        |
|    time_elapsed         | 6039       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.16813597 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.85      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.157      |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.032     |
|    std                  | 0.732      |
|    value_loss           | 0.675      |
----------------------------------------
---------------------------------------
| reward                  | 0.218     |
| reward_contact          | 0.0288    |
| reward_ctrl             | 0.0475    |
| reward_motion           | 0         |
| reward_orientation      | 0.0382    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0382    |
| reward_torque           | 0.0475    |
| reward_velocity         | 0.0178    |
| rollout/                |           |
|    ep_len_mean          | 2.04e+03  |
|    ep_rew_mean          | 557       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 119       |
|    time_elapsed         | 6091      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.2313352 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.82     |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.253     |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.0411   |
|    std                  | 0.728     |
|    value_loss           | 0.38      |
---------------------------------------
---------------------------------------
| reward                  | 0.217     |
| reward_contact          | 0.0282    |
| reward_ctrl             | 0.0476    |
| reward_motion           | 0         |
| reward_orientation      | 0.0379    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0405    |
| reward_torque           | 0.0476    |
| reward_velocity         | 0.0157    |
| rollout/                |           |
|    ep_len_mean          | 2.04e+03  |
|    ep_rew_mean          | 558       |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 120       |
|    time_elapsed         | 6143      |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.2383852 |
|    clip_fraction        | 0.688     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.79     |
|    explained_variance   | 0.97      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0216    |
|    n_updates            | 1190      |
|    policy_gradient_loss | -0.0348   |
|    std                  | 0.727     |
|    value_loss           | 0.304     |
---------------------------------------
Num timesteps: 246000
Best mean reward: 550.49 - Last mean reward per episode: 557.76
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.215     |
| reward_contact          | 0.0282    |
| reward_ctrl             | 0.0467    |
| reward_motion           | 0         |
| reward_orientation      | 0.0376    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0414    |
| reward_torque           | 0.0474    |
| reward_velocity         | 0.0143    |
| rollout/                |           |
|    ep_len_mean          | 2.06e+03  |
|    ep_rew_mean          | 564       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 121       |
|    time_elapsed         | 6196      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.1566731 |
|    clip_fraction        | 0.681     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.76     |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.549     |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.0101   |
|    std                  | 0.725     |
|    value_loss           | 0.91      |
---------------------------------------
----------------------------------------
| reward                  | 0.215      |
| reward_contact          | 0.0282     |
| reward_ctrl             | 0.0467     |
| reward_motion           | 0          |
| reward_orientation      | 0.0376     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0414     |
| reward_torque           | 0.0474     |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 564        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 122        |
|    time_elapsed         | 6248       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.25532693 |
|    clip_fraction        | 0.724      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.76      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0995     |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0265    |
|    std                  | 0.724      |
|    value_loss           | 0.703      |
----------------------------------------
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.0276     |
| reward_ctrl             | 0.0476     |
| reward_motion           | 0          |
| reward_orientation      | 0.0376     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.043      |
| reward_torque           | 0.0476     |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 567        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 123        |
|    time_elapsed         | 6299       |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.18221506 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.75      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0643     |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 0.724      |
|    value_loss           | 0.306      |
----------------------------------------
Num timesteps: 252000
Best mean reward: 557.76 - Last mean reward per episode: 566.85
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.218     |
| reward_contact          | 0.0276    |
| reward_ctrl             | 0.0481    |
| reward_motion           | 0         |
| reward_orientation      | 0.0376    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0428    |
| reward_torque           | 0.0477    |
| reward_velocity         | 0.0146    |
| rollout/                |           |
|    ep_len_mean          | 2.06e+03  |
|    ep_rew_mean          | 569       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 124       |
|    time_elapsed         | 6352      |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.2122133 |
|    clip_fraction        | 0.687     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.75     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0714    |
|    n_updates            | 1230      |
|    policy_gradient_loss | -0.0212   |
|    std                  | 0.724     |
|    value_loss           | 0.922     |
---------------------------------------
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.027      |
| reward_ctrl             | 0.0487     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0428     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 571        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 125        |
|    time_elapsed         | 6405       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.21267049 |
|    clip_fraction        | 0.679      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.75      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.145      |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0146    |
|    std                  | 0.725      |
|    value_loss           | 0.661      |
----------------------------------------
Num timesteps: 258000
Best mean reward: 566.85 - Last mean reward per episode: 572.53
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.0264     |
| reward_ctrl             | 0.0484     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0444     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 573        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 126        |
|    time_elapsed         | 6457       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.15467799 |
|    clip_fraction        | 0.636      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.76      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.029     |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0416    |
|    std                  | 0.725      |
|    value_loss           | 0.605      |
----------------------------------------
----------------------------------------
| reward                  | 0.22       |
| reward_contact          | 0.0258     |
| reward_ctrl             | 0.0485     |
| reward_motion           | 0          |
| reward_orientation      | 0.0374     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0461     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 574        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 127        |
|    time_elapsed         | 6510       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.13878039 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.75      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0297    |
|    std                  | 0.724      |
|    value_loss           | 0.449      |
----------------------------------------
----------------------------------------
| reward                  | 0.219      |
| reward_contact          | 0.0252     |
| reward_ctrl             | 0.0487     |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0466     |
| reward_torque           | 0.0478     |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 2.07e+03   |
|    ep_rew_mean          | 578        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 128        |
|    time_elapsed         | 6562       |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.21471857 |
|    clip_fraction        | 0.662      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.74      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0137     |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0257    |
|    std                  | 0.724      |
|    value_loss           | 0.413      |
----------------------------------------
Num timesteps: 264000
Best mean reward: 572.53 - Last mean reward per episode: 584.38
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.218      |
| reward_contact          | 0.0246     |
| reward_ctrl             | 0.0486     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0467     |
| reward_torque           | 0.0477     |
| reward_velocity         | 0.0127     |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 584        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 129        |
|    time_elapsed         | 6613       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.22423033 |
|    clip_fraction        | 0.686      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.76      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.195      |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0226    |
|    std                  | 0.725      |
|    value_loss           | 0.853      |
----------------------------------------
---------------------------------------
| reward                  | 0.219     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0492    |
| reward_motion           | 0         |
| reward_orientation      | 0.038     |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0469    |
| reward_torque           | 0.048     |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 586       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 130       |
|    time_elapsed         | 6665      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.4696163 |
|    clip_fraction        | 0.748     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.73     |
|    explained_variance   | 0.851     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.106     |
|    n_updates            | 1290      |
|    policy_gradient_loss | 0.0053    |
|    std                  | 0.723     |
|    value_loss           | 1.14      |
---------------------------------------
---------------------------------------
| reward                  | 0.219     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0492    |
| reward_motion           | 0         |
| reward_orientation      | 0.038     |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0469    |
| reward_torque           | 0.048     |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 586       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 131       |
|    time_elapsed         | 6717      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.2829131 |
|    clip_fraction        | 0.721     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.72     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.141     |
|    n_updates            | 1300      |
|    policy_gradient_loss | 0.00224   |
|    std                  | 0.721     |
|    value_loss           | 0.565     |
---------------------------------------
Num timesteps: 270000
Best mean reward: 584.38 - Last mean reward per episode: 587.34
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.221      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0496     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0489     |
| reward_torque           | 0.048      |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 587        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 132        |
|    time_elapsed         | 6769       |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.28810835 |
|    clip_fraction        | 0.76       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.7       |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0784     |
|    n_updates            | 1310       |
|    policy_gradient_loss | 0.00278    |
|    std                  | 0.719      |
|    value_loss           | 0.243      |
----------------------------------------
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0503     |
| reward_motion           | 0          |
| reward_orientation      | 0.0381     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0486     |
| reward_torque           | 0.0482     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 589        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 133        |
|    time_elapsed         | 6820       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.28868806 |
|    clip_fraction        | 0.743      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0253    |
|    std                  | 0.72       |
|    value_loss           | 0.613      |
----------------------------------------
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0501     |
| reward_motion           | 0          |
| reward_orientation      | 0.0379     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0496     |
| reward_torque           | 0.0482     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.08e+03   |
|    ep_rew_mean          | 589        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 134        |
|    time_elapsed         | 6872       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.19268681 |
|    clip_fraction        | 0.682      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.68      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0396    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0242    |
|    std                  | 0.719      |
|    value_loss           | 0.698      |
----------------------------------------
Num timesteps: 276000
Best mean reward: 587.34 - Last mean reward per episode: 596.17
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.222      |
| reward_contact          | 0.0229     |
| reward_ctrl             | 0.051      |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0494     |
| reward_torque           | 0.0484     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 596        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 135        |
|    time_elapsed         | 6924       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.22758305 |
|    clip_fraction        | 0.668      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.67      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.152      |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0271    |
|    std                  | 0.718      |
|    value_loss           | 1.48       |
----------------------------------------
----------------------------------------
| reward                  | 0.223      |
| reward_contact          | 0.0229     |
| reward_ctrl             | 0.0516     |
| reward_motion           | 0          |
| reward_orientation      | 0.0375     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0489     |
| reward_torque           | 0.0485     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 597        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 136        |
|    time_elapsed         | 6975       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.21068835 |
|    clip_fraction        | 0.676      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.66      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0077     |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0151    |
|    std                  | 0.719      |
|    value_loss           | 0.848      |
----------------------------------------
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0513     |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0507     |
| reward_torque           | 0.0484     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 599        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 137        |
|    time_elapsed         | 7027       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.21989466 |
|    clip_fraction        | 0.663      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.68      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.597      |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.00996   |
|    std                  | 0.72       |
|    value_loss           | 1.03       |
----------------------------------------
Num timesteps: 282000
Best mean reward: 596.17 - Last mean reward per episode: 598.88
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.225      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0513     |
| reward_motion           | 0          |
| reward_orientation      | 0.0377     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0507     |
| reward_torque           | 0.0484     |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 599        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 138        |
|    time_elapsed         | 7079       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.18982515 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.68      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0647     |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0136    |
|    std                  | 0.719      |
|    value_loss           | 0.405      |
----------------------------------------
----------------------------------------
| reward                  | 0.226      |
| reward_contact          | 0.0241     |
| reward_ctrl             | 0.0514     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.81e-10   |
| reward_rotation         | 0.0512     |
| reward_torque           | 0.0485     |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 601        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 139        |
|    time_elapsed         | 7132       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.21435162 |
|    clip_fraction        | 0.663      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0683     |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0145    |
|    std                  | 0.721      |
|    value_loss           | 0.266      |
----------------------------------------
---------------------------------------
| reward                  | 0.227     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0516    |
| reward_motion           | 0         |
| reward_orientation      | 0.0378    |
| reward_position         | 1.81e-10  |
| reward_rotation         | 0.0511    |
| reward_torque           | 0.0486    |
| reward_velocity         | 0.013     |
| rollout/                |           |
|    ep_len_mean          | 2.11e+03  |
|    ep_rew_mean          | 603       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 140       |
|    time_elapsed         | 7183      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.1676096 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.68     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0194   |
|    n_updates            | 1390      |
|    policy_gradient_loss | -0.0439   |
|    std                  | 0.718     |
|    value_loss           | 0.548     |
---------------------------------------
Num timesteps: 288000
Best mean reward: 598.88 - Last mean reward per episode: 608.96
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.227      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0517     |
| reward_motion           | 0          |
| reward_orientation      | 0.0378     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0519     |
| reward_torque           | 0.0486     |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 609        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 141        |
|    time_elapsed         | 7234       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.36176997 |
|    clip_fraction        | 0.796      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.7       |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0587     |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0103    |
|    std                  | 0.72       |
|    value_loss           | 0.356      |
----------------------------------------
----------------------------------------
| reward                  | 0.228      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0519     |
| reward_motion           | 0          |
| reward_orientation      | 0.0376     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0534     |
| reward_torque           | 0.0486     |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 609        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 142        |
|    time_elapsed         | 7286       |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.37805703 |
|    clip_fraction        | 0.764      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.7       |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.163      |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.00206   |
|    std                  | 0.72       |
|    value_loss           | 0.824      |
----------------------------------------
----------------------------------------
| reward                  | 0.229      |
| reward_contact          | 0.0232     |
| reward_ctrl             | 0.0522     |
| reward_motion           | 0          |
| reward_orientation      | 0.038      |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0539     |
| reward_torque           | 0.0487     |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 611        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 143        |
|    time_elapsed         | 7338       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.26799172 |
|    clip_fraction        | 0.742      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.145      |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0164    |
|    std                  | 0.719      |
|    value_loss           | 0.775      |
----------------------------------------
Num timesteps: 294000
Best mean reward: 608.96 - Last mean reward per episode: 610.61
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.023     |
| reward_ctrl             | 0.0529    |
| reward_motion           | 0         |
| reward_orientation      | 0.0383    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0542    |
| reward_torque           | 0.0488    |
| reward_velocity         | 0.0123    |
| rollout/                |           |
|    ep_len_mean          | 2.13e+03  |
|    ep_rew_mean          | 611       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 144       |
|    time_elapsed         | 7389      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.2609117 |
|    clip_fraction        | 0.687     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.69     |
|    explained_variance   | 0.966     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0288   |
|    n_updates            | 1430      |
|    policy_gradient_loss | -0.0226   |
|    std                  | 0.721     |
|    value_loss           | 0.389     |
---------------------------------------
----------------------------------------
| reward                  | 0.23       |
| reward_contact          | 0.0224     |
| reward_ctrl             | 0.0531     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0544     |
| reward_torque           | 0.0489     |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 613        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 145        |
|    time_elapsed         | 7440       |
|    total_timesteps      | 296960     |
| train/                  |            |
|    approx_kl            | 0.17324759 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0484     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0231    |
|    std                  | 0.72       |
|    value_loss           | 0.328      |
----------------------------------------
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.023     |
| reward_ctrl             | 0.0531    |
| reward_motion           | 0         |
| reward_orientation      | 0.0382    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0539    |
| reward_torque           | 0.0489    |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 2.13e+03  |
|    ep_rew_mean          | 613       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 146       |
|    time_elapsed         | 7492      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.2839511 |
|    clip_fraction        | 0.746     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.69     |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0738    |
|    n_updates            | 1450      |
|    policy_gradient_loss | -0.0183   |
|    std                  | 0.718     |
|    value_loss           | 0.71      |
---------------------------------------
Num timesteps: 300000
Best mean reward: 610.61 - Last mean reward per episode: 613.27
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.23      |
| reward_contact          | 0.023     |
| reward_ctrl             | 0.0531    |
| reward_motion           | 0         |
| reward_orientation      | 0.0382    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0539    |
| reward_torque           | 0.0489    |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 2.13e+03  |
|    ep_rew_mean          | 613       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 147       |
|    time_elapsed         | 7544      |
|    total_timesteps      | 301056    |
| train/                  |           |
|    approx_kl            | 0.2802563 |
|    clip_fraction        | 0.716     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.68     |
|    explained_variance   | 0.969     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0104    |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0353   |
|    std                  | 0.718     |
|    value_loss           | 0.312     |
---------------------------------------
----------------------------------------
| reward                  | 0.231      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0539     |
| reward_motion           | 0          |
| reward_orientation      | 0.0382     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0546     |
| reward_torque           | 0.049      |
| reward_velocity         | 0.0127     |
| rollout/                |            |
|    ep_len_mean          | 2.14e+03   |
|    ep_rew_mean          | 617        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 148        |
|    time_elapsed         | 7596       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.20992596 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.66      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00321   |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0248    |
|    std                  | 0.716      |
|    value_loss           | 0.277      |
----------------------------------------
----------------------------------------
| reward                  | 0.233      |
| reward_contact          | 0.0219     |
| reward_ctrl             | 0.0545     |
| reward_motion           | 0          |
| reward_orientation      | 0.0384     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0562     |
| reward_torque           | 0.0491     |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 2.14e+03   |
|    ep_rew_mean          | 618        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 149        |
|    time_elapsed         | 7648       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.28855288 |
|    clip_fraction        | 0.726      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.64      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0232     |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0334    |
|    std                  | 0.716      |
|    value_loss           | 0.583      |
----------------------------------------
Num timesteps: 306000
Best mean reward: 613.27 - Last mean reward per episode: 619.23
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.235     |
| reward_contact          | 0.0213    |
| reward_ctrl             | 0.055     |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0579    |
| reward_torque           | 0.0492    |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 2.14e+03  |
|    ep_rew_mean          | 619       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 150       |
|    time_elapsed         | 7701      |
|    total_timesteps      | 307200    |
| train/                  |           |
|    approx_kl            | 0.3717386 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.65     |
|    explained_variance   | 0.756     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.19      |
|    n_updates            | 1490      |
|    policy_gradient_loss | -0.00654  |
|    std                  | 0.716     |
|    value_loss           | 1.85      |
---------------------------------------
----------------------------------------
| reward                  | 0.233      |
| reward_contact          | 0.0207     |
| reward_ctrl             | 0.0552     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0571     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 2.14e+03   |
|    ep_rew_mean          | 619        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 151        |
|    time_elapsed         | 7754       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.23082477 |
|    clip_fraction        | 0.685      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.65      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.416      |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0225    |
|    std                  | 0.716      |
|    value_loss           | 0.7        |
----------------------------------------
----------------------------------------
| reward                  | 0.233      |
| reward_contact          | 0.0213     |
| reward_ctrl             | 0.055      |
| reward_motion           | 0          |
| reward_orientation      | 0.0389     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0569     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 2.14e+03   |
|    ep_rew_mean          | 619        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 152        |
|    time_elapsed         | 7806       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.28811255 |
|    clip_fraction        | 0.721      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.65      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.1        |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.024     |
|    std                  | 0.717      |
|    value_loss           | 0.303      |
----------------------------------------
Num timesteps: 312000
Best mean reward: 619.23 - Last mean reward per episode: 619.23
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0213     |
| reward_ctrl             | 0.0557     |
| reward_motion           | 0          |
| reward_orientation      | 0.0389     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0588     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.0123     |
| rollout/                |            |
|    ep_len_mean          | 2.14e+03   |
|    ep_rew_mean          | 622        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 153        |
|    time_elapsed         | 7859       |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.38938716 |
|    clip_fraction        | 0.728      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.67      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.021     |
|    std                  | 0.72       |
|    value_loss           | 0.963      |
----------------------------------------
---------------------------------------
| reward                  | 0.234     |
| reward_contact          | 0.0208    |
| reward_ctrl             | 0.0556    |
| reward_motion           | 0         |
| reward_orientation      | 0.0385    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0576    |
| reward_torque           | 0.0493    |
| reward_velocity         | 0.0124    |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 628       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 154       |
|    time_elapsed         | 7910      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.2833447 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.69     |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.518     |
|    n_updates            | 1530      |
|    policy_gradient_loss | 0.00179   |
|    std                  | 0.72      |
|    value_loss           | 2.07      |
---------------------------------------
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0202     |
| reward_ctrl             | 0.0559     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0576     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 629        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 155        |
|    time_elapsed         | 7962       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.75892115 |
|    clip_fraction        | 0.739      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.66      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00556    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0323    |
|    std                  | 0.714      |
|    value_loss           | 0.299      |
----------------------------------------
Num timesteps: 318000
Best mean reward: 619.23 - Last mean reward per episode: 628.96
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0202     |
| reward_ctrl             | 0.0559     |
| reward_motion           | 0          |
| reward_orientation      | 0.0385     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0576     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 629        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 156        |
|    time_elapsed         | 8014       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.13379605 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.61      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.186      |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0319    |
|    std                  | 0.714      |
|    value_loss           | 0.441      |
----------------------------------------
---------------------------------------
| reward                  | 0.234     |
| reward_contact          | 0.0207    |
| reward_ctrl             | 0.0556    |
| reward_motion           | 0         |
| reward_orientation      | 0.0389    |
| reward_position         | 1.98e-138 |
| reward_rotation         | 0.0574    |
| reward_torque           | 0.0492    |
| reward_velocity         | 0.0124    |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 629       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 157       |
|    time_elapsed         | 8065      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.2710591 |
|    clip_fraction        | 0.744     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.6      |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.014     |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.0232   |
|    std                  | 0.711     |
|    value_loss           | 0.231     |
---------------------------------------
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.055      |
| reward_motion           | 0          |
| reward_orientation      | 0.0392     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.057      |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.0127     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 158        |
|    time_elapsed         | 8117       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.29843697 |
|    clip_fraction        | 0.721      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.57      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0364     |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0313    |
|    std                  | 0.71       |
|    value_loss           | 0.455      |
----------------------------------------
Num timesteps: 324000
Best mean reward: 628.96 - Last mean reward per episode: 630.64
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0552     |
| reward_motion           | 0          |
| reward_orientation      | 0.0392     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0572     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 159        |
|    time_elapsed         | 8169       |
|    total_timesteps      | 325632     |
| train/                  |            |
|    approx_kl            | 0.26496625 |
|    clip_fraction        | 0.724      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.57      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0382     |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.709      |
|    value_loss           | 0.397      |
----------------------------------------
----------------------------------------
| reward                  | 0.233      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0551     |
| reward_motion           | 0          |
| reward_orientation      | 0.0396     |
| reward_position         | 1.98e-138  |
| reward_rotation         | 0.0574     |
| reward_torque           | 0.0492     |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 160        |
|    time_elapsed         | 8222       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.29263696 |
|    clip_fraction        | 0.771      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.56      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0363     |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0122    |
|    std                  | 0.707      |
|    value_loss           | 0.496      |
----------------------------------------
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0204     |
| reward_ctrl             | 0.0562     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0586     |
| reward_torque           | 0.0494     |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 2.15e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 161        |
|    time_elapsed         | 8275       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.36204118 |
|    clip_fraction        | 0.728      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.54      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.218      |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.00266   |
|    std                  | 0.709      |
|    value_loss           | 1.67       |
----------------------------------------
Num timesteps: 330000
Best mean reward: 630.64 - Last mean reward per episode: 628.14
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0199     |
| reward_ctrl             | 0.0566     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0581     |
| reward_torque           | 0.0495     |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 2.15e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 162        |
|    time_elapsed         | 8326       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.21239832 |
|    clip_fraction        | 0.676      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.56      |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.414      |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0142    |
|    std                  | 0.707      |
|    value_loss           | 2.11       |
----------------------------------------
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0199     |
| reward_ctrl             | 0.0556     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0581     |
| reward_torque           | 0.0493     |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 633        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 163        |
|    time_elapsed         | 8377       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.26541921 |
|    clip_fraction        | 0.732      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.56      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0729     |
|    n_updates            | 1620       |
|    policy_gradient_loss | 0.00589    |
|    std                  | 0.708      |
|    value_loss           | 0.251      |
----------------------------------------
---------------------------------------
| reward                  | 0.235     |
| reward_contact          | 0.0199    |
| reward_ctrl             | 0.0556    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0581    |
| reward_torque           | 0.0493    |
| reward_velocity         | 0.0115    |
| rollout/                |           |
|    ep_len_mean          | 2.17e+03  |
|    ep_rew_mean          | 633       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 164       |
|    time_elapsed         | 8428      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.3169012 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.56     |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.419     |
|    n_updates            | 1630      |
|    policy_gradient_loss | -0.0257   |
|    std                  | 0.709     |
|    value_loss           | 0.495     |
---------------------------------------
Num timesteps: 336000
Best mean reward: 630.64 - Last mean reward per episode: 632.67
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0557     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0581     |
| reward_torque           | 0.0494     |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 633        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 165        |
|    time_elapsed         | 8480       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.34876698 |
|    clip_fraction        | 0.768      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.57      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.028     |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 0.711      |
|    value_loss           | 0.18       |
----------------------------------------
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.056      |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0581     |
| reward_torque           | 0.0495     |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 166        |
|    time_elapsed         | 8532       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.56642985 |
|    clip_fraction        | 0.775      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.59      |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0123    |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0389    |
|    std                  | 0.712      |
|    value_loss           | 0.31       |
----------------------------------------
Num timesteps: 342000
Best mean reward: 632.67 - Last mean reward per episode: 637.86
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0205     |
| reward_ctrl             | 0.0564     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0583     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 638        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 167        |
|    time_elapsed         | 8585       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.16546223 |
|    clip_fraction        | 0.637      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.58      |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0704     |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0286    |
|    std                  | 0.712      |
|    value_loss           | 1.14       |
----------------------------------------
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0563     |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0584     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 637        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 168        |
|    time_elapsed         | 8636       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.16756406 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.58      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0349     |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0212    |
|    std                  | 0.712      |
|    value_loss           | 0.61       |
----------------------------------------
--------------------------------------
| reward                  | 0.236    |
| reward_contact          | 0.0212   |
| reward_ctrl             | 0.0565   |
| reward_motion           | 0        |
| reward_orientation      | 0.0396   |
| reward_position         | 5.97e-32 |
| reward_rotation         | 0.0583   |
| reward_torque           | 0.0497   |
| reward_velocity         | 0.0107   |
| rollout/                |          |
|    ep_len_mean          | 2.16e+03 |
|    ep_rew_mean          | 634      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 169      |
|    time_elapsed         | 8688     |
|    total_timesteps      | 346112   |
| train/                  |          |
|    approx_kl            | 3.887515 |
|    clip_fraction        | 0.709    |
|    clip_range           | 0.2      |
|    entropy_loss         | -8.58    |
|    explained_variance   | 0.431    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.151    |
|    n_updates            | 1680     |
|    policy_gradient_loss | -0.0268  |
|    std                  | 0.71     |
|    value_loss           | 1.83     |
--------------------------------------
Num timesteps: 348000
Best mean reward: 637.86 - Last mean reward per episode: 634.18
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0212     |
| reward_ctrl             | 0.056      |
| reward_motion           | 0          |
| reward_orientation      | 0.0399     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.058      |
| reward_torque           | 0.0496     |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 170        |
|    time_elapsed         | 8740       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.25142664 |
|    clip_fraction        | 0.717      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.56      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.606      |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0272    |
|    std                  | 0.708      |
|    value_loss           | 2.12       |
----------------------------------------
----------------------------------------
| reward                  | 0.234      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0564     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0572     |
| reward_torque           | 0.0496     |
| reward_velocity         | 0.01       |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 639        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 171        |
|    time_elapsed         | 8790       |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.26600373 |
|    clip_fraction        | 0.695      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.53      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0905     |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0299    |
|    std                  | 0.707      |
|    value_loss           | 0.687      |
----------------------------------------
---------------------------------------
| reward                  | 0.234     |
| reward_contact          | 0.0206    |
| reward_ctrl             | 0.0564    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0572    |
| reward_torque           | 0.0496    |
| reward_velocity         | 0.01      |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 639       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 172       |
|    time_elapsed         | 8841      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.3258689 |
|    clip_fraction        | 0.734     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.5      |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0343    |
|    n_updates            | 1710      |
|    policy_gradient_loss | -0.0388   |
|    std                  | 0.702     |
|    value_loss           | 0.245     |
---------------------------------------
Num timesteps: 354000
Best mean reward: 637.86 - Last mean reward per episode: 639.51
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.235      |
| reward_contact          | 0.0206     |
| reward_ctrl             | 0.0566     |
| reward_motion           | 0          |
| reward_orientation      | 0.0396     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0583     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.01       |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 173        |
|    time_elapsed         | 8894       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.45299816 |
|    clip_fraction        | 0.787      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.45      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0291     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0327    |
|    std                  | 0.699      |
|    value_loss           | 0.221      |
----------------------------------------
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0212     |
| reward_ctrl             | 0.0566     |
| reward_motion           | 0          |
| reward_orientation      | 0.0393     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0595     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 174        |
|    time_elapsed         | 8946       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.27387643 |
|    clip_fraction        | 0.704      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.44      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00913    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0294    |
|    std                  | 0.7        |
|    value_loss           | 1.01       |
----------------------------------------
----------------------------------------
| reward                  | 0.236      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0561     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0584     |
| reward_torque           | 0.0496     |
| reward_velocity         | 0.01       |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 641        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 175        |
|    time_elapsed         | 8999       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.25595337 |
|    clip_fraction        | 0.676      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.43      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.022      |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0492    |
|    std                  | 0.698      |
|    value_loss           | 0.396      |
----------------------------------------
Num timesteps: 360000
Best mean reward: 639.51 - Last mean reward per episode: 643.39
Saving new best model to rl/out_dir/models/exp68/best_model.zip
--------------------------------------
| reward                  | 0.239    |
| reward_contact          | 0.0223   |
| reward_ctrl             | 0.0573   |
| reward_motion           | 0        |
| reward_orientation      | 0.04     |
| reward_position         | 5.97e-32 |
| reward_rotation         | 0.0598   |
| reward_torque           | 0.0498   |
| reward_velocity         | 0.01     |
| rollout/                |          |
|    ep_len_mean          | 2.18e+03 |
|    ep_rew_mean          | 643      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 176      |
|    time_elapsed         | 9052     |
|    total_timesteps      | 360448   |
| train/                  |          |
|    approx_kl            | 0.407978 |
|    clip_fraction        | 0.754    |
|    clip_range           | 0.2      |
|    entropy_loss         | -8.4     |
|    explained_variance   | 0.982    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0423  |
|    n_updates            | 1750     |
|    policy_gradient_loss | -0.049   |
|    std                  | 0.694    |
|    value_loss           | 0.36     |
--------------------------------------
---------------------------------------
| reward                  | 0.242     |
| reward_contact          | 0.0222    |
| reward_ctrl             | 0.0583    |
| reward_motion           | 0         |
| reward_orientation      | 0.0403    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0612    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.01      |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 645       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 177       |
|    time_elapsed         | 9104      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.3711501 |
|    clip_fraction        | 0.749     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.38     |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.107     |
|    n_updates            | 1760      |
|    policy_gradient_loss | -0.0297   |
|    std                  | 0.693     |
|    value_loss           | 0.483     |
---------------------------------------
----------------------------------------
| reward                  | 0.244      |
| reward_contact          | 0.0222     |
| reward_ctrl             | 0.058      |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0634     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00999    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 651        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 178        |
|    time_elapsed         | 9157       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.19714594 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.35      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0763     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0355    |
|    std                  | 0.691      |
|    value_loss           | 1.16       |
----------------------------------------
Num timesteps: 366000
Best mean reward: 643.39 - Last mean reward per episode: 650.73
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.246      |
| reward_contact          | 0.0222     |
| reward_ctrl             | 0.0582     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0653     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.01       |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 179        |
|    time_elapsed         | 9210       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.34653503 |
|    clip_fraction        | 0.747      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.33      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0397    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0519    |
|    std                  | 0.69       |
|    value_loss           | 0.279      |
----------------------------------------
----------------------------------------
| reward                  | 0.245      |
| reward_contact          | 0.0216     |
| reward_ctrl             | 0.0579     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0656     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00986    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 180        |
|    time_elapsed         | 9261       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.38436735 |
|    clip_fraction        | 0.759      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.32      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.792      |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.00234   |
|    std                  | 0.689      |
|    value_loss           | 0.902      |
----------------------------------------
----------------------------------------
| reward                  | 0.245      |
| reward_contact          | 0.0216     |
| reward_ctrl             | 0.0579     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0656     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00986    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 181        |
|    time_elapsed         | 9314       |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.23653261 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.32      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.285      |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.013     |
|    std                  | 0.69       |
|    value_loss           | 0.679      |
----------------------------------------
Num timesteps: 372000
Best mean reward: 650.73 - Last mean reward per episode: 651.74
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.246     |
| reward_contact          | 0.0222    |
| reward_ctrl             | 0.0579    |
| reward_motion           | 0         |
| reward_orientation      | 0.0408    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0651    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00979   |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 652       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 182       |
|    time_elapsed         | 9366      |
|    total_timesteps      | 372736    |
| train/                  |           |
|    approx_kl            | 0.3469035 |
|    clip_fraction        | 0.751     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.35     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0527   |
|    n_updates            | 1810      |
|    policy_gradient_loss | -0.0173   |
|    std                  | 0.692     |
|    value_loss           | 0.218     |
---------------------------------------
----------------------------------------
| reward                  | 0.246      |
| reward_contact          | 0.0228     |
| reward_ctrl             | 0.0574     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0672     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00831    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 183        |
|    time_elapsed         | 9418       |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.33334845 |
|    clip_fraction        | 0.726      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.35      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0056     |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0478    |
|    std                  | 0.69       |
|    value_loss           | 0.249      |
----------------------------------------
---------------------------------------
| reward                  | 0.246     |
| reward_contact          | 0.0228    |
| reward_ctrl             | 0.058     |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0655    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.0083    |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 651       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 184       |
|    time_elapsed         | 9470      |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.1849912 |
|    clip_fraction        | 0.651     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.33     |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0712    |
|    n_updates            | 1830      |
|    policy_gradient_loss | -0.0359   |
|    std                  | 0.689     |
|    value_loss           | 0.346     |
---------------------------------------
Num timesteps: 378000
Best mean reward: 651.74 - Last mean reward per episode: 653.11
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.248     |
| reward_contact          | 0.0229    |
| reward_ctrl             | 0.0586    |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0665    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00855   |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 653       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 185       |
|    time_elapsed         | 9523      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 1.3503845 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.31     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.148     |
|    n_updates            | 1840      |
|    policy_gradient_loss | -0.0314   |
|    std                  | 0.688     |
|    value_loss           | 0.994     |
---------------------------------------
---------------------------------------
| reward                  | 0.247     |
| reward_contact          | 0.0223    |
| reward_ctrl             | 0.0576    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0669    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00904   |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 653       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 186       |
|    time_elapsed         | 9575      |
|    total_timesteps      | 380928    |
| train/                  |           |
|    approx_kl            | 0.4141099 |
|    clip_fraction        | 0.729     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32     |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.436     |
|    n_updates            | 1850      |
|    policy_gradient_loss | -0.03     |
|    std                  | 0.69      |
|    value_loss           | 0.888     |
---------------------------------------
----------------------------------------
| reward                  | 0.246      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0576     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0665     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00904    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 654        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 187        |
|    time_elapsed         | 9627       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.28468233 |
|    clip_fraction        | 0.711      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.31      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.283      |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0304    |
|    std                  | 0.688      |
|    value_loss           | 0.82       |
----------------------------------------
Num timesteps: 384000
Best mean reward: 653.11 - Last mean reward per episode: 653.97
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.247      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0582     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0674     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.00903    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 655        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 188        |
|    time_elapsed         | 9679       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.31259963 |
|    clip_fraction        | 0.753      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.32      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0062    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.00729   |
|    std                  | 0.69       |
|    value_loss           | 0.38       |
----------------------------------------
----------------------------------------
| reward                  | 0.247      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0574     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0672     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00911    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 656        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 189        |
|    time_elapsed         | 9731       |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.37048164 |
|    clip_fraction        | 0.701      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.33      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.00351   |
|    std                  | 0.691      |
|    value_loss           | 1.82       |
----------------------------------------
---------------------------------------
| reward                  | 0.247     |
| reward_contact          | 0.0217    |
| reward_ctrl             | 0.0574    |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0672    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00911   |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 656       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 190       |
|    time_elapsed         | 9783      |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 0.3416584 |
|    clip_fraction        | 0.737     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32     |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0746    |
|    n_updates            | 1890      |
|    policy_gradient_loss | -0.0163   |
|    std                  | 0.688     |
|    value_loss           | 0.431     |
---------------------------------------
Num timesteps: 390000
Best mean reward: 653.97 - Last mean reward per episode: 656.18
Saving new best model to rl/out_dir/models/exp68/best_model.zip
---------------------------------------
| reward                  | 0.249     |
| reward_contact          | 0.0217    |
| reward_ctrl             | 0.0581    |
| reward_motion           | 0         |
| reward_orientation      | 0.0409    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0688    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.0091    |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 656       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 191       |
|    time_elapsed         | 9836      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.3152548 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.31     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0332    |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.00451  |
|    std                  | 0.69      |
|    value_loss           | 0.308     |
---------------------------------------
----------------------------------------
| reward                  | 0.248      |
| reward_contact          | 0.0217     |
| reward_ctrl             | 0.0583     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0675     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.0091     |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 192        |
|    time_elapsed         | 9887       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.31711233 |
|    clip_fraction        | 0.737      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.31      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00178   |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0222    |
|    std                  | 0.689      |
|    value_loss           | 0.256      |
----------------------------------------
----------------------------------------
| reward                  | 0.247      |
| reward_contact          | 0.0223     |
| reward_ctrl             | 0.0578     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.067      |
| reward_torque           | 0.0498     |
| reward_velocity         | 0.00914    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 658        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 193        |
|    time_elapsed         | 9940       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.18512824 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.31      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0107    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0505    |
|    std                  | 0.689      |
|    value_loss           | 0.479      |
----------------------------------------
Num timesteps: 396000
Best mean reward: 656.18 - Last mean reward per episode: 657.89
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.248      |
| reward_contact          | 0.0218     |
| reward_ctrl             | 0.0579     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0682     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00913    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 658        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 194        |
|    time_elapsed         | 9992       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.60473204 |
|    clip_fraction        | 0.821      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.32      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0283    |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.00684   |
|    std                  | 0.69       |
|    value_loss           | 0.198      |
----------------------------------------
----------------------------------------
| reward                  | 0.249      |
| reward_contact          | 0.0219     |
| reward_ctrl             | 0.0584     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0685     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00913    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 658        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 195        |
|    time_elapsed         | 10043      |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.14461003 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.32      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0704     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.035     |
|    std                  | 0.691      |
|    value_loss           | 0.668      |
----------------------------------------
---------------------------------------
| reward                  | 0.25      |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0586    |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.069     |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00913   |
| rollout/                |           |
|    ep_len_mean          | 2.2e+03   |
|    ep_rew_mean          | 659       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 196       |
|    time_elapsed         | 10095     |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.3096094 |
|    clip_fraction        | 0.741     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.3      |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0693    |
|    n_updates            | 1950      |
|    policy_gradient_loss | 0.0171    |
|    std                  | 0.688     |
|    value_loss           | 0.368     |
---------------------------------------
Num timesteps: 402000
Best mean reward: 657.89 - Last mean reward per episode: 658.89
Saving new best model to rl/out_dir/models/exp68/best_model.zip
----------------------------------------
| reward                  | 0.251      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.059      |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0693     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.00909    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 659        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 197        |
|    time_elapsed         | 10146      |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.16517107 |
|    clip_fraction        | 0.637      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.28      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0701     |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0391    |
|    std                  | 0.687      |
|    value_loss           | 0.605      |
----------------------------------------
----------------------------------------
| reward                  | 0.25       |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0595     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.068      |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00904    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 658        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 198        |
|    time_elapsed         | 10198      |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.31189406 |
|    clip_fraction        | 0.74       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.25      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.781      |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.00845   |
|    std                  | 0.683      |
|    value_loss           | 0.876      |
----------------------------------------
----------------------------------------
| reward                  | 0.25       |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0595     |
| reward_motion           | 0          |
| reward_orientation      | 0.0414     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.068      |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00904    |
| rollout/                |            |
|    ep_len_mean          | 2.2e+03    |
|    ep_rew_mean          | 658        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 199        |
|    time_elapsed         | 10250      |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.43008292 |
|    clip_fraction        | 0.794      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.24      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0169    |
|    n_updates            | 1980       |
|    policy_gradient_loss | 0.0228     |
|    std                  | 0.684      |
|    value_loss           | 0.119      |
----------------------------------------
Num timesteps: 408000
Best mean reward: 658.89 - Last mean reward per episode: 657.87
----------------------------------------
| reward                  | 0.252      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0602     |
| reward_motion           | 0          |
| reward_orientation      | 0.0408     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0683     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00969    |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 200        |
|    time_elapsed         | 10301      |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.21819198 |
|    clip_fraction        | 0.693      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.22      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.0222    |
|    std                  | 0.68       |
|    value_loss           | 0.429      |
----------------------------------------
---------------------------------------
| reward                  | 0.253     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0614    |
| reward_motion           | 0         |
| reward_orientation      | 0.0408    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0685    |
| reward_torque           | 0.0503    |
| reward_velocity         | 0.00923   |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 653       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 201       |
|    time_elapsed         | 10352     |
|    total_timesteps      | 411648    |
| train/                  |           |
|    approx_kl            | 0.4989283 |
|    clip_fraction        | 0.799     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.18     |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.161     |
|    n_updates            | 2000      |
|    policy_gradient_loss | -0.00604  |
|    std                  | 0.676     |
|    value_loss           | 1.36      |
---------------------------------------
---------------------------------------
| reward                  | 0.253     |
| reward_contact          | 0.0219    |
| reward_ctrl             | 0.0611    |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0694    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.00882   |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 653       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 202       |
|    time_elapsed         | 10404     |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 1.0997429 |
|    clip_fraction        | 0.796     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.16     |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.178     |
|    n_updates            | 2010      |
|    policy_gradient_loss | 0.0186    |
|    std                  | 0.677     |
|    value_loss           | 2.43      |
---------------------------------------
Num timesteps: 414000
Best mean reward: 658.89 - Last mean reward per episode: 652.72
---------------------------------------
| reward                  | 0.254     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0613    |
| reward_motion           | 0         |
| reward_orientation      | 0.0413    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0693    |
| reward_torque           | 0.0503    |
| reward_velocity         | 0.00878   |
| rollout/                |           |
|    ep_len_mean          | 2.17e+03  |
|    ep_rew_mean          | 651       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 203       |
|    time_elapsed         | 10455     |
|    total_timesteps      | 415744    |
| train/                  |           |
|    approx_kl            | 0.2051859 |
|    clip_fraction        | 0.682     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.16     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00453   |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0267   |
|    std                  | 0.677     |
|    value_loss           | 0.407     |
---------------------------------------
----------------------------------------
| reward                  | 0.254      |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0618     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0699     |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.00874    |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 650        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 204        |
|    time_elapsed         | 10507      |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.40850285 |
|    clip_fraction        | 0.766      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.16      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0941     |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0211    |
|    std                  | 0.676      |
|    value_loss           | 0.501      |
----------------------------------------
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0625     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0696     |
| reward_torque           | 0.0504     |
| reward_velocity         | 0.00893    |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 205        |
|    time_elapsed         | 10558      |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.33897743 |
|    clip_fraction        | 0.742      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.14      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.043      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.033     |
|    std                  | 0.674      |
|    value_loss           | 0.372      |
----------------------------------------
Num timesteps: 420000
Best mean reward: 658.89 - Last mean reward per episode: 648.98
----------------------------------------
| reward                  | 0.256      |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0627     |
| reward_motion           | 0          |
| reward_orientation      | 0.0413     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0696     |
| reward_torque           | 0.0504     |
| reward_velocity         | 0.00887    |
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 206        |
|    time_elapsed         | 10610      |
|    total_timesteps      | 421888     |
| train/                  |            |
|    approx_kl            | 0.28679806 |
|    clip_fraction        | 0.717      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.12      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.761      |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0453    |
|    std                  | 0.674      |
|    value_loss           | 0.541      |
----------------------------------------
---------------------------------------
| reward                  | 0.255     |
| reward_contact          | 0.0221    |
| reward_ctrl             | 0.0624    |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0699    |
| reward_torque           | 0.0504    |
| reward_velocity         | 0.00878   |
| rollout/                |           |
|    ep_len_mean          | 2.17e+03  |
|    ep_rew_mean          | 649       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 207       |
|    time_elapsed         | 10661     |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 0.2989729 |
|    clip_fraction        | 0.697     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.1      |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0226    |
|    n_updates            | 2060      |
|    policy_gradient_loss | -0.0284   |
|    std                  | 0.671     |
|    value_loss           | 0.712     |
---------------------------------------
---------------------------------------
| reward                  | 0.254     |
| reward_contact          | 0.0227    |
| reward_ctrl             | 0.0624    |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 5.97e-32  |
| reward_rotation         | 0.0697    |
| reward_torque           | 0.0503    |
| reward_velocity         | 0.00783   |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 642       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 208       |
|    time_elapsed         | 10713     |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.3490117 |
|    clip_fraction        | 0.758     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.08     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0536    |
|    n_updates            | 2070      |
|    policy_gradient_loss | -0.0168   |
|    std                  | 0.669     |
|    value_loss           | 0.268     |
---------------------------------------
Num timesteps: 426000
Best mean reward: 658.89 - Last mean reward per episode: 642.31
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0227     |
| reward_ctrl             | 0.0626     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0708     |
| reward_torque           | 0.0504     |
| reward_velocity         | 0.00782    |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 642        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 209        |
|    time_elapsed         | 10763      |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.28182438 |
|    clip_fraction        | 0.746      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.08      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.218      |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.00201   |
|    std                  | 0.669      |
|    value_loss           | 0.928      |
----------------------------------------
----------------------------------------
| reward                  | 0.254      |
| reward_contact          | 0.0221     |
| reward_ctrl             | 0.0621     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 5.97e-32   |
| reward_rotation         | 0.0711     |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.00781    |
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 648        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 210        |
|    time_elapsed         | 10816      |
|    total_timesteps      | 430080     |
| train/                  |            |
|    approx_kl            | 0.20391029 |
|    clip_fraction        | 0.683      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.06      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.287      |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0343    |
|    std                  | 0.668      |
|    value_loss           | 0.784      |
----------------------------------------
Num timesteps: 432000
Best mean reward: 658.89 - Last mean reward per episode: 642.76
---------------------------------------
| reward                  | 0.253     |
| reward_contact          | 0.0226    |
| reward_ctrl             | 0.0609    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0704    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00778   |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 643       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 211       |
|    time_elapsed         | 10868     |
|    total_timesteps      | 432128    |
| train/                  |           |
|    approx_kl            | 0.4233724 |
|    clip_fraction        | 0.764     |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.04     |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0317   |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.0385   |
|    std                  | 0.665     |
|    value_loss           | 0.307     |
---------------------------------------
----------------------------------------
| reward                  | 0.252      |
| reward_contact          | 0.022      |
| reward_ctrl             | 0.0609     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0701     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.00757    |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 643        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 212        |
|    time_elapsed         | 10920      |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.39995623 |
|    clip_fraction        | 0.748      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.01      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0372    |
|    std                  | 0.662      |
|    value_loss           | 1.39       |
----------------------------------------
----------------------------------------
| reward                  | 0.251      |
| reward_contact          | 0.022      |
| reward_ctrl             | 0.0602     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0706     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00757    |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 643        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 213        |
|    time_elapsed         | 10972      |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.37167236 |
|    clip_fraction        | 0.724      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.99      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00688    |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0397    |
|    std                  | 0.663      |
|    value_loss           | 0.315      |
----------------------------------------
Num timesteps: 438000
Best mean reward: 658.89 - Last mean reward per episode: 642.53
----------------------------------------
| reward                  | 0.251      |
| reward_contact          | 0.022      |
| reward_ctrl             | 0.0602     |
| reward_motion           | 0          |
| reward_orientation      | 0.041      |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0706     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00757    |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 643        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 214        |
|    time_elapsed         | 11023      |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.26348346 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.96      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00954   |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0306    |
|    std                  | 0.658      |
|    value_loss           | 0.44       |
----------------------------------------
---------------------------------------
| reward                  | 0.253     |
| reward_contact          | 0.0226    |
| reward_ctrl             | 0.0598    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0725    |
| reward_torque           | 0.0498    |
| reward_velocity         | 0.00741   |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 643       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 215       |
|    time_elapsed         | 11075     |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.3152765 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.92     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.156     |
|    n_updates            | 2140      |
|    policy_gradient_loss | -0.0202   |
|    std                  | 0.656     |
|    value_loss           | 0.708     |
---------------------------------------
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0226     |
| reward_ctrl             | 0.0602     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0737     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.0074     |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 645        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 216        |
|    time_elapsed         | 11127      |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.34939376 |
|    clip_fraction        | 0.752      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.92      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0229     |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.035     |
|    std                  | 0.656      |
|    value_loss           | 0.31       |
----------------------------------------
Num timesteps: 444000
Best mean reward: 658.89 - Last mean reward per episode: 652.27
---------------------------------------
| reward                  | 0.257     |
| reward_contact          | 0.022     |
| reward_ctrl             | 0.0611    |
| reward_motion           | 0         |
| reward_orientation      | 0.0408    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0759    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00744   |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 652       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 217       |
|    time_elapsed         | 11179     |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 0.3621136 |
|    clip_fraction        | 0.709     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.91     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0377    |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.05     |
|    std                  | 0.654     |
|    value_loss           | 0.43      |
---------------------------------------
--------------------------------------
| reward                  | 0.26     |
| reward_contact          | 0.0225   |
| reward_ctrl             | 0.0616   |
| reward_motion           | 0        |
| reward_orientation      | 0.0412   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.0765   |
| reward_torque           | 0.0501   |
| reward_velocity         | 0.00781  |
| rollout/                |          |
|    ep_len_mean          | 2.18e+03 |
|    ep_rew_mean          | 655      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 218      |
|    time_elapsed         | 11231    |
|    total_timesteps      | 446464   |
| train/                  |          |
|    approx_kl            | 0.390864 |
|    clip_fraction        | 0.733    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.89    |
|    explained_variance   | 0.559    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0689   |
|    n_updates            | 2170     |
|    policy_gradient_loss | -0.0313  |
|    std                  | 0.651    |
|    value_loss           | 0.826    |
--------------------------------------
---------------------------------------
| reward                  | 0.26      |
| reward_contact          | 0.0224    |
| reward_ctrl             | 0.0608    |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.078     |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00782   |
| rollout/                |           |
|    ep_len_mean          | 2.18e+03  |
|    ep_rew_mean          | 655       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 219       |
|    time_elapsed         | 11283     |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.3126086 |
|    clip_fraction        | 0.749     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.89     |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.234     |
|    n_updates            | 2180      |
|    policy_gradient_loss | -0.0185   |
|    std                  | 0.653     |
|    value_loss           | 1.79      |
---------------------------------------
Num timesteps: 450000
Best mean reward: 658.89 - Last mean reward per episode: 648.81
----------------------------------------
| reward                  | 0.263      |
| reward_contact          | 0.0224     |
| reward_ctrl             | 0.0618     |
| reward_motion           | 0          |
| reward_orientation      | 0.0415     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0796     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00798    |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 220        |
|    time_elapsed         | 11335      |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.30191302 |
|    clip_fraction        | 0.712      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.89      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0343     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0117    |
|    std                  | 0.654      |
|    value_loss           | 1.26       |
----------------------------------------
----------------------------------------
| reward                  | 0.263      |
| reward_contact          | 0.0224     |
| reward_ctrl             | 0.0612     |
| reward_motion           | 0          |
| reward_orientation      | 0.0416     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0795     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.008      |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 649        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 221        |
|    time_elapsed         | 11387      |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.34711233 |
|    clip_fraction        | 0.743      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.9       |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00225    |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0235    |
|    std                  | 0.654      |
|    value_loss           | 0.83       |
----------------------------------------
---------------------------------------
| reward                  | 0.263     |
| reward_contact          | 0.0218    |
| reward_ctrl             | 0.0616    |
| reward_motion           | 0         |
| reward_orientation      | 0.0415    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0797    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.008     |
| rollout/                |           |
|    ep_len_mean          | 2.16e+03  |
|    ep_rew_mean          | 650       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 222       |
|    time_elapsed         | 11438     |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 0.3547344 |
|    clip_fraction        | 0.725     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.89     |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.172     |
|    n_updates            | 2210      |
|    policy_gradient_loss | -0.012    |
|    std                  | 0.654     |
|    value_loss           | 0.588     |
---------------------------------------
Num timesteps: 456000
Best mean reward: 658.89 - Last mean reward per episode: 649.58
----------------------------------------
| reward                  | 0.263      |
| reward_contact          | 0.0218     |
| reward_ctrl             | 0.0616     |
| reward_motion           | 0          |
| reward_orientation      | 0.0415     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0797     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.008      |
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 650        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 223        |
|    time_elapsed         | 11490      |
|    total_timesteps      | 456704     |
| train/                  |            |
|    approx_kl            | 0.44501355 |
|    clip_fraction        | 0.773      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.9       |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0482    |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0229    |
|    std                  | 0.654      |
|    value_loss           | 0.28       |
----------------------------------------
---------------------------------------
| reward                  | 0.263     |
| reward_contact          | 0.0224    |
| reward_ctrl             | 0.061     |
| reward_motion           | 0         |
| reward_orientation      | 0.0411    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0799    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00841   |
| rollout/                |           |
|    ep_len_mean          | 2.15e+03  |
|    ep_rew_mean          | 648       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 224       |
|    time_elapsed         | 11541     |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.4023441 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.88     |
|    explained_variance   | 0.947     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0717    |
|    n_updates            | 2230      |
|    policy_gradient_loss | -0.0414   |
|    std                  | 0.653     |
|    value_loss           | 0.406     |
---------------------------------------
----------------------------------------
| reward                  | 0.261      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0611     |
| reward_motion           | 0          |
| reward_orientation      | 0.0412     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0777     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00824    |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 641        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 225        |
|    time_elapsed         | 11593      |
|    total_timesteps      | 460800     |
| train/                  |            |
|    approx_kl            | 0.46849632 |
|    clip_fraction        | 0.748      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.87      |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0501     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0473    |
|    std                  | 0.651      |
|    value_loss           | 1.03       |
----------------------------------------
Num timesteps: 462000
Best mean reward: 658.89 - Last mean reward per episode: 640.67
---------------------------------------
| reward                  | 0.261     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0611    |
| reward_motion           | 0         |
| reward_orientation      | 0.0412    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0777    |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00824   |
| rollout/                |           |
|    ep_len_mean          | 2.13e+03  |
|    ep_rew_mean          | 641       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 226       |
|    time_elapsed         | 11644     |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.3000974 |
|    clip_fraction        | 0.733     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.83     |
|    explained_variance   | 0.934     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0327   |
|    n_updates            | 2250      |
|    policy_gradient_loss | -0.0215   |
|    std                  | 0.648     |
|    value_loss           | 0.711     |
---------------------------------------
---------------------------------------
| reward                  | 0.259     |
| reward_contact          | 0.0225    |
| reward_ctrl             | 0.0607    |
| reward_motion           | 0         |
| reward_orientation      | 0.0409    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0765    |
| reward_torque           | 0.0498    |
| reward_velocity         | 0.00825   |
| rollout/                |           |
|    ep_len_mean          | 2.13e+03  |
|    ep_rew_mean          | 639       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 227       |
|    time_elapsed         | 11696     |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.5681244 |
|    clip_fraction        | 0.791     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.82     |
|    explained_variance   | 0.936     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0123    |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.025    |
|    std                  | 0.648     |
|    value_loss           | 0.22      |
---------------------------------------
----------------------------------------
| reward                  | 0.257      |
| reward_contact          | 0.0219     |
| reward_ctrl             | 0.0601     |
| reward_motion           | 0          |
| reward_orientation      | 0.0409     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0769     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.00797    |
| rollout/                |            |
|    ep_len_mean          | 2.13e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 228        |
|    time_elapsed         | 11748      |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.37811136 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.82      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0696    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0464    |
|    std                  | 0.648      |
|    value_loss           | 0.367      |
----------------------------------------
Num timesteps: 468000
Best mean reward: 658.89 - Last mean reward per episode: 634.41
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0225     |
| reward_ctrl             | 0.0593     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0756     |
| reward_torque           | 0.0494     |
| reward_velocity         | 0.00771    |
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 229        |
|    time_elapsed         | 11800      |
|    total_timesteps      | 468992     |
| train/                  |            |
|    approx_kl            | 0.32049406 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.81      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.213      |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.647      |
|    value_loss           | 0.393      |
----------------------------------------
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0603     |
| reward_motion           | 0          |
| reward_orientation      | 0.0405     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0738     |
| reward_torque           | 0.0495     |
| reward_velocity         | 0.00772    |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 629        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 230        |
|    time_elapsed         | 11852      |
|    total_timesteps      | 471040     |
| train/                  |            |
|    approx_kl            | 0.31811652 |
|    clip_fraction        | 0.72       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.8       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0612     |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0468    |
|    std                  | 0.646      |
|    value_loss           | 0.874      |
----------------------------------------
---------------------------------------
| reward                  | 0.256     |
| reward_contact          | 0.0235    |
| reward_ctrl             | 0.0611    |
| reward_motion           | 0         |
| reward_orientation      | 0.0406    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0739    |
| reward_torque           | 0.0497    |
| reward_velocity         | 0.0077    |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 628       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 231       |
|    time_elapsed         | 11904     |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.3522588 |
|    clip_fraction        | 0.704     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.77     |
|    explained_variance   | 0.882     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0287    |
|    n_updates            | 2300      |
|    policy_gradient_loss | -0.0273   |
|    std                  | 0.644     |
|    value_loss           | 1.15      |
---------------------------------------
Num timesteps: 474000
Best mean reward: 658.89 - Last mean reward per episode: 628.39
----------------------------------------
| reward                  | 0.256      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0611     |
| reward_motion           | 0          |
| reward_orientation      | 0.0406     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0739     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.0077     |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 232        |
|    time_elapsed         | 11956      |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.33975416 |
|    clip_fraction        | 0.754      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.77      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0259     |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0104    |
|    std                  | 0.644      |
|    value_loss           | 0.363      |
----------------------------------------
--------------------------------------
| reward                  | 0.257    |
| reward_contact          | 0.0235   |
| reward_ctrl             | 0.0617   |
| reward_motion           | 0        |
| reward_orientation      | 0.0402   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.0739   |
| reward_torque           | 0.0498   |
| reward_velocity         | 0.00835  |
| rollout/                |          |
|    ep_len_mean          | 2.09e+03 |
|    ep_rew_mean          | 630      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 233      |
|    time_elapsed         | 12007    |
|    total_timesteps      | 477184   |
| train/                  |          |
|    approx_kl            | 3.923407 |
|    clip_fraction        | 0.883    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.79    |
|    explained_variance   | 0.213    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0191  |
|    n_updates            | 2320     |
|    policy_gradient_loss | -0.0043  |
|    std                  | 0.645    |
|    value_loss           | 0.322    |
--------------------------------------
----------------------------------------
| reward                  | 0.255      |
| reward_contact          | 0.0234     |
| reward_ctrl             | 0.0615     |
| reward_motion           | 0          |
| reward_orientation      | 0.0402     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0719     |
| reward_torque           | 0.0497     |
| reward_velocity         | 0.00831    |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 629        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 234        |
|    time_elapsed         | 12058      |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.35816884 |
|    clip_fraction        | 0.713      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.75      |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00731   |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0439    |
|    std                  | 0.641      |
|    value_loss           | 1.57       |
----------------------------------------
Num timesteps: 480000
Best mean reward: 658.89 - Last mean reward per episode: 629.86
---------------------------------------
| reward                  | 0.257     |
| reward_contact          | 0.0239    |
| reward_ctrl             | 0.0621    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0719    |
| reward_torque           | 0.0498    |
| reward_velocity         | 0.00892   |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 630       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 235       |
|    time_elapsed         | 12110     |
|    total_timesteps      | 481280    |
| train/                  |           |
|    approx_kl            | 0.4874789 |
|    clip_fraction        | 0.786     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.71     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0181    |
|    n_updates            | 2340      |
|    policy_gradient_loss | -0.0387   |
|    std                  | 0.638     |
|    value_loss           | 0.253     |
---------------------------------------
--------------------------------------
| reward                  | 0.257    |
| reward_contact          | 0.0245   |
| reward_ctrl             | 0.0623   |
| reward_motion           | 0        |
| reward_orientation      | 0.0399   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.0715   |
| reward_torque           | 0.0498   |
| reward_velocity         | 0.00892  |
| rollout/                |          |
|    ep_len_mean          | 2.09e+03 |
|    ep_rew_mean          | 631      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 236      |
|    time_elapsed         | 12162    |
|    total_timesteps      | 483328   |
| train/                  |          |
|    approx_kl            | 0.581377 |
|    clip_fraction        | 0.767    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.68    |
|    explained_variance   | 0.78     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0648   |
|    n_updates            | 2350     |
|    policy_gradient_loss | -0.0535  |
|    std                  | 0.636    |
|    value_loss           | 3.35     |
--------------------------------------
--------------------------------------
| reward                  | 0.259    |
| reward_contact          | 0.0246   |
| reward_ctrl             | 0.0624   |
| reward_motion           | 0        |
| reward_orientation      | 0.0398   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.073    |
| reward_torque           | 0.0499   |
| reward_velocity         | 0.00942  |
| rollout/                |          |
|    ep_len_mean          | 2.09e+03 |
|    ep_rew_mean          | 631      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 237      |
|    time_elapsed         | 12214    |
|    total_timesteps      | 485376   |
| train/                  |          |
|    approx_kl            | 4.081908 |
|    clip_fraction        | 0.832    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.64    |
|    explained_variance   | 0.978    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0871   |
|    n_updates            | 2360     |
|    policy_gradient_loss | -0.041   |
|    std                  | 0.633    |
|    value_loss           | 0.322    |
--------------------------------------
Num timesteps: 486000
Best mean reward: 658.89 - Last mean reward per episode: 631.02
---------------------------------------
| reward                  | 0.259     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0624    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.073     |
| reward_torque           | 0.0499    |
| reward_velocity         | 0.00942   |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 632       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 238       |
|    time_elapsed         | 12267     |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.1911989 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.6      |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.303     |
|    n_updates            | 2370      |
|    policy_gradient_loss | -0.0213   |
|    std                  | 0.631     |
|    value_loss           | 2.57      |
---------------------------------------
----------------------------------------
| reward                  | 0.261      |
| reward_contact          | 0.0252     |
| reward_ctrl             | 0.0627     |
| reward_motion           | 0          |
| reward_orientation      | 0.04       |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0733     |
| reward_torque           | 0.0499     |
| reward_velocity         | 0.00942    |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 239        |
|    time_elapsed         | 12318      |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.20093825 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.6       |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0578     |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0259    |
|    std                  | 0.632      |
|    value_loss           | 0.803      |
----------------------------------------
---------------------------------------
| reward                  | 0.26      |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0632    |
| reward_motion           | 0         |
| reward_orientation      | 0.0398    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0724    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00952   |
| rollout/                |           |
|    ep_len_mean          | 2.09e+03  |
|    ep_rew_mean          | 632       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 240       |
|    time_elapsed         | 12369     |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 3.6562424 |
|    clip_fraction        | 0.867     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.64     |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0169    |
|    n_updates            | 2390      |
|    policy_gradient_loss | 0.0455    |
|    std                  | 0.632     |
|    value_loss           | 0.289     |
---------------------------------------
Num timesteps: 492000
Best mean reward: 658.89 - Last mean reward per episode: 632.23
----------------------------------------
| reward                  | 0.262      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0642     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0743     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00952    |
| rollout/                |            |
|    ep_len_mean          | 2.09e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 241        |
|    time_elapsed         | 12420      |
|    total_timesteps      | 493568     |
| train/                  |            |
|    approx_kl            | 0.23313436 |
|    clip_fraction        | 0.701      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.6       |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.562      |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 0.632      |
|    value_loss           | 0.814      |
----------------------------------------
----------------------------------------
| reward                  | 0.262      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0643     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0742     |
| reward_torque           | 0.05       |
| reward_velocity         | 0.00947    |
| rollout/                |            |
|    ep_len_mean          | 2.08e+03   |
|    ep_rew_mean          | 630        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 242        |
|    time_elapsed         | 12472      |
|    total_timesteps      | 495616     |
| train/                  |            |
|    approx_kl            | 0.23341054 |
|    clip_fraction        | 0.709      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.62      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.4        |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.00193   |
|    std                  | 0.634      |
|    value_loss           | 0.647      |
----------------------------------------
--------------------------------------
| reward                  | 0.264    |
| reward_contact          | 0.024    |
| reward_ctrl             | 0.0653   |
| reward_motion           | 0        |
| reward_orientation      | 0.0407   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.0748   |
| reward_torque           | 0.0501   |
| reward_velocity         | 0.00943  |
| rollout/                |          |
|    ep_len_mean          | 2.08e+03 |
|    ep_rew_mean          | 632      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 243      |
|    time_elapsed         | 12524    |
|    total_timesteps      | 497664   |
| train/                  |          |
|    approx_kl            | 0.362539 |
|    clip_fraction        | 0.739    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.62    |
|    explained_variance   | 0.881    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0309  |
|    n_updates            | 2420     |
|    policy_gradient_loss | -0.0548  |
|    std                  | 0.633    |
|    value_loss           | 0.764    |
--------------------------------------
Num timesteps: 498000
Best mean reward: 658.89 - Last mean reward per episode: 631.51
----------------------------------------
| reward                  | 0.264      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0653     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0748     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00943    |
| rollout/                |            |
|    ep_len_mean          | 2.08e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 244        |
|    time_elapsed         | 12576      |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.34461898 |
|    clip_fraction        | 0.738      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.61      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0578     |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 0.634      |
|    value_loss           | 0.487      |
----------------------------------------
---------------------------------------
| reward                  | 0.265     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0659    |
| reward_motion           | 0         |
| reward_orientation      | 0.0405    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0755    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.00932   |
| rollout/                |           |
|    ep_len_mean          | 2.08e+03  |
|    ep_rew_mean          | 633       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 245       |
|    time_elapsed         | 12629     |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.4934681 |
|    clip_fraction        | 0.772     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.62     |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00334  |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.0243   |
|    std                  | 0.633     |
|    value_loss           | 0.469     |
---------------------------------------
---------------------------------------
| reward                  | 0.266     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0661    |
| reward_motion           | 0         |
| reward_orientation      | 0.0406    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0759    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.0094    |
| rollout/                |           |
|    ep_len_mean          | 2.08e+03  |
|    ep_rew_mean          | 633       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 246       |
|    time_elapsed         | 12680     |
|    total_timesteps      | 503808    |
| train/                  |           |
|    approx_kl            | 0.5186087 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.61     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0347   |
|    n_updates            | 2450      |
|    policy_gradient_loss | -0.0439   |
|    std                  | 0.632     |
|    value_loss           | 0.285     |
---------------------------------------
Num timesteps: 504000
Best mean reward: 658.89 - Last mean reward per episode: 633.38
----------------------------------------
| reward                  | 0.269      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0668     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0775     |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.00917    |
| rollout/                |            |
|    ep_len_mean          | 2.08e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 247        |
|    time_elapsed         | 12732      |
|    total_timesteps      | 505856     |
| train/                  |            |
|    approx_kl            | 0.60353285 |
|    clip_fraction        | 0.799      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.62      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0315    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0426    |
|    std                  | 0.634      |
|    value_loss           | 0.248      |
----------------------------------------
----------------------------------------
| reward                  | 0.266      |
| reward_contact          | 0.0241     |
| reward_ctrl             | 0.0664     |
| reward_motion           | 0          |
| reward_orientation      | 0.0403     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0759     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00924    |
| rollout/                |            |
|    ep_len_mean          | 2.05e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 248        |
|    time_elapsed         | 12783      |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.43158036 |
|    clip_fraction        | 0.706      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.6       |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.313      |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 0.632      |
|    value_loss           | 1.76       |
----------------------------------------
---------------------------------------
| reward                  | 0.264     |
| reward_contact          | 0.0241    |
| reward_ctrl             | 0.0662    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0744    |
| reward_torque           | 0.05      |
| reward_velocity         | 0.00928   |
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 624       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 249       |
|    time_elapsed         | 12835     |
|    total_timesteps      | 509952    |
| train/                  |           |
|    approx_kl            | 0.3939988 |
|    clip_fraction        | 0.734     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.61     |
|    explained_variance   | 0.896     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0385    |
|    n_updates            | 2480      |
|    policy_gradient_loss | -0.0273   |
|    std                  | 0.633     |
|    value_loss           | 0.668     |
---------------------------------------
Num timesteps: 510000
Best mean reward: 658.89 - Last mean reward per episode: 624.44
----------------------------------------
| reward                  | 0.264      |
| reward_contact          | 0.0241     |
| reward_ctrl             | 0.0669     |
| reward_motion           | 0          |
| reward_orientation      | 0.0398     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0736     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00925    |
| rollout/                |            |
|    ep_len_mean          | 2.05e+03   |
|    ep_rew_mean          | 624        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 250        |
|    time_elapsed         | 12886      |
|    total_timesteps      | 512000     |
| train/                  |            |
|    approx_kl            | 0.37242875 |
|    clip_fraction        | 0.75       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.6       |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0278     |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.022     |
|    std                  | 0.63       |
|    value_loss           | 0.804      |
----------------------------------------
----------------------------------------
| reward                  | 0.266      |
| reward_contact          | 0.0242     |
| reward_ctrl             | 0.0673     |
| reward_motion           | 0          |
| reward_orientation      | 0.0395     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0756     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00913    |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 621        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 251        |
|    time_elapsed         | 12938      |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.16801083 |
|    clip_fraction        | 0.653      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.56      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.318      |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0319    |
|    std                  | 0.629      |
|    value_loss           | 0.734      |
----------------------------------------
Num timesteps: 516000
Best mean reward: 658.89 - Last mean reward per episode: 622.31
----------------------------------------
| reward                  | 0.265      |
| reward_contact          | 0.0236     |
| reward_ctrl             | 0.0676     |
| reward_motion           | 0          |
| reward_orientation      | 0.0397     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0755     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00833    |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 622        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 252        |
|    time_elapsed         | 12989      |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.34670603 |
|    clip_fraction        | 0.749      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.57      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0412     |
|    n_updates            | 2510       |
|    policy_gradient_loss | -0.0147    |
|    std                  | 0.632      |
|    value_loss           | 1.63       |
----------------------------------------
----------------------------------------
| reward                  | 0.266      |
| reward_contact          | 0.0235     |
| reward_ctrl             | 0.0683     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0758     |
| reward_torque           | 0.0501     |
| reward_velocity         | 0.00828    |
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 623        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 253        |
|    time_elapsed         | 13041      |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.28622037 |
|    clip_fraction        | 0.717      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.57      |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.419      |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0345    |
|    std                  | 0.63       |
|    value_loss           | 0.93       |
----------------------------------------
---------------------------------------
| reward                  | 0.267     |
| reward_contact          | 0.024     |
| reward_ctrl             | 0.0691    |
| reward_motion           | 0         |
| reward_orientation      | 0.0401    |
| reward_position         | 1.74e-22  |
| reward_rotation         | 0.0752    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.00834   |
| rollout/                |           |
|    ep_len_mean          | 2.01e+03  |
|    ep_rew_mean          | 619       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 254       |
|    time_elapsed         | 13093     |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 0.6230756 |
|    clip_fraction        | 0.787     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.57     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0141   |
|    n_updates            | 2530      |
|    policy_gradient_loss | -0.0303   |
|    std                  | 0.627     |
|    value_loss           | 0.28      |
---------------------------------------
Num timesteps: 522000
Best mean reward: 658.89 - Last mean reward per episode: 619.48
----------------------------------------
| reward                  | 0.267      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.0691     |
| reward_motion           | 0          |
| reward_orientation      | 0.0401     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0752     |
| reward_torque           | 0.0502     |
| reward_velocity         | 0.00834    |
| rollout/                |            |
|    ep_len_mean          | 2.01e+03   |
|    ep_rew_mean          | 619        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 255        |
|    time_elapsed         | 13145      |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.42017162 |
|    clip_fraction        | 0.725      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.52      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.078      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0376    |
|    std                  | 0.623      |
|    value_loss           | 1.21       |
----------------------------------------
--------------------------------------
| reward                  | 0.268    |
| reward_contact          | 0.024    |
| reward_ctrl             | 0.0697   |
| reward_motion           | 0        |
| reward_orientation      | 0.0401   |
| reward_position         | 1.74e-22 |
| reward_rotation         | 0.0759   |
| reward_torque           | 0.0503   |
| reward_velocity         | 0.00797  |
| rollout/                |          |
|    ep_len_mean          | 2.01e+03 |
|    ep_rew_mean          | 619      |
| time/                   |          |
|    fps                  | 39       |
|    iterations           | 256      |
|    time_elapsed         | 13197    |
|    total_timesteps      | 524288   |
| train/                  |          |
|    approx_kl            | 0.530477 |
|    clip_fraction        | 0.776    |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.5     |
|    explained_variance   | 0.96     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0336  |
|    n_updates            | 2550     |
|    policy_gradient_loss | -0.0368  |
|    std                  | 0.624    |
|    value_loss           | 0.227    |
--------------------------------------
----------------------------------------
| reward                  | 0.269      |
| reward_contact          | 0.024      |
| reward_ctrl             | 0.07       |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 1.74e-22   |
| reward_rotation         | 0.0769     |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.00771    |
| rollout/                |            |
|    ep_len_mean          | 2.01e+03   |
|    ep_rew_mean          | 620        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 257        |
|    time_elapsed         | 13251      |
|    total_timesteps      | 526336     |
| train/                  |            |
|    approx_kl            | 0.34337175 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.5       |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0948     |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0502    |
|    std                  | 0.624      |
|    value_loss           | 0.498      |
----------------------------------------
Num timesteps: 528000
Best mean reward: 658.89 - Last mean reward per episode: 613.74
----------------------------------------
| reward                  | 0.27       |
| reward_contact          | 0.0252     |
| reward_ctrl             | 0.0694     |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 2.93e-08   |
| reward_rotation         | 0.0773     |
| reward_torque           | 0.0502     |
| reward_velocity         | 0.00761    |
| rollout/                |            |
|    ep_len_mean          | 1.98e+03   |
|    ep_rew_mean          | 614        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 258        |
|    time_elapsed         | 13306      |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.32745886 |
|    clip_fraction        | 0.773      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.49      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.028      |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0357    |
|    std                  | 0.623      |
|    value_loss           | 0.415      |
----------------------------------------
----------------------------------------
| reward                  | 0.271      |
| reward_contact          | 0.0246     |
| reward_ctrl             | 0.07       |
| reward_motion           | 0          |
| reward_orientation      | 0.0404     |
| reward_position         | 2.93e-08   |
| reward_rotation         | 0.0781     |
| reward_torque           | 0.0503     |
| reward_velocity         | 0.0076     |
| rollout/                |            |
|    ep_len_mean          | 2e+03      |
|    ep_rew_mean          | 621        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 259        |
|    time_elapsed         | 13365      |
|    total_timesteps      | 530432     |
| train/                  |            |
|    approx_kl            | 0.25063777 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.5       |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0416     |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 0.625      |
|    value_loss           | 1.41       |
----------------------------------------
---------------------------------------
| reward                  | 0.272     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0704    |
| reward_motion           | 0         |
| reward_orientation      | 0.0408    |
| reward_position         | 2.93e-08  |
| reward_rotation         | 0.0781    |
| reward_torque           | 0.0503    |
| reward_velocity         | 0.0076    |
| rollout/                |           |
|    ep_len_mean          | 2e+03     |
|    ep_rew_mean          | 622       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 260       |
|    time_elapsed         | 13419     |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.2949053 |
|    clip_fraction        | 0.712     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.52     |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0769    |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.0293   |
|    std                  | 0.625     |
|    value_loss           | 0.585     |
---------------------------------------
Num timesteps: 534000
Best mean reward: 658.89 - Last mean reward per episode: 621.70
----------------------------------------
| reward                  | 0.273      |
| reward_contact          | 0.0246     |
| reward_ctrl             | 0.0708     |
| reward_motion           | 0          |
| reward_orientation      | 0.0407     |
| reward_position         | 1.11e-06   |
| reward_rotation         | 0.0787     |
| reward_torque           | 0.0502     |
| reward_velocity         | 0.00842    |
| rollout/                |            |
|    ep_len_mean          | 1.96e+03   |
|    ep_rew_mean          | 610        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 261        |
|    time_elapsed         | 13471      |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.37138414 |
|    clip_fraction        | 0.746      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.51      |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0478     |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0393    |
|    std                  | 0.625      |
|    value_loss           | 0.463      |
----------------------------------------
---------------------------------------
| reward                  | 0.273     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0708    |
| reward_motion           | 0         |
| reward_orientation      | 0.0407    |
| reward_position         | 1.11e-06  |
| reward_rotation         | 0.0787    |
| reward_torque           | 0.0502    |
| reward_velocity         | 0.00842   |
| rollout/                |           |
|    ep_len_mean          | 1.96e+03  |
|    ep_rew_mean          | 610       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 262       |
|    time_elapsed         | 13530     |
|    total_timesteps      | 536576    |
| train/                  |           |
|    approx_kl            | 0.2973291 |
|    clip_fraction        | 0.687     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.5      |
|    explained_variance   | 0.843     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.386     |
|    n_updates            | 2610      |
|    policy_gradient_loss | -0.0306   |
|    std                  | 0.624     |
|    value_loss           | 2.02      |
---------------------------------------
---------------------------------------
| reward                  | 0.275     |
| reward_contact          | 0.0246    |
| reward_ctrl             | 0.0709    |
| reward_motion           | 0         |
| reward_orientation      | 0.041     |
| reward_position         | 1.11e-06  |
| reward_rotation         | 0.0799    |
| reward_torque           | 0.0503    |
| reward_velocity         | 0.00842   |
| rollout/                |           |
|    ep_len_mean          | 1.96e+03  |
|    ep_rew_mean          | 611       |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 263       |
|    time_elapsed         | 13581     |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.8389883 |
|    clip_fraction        | 0.828     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.53     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0281   |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.00651  |
|    std                  | 0.626     |
|    value_loss           | 0.184     |
---------------------------------------
