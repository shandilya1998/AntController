running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 308      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0461    |
| reward_ctrl             | 6.04e-05   |
| reward_motion           | 1.47e-06   |
| reward_position         | 0.00022    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.677      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 392        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.15049204 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | -7.55e-05  |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.368      |
|    value_loss           | 1.18e+03   |
----------------------------------------
----------------------------------------
| reward                  | -2.23      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.645      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 3          |
|    time_elapsed         | 13         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.10099134 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.369      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0474    |
| reward_ctrl             | 3.22e-05   |
| reward_motion           | 8.05e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.706      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 4          |
|    time_elapsed         | 18         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.15403327 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.2      |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.368      |
|    value_loss           | 672        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 2.92e-05   |
| reward_motion           | 7.39e-07   |
| reward_position         | 0.000111   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 5          |
|    time_elapsed         | 23         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.13035062 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.1      |
|    explained_variance   | 0.408      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.5       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.368      |
|    value_loss           | 684        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 630.72
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.047      |
| reward_ctrl             | 2.97e-05    |
| reward_motion           | 6.88e-07    |
| reward_position         | 0.000103    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 6           |
|    time_elapsed         | 28          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.052000042 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.7        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.368       |
|    value_loss           | 745         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.03       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 2.66e-05    |
| reward_motion           | 6.1e-07     |
| reward_position         | 9.15e-05    |
| reward_torque           | -2.75       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 550         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 7           |
|    time_elapsed         | 34          |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.037831835 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.3       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.6        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 572         |
-----------------------------------------
----------------------------------------
| reward                  | -2.04      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 2.74e-05   |
| reward_motion           | 6.05e-07   |
| reward_position         | 9.08e-05   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 8          |
|    time_elapsed         | 39         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04968562 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 62.2       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.368      |
|    value_loss           | 608        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 2.63e-05   |
| reward_motion           | 5.87e-07   |
| reward_position         | 8.8e-05    |
| reward_torque           | -2.66      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 618        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 9          |
|    time_elapsed         | 44         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.10000177 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 53.1       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.368      |
|    value_loss           | 664        |
----------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.54e-05    |
| reward_motion           | 5.8e-07     |
| reward_position         | 8.71e-05    |
| reward_torque           | -2.72       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 659         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 10          |
|    time_elapsed         | 49          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.045438074 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0477     |
|    std                  | 0.368       |
|    value_loss           | 853         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.87       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.56e-05    |
| reward_motion           | 5.89e-07    |
| reward_position         | 8.84e-05    |
| reward_torque           | -2.59       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 11          |
|    time_elapsed         | 54          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.087017946 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.4        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.368       |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 630.72 - Last mean reward per episode: 678.77
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.5e-05     |
| reward_motion           | 5.78e-07    |
| reward_position         | 8.67e-05    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.786       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 666         |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 12          |
|    time_elapsed         | 59          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.037214965 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.8       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0488     |
|    std                  | 0.368       |
|    value_loss           | 700         |
-----------------------------------------
----------------------------------------
| reward                  | -1.95      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.45e-05   |
| reward_motion           | 7.29e-07   |
| reward_position         | 0.000109   |
| reward_torque           | -2.69      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 13         |
|    time_elapsed         | 65         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.03277895 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 91.2       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0453    |
|    std                  | 0.368      |
|    value_loss           | 757        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.49e-05   |
| reward_motion           | 7.56e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -2.67      |
| reward_velocity         | 0.782      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 730        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 14         |
|    time_elapsed         | 70         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09893784 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14        |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 88         |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.368      |
|    value_loss           | 739        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 7.95e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 15          |
|    time_elapsed         | 75          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.059402347 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16         |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0687     |
|    std                  | 0.368       |
|    value_loss           | 501         |
-----------------------------------------
----------------------------------------
| reward                  | -1.92      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.68      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 772        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 16         |
|    time_elapsed         | 80         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05526676 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0708    |
|    std                  | 0.368      |
|    value_loss           | 819        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.65e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -2.68       |
| reward_velocity         | 0.803       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 792         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 17          |
|    time_elapsed         | 85          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.072550446 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0741     |
|    std                  | 0.368       |
|    value_loss           | 811         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 678.77 - Last mean reward per episode: 791.68
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.88       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.31e-05    |
| reward_motion           | 9e-07       |
| reward_position         | 0.000135    |
| reward_torque           | -2.64       |
| reward_velocity         | 0.809       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 18          |
|    time_elapsed         | 91          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.050225984 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 94          |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.368       |
|    value_loss           | 709         |
-----------------------------------------
----------------------------------------
| reward                  | -1.88      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.82       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 19         |
|    time_elapsed         | 96         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.04830476 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 161        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.368      |
|    value_loss           | 733        |
----------------------------------------
----------------------------------------
| reward                  | -1.89      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.814      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 20         |
|    time_elapsed         | 101        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.06929466 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.368      |
|    value_loss           | 760        |
----------------------------------------
-----------------------------------------
| reward                  | -1.91       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.11e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.67       |
| reward_velocity         | 0.808       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 21          |
|    time_elapsed         | 106         |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.062169574 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27e+03    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.368       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 3.99e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.806       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 771         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 22          |
|    time_elapsed         | 111         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.046546943 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.1       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 722         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0547     |
|    std                  | 0.368       |
|    value_loss           | 815         |
-----------------------------------------
----------------------------------------
| reward                  | -1.98      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.12e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.74      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 792        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 23         |
|    time_elapsed         | 116        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.03915607 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 146        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0404    |
|    std                  | 0.368      |
|    value_loss           | 648        |
----------------------------------------
Num timesteps: 24000
Best mean reward: 791.68 - Last mean reward per episode: 791.71
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2.04       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.804       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 765         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 24          |
|    time_elapsed         | 121         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.039605044 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 219         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0567     |
|    std                  | 0.368       |
|    value_loss           | 903         |
-----------------------------------------
---------------------------------------
| reward                  | -2.02     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 4.09e-05  |
| reward_motion           | 8.53e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.797     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 25        |
|    time_elapsed         | 126       |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.0371536 |
|    clip_fraction        | 0.0811    |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.7     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 354       |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0502   |
|    std                  | 0.368     |
|    value_loss           | 939       |
---------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.38e-07    |
| reward_position         | 0.000126    |
| reward_torque           | -2.74       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 768         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 26          |
|    time_elapsed         | 131         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.063086435 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.9        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0678     |
|    std                  | 0.368       |
|    value_loss           | 771         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.99       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.27e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.73       |
| reward_velocity         | 0.788       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 769         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 27          |
|    time_elapsed         | 136         |
|    total_timesteps      | 27648       |
| train/                  |             |
|    approx_kl            | 0.055129383 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1e+03     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0717     |
|    std                  | 0.368       |
|    value_loss           | 819         |
-----------------------------------------
---------------------------------------
| reward                  | -2.03     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.95e-05  |
| reward_motion           | 8.25e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.783     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 744       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 28        |
|    time_elapsed         | 141       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0534485 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.964     |
|    learning_rate        | 0.0003    |
|    loss                 | 203       |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.0704   |
|    std                  | 0.368     |
|    value_loss           | 804       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.01e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -2.81      |
| reward_velocity         | 0.786      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 723        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 29         |
|    time_elapsed         | 146        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.05150444 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27e+03   |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0435    |
|    std                  | 0.368      |
|    value_loss           | 829        |
----------------------------------------
Num timesteps: 30000
Best mean reward: 791.71 - Last mean reward per episode: 722.75
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -2.84      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 30         |
|    time_elapsed         | 151        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06510179 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35e+03   |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.368      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.55e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 31          |
|    time_elapsed         | 156         |
|    total_timesteps      | 31744       |
| train/                  |             |
|    approx_kl            | 0.051414184 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0474     |
|    std                  | 0.368       |
|    value_loss           | 672         |
-----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.57e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 32         |
|    time_elapsed         | 161        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07030794 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 680        |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0665    |
|    std                  | 0.368      |
|    value_loss           | 708        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.1e-05    |
| reward_motion           | 8.48e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 728        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 33         |
|    time_elapsed         | 166        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.07115512 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.368      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.23e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 735        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 34         |
|    time_elapsed         | 171        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05076515 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.367      |
|    value_loss           | 843        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.14e-05    |
| reward_motion           | 8.46e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.82       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 720         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 35          |
|    time_elapsed         | 177         |
|    total_timesteps      | 35840       |
| train/                  |             |
|    approx_kl            | 0.095722534 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0874     |
|    std                  | 0.367       |
|    value_loss           | 699         |
-----------------------------------------
Num timesteps: 36000
Best mean reward: 791.71 - Last mean reward per episode: 720.11
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 709         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 36          |
|    time_elapsed         | 182         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.056049634 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0406     |
|    std                  | 0.367       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 718         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 37          |
|    time_elapsed         | 187         |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.044259906 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 88.2        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0577     |
|    std                  | 0.367       |
|    value_loss           | 972         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 38         |
|    time_elapsed         | 192        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06741989 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 185        |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.5e-07    |
| reward_position         | 0.000143   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 720        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 39         |
|    time_elapsed         | 197        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.05378554 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 598        |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.056     |
|    std                  | 0.367      |
|    value_loss           | 841        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.42e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 706        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 40         |
|    time_elapsed         | 202        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.05185401 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0418    |
|    std                  | 0.367      |
|    value_loss           | 594        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 705        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 41         |
|    time_elapsed         | 207        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.08426472 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0562    |
|    std                  | 0.367      |
|    value_loss           | 873        |
----------------------------------------
Num timesteps: 42000
Best mean reward: 791.71 - Last mean reward per episode: 705.46
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 42         |
|    time_elapsed         | 212        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04333145 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0615    |
|    std                  | 0.367      |
|    value_loss           | 876        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 709        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 43         |
|    time_elapsed         | 217        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.06598474 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.2       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.367      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 706         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 44          |
|    time_elapsed         | 223         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.065347165 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.367       |
|    value_loss           | 765         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.59e-05    |
| reward_motion           | 9.43e-07    |
| reward_position         | 0.000141    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 703         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 45          |
|    time_elapsed         | 228         |
|    total_timesteps      | 46080       |
| train/                  |             |
|    approx_kl            | 0.054258935 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.3       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 200         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0616     |
|    std                  | 0.367       |
|    value_loss           | 722         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 717        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 46         |
|    time_elapsed         | 233        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06634159 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.367      |
|    value_loss           | 761        |
----------------------------------------
Num timesteps: 48000
Best mean reward: 791.71 - Last mean reward per episode: 716.92
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.87e-05   |
| reward_motion           | 9.85e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 712        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 47         |
|    time_elapsed         | 238        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.05172252 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.2      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.367      |
|    value_loss           | 678        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.97e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000153    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 713         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 48          |
|    time_elapsed         | 243         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.035609327 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0352     |
|    std                  | 0.367       |
|    value_loss           | 728         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.93e-05   |
| reward_motion           | 1.02e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 697        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 49         |
|    time_elapsed         | 248        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.05193235 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 342        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.367      |
|    value_loss           | 686        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.98e-05    |
| reward_motion           | 1.03e-06    |
| reward_position         | 0.000155    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 687         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 50          |
|    time_elapsed         | 253         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.053019665 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43e+03    |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0614     |
|    std                  | 0.367       |
|    value_loss           | 727         |
-----------------------------------------
---------------------------------------
| reward                  | -2.08     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.91e-05  |
| reward_motion           | 1.02e-06  |
| reward_position         | 0.000153  |
| reward_torque           | -2.8      |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 671       |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 51        |
|    time_elapsed         | 259       |
|    total_timesteps      | 52224     |
| train/                  |           |
|    approx_kl            | 0.0752333 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 125       |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0765   |
|    std                  | 0.367     |
|    value_loss           | 665       |
---------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.96e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000154    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 669         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 52          |
|    time_elapsed         | 264         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.042688258 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0545     |
|    std                  | 0.367       |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 54000
Best mean reward: 791.71 - Last mean reward per episode: 669.18
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.93e-05    |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000152    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 667         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 53          |
|    time_elapsed         | 269         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.032322936 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.04       |
|    std                  | 0.367       |
|    value_loss           | 814         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.88e-05   |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000151   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 673        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 54         |
|    time_elapsed         | 274        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06756666 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 138        |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.367      |
|    value_loss           | 715        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.84e-05    |
| reward_motion           | 9.95e-07    |
| reward_position         | 0.000149    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.756       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 676         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 55          |
|    time_elapsed         | 280         |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.044751495 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.367       |
|    value_loss           | 824         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.9e-05    |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 681        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 56         |
|    time_elapsed         | 285        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08345644 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 158        |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.367      |
|    value_loss           | 586        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.9e-05     |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000151    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 57          |
|    time_elapsed         | 290         |
|    total_timesteps      | 58368       |
| train/                  |             |
|    approx_kl            | 0.043336414 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.6        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0627     |
|    std                  | 0.367       |
|    value_loss           | 733         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.86e-05    |
| reward_motion           | 9.99e-07    |
| reward_position         | 0.00015     |
| reward_torque           | -2.77       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 672         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 58          |
|    time_elapsed         | 295         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.057172395 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 0.367       |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 60000
Best mean reward: 791.71 - Last mean reward per episode: 672.44
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.79e-05   |
| reward_motion           | 9.88e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 676        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 59         |
|    time_elapsed         | 300        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.07305443 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 160        |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0693    |
|    std                  | 0.367      |
|    value_loss           | 658        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.83e-05    |
| reward_motion           | 9.9e-07     |
| reward_position         | 0.000148    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 675         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 60          |
|    time_elapsed         | 305         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.048278354 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 562         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0548     |
|    std                  | 0.367       |
|    value_loss           | 745         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.78e-05   |
| reward_motion           | 9.8e-07    |
| reward_position         | 0.000147   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 672        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 61         |
|    time_elapsed         | 310        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.06619827 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 646        |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 901        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.69e-07   |
| reward_position         | 0.000145   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 666        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 62         |
|    time_elapsed         | 315        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.07789113 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0805    |
|    std                  | 0.367      |
|    value_loss           | 870        |
----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.68e-05    |
| reward_motion           | 9.63e-07    |
| reward_position         | 0.000144    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 664         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 63          |
|    time_elapsed         | 320         |
|    total_timesteps      | 64512       |
| train/                  |             |
|    approx_kl            | 0.051378116 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.8        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0606     |
|    std                  | 0.367       |
|    value_loss           | 646         |
-----------------------------------------
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.63e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 64         |
|    time_elapsed         | 325        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.07146649 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
Num timesteps: 66000
Best mean reward: 791.71 - Last mean reward per episode: 656.82
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.62e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 65         |
|    time_elapsed         | 329        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.06033282 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.9       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0711    |
|    std                  | 0.367      |
|    value_loss           | 651        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.54e-07    |
| reward_position         | 0.000143    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 657         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 66          |
|    time_elapsed         | 333         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.047285896 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 277         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.367       |
|    value_loss           | 624         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 9.48e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 67         |
|    time_elapsed         | 337        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.07419628 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 111        |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.367      |
|    value_loss           | 697        |
----------------------------------------
---------------------------------------
| reward                  | -2.07     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 9.41e-07  |
| reward_position         | 0.000141  |
| reward_torque           | -2.78     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 654       |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 68        |
|    time_elapsed         | 341       |
|    total_timesteps      | 69632     |
| train/                  |           |
|    approx_kl            | 0.0687863 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 348       |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0664   |
|    std                  | 0.367     |
|    value_loss           | 631       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 653        |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 69         |
|    time_elapsed         | 344        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.06904027 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.4       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.367      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.67e-05   |
| reward_motion           | 9.54e-07   |
| reward_position         | 0.000143   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 651        |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 70         |
|    time_elapsed         | 347        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.08205227 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 159        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.367      |
|    value_loss           | 587        |
----------------------------------------
Num timesteps: 72000
Best mean reward: 791.71 - Last mean reward per episode: 650.60
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 646         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 71          |
|    time_elapsed         | 351         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.059771188 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.367       |
|    value_loss           | 594         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 645        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 72         |
|    time_elapsed         | 355        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.04455894 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0585    |
|    std                  | 0.367      |
|    value_loss           | 499        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.53e-05    |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 73          |
|    time_elapsed         | 360         |
|    total_timesteps      | 74752       |
| train/                  |             |
|    approx_kl            | 0.051120788 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 659         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.055      |
|    std                  | 0.367       |
|    value_loss           | 591         |
-----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 642        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 74         |
|    time_elapsed         | 364        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.07270974 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 657        |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 553        |
----------------------------------------
----------------------------------------
| reward                  | -2.08      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.26e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 639        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 75         |
|    time_elapsed         | 368        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.08604793 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 810        |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0809    |
|    std                  | 0.367      |
|    value_loss           | 433        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.25e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 76         |
|    time_elapsed         | 373        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07724598 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 512        |
----------------------------------------
Num timesteps: 78000
Best mean reward: 791.71 - Last mean reward per episode: 640.18
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 77         |
|    time_elapsed         | 377        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.06728281 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 617        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.46e-05    |
| reward_motion           | 9.16e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 633         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 78          |
|    time_elapsed         | 381         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.052631572 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0682     |
|    std                  | 0.367       |
|    value_loss           | 420         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.65e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 638         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 79          |
|    time_elapsed         | 386         |
|    total_timesteps      | 80896       |
| train/                  |             |
|    approx_kl            | 0.091547176 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.4        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0856     |
|    std                  | 0.367       |
|    value_loss           | 432         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.47e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 636        |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 80         |
|    time_elapsed         | 390        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.07504402 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 103        |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.367      |
|    value_loss           | 492        |
----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.39e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 81         |
|    time_elapsed         | 394        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.05894314 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.3       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.367      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.34e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -2.82       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 82          |
|    time_elapsed         | 399         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.037166715 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0523     |
|    std                  | 0.367       |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 791.71 - Last mean reward per episode: 631.66
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 83         |
|    time_elapsed         | 403        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.12127335 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.3       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0993    |
|    std                  | 0.367      |
|    value_loss           | 340        |
----------------------------------------
---------------------------------------
| reward                  | -2.1      |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.52e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -2.82     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 634       |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 84        |
|    time_elapsed         | 408       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.0711868 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 228       |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.0729   |
|    std                  | 0.367     |
|    value_loss           | 634       |
---------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.51e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 631         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 85          |
|    time_elapsed         | 412         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.059059467 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 472         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0646     |
|    std                  | 0.367       |
|    value_loss           | 605         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.18e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 637        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 86         |
|    time_elapsed         | 416        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10163675 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 487        |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.367      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 9.15e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 87          |
|    time_elapsed         | 420         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.051552266 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0379     |
|    std                  | 0.367       |
|    value_loss           | 485         |
-----------------------------------------
Num timesteps: 90000
Best mean reward: 791.71 - Last mean reward per episode: 630.24
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 88         |
|    time_elapsed         | 425        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.04710511 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 201        |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0552    |
|    std                  | 0.367      |
|    value_loss           | 560        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.44e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 625        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 89         |
|    time_elapsed         | 429        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.05163174 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 78         |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 0.367      |
|    value_loss           | 513        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 621         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 90          |
|    time_elapsed         | 434         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.062439263 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 524         |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.048      |
|    std                  | 0.367       |
|    value_loss           | 502         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.43e-05   |
| reward_motion           | 9.06e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 91         |
|    time_elapsed         | 438        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.07532377 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.367      |
|    value_loss           | 565        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.06e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 611         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 92          |
|    time_elapsed         | 442         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.038080342 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.1        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0483     |
|    std                  | 0.367       |
|    value_loss           | 469         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 8.99e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 612         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 93          |
|    time_elapsed         | 447         |
|    total_timesteps      | 95232       |
| train/                  |             |
|    approx_kl            | 0.057437897 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.367       |
|    value_loss           | 542         |
-----------------------------------------
Num timesteps: 96000
Best mean reward: 791.71 - Last mean reward per episode: 612.12
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 94         |
|    time_elapsed         | 451        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.06996028 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 107        |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.367      |
|    value_loss           | 526        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.34e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 606         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 95          |
|    time_elapsed         | 456         |
|    total_timesteps      | 97280       |
| train/                  |             |
|    approx_kl            | 0.056031484 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.5       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.057      |
|    std                  | 0.367       |
|    value_loss           | 495         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 96         |
|    time_elapsed         | 460        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.07183945 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 175        |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0801    |
|    std                  | 0.367      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.97e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 603        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 97         |
|    time_elapsed         | 464        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.08660442 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0682    |
|    std                  | 0.367      |
|    value_loss           | 556        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.02e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 608         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 98          |
|    time_elapsed         | 469         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.036415048 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.367       |
|    value_loss           | 499         |
-----------------------------------------
---------------------------------------
| reward                  | -2.15     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 9.02e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -2.87     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 606       |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 99        |
|    time_elapsed         | 473       |
|    total_timesteps      | 101376    |
| train/                  |           |
|    approx_kl            | 0.0719479 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.8     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 133       |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0645   |
|    std                  | 0.367     |
|    value_loss           | 479       |
---------------------------------------
Num timesteps: 102000
Best mean reward: 791.71 - Last mean reward per episode: 605.58
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.98e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 602         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 100         |
|    time_elapsed         | 477         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.052123062 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 488         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0443     |
|    std                  | 0.367       |
|    value_loss           | 509         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.96e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 600         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 101         |
|    time_elapsed         | 482         |
|    total_timesteps      | 103424      |
| train/                  |             |
|    approx_kl            | 0.060172085 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0666     |
|    std                  | 0.367       |
|    value_loss           | 449         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 599        |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 102        |
|    time_elapsed         | 485        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07174431 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 79.1       |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.367      |
|    value_loss           | 373        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 595         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 103         |
|    time_elapsed         | 489         |
|    total_timesteps      | 105472      |
| train/                  |             |
|    approx_kl            | 0.061719283 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0671     |
|    std                  | 0.366       |
|    value_loss           | 384         |
-----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.2e-07    |
| reward_position         | 0.000138   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 588        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 104        |
|    time_elapsed         | 492        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.09381586 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.366      |
|    value_loss           | 346        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.24e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 588         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 105         |
|    time_elapsed         | 496         |
|    total_timesteps      | 107520      |
| train/                  |             |
|    approx_kl            | 0.056681145 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 169         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.366       |
|    value_loss           | 358         |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 791.71 - Last mean reward per episode: 587.83
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 597        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 106        |
|    time_elapsed         | 500        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.06581798 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0689    |
|    std                  | 0.366      |
|    value_loss           | 276        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 596        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 107        |
|    time_elapsed         | 504        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.09682576 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 391        |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0893    |
|    std                  | 0.366      |
|    value_loss           | 469        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 585        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 108        |
|    time_elapsed         | 508        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.08974074 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 234        |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.366      |
|    value_loss           | 507        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 583        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 109        |
|    time_elapsed         | 513        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.07364488 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 216        |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0762    |
|    std                  | 0.366      |
|    value_loss           | 431        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.5e-05     |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 571         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 110         |
|    time_elapsed         | 517         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.060291555 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0624     |
|    std                  | 0.366       |
|    value_loss           | 496         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.26e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 111         |
|    time_elapsed         | 521         |
|    total_timesteps      | 113664      |
| train/                  |             |
|    approx_kl            | 0.054138087 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0621     |
|    std                  | 0.366       |
|    value_loss           | 498         |
-----------------------------------------
Num timesteps: 114000
Best mean reward: 791.71 - Last mean reward per episode: 560.01
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 560        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 112        |
|    time_elapsed         | 526        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.06798768 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.366      |
|    value_loss           | 359        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.88       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 546         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 113         |
|    time_elapsed         | 530         |
|    total_timesteps      | 115712      |
| train/                  |             |
|    approx_kl            | 0.046133902 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 431         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.366       |
|    value_loss           | 599         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.18       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.08e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.89       |
| reward_velocity         | 0.757       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 537         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 114         |
|    time_elapsed         | 534         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.068029776 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0635     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 531        |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 115        |
|    time_elapsed         | 539        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.09138459 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0855    |
|    std                  | 0.366      |
|    value_loss           | 305        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.33e-05    |
| reward_motion           | 8.97e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.9        |
| reward_velocity         | 0.754       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 522         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 116         |
|    time_elapsed         | 542         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.084602855 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.5       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0706     |
|    std                  | 0.366       |
|    value_loss           | 458         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.3e-05    |
| reward_motion           | 8.93e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 517        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 117        |
|    time_elapsed         | 546        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.07564285 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 410        |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.366      |
|    value_loss           | 468        |
----------------------------------------
Num timesteps: 120000
Best mean reward: 791.71 - Last mean reward per episode: 516.74
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 505         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 118         |
|    time_elapsed         | 549         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.093632296 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.1        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0891     |
|    std                  | 0.366       |
|    value_loss           | 297         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.19e-05    |
| reward_motion           | 8.75e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 495         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 119         |
|    time_elapsed         | 553         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.042702064 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 156         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0479     |
|    std                  | 0.366       |
|    value_loss           | 528         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.17e-05    |
| reward_motion           | 8.74e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 496         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 120         |
|    time_elapsed         | 557         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.053215243 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 384         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 491         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 121         |
|    time_elapsed         | 561         |
|    total_timesteps      | 123904      |
| train/                  |             |
|    approx_kl            | 0.079917565 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.4        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0821     |
|    std                  | 0.366       |
|    value_loss           | 598         |
-----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.21e-05   |
| reward_motion           | 8.84e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 485        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 122        |
|    time_elapsed         | 565        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.09118663 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 265        |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.069     |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.85e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 477        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 123        |
|    time_elapsed         | 570        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.03758951 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0571    |
|    std                  | 0.366      |
|    value_loss           | 473        |
----------------------------------------
Num timesteps: 126000
Best mean reward: 791.71 - Last mean reward per episode: 476.83
---------------------------------------
| reward                  | -2.21     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.22e-05  |
| reward_motion           | 8.79e-07  |
| reward_position         | 0.000132  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.748     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 478       |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 124       |
|    time_elapsed         | 574       |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.0718819 |
|    clip_fraction        | 0.186     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 352       |
|    n_updates            | 2460      |
|    policy_gradient_loss | -0.069    |
|    std                  | 0.366     |
|    value_loss           | 473       |
---------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.95e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.751      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 475        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 125        |
|    time_elapsed         | 579        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.05870478 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 343        |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0582    |
|    std                  | 0.366      |
|    value_loss           | 510        |
----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.93      |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 467        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 126        |
|    time_elapsed         | 583        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.09303458 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0826    |
|    std                  | 0.366      |
|    value_loss           | 333        |
----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.27e-05    |
| reward_motion           | 8.94e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 457         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 127         |
|    time_elapsed         | 588         |
|    total_timesteps      | 130048      |
| train/                  |             |
|    approx_kl            | 0.075280145 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.064      |
|    std                  | 0.366       |
|    value_loss           | 423         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.25e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.752       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 466         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 128         |
|    time_elapsed         | 592         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.089382544 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.366       |
|    value_loss           | 334         |
-----------------------------------------
Num timesteps: 132000
Best mean reward: 791.71 - Last mean reward per episode: 466.43
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.753      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 469        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 129        |
|    time_elapsed         | 596        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.07682991 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 221        |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.366      |
|    value_loss           | 405        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 473        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 130        |
|    time_elapsed         | 601        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.07019861 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 169        |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.366      |
|    value_loss           | 317        |
----------------------------------------
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.2e-05    |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 464        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 131        |
|    time_elapsed         | 605        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.07635625 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.366      |
|    value_loss           | 448        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.22e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 452        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 132        |
|    time_elapsed         | 610        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.06735976 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.3       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0696    |
|    std                  | 0.366      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 448        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 133        |
|    time_elapsed         | 614        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.07325928 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0698    |
|    std                  | 0.366      |
|    value_loss           | 398        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 443        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 134        |
|    time_elapsed         | 618        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.07183257 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 311        |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
Num timesteps: 138000
Best mean reward: 791.71 - Last mean reward per episode: 443.06
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 444        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 135        |
|    time_elapsed         | 623        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.09175141 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 168        |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.366      |
|    value_loss           | 425        |
----------------------------------------
---------------------------------------
| reward                  | -2.18     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.26e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -2.89     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 445       |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 136       |
|    time_elapsed         | 626       |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.0662691 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 152       |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.0719   |
|    std                  | 0.366     |
|    value_loss           | 433       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.98e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 434        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 137        |
|    time_elapsed         | 630        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.05629784 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.8       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.366      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.13e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 430         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 138         |
|    time_elapsed         | 633         |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.063325554 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 362         |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0723     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
---------------------------------------
| reward                  | -2.19     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.98e-05  |
| reward_motion           | 8.44e-07  |
| reward_position         | 0.000127  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 424       |
| time/                   |           |
|    fps                  | 223       |
|    iterations           | 139       |
|    time_elapsed         | 636       |
|    total_timesteps      | 142336    |
| train/                  |           |
|    approx_kl            | 0.1082619 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 39.6      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.104    |
|    std                  | 0.366     |
|    value_loss           | 306       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.43e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.91      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 424        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 140        |
|    time_elapsed         | 640        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.07125464 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 132        |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.366      |
|    value_loss           | 340        |
----------------------------------------
Num timesteps: 144000
Best mean reward: 791.71 - Last mean reward per episode: 424.38
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.41e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 420        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 141        |
|    time_elapsed         | 644        |
|    total_timesteps      | 144384     |
| train/                  |            |
|    approx_kl            | 0.08280164 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 347        |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0521    |
|    std                  | 0.366      |
|    value_loss           | 378        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.47e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 415        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 142        |
|    time_elapsed         | 649        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.09441495 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 56.9       |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.366      |
|    value_loss           | 307        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.49e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 143         |
|    time_elapsed         | 654         |
|    total_timesteps      | 146432      |
| train/                  |             |
|    approx_kl            | 0.084700994 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 189         |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0941     |
|    std                  | 0.366       |
|    value_loss           | 435         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 144         |
|    time_elapsed         | 658         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.078906074 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0745     |
|    std                  | 0.366       |
|    value_loss           | 474         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.93e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 407         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 145         |
|    time_elapsed         | 663         |
|    total_timesteps      | 148480      |
| train/                  |             |
|    approx_kl            | 0.079785354 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0749     |
|    std                  | 0.366       |
|    value_loss           | 373         |
-----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 399        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 146        |
|    time_elapsed         | 667        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.06602247 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 511        |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.366      |
|    value_loss           | 481        |
----------------------------------------
Num timesteps: 150000
Best mean reward: 791.71 - Last mean reward per episode: 399.17
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 394         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 147         |
|    time_elapsed         | 672         |
|    total_timesteps      | 150528      |
| train/                  |             |
|    approx_kl            | 0.074376225 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0707     |
|    std                  | 0.366       |
|    value_loss           | 408         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.84e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 393         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 148         |
|    time_elapsed         | 676         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.064057305 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.0699     |
|    std                  | 0.366       |
|    value_loss           | 445         |
-----------------------------------------
---------------------------------------
| reward                  | -2.22     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.67e-05  |
| reward_motion           | 7.81e-07  |
| reward_position         | 0.000117  |
| reward_torque           | -2.94     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 393       |
| time/                   |           |
|    fps                  | 224       |
|    iterations           | 149       |
|    time_elapsed         | 680       |
|    total_timesteps      | 152576    |
| train/                  |           |
|    approx_kl            | 0.0948907 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.0826   |
|    std                  | 0.365     |
|    value_loss           | 441       |
---------------------------------------
-----------------------------------------
| reward                  | -2.25       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.67e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -2.96       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 390         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 150         |
|    time_elapsed         | 685         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.051997818 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0506     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.63e-05   |
| reward_motion           | 7.71e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 398        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 151        |
|    time_elapsed         | 689        |
|    total_timesteps      | 154624     |
| train/                  |            |
|    approx_kl            | 0.08013227 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.365      |
|    value_loss           | 373        |
----------------------------------------
----------------------------------------
| reward                  | -2.28      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.83e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 394        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 152        |
|    time_elapsed         | 693        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.05597806 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 156000
Best mean reward: 791.71 - Last mean reward per episode: 393.96
----------------------------------------
| reward                  | -2.27      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 386        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 153        |
|    time_elapsed         | 698        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.06945801 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 203        |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.365      |
|    value_loss           | 537        |
----------------------------------------
-----------------------------------------
| reward                  | -2.28       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.67e-05    |
| reward_motion           | 7.78e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3          |
| reward_velocity         | 0.765       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 378         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 154         |
|    time_elapsed         | 702         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.075821295 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0732     |
|    std                  | 0.365       |
|    value_loss           | 319         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.66e-05    |
| reward_motion           | 7.8e-07     |
| reward_position         | 0.000117    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 372         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 155         |
|    time_elapsed         | 707         |
|    total_timesteps      | 158720      |
| train/                  |             |
|    approx_kl            | 0.065256454 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.4        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.066      |
|    std                  | 0.365       |
|    value_loss           | 472         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.66e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 364         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 156         |
|    time_elapsed         | 711         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.059843626 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.0743     |
|    std                  | 0.365       |
|    value_loss           | 415         |
-----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 157        |
|    time_elapsed         | 716        |
|    total_timesteps      | 160768     |
| train/                  |            |
|    approx_kl            | 0.09143843 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 94.6       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.0821    |
|    std                  | 0.365      |
|    value_loss           | 311        |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.57e-05   |
| reward_motion           | 7.61e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.03      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 158        |
|    time_elapsed         | 720        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.09160215 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 162000
Best mean reward: 791.71 - Last mean reward per episode: 365.98
-----------------------------------------
| reward                  | -2.32       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.58e-05    |
| reward_motion           | 7.65e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.04       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 357         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 159         |
|    time_elapsed         | 725         |
|    total_timesteps      | 162816      |
| train/                  |             |
|    approx_kl            | 0.071382515 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 415         |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 0.365       |
|    value_loss           | 609         |
-----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.53e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.04      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 350        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 160        |
|    time_elapsed         | 729        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.07973179 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 217        |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.365      |
|    value_loss           | 483        |
----------------------------------------
-----------------------------------------
| reward                  | -2.34       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.64e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.05       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 161         |
|    time_elapsed         | 734         |
|    total_timesteps      | 164864      |
| train/                  |             |
|    approx_kl            | 0.062193085 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.8        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0636     |
|    std                  | 0.365       |
|    value_loss           | 465         |
-----------------------------------------
----------------------------------------
| reward                  | -2.35      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.67e-07   |
| reward_position         | 0.000115   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 344        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 162        |
|    time_elapsed         | 738        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.08855179 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 277        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.365      |
|    value_loss           | 418        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000114    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 342         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 163         |
|    time_elapsed         | 742         |
|    total_timesteps      | 166912      |
| train/                  |             |
|    approx_kl            | 0.090915054 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 447         |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.365       |
|    value_loss           | 441         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.53e-05    |
| reward_motion           | 7.55e-07    |
| reward_position         | 0.000113    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 164         |
|    time_elapsed         | 746         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.056085087 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 256         |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0711     |
|    std                  | 0.365       |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 168000
Best mean reward: 791.71 - Last mean reward per episode: 345.20
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.54e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 165        |
|    time_elapsed         | 751        |
|    total_timesteps      | 168960     |
| train/                  |            |
|    approx_kl            | 0.07051596 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.4       |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.365      |
|    value_loss           | 540        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.77e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 335        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 166        |
|    time_elapsed         | 755        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.07950926 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.2       |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 432        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 328        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 167        |
|    time_elapsed         | 759        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.08645288 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 350        |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.0779    |
|    std                  | 0.365      |
|    value_loss           | 459        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.87e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -3.08       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 326         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 168         |
|    time_elapsed         | 764         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.062871024 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 406         |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.0655     |
|    std                  | 0.365       |
|    value_loss           | 491         |
-----------------------------------------
----------------------------------------
| reward                  | -2.37      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.9e-07    |
| reward_position         | 0.000119   |
| reward_torque           | -3.08      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 169        |
|    time_elapsed         | 768        |
|    total_timesteps      | 173056     |
| train/                  |            |
|    approx_kl            | 0.07585441 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 227        |
|    n_updates            | 3360       |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.365      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 174000
Best mean reward: 791.71 - Last mean reward per episode: 318.08
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.61e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 170        |
|    time_elapsed         | 773        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.06525008 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0433    |
|    std                  | 0.365      |
|    value_loss           | 378        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.69e-05  |
| reward_motion           | 7.89e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.09     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 320       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 171       |
|    time_elapsed         | 777       |
|    total_timesteps      | 175104    |
| train/                  |           |
|    approx_kl            | 0.0965157 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 290       |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.0908   |
|    std                  | 0.365     |
|    value_loss           | 494       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.95e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 172        |
|    time_elapsed         | 781        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.06046278 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 172        |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 522        |
----------------------------------------
-----------------------------------------
| reward                  | -2.38       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.74e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.1        |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 317         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 173         |
|    time_elapsed         | 786         |
|    total_timesteps      | 177152      |
| train/                  |             |
|    approx_kl            | 0.076122925 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 284         |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.0637     |
|    std                  | 0.365       |
|    value_loss           | 425         |
-----------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.78e-05   |
| reward_motion           | 8.02e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 174        |
|    time_elapsed         | 790        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.08470478 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.4       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.365      |
|    value_loss           | 270        |
----------------------------------------
-----------------------------------------
| reward                  | -2.4        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.77e-05    |
| reward_motion           | 8.02e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.12       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 175         |
|    time_elapsed         | 795         |
|    total_timesteps      | 179200      |
| train/                  |             |
|    approx_kl            | 0.071893886 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0697     |
|    std                  | 0.365       |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 180000
Best mean reward: 791.71 - Last mean reward per episode: 310.27
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.75e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.12      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 300        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 176        |
|    time_elapsed         | 799        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.08430763 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 346        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.365      |
|    value_loss           | 462        |
----------------------------------------
-----------------------------------------
| reward                  | -2.41       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.73e-05    |
| reward_motion           | 7.94e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.13       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 299         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 177         |
|    time_elapsed         | 803         |
|    total_timesteps      | 181248      |
| train/                  |             |
|    approx_kl            | 0.082156904 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.1        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0705     |
|    std                  | 0.365       |
|    value_loss           | 398         |
-----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.96e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 291        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 178        |
|    time_elapsed         | 808        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.07415432 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 173        |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.365      |
|    value_loss           | 450        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.62e-05   |
| reward_motion           | 7.75e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 283        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 179        |
|    time_elapsed         | 812        |
|    total_timesteps      | 183296     |
| train/                  |            |
|    approx_kl            | 0.08652607 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.365      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.92e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 180        |
|    time_elapsed         | 816        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.09836302 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 271        |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.365      |
|    value_loss           | 437        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000122   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 281        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 181        |
|    time_elapsed         | 821        |
|    total_timesteps      | 185344     |
| train/                  |            |
|    approx_kl            | 0.10941856 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.365      |
|    value_loss           | 280        |
----------------------------------------
Num timesteps: 186000
Best mean reward: 791.71 - Last mean reward per episode: 281.13
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 273        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 182        |
|    time_elapsed         | 825        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.08115048 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.7       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.365      |
|    value_loss           | 420        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.18e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 183        |
|    time_elapsed         | 829        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.06963568 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.365      |
|    value_loss           | 281        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 262        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 184        |
|    time_elapsed         | 834        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.08650813 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.9       |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.365      |
|    value_loss           | 342        |
----------------------------------------
-----------------------------------------
| reward                  | -2.45       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.16       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 261         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 185         |
|    time_elapsed         | 838         |
|    total_timesteps      | 189440      |
| train/                  |             |
|    approx_kl            | 0.059413694 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.0513     |
|    std                  | 0.365       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 186        |
|    time_elapsed         | 842        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.08431653 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 143        |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 379        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.87e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 187        |
|    time_elapsed         | 847        |
|    total_timesteps      | 191488     |
| train/                  |            |
|    approx_kl            | 0.08574592 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.4       |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.0862    |
|    std                  | 0.365      |
|    value_loss           | 314        |
----------------------------------------
Num timesteps: 192000
Best mean reward: 791.71 - Last mean reward per episode: 255.42
-----------------------------------------
| reward                  | -2.46       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.86e-05    |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 188         |
|    time_elapsed         | 851         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.098878115 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.8        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.0883     |
|    std                  | 0.365       |
|    value_loss           | 382         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.47       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.88e-05    |
| reward_motion           | 8.23e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.19       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 245         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 189         |
|    time_elapsed         | 856         |
|    total_timesteps      | 193536      |
| train/                  |             |
|    approx_kl            | 0.031861797 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.0382     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 190        |
|    time_elapsed         | 860        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.05897156 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.365      |
|    value_loss           | 388        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.84e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 191        |
|    time_elapsed         | 865        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.09375177 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0871    |
|    std                  | 0.365      |
|    value_loss           | 395        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.2       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 192        |
|    time_elapsed         | 870        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.09083923 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 300        |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.365      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.95e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 241        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 193        |
|    time_elapsed         | 874        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.08830097 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 80.5       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.0854    |
|    std                  | 0.365      |
|    value_loss           | 393        |
----------------------------------------
Num timesteps: 198000
Best mean reward: 791.71 - Last mean reward per episode: 241.05
-----------------------------------------
| reward                  | -2.49       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.2        |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 238         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 194         |
|    time_elapsed         | 879         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.045449574 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 66          |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.0453     |
|    std                  | 0.365       |
|    value_loss           | 317         |
-----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.28e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 195        |
|    time_elapsed         | 884        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.09413753 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 332        |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.365      |
|    value_loss           | 472        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.22e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 196        |
|    time_elapsed         | 888        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.08775077 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.365      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.01e-05   |
| reward_motion           | 8.35e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 197        |
|    time_elapsed         | 893        |
|    total_timesteps      | 201728     |
| train/                  |            |
|    approx_kl            | 0.08543901 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0776    |
|    std                  | 0.365      |
|    value_loss           | 441        |
----------------------------------------
-----------------------------------------
| reward                  | -2.51       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.23       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 220         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 198         |
|    time_elapsed         | 898         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.101773694 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.093      |
|    std                  | 0.365       |
|    value_loss           | 291         |
-----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.21e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 219        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 199        |
|    time_elapsed         | 902        |
|    total_timesteps      | 203776     |
| train/                  |            |
|    approx_kl            | 0.08246793 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.6       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.0725    |
|    std                  | 0.365      |
|    value_loss           | 366        |
----------------------------------------
Num timesteps: 204000
Best mean reward: 791.71 - Last mean reward per episode: 218.92
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 220        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 200        |
|    time_elapsed         | 907        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.07519507 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.365      |
|    value_loss           | 446        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.29e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 217        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 201        |
|    time_elapsed         | 912        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.07404611 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.365      |
|    value_loss           | 417        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.93e-05  |
| reward_motion           | 8.14e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.24     |
| reward_velocity         | 0.764     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 215       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 202       |
|    time_elapsed         | 916       |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.0682548 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.7     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 275       |
|    n_updates            | 4020      |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.365     |
|    value_loss           | 386       |
---------------------------------------
-----------------------------------------
| reward                  | -2.53       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.85e-05    |
| reward_motion           | 7.96e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.25       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 215         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 203         |
|    time_elapsed         | 921         |
|    total_timesteps      | 207872      |
| train/                  |             |
|    approx_kl            | 0.058709525 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.0601     |
|    std                  | 0.365       |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.84e-05  |
| reward_motion           | 7.97e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.25     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 210       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 204       |
|    time_elapsed         | 925       |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.0821961 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 244       |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.365     |
|    value_loss           | 486       |
---------------------------------------
----------------------------------------
| reward                  | -2.54      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.00012    |
| reward_torque           | -3.26      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 205        |
|    time_elapsed         | 930        |
|    total_timesteps      | 209920     |
| train/                  |            |
|    approx_kl            | 0.09615535 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 118        |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.365      |
|    value_loss           | 424        |
----------------------------------------
Num timesteps: 210000
Best mean reward: 791.71 - Last mean reward per episode: 209.18
----------------------------------------
| reward                  | -2.55      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.04e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.27      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 200        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 206        |
|    time_elapsed         | 935        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.10412278 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.5       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.0897    |
|    std                  | 0.365      |
|    value_loss           | 422        |
----------------------------------------
-----------------------------------------
| reward                  | -2.57       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -3.29       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 197         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 207         |
|    time_elapsed         | 939         |
|    total_timesteps      | 211968      |
| train/                  |             |
|    approx_kl            | 0.078166276 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.0763     |
|    std                  | 0.365       |
|    value_loss           | 390         |
-----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.09e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.3       |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 199        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 208        |
|    time_elapsed         | 944        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.09233539 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.5       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.365      |
|    value_loss           | 307        |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 209        |
|    time_elapsed         | 948        |
|    total_timesteps      | 214016     |
| train/                  |            |
|    approx_kl            | 0.07892518 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.1       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 203        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 210        |
|    time_elapsed         | 952        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.09467095 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.0849    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
Num timesteps: 216000
Best mean reward: 791.71 - Last mean reward per episode: 203.38
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 211        |
|    time_elapsed         | 957        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.09579381 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 510        |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.365      |
|    value_loss           | 494        |
----------------------------------------
-----------------------------------------
| reward                  | -2.6        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.32       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 202         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 212         |
|    time_elapsed         | 961         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.118106246 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0861     |
|    std                  | 0.364       |
|    value_loss           | 381         |
-----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 213        |
|    time_elapsed         | 966        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.08759506 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 97.6       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.364      |
|    value_loss           | 310        |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.14e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 214        |
|    time_elapsed         | 970        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.08177506 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 250        |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.364      |
|    value_loss           | 445        |
----------------------------------------
---------------------------------------
| reward                  | -2.59     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.92e-05  |
| reward_motion           | 8.03e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.32     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 200       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 215       |
|    time_elapsed         | 975       |
|    total_timesteps      | 220160    |
| train/                  |           |
|    approx_kl            | 0.0946173 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | 43.8      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.0792   |
|    std                  | 0.364     |
|    value_loss           | 362       |
---------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000121   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 216        |
|    time_elapsed         | 979        |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.06670367 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 222000
Best mean reward: 791.71 - Last mean reward per episode: 197.53
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.1e-07     |
| reward_position         | 0.000122    |
| reward_torque           | -3.34       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 190         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 217         |
|    time_elapsed         | 984         |
|    total_timesteps      | 222208      |
| train/                  |             |
|    approx_kl            | 0.061054613 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.364       |
|    value_loss           | 424         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 190        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 218        |
|    time_elapsed         | 988        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.10701846 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.364      |
|    value_loss           | 423        |
----------------------------------------
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.98e-05    |
| reward_motion           | 8.14e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.35       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 219         |
|    time_elapsed         | 992         |
|    total_timesteps      | 224256      |
| train/                  |             |
|    approx_kl            | 0.083152995 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.364       |
|    value_loss           | 344         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.34      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 184        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 220        |
|    time_elapsed         | 997        |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.07975471 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.1       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 461        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.02e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 221        |
|    time_elapsed         | 1001       |
|    total_timesteps      | 226304     |
| train/                  |            |
|    approx_kl            | 0.06731045 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 434        |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.364      |
|    value_loss           | 550        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.38e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 222        |
|    time_elapsed         | 1005       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.06061294 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 151        |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 555        |
----------------------------------------
Num timesteps: 228000
Best mean reward: 791.71 - Last mean reward per episode: 181.53
-----------------------------------------
| reward                  | -2.64       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.36       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 223         |
|    time_elapsed         | 1010        |
|    total_timesteps      | 228352      |
| train/                  |             |
|    approx_kl            | 0.090596244 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0788     |
|    std                  | 0.364       |
|    value_loss           | 346         |
-----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.06e-05   |
| reward_motion           | 8.27e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.37      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 224        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.07922794 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 574        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 225        |
|    time_elapsed         | 1018       |
|    total_timesteps      | 230400     |
| train/                  |            |
|    approx_kl            | 0.08420156 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 226        |
|    time_elapsed         | 1023       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.11329693 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.0819    |
|    std                  | 0.364      |
|    value_loss           | 496        |
----------------------------------------
----------------------------------------
| reward                  | -2.66      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.39      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 176        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 227        |
|    time_elapsed         | 1027       |
|    total_timesteps      | 232448     |
| train/                  |            |
|    approx_kl            | 0.08218557 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.8       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.67      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.05e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.4       |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 168        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 228        |
|    time_elapsed         | 1031       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09102081 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0842    |
|    std                  | 0.364      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 234000
Best mean reward: 791.71 - Last mean reward per episode: 167.55
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 159        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 229        |
|    time_elapsed         | 1036       |
|    total_timesteps      | 234496     |
| train/                  |            |
|    approx_kl            | 0.06711652 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.364      |
|    value_loss           | 313        |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.03e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 230        |
|    time_elapsed         | 1040       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.08161515 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 247        |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.0668    |
|    std                  | 0.364      |
|    value_loss           | 621        |
----------------------------------------
---------------------------------------
| reward                  | -2.7      |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.96e-05  |
| reward_motion           | 8.01e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.42     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 155       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 231       |
|    time_elapsed         | 1044      |
|    total_timesteps      | 236544    |
| train/                  |           |
|    approx_kl            | 0.0906602 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.8     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 103       |
|    n_updates            | 4600      |
|    policy_gradient_loss | -0.0795   |
|    std                  | 0.364     |
|    value_loss           | 404       |
---------------------------------------
----------------------------------------
| reward                  | -2.71      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-05   |
| reward_motion           | 8.26e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.43      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 232        |
|    time_elapsed         | 1049       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.08881818 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 223        |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.364      |
|    value_loss           | 533        |
----------------------------------------
---------------------------------------
| reward                  | -2.72     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-05  |
| reward_motion           | 8.24e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -3.44     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 233       |
|    time_elapsed         | 1053      |
|    total_timesteps      | 238592    |
| train/                  |           |
|    approx_kl            | 0.0750297 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 224       |
|    n_updates            | 4640      |
|    policy_gradient_loss | -0.0521   |
|    std                  | 0.364     |
|    value_loss           | 468       |
---------------------------------------
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.07e-05   |
| reward_motion           | 8.23e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 141        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 234        |
|    time_elapsed         | 1057       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.07345635 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 215        |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.364      |
|    value_loss           | 490        |
----------------------------------------
Num timesteps: 240000
Best mean reward: 791.71 - Last mean reward per episode: 141.28
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.34e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 143        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 235        |
|    time_elapsed         | 1062       |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.06890334 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.4       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.0613    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.53e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 236        |
|    time_elapsed         | 1066       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.07793336 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.2       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.364      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.52e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 142        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 237        |
|    time_elapsed         | 1071       |
|    total_timesteps      | 242688     |
| train/                  |            |
|    approx_kl            | 0.08569087 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 81.3       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.364      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.76      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.55e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.48      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 238        |
|    time_elapsed         | 1075       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.09417659 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 378        |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.364      |
|    value_loss           | 548        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000129    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 239         |
|    time_elapsed         | 1079        |
|    total_timesteps      | 244736      |
| train/                  |             |
|    approx_kl            | 0.076811194 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.3        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.0673     |
|    std                  | 0.364       |
|    value_loss           | 519         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.49      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 240        |
|    time_elapsed         | 1084       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.06444687 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.364      |
|    value_loss           | 506        |
----------------------------------------
Num timesteps: 246000
Best mean reward: 791.71 - Last mean reward per episode: 127.14
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 123        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 241        |
|    time_elapsed         | 1088       |
|    total_timesteps      | 246784     |
| train/                  |            |
|    approx_kl            | 0.07253139 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 113        |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0679    |
|    std                  | 0.364      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.768       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 242         |
|    time_elapsed         | 1092        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.081109695 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.364       |
|    value_loss           | 514         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 243        |
|    time_elapsed         | 1097       |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.07144873 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.0479    |
|    std                  | 0.364      |
|    value_loss           | 353        |
----------------------------------------
---------------------------------------
| reward                  | -2.77     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.38e-05  |
| reward_motion           | 8.66e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.49     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 105       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 244       |
|    time_elapsed         | 1102      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0925653 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 230       |
|    n_updates            | 4860      |
|    policy_gradient_loss | -0.0768   |
|    std                  | 0.364     |
|    value_loss           | 403       |
---------------------------------------
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.36e-05  |
| reward_motion           | 8.63e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.5      |
| reward_velocity         | 0.769     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 99.4      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 245       |
|    time_elapsed         | 1106      |
|    total_timesteps      | 250880    |
| train/                  |           |
|    approx_kl            | 0.0684265 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.7     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 381       |
|    n_updates            | 4880      |
|    policy_gradient_loss | -0.0741   |
|    std                  | 0.364     |
|    value_loss           | 536       |
---------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.37e-05    |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -3.5        |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 97.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 246         |
|    time_elapsed         | 1111        |
|    total_timesteps      | 251904      |
| train/                  |             |
|    approx_kl            | 0.057845965 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0739     |
|    std                  | 0.364       |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 252000
Best mean reward: 791.71 - Last mean reward per episode: 97.37
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 247        |
|    time_elapsed         | 1116       |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.06736165 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 564        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 93.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 248        |
|    time_elapsed         | 1120       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.08807652 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 580        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 90.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 249        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 254976     |
| train/                  |            |
|    approx_kl            | 0.06194989 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 112        |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.364      |
|    value_loss           | 427        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 250        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.06726232 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.56e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 91.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 251        |
|    time_elapsed         | 1134       |
|    total_timesteps      | 257024     |
| train/                  |            |
|    approx_kl            | 0.09167318 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 295        |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.364      |
|    value_loss           | 558        |
----------------------------------------
Num timesteps: 258000
Best mean reward: 791.71 - Last mean reward per episode: 91.10
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.75e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 252        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.08047421 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 205        |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.364      |
|    value_loss           | 417        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.77e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 84.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 253        |
|    time_elapsed         | 1143       |
|    total_timesteps      | 259072     |
| train/                  |            |
|    approx_kl            | 0.06514632 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.364      |
|    value_loss           | 485        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 254        |
|    time_elapsed         | 1148       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.07208417 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 284        |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.0785    |
|    std                  | 0.364      |
|    value_loss           | 568        |
----------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.52e-05    |
| reward_motion           | 8.89e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.5        |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 78.3        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 255         |
|    time_elapsed         | 1152        |
|    total_timesteps      | 261120      |
| train/                  |             |
|    approx_kl            | 0.068733916 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.3        |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.364       |
|    value_loss           | 279         |
-----------------------------------------
---------------------------------------
| reward                  | -2.79     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.51     |
| reward_velocity         | 0.768     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 73.8      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 256       |
|    time_elapsed         | 1157      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0644877 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.5     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 312       |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.0623   |
|    std                  | 0.364     |
|    value_loss           | 532       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 69.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 257        |
|    time_elapsed         | 1162       |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.09269175 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 208        |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 264000
Best mean reward: 791.71 - Last mean reward per episode: 69.46
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.61e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 71         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 258        |
|    time_elapsed         | 1166       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.08261158 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 84.6       |
|    n_updates            | 5140       |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.364      |
|    value_loss           | 444        |
----------------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 73         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 259        |
|    time_elapsed         | 1171       |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.15402758 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 48         |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.364      |
|    value_loss           | 459        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 68.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 260        |
|    time_elapsed         | 1175       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.09338033 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 225        |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.363      |
|    value_loss           | 488        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 65.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 261        |
|    time_elapsed         | 1180       |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.07121593 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.363      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.14e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 63.4       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 262        |
|    time_elapsed         | 1185       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.08495103 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 5220       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.363      |
|    value_loss           | 465        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 66.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 263        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 269312     |
| train/                  |            |
|    approx_kl            | 0.11406556 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.363      |
|    value_loss           | 663        |
----------------------------------------
Num timesteps: 270000
Best mean reward: 791.71 - Last mean reward per episode: 66.95
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.69e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -3.5      |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 58.3      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 264       |
|    time_elapsed         | 1194      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0768143 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 58.3      |
|    n_updates            | 5260      |
|    policy_gradient_loss | -0.0761   |
|    std                  | 0.363     |
|    value_loss           | 505       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.21e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.52      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 59.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 265        |
|    time_elapsed         | 1199       |
|    total_timesteps      | 271360     |
| train/                  |            |
|    approx_kl            | 0.08713607 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.363      |
|    value_loss           | 467        |
----------------------------------------
-----------------------------------------
| reward                  | -2.8        |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.62e-05    |
| reward_motion           | 9.17e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -3.53       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 266         |
|    time_elapsed         | 1203        |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.088511154 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 327         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0809     |
|    std                  | 0.363       |
|    value_loss           | 423         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 67.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 267        |
|    time_elapsed         | 1208       |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.08538949 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.1       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.363      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 62.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 268        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.10191356 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 124        |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.0903    |
|    std                  | 0.363      |
|    value_loss           | 570        |
----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 64.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 269        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 275456     |
| train/                  |            |
|    approx_kl            | 0.06791703 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.0707    |
|    std                  | 0.363      |
|    value_loss           | 403        |
----------------------------------------
Num timesteps: 276000
Best mean reward: 791.71 - Last mean reward per episode: 64.63
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.23e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 56.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 270        |
|    time_elapsed         | 1221       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.09127189 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 48.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 271         |
|    time_elapsed         | 1225        |
|    total_timesteps      | 277504      |
| train/                  |             |
|    approx_kl            | 0.053911544 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.363       |
|    value_loss           | 436         |
-----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.12e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 45.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 272        |
|    time_elapsed         | 1230       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.08128683 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 72         |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.363      |
|    value_loss           | 369        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 41         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 273        |
|    time_elapsed         | 1234       |
|    total_timesteps      | 279552     |
| train/                  |            |
|    approx_kl            | 0.07408503 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 269        |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.363      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 34.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 274        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.06615147 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 213        |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 275        |
|    time_elapsed         | 1243       |
|    total_timesteps      | 281600     |
| train/                  |            |
|    approx_kl            | 0.07603255 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.363      |
|    value_loss           | 395        |
----------------------------------------
Num timesteps: 282000
Best mean reward: 791.71 - Last mean reward per episode: 36.90
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 35.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 276        |
|    time_elapsed         | 1247       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.08953834 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 277        |
|    time_elapsed         | 1251       |
|    total_timesteps      | 283648     |
| train/                  |            |
|    approx_kl            | 0.09077023 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 81.7       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 37.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 278         |
|    time_elapsed         | 1255        |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.098774806 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 441         |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.363       |
|    value_loss           | 492         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 29.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 279        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 285696     |
| train/                  |            |
|    approx_kl            | 0.10155187 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 123        |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.363      |
|    value_loss           | 481        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.82e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 280        |
|    time_elapsed         | 1264       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.07861796 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.363      |
|    value_loss           | 411        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.64e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.5        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 281         |
|    time_elapsed         | 1268        |
|    total_timesteps      | 287744      |
| train/                  |             |
|    approx_kl            | 0.121506184 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.363       |
|    value_loss           | 327         |
-----------------------------------------
Num timesteps: 288000
Best mean reward: 791.71 - Last mean reward per episode: 19.47
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.29e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 26.8       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 282        |
|    time_elapsed         | 1273       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.06990592 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0652    |
|    std                  | 0.363      |
|    value_loss           | 336        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.6e-07     |
| reward_position         | 0.000131    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.78        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 283         |
|    time_elapsed         | 1277        |
|    total_timesteps      | 289792      |
| train/                  |             |
|    approx_kl            | 0.100373924 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.099      |
|    std                  | 0.363       |
|    value_loss           | 367         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 284         |
|    time_elapsed         | 1281        |
|    total_timesteps      | 290816      |
| train/                  |             |
|    approx_kl            | 0.100580186 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.363       |
|    value_loss           | 416         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 285        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.09491367 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 177        |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.74e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 6.95       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 286        |
|    time_elapsed         | 1290       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.05984002 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 144        |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.0675    |
|    std                  | 0.363      |
|    value_loss           | 372        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 2.41       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 287        |
|    time_elapsed         | 1295       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.10393738 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.363      |
|    value_loss           | 446        |
----------------------------------------
Num timesteps: 294000
Best mean reward: 791.71 - Last mean reward per episode: 2.41
---------------------------------------
| reward                  | -2.83     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.41e-05  |
| reward_motion           | 8.84e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.56     |
| reward_velocity         | 0.777     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 4.24      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 288       |
|    time_elapsed         | 1299      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0791928 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 197       |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.0775   |
|    std                  | 0.363     |
|    value_loss           | 445       |
---------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 289        |
|    time_elapsed         | 1303       |
|    total_timesteps      | 295936     |
| train/                  |            |
|    approx_kl            | 0.11338747 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 102        |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0832    |
|    std                  | 0.363      |
|    value_loss           | 453        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 4.88        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 290         |
|    time_elapsed         | 1308        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.085854016 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0722     |
|    std                  | 0.363       |
|    value_loss           | 467         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.45e-05    |
| reward_motion           | 8.82e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 5.61        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 291         |
|    time_elapsed         | 1312        |
|    total_timesteps      | 297984      |
| train/                  |             |
|    approx_kl            | 0.122409046 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.0768     |
|    std                  | 0.363       |
|    value_loss           | 324         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.01e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.49       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 292         |
|    time_elapsed         | 1316        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.106792346 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.4        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.0909     |
|    std                  | 0.363       |
|    value_loss           | 537         |
-----------------------------------------
Num timesteps: 300000
Best mean reward: 791.71 - Last mean reward per episode: -4.49
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 8.85e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 293         |
|    time_elapsed         | 1321        |
|    total_timesteps      | 300032      |
| train/                  |             |
|    approx_kl            | 0.081342936 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 87.6        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.363       |
|    value_loss           | 442         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -5.69      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 294        |
|    time_elapsed         | 1325       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.08617334 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 110        |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 528        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.89e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -4.42      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 295        |
|    time_elapsed         | 1330       |
|    total_timesteps      | 302080     |
| train/                  |            |
|    approx_kl            | 0.05853964 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 181        |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.363      |
|    value_loss           | 525        |
----------------------------------------
----------------------------------------
| reward                  | -2.85      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.58      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -9.75      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 296        |
|    time_elapsed         | 1334       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.08167342 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.6       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.363      |
|    value_loss           | 267        |
----------------------------------------
-----------------------------------------
| reward                  | -2.85       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.68e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.24       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 297         |
|    time_elapsed         | 1339        |
|    total_timesteps      | 304128      |
| train/                  |             |
|    approx_kl            | 0.079787806 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.363       |
|    value_loss           | 554         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -8.79      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 298        |
|    time_elapsed         | 1343       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.10891131 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 320        |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0834    |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
Num timesteps: 306000
Best mean reward: 791.71 - Last mean reward per episode: -8.79
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.45e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -11.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 299        |
|    time_elapsed         | 1348       |
|    total_timesteps      | 306176     |
| train/                  |            |
|    approx_kl            | 0.09185974 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 243        |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.363      |
|    value_loss           | 389        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -17.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 300        |
|    time_elapsed         | 1352       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.07487345 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.363      |
|    value_loss           | 362        |
----------------------------------------
---------------------------------------
| reward                  | -2.85     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 8.75e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.58     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -22.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 301       |
|    time_elapsed         | 1357      |
|    total_timesteps      | 308224    |
| train/                  |           |
|    approx_kl            | 0.0672723 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.0003    |
|    loss                 | 94.2      |
|    n_updates            | 6000      |
|    policy_gradient_loss | -0.0552   |
|    std                  | 0.363     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.4e-05    |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -22.7      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 302        |
|    time_elapsed         | 1361       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.07330081 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.3       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.87       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.76e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -20.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 303         |
|    time_elapsed         | 1365        |
|    total_timesteps      | 310272      |
| train/                  |             |
|    approx_kl            | 0.060410976 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 290         |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0609     |
|    std                  | 0.363       |
|    value_loss           | 535         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -19        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 304        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.09256604 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0847    |
|    std                  | 0.363      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 312000
Best mean reward: 791.71 - Last mean reward per episode: -18.98
---------------------------------------
| reward                  | -2.86     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.42e-05  |
| reward_motion           | 8.81e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.59     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -28.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 305       |
|    time_elapsed         | 1374      |
|    total_timesteps      | 312320    |
| train/                  |           |
|    approx_kl            | 0.0849249 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 33.1      |
|    n_updates            | 6080      |
|    policy_gradient_loss | -0.0641   |
|    std                  | 0.362     |
|    value_loss           | 464       |
---------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -31.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 306         |
|    time_elapsed         | 1378        |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.056493938 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 223         |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.362       |
|    value_loss           | 455         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -29.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 307        |
|    time_elapsed         | 1383       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.09777212 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.0864    |
|    std                  | 0.362      |
|    value_loss           | 448        |
----------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 308         |
|    time_elapsed         | 1387        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.090832666 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 323         |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.0817     |
|    std                  | 0.362       |
|    value_loss           | 542         |
-----------------------------------------
---------------------------------------
| reward                  | -2.87     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.31e-05  |
| reward_motion           | 8.68e-07  |
| reward_position         | 0.000133  |
| reward_torque           | -3.6      |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -46.8     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 309       |
|    time_elapsed         | 1392      |
|    total_timesteps      | 316416    |
| train/                  |           |
|    approx_kl            | 0.0758764 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | 92.8      |
|    n_updates            | 6160      |
|    policy_gradient_loss | -0.0626   |
|    std                  | 0.362     |
|    value_loss           | 457       |
---------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.3e-05     |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.6        |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -49.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 310         |
|    time_elapsed         | 1396        |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.074833825 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.362       |
|    value_loss           | 283         |
-----------------------------------------
Num timesteps: 318000
Best mean reward: 791.71 - Last mean reward per episode: -49.09
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.6       |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -52.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 311        |
|    time_elapsed         | 1400       |
|    total_timesteps      | 318464     |
| train/                  |            |
|    approx_kl            | 0.07202639 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.362      |
|    value_loss           | 443        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -49.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 312        |
|    time_elapsed         | 1405       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.07316707 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 176        |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.362      |
|    value_loss           | 487        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 313        |
|    time_elapsed         | 1409       |
|    total_timesteps      | 320512     |
| train/                  |            |
|    approx_kl            | 0.08623876 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.6       |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 419        |
----------------------------------------
-----------------------------------------
| reward                  | -2.89       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 314         |
|    time_elapsed         | 1414        |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.054401636 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 6260        |
|    policy_gradient_loss | -0.0629     |
|    std                  | 0.362       |
|    value_loss           | 473         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.9        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.84e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.62       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 315         |
|    time_elapsed         | 1419        |
|    total_timesteps      | 322560      |
| train/                  |             |
|    approx_kl            | 0.097741075 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 174         |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.0815     |
|    std                  | 0.362       |
|    value_loss           | 487         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -60        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 316        |
|    time_elapsed         | 1423       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.08981447 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 178        |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.0756    |
|    std                  | 0.362      |
|    value_loss           | 421        |
----------------------------------------
Num timesteps: 324000
Best mean reward: 791.71 - Last mean reward per episode: -59.96
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -64.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 317        |
|    time_elapsed         | 1428       |
|    total_timesteps      | 324608     |
| train/                  |            |
|    approx_kl            | 0.07262496 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.362      |
|    value_loss           | 495        |
----------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -58.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 318        |
|    time_elapsed         | 1433       |
|    total_timesteps      | 325632     |
| train/                  |            |
|    approx_kl            | 0.08142903 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6340       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.362      |
|    value_loss           | 393        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.78e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 319         |
|    time_elapsed         | 1437        |
|    total_timesteps      | 326656      |
| train/                  |             |
|    approx_kl            | 0.057203952 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 149         |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.0562     |
|    std                  | 0.362       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -61.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 320        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.07968338 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.0626    |
|    std                  | 0.362      |
|    value_loss           | 355        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -63.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 321        |
|    time_elapsed         | 1447       |
|    total_timesteps      | 328704     |
| train/                  |            |
|    approx_kl            | 0.07895845 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.362      |
|    value_loss           | 415        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.2e-05     |
| reward_motion           | 8.5e-07     |
| reward_position         | 0.00013     |
| reward_torque           | -3.62       |
| reward_velocity         | 0.779       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -62.5       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 322         |
|    time_elapsed         | 1451        |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.082525745 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.1        |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.362       |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 330000
Best mean reward: 791.71 - Last mean reward per episode: -62.45
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.56e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -66.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 323        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.08526311 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.362      |
|    value_loss           | 503        |
----------------------------------------
---------------------------------------
| reward                  | -2.88     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.24e-05  |
| reward_motion           | 8.57e-07  |
| reward_position         | 0.000131  |
| reward_torque           | -3.61     |
| reward_velocity         | 0.776     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 324       |
|    time_elapsed         | 1460      |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0881749 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | 281       |
|    n_updates            | 6460      |
|    policy_gradient_loss | -0.0783   |
|    std                  | 0.362     |
|    value_loss           | 405       |
---------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 325        |
|    time_elapsed         | 1465       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.08563089 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.2       |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 0.362      |
|    value_loss           | 386        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 326        |
|    time_elapsed         | 1469       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.06256071 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 6500       |
|    policy_gradient_loss | -0.0566    |
|    std                  | 0.362      |
|    value_loss           | 392        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 327        |
|    time_elapsed         | 1473       |
|    total_timesteps      | 334848     |
| train/                  |            |
|    approx_kl            | 0.08924619 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.8       |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.362      |
|    value_loss           | 366        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 328        |
|    time_elapsed         | 1478       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.09193449 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 96.4       |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.0824    |
|    std                  | 0.362      |
|    value_loss           | 319        |
----------------------------------------
Num timesteps: 336000
Best mean reward: 791.71 - Last mean reward per episode: -74.94
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 329        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.08618444 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.5       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.362      |
|    value_loss           | 396        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -67.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 330        |
|    time_elapsed         | 1486       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.05716681 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 6580       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.362      |
|    value_loss           | 581        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -70.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 331        |
|    time_elapsed         | 1491       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.09605715 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 164        |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.0876    |
|    std                  | 0.362      |
|    value_loss           | 537        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.39e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 332        |
|    time_elapsed         | 1495       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.08115013 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.071     |
|    std                  | 0.362      |
|    value_loss           | 578        |
----------------------------------------
---------------------------------------
| reward                  | -2.91     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.17e-05  |
| reward_motion           | 8.39e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -3.63     |
| reward_velocity         | 0.767     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -70.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 333       |
|    time_elapsed         | 1500      |
|    total_timesteps      | 340992    |
| train/                  |           |
|    approx_kl            | 0.0719579 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 6640      |
|    policy_gradient_loss | -0.0657   |
|    std                  | 0.362     |
|    value_loss           | 449       |
---------------------------------------
Num timesteps: 342000
Best mean reward: 791.71 - Last mean reward per episode: -70.20
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.44e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 334        |
|    time_elapsed         | 1504       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.07477051 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.0692    |
|    std                  | 0.362      |
|    value_loss           | 357        |
----------------------------------------
-----------------------------------------
| reward                  | -2.91       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.05e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -3.63       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -82.2       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 335         |
|    time_elapsed         | 1509        |
|    total_timesteps      | 343040      |
| train/                  |             |
|    approx_kl            | 0.038849957 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.0445     |
|    std                  | 0.362       |
|    value_loss           | 342         |
-----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -83.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 336        |
|    time_elapsed         | 1513       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.08295447 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 358        |
|    n_updates            | 6700       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.362      |
|    value_loss           | 511        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 337        |
|    time_elapsed         | 1518       |
|    total_timesteps      | 345088     |
| train/                  |            |
|    approx_kl            | 0.10497835 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 6720       |
|    policy_gradient_loss | -0.0865    |
|    std                  | 0.362      |
|    value_loss           | 490        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -89.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 338        |
|    time_elapsed         | 1522       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.11712283 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.1       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -81.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 339        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.10852885 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.362      |
|    value_loss           | 504        |
----------------------------------------
Num timesteps: 348000
Best mean reward: 791.71 - Last mean reward per episode: -81.56
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 7.89e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -75.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 340        |
|    time_elapsed         | 1531       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.06358122 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 376        |
|    n_updates            | 6780       |
|    policy_gradient_loss | -0.0604    |
|    std                  | 0.362      |
|    value_loss           | 440        |
----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 341        |
|    time_elapsed         | 1536       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.08850199 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.362      |
|    value_loss           | 465        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.9e-05     |
| reward_motion           | 7.97e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -74.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 342         |
|    time_elapsed         | 1540        |
|    total_timesteps      | 350208      |
| train/                  |             |
|    approx_kl            | 0.098224916 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 6820        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.362       |
|    value_loss           | 438         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -77.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 343        |
|    time_elapsed         | 1544       |
|    total_timesteps      | 351232     |
| train/                  |            |
|    approx_kl            | 0.10913843 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 98         |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.0914    |
|    std                  | 0.362      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -75.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 344         |
|    time_elapsed         | 1549        |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.086836606 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 214         |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.362       |
|    value_loss           | 413         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.99e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -69.6       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 345         |
|    time_elapsed         | 1553        |
|    total_timesteps      | 353280      |
| train/                  |             |
|    approx_kl            | 0.060340073 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.9        |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.0478     |
|    std                  | 0.362       |
|    value_loss           | 399         |
-----------------------------------------
Num timesteps: 354000
Best mean reward: 791.71 - Last mean reward per episode: -69.57
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.94e-05    |
| reward_motion           | 8.01e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -76.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 346         |
|    time_elapsed         | 1557        |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.095809594 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 6900        |
|    policy_gradient_loss | -0.0825     |
|    std                  | 0.362       |
|    value_loss           | 411         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.66       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -77.8       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 347         |
|    time_elapsed         | 1562        |
|    total_timesteps      | 355328      |
| train/                  |             |
|    approx_kl            | 0.097013265 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 6920        |
|    policy_gradient_loss | -0.0865     |
|    std                  | 0.362       |
|    value_loss           | 374         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 348        |
|    time_elapsed         | 1566       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.06848779 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 94.4       |
|    n_updates            | 6940       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.362      |
|    value_loss           | 434        |
----------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.97e-05  |
| reward_motion           | 8.1e-07   |
| reward_position         | 0.000124  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -85       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 349       |
|    time_elapsed         | 1571      |
|    total_timesteps      | 357376    |
| train/                  |           |
|    approx_kl            | 0.0612828 |
|    clip_fraction        | 0.178     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 118       |
|    n_updates            | 6960      |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.362     |
|    value_loss           | 382       |
---------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.9e-05   |
| reward_motion           | 7.98e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.775     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -90.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 350       |
|    time_elapsed         | 1575      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0758909 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 124       |
|    n_updates            | 6980      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.362     |
|    value_loss           | 488       |
---------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.8e-05     |
| reward_motion           | 7.85e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.66       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -94.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 351         |
|    time_elapsed         | 1579        |
|    total_timesteps      | 359424      |
| train/                  |             |
|    approx_kl            | 0.101735264 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 7000        |
|    policy_gradient_loss | -0.0952     |
|    std                  | 0.362       |
|    value_loss           | 491         |
-----------------------------------------
Num timesteps: 360000
Best mean reward: 791.71 - Last mean reward per episode: -94.37
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.77e-05  |
| reward_motion           | 7.8e-07   |
| reward_position         | 0.000119  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -89.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 352       |
|    time_elapsed         | 1584      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.1101183 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 89.5      |
|    n_updates            | 7020      |
|    policy_gradient_loss | -0.0816   |
|    std                  | 0.362     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.77e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -91.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 353        |
|    time_elapsed         | 1588       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.07463523 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.8       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.362      |
|    value_loss           | 331        |
----------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.66e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.783      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 354        |
|    time_elapsed         | 1593       |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.09484478 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 41.7       |
|    n_updates            | 7060       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.69e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -96        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 355        |
|    time_elapsed         | 1598       |
|    total_timesteps      | 363520     |
| train/                  |            |
|    approx_kl            | 0.07948056 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 230        |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.362      |
|    value_loss           | 342        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.68e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 356        |
|    time_elapsed         | 1602       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.09544092 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.2       |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.362      |
|    value_loss           | 335        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -97.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 357        |
|    time_elapsed         | 1607       |
|    total_timesteps      | 365568     |
| train/                  |            |
|    approx_kl            | 0.07863753 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.6       |
|    n_updates            | 7120       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.362      |
|    value_loss           | 478        |
----------------------------------------
Num timesteps: 366000
Best mean reward: 791.71 - Last mean reward per episode: -97.89
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.57e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.68      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 358        |
|    time_elapsed         | 1611       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.09423084 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 226        |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.0858    |
|    std                  | 0.362      |
|    value_loss           | 437        |
----------------------------------------
---------------------------------------
| reward                  | -2.94     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.63e-05  |
| reward_motion           | 7.54e-07  |
| reward_position         | 0.000116  |
| reward_torque           | -3.67     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 359       |
|    time_elapsed         | 1616      |
|    total_timesteps      | 367616    |
| train/                  |           |
|    approx_kl            | 0.1212948 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 97.7      |
|    n_updates            | 7160      |
|    policy_gradient_loss | -0.0898   |
|    std                  | 0.362     |
|    value_loss           | 416       |
---------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.65e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -99        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 360        |
|    time_elapsed         | 1620       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.10194968 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0818    |
|    std                  | 0.362      |
|    value_loss           | 470        |
----------------------------------------
-----------------------------------------
| reward                  | -2.95       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.7e-05     |
| reward_motion           | 7.61e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.781       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 361         |
|    time_elapsed         | 1624        |
|    total_timesteps      | 369664      |
| train/                  |             |
|    approx_kl            | 0.096999794 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 7200        |
|    policy_gradient_loss | -0.0912     |
|    std                  | 0.362       |
|    value_loss           | 318         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.94       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.53e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 362         |
|    time_elapsed         | 1629        |
|    total_timesteps      | 370688      |
| train/                  |             |
|    approx_kl            | 0.081590526 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 51          |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.362       |
|    value_loss           | 373         |
-----------------------------------------
---------------------------------------
| reward                  | -2.95     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.76e-05  |
| reward_motion           | 7.68e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.68     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 363       |
|    time_elapsed         | 1633      |
|    total_timesteps      | 371712    |
| train/                  |           |
|    approx_kl            | 0.0668627 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 44.1      |
|    n_updates            | 7240      |
|    policy_gradient_loss | -0.0582   |
|    std                  | 0.362     |
|    value_loss           | 240       |
---------------------------------------
Num timesteps: 372000
Best mean reward: 791.71 - Last mean reward per episode: -112.40
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.72e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 364         |
|    time_elapsed         | 1637        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.094104856 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 7260        |
|    policy_gradient_loss | -0.0816     |
|    std                  | 0.361       |
|    value_loss           | 385         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.59e-07    |
| reward_position         | 0.000116    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 365         |
|    time_elapsed         | 1642        |
|    total_timesteps      | 373760      |
| train/                  |             |
|    approx_kl            | 0.049591344 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.0509     |
|    std                  | 0.361       |
|    value_loss           | 347         |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Traceback (most recent call last):
  File "ddpg.py", line 224, in <module>
    'info_keywords' : info_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 977, in __init__
    super(AntEnvV8, self).__init__('ant.xml')
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 467, in __init__
    super(AntEnvV4, self).__init__(path)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 84, in __init__
    mujoco_env.MujocoEnv.__init__(self, path, 5)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py", line 64, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 1086, in step
    reward = info['reward_ctrl'] + info['reward_torque'] + info['reward_motion'] + info['reward_position'] + info['reward_velocity'] + info['reward_contact']
KeyError: 'reward_position'
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_4
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    fps             | 501       |
|    iterations      | 1         |
|    time_elapsed    | 2         |
|    total_timesteps | 1024      |
----------------------------------
----------------------------------------
| reward                  | -1.59      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 1.48e-06   |
| reward_motion           | 3.8e-07    |
| reward_torque           | -2.33      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -1.68e+03  |
| time/                   |            |
|    fps                  | 408        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.36854783 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.6      |
|    explained_variance   | -0.00194   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.364      |
|    value_loss           | 27.3       |
----------------------------------------
---------------------------------------
| reward                  | -1.72     |
| reward_contact          | -0.0474   |
| reward_ctrl             | 5.05e-06  |
| reward_motion           | 1.15e-06  |
| reward_torque           | -2.54     |
| reward_velocity         | 0.869     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -1.97e+03 |
| time/                   |           |
|    fps                  | 384       |
|    iterations           | 3         |
|    time_elapsed         | 7         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.3725308 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.8     |
|    explained_variance   | 0.0732    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0945   |
|    std                  | 0.362     |
|    value_loss           | 27.4      |
---------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.95e-06  |
| reward_motion           | 9.14e-07  |
| reward_torque           | -3.03     |
| reward_velocity         | 0.814     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.03e+03 |
| time/                   |           |
|    fps                  | 373       |
|    iterations           | 4         |
|    time_elapsed         | 10        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.133129  |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19       |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.01      |
|    n_updates            | 60        |
|    policy_gradient_loss | -2.98e-05 |
|    std                  | 0.362     |
|    value_loss           | 126       |
---------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.5e-06    |
| reward_motion           | 8.15e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.839      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.06e+03  |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 5          |
|    time_elapsed         | 13         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.20465791 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.361      |
|    value_loss           | 66.6       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -2059.00
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.93e-06   |
| reward_motion           | 1.06e-06   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.849      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.13e+03  |
| time/                   |            |
|    fps                  | 362        |
|    iterations           | 6          |
|    time_elapsed         | 16         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.11940461 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.92       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0845    |
|    std                  | 0.36       |
|    value_loss           | 29.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.4e-06    |
| reward_motion           | 9.54e-07   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.837      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 359        |
|    iterations           | 7          |
|    time_elapsed         | 19         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09918121 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.359      |
|    value_loss           | 40.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.0466    |
| reward_ctrl             | 3.99e-06   |
| reward_motion           | 8.61e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 357        |
|    iterations           | 8          |
|    time_elapsed         | 22         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04906732 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | -1.03      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0508    |
|    std                  | 0.359      |
|    value_loss           | 50.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0465    |
| reward_ctrl             | 3.78e-06   |
| reward_motion           | 8.16e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.815      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 356        |
|    iterations           | 9          |
|    time_elapsed         | 25         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.09477489 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.31       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0674    |
|    std                  | 0.358      |
|    value_loss           | 39.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 3.46e-06   |
| reward_motion           | 7.53e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.18e+03  |
| time/                   |            |
|    fps                  | 354        |
|    iterations           | 10         |
|    time_elapsed         | 28         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.64199543 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.0344     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0534    |
|    std                  | 0.357      |
|    value_loss           | 47.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.24      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.2e-06    |
| reward_motion           | 7.02e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 11         |
|    time_elapsed         | 31         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.22036704 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.164      |
|    learning_rate        | 0.0003     |
|    loss                 | 35         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.355      |
|    value_loss           | 61.6       |
----------------------------------------
Num timesteps: 12000
Best mean reward: -2059.00 - Last mean reward per episode: -2203.10
----------------------------------------
| reward                  | -2.34      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.88e-07   |
| reward_torque           | -3.11      |
| reward_velocity         | 0.812      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 12         |
|    time_elapsed         | 34         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16806449 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.0587     |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.355      |
|    value_loss           | 56.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.33      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.13e-06   |
| reward_motion           | 7.12e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 352        |
|    iterations           | 13         |
|    time_elapsed         | 37         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.22779572 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | -0.00966   |
|    learning_rate        | 0.0003     |
|    loss                 | 30         |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.355      |
|    value_loss           | 70.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.98e-06   |
| reward_motion           | 6.79e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.813      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 14         |
|    time_elapsed         | 40         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.25915498 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.0405     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.39       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.043     |
|    std                  | 0.354      |
|    value_loss           | 75.4       |
----------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.97e-06  |
| reward_motion           | 6.71e-07  |
| reward_torque           | -3.04     |
| reward_velocity         | 0.82      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.2e+03  |
| time/                   |           |
|    fps                  | 351       |
|    iterations           | 15        |
|    time_elapsed         | 43        |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.2859522 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.6     |
|    explained_variance   | -0.0266   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0304   |
|    std                  | 0.353     |
|    value_loss           | 74.5      |
---------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.85e-06   |
| reward_motion           | 6.45e-07   |
| reward_torque           | -3.05      |
| reward_velocity         | 0.809      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 16         |
|    time_elapsed         | 46         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.18760654 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | -0.237     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.8       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.352      |
|    value_loss           | 92.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.8e-06    |
| reward_motion           | 6.31e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 17         |
|    time_elapsed         | 49         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.29463354 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.0503     |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.352      |
|    value_loss           | 102        |
----------------------------------------
Num timesteps: 18000
Best mean reward: -2059.00 - Last mean reward per episode: -2224.99
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.81e-06   |
| reward_motion           | 6.25e-07   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 18         |
|    time_elapsed         | 52         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.36696294 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.032      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.1       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.351      |
|    value_loss           | 107        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.16e-06  |
| reward_motion           | 6.89e-07  |
| reward_torque           | -3.12     |
| reward_velocity         | 0.8       |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.25e+03 |
| time/                   |           |
|    fps                  | 349       |
|    iterations           | 19        |
|    time_elapsed         | 55        |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.0930747 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | -0.0214   |
|    learning_rate        | 0.0003    |
|    loss                 | 75.1      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0604   |
|    std                  | 0.351     |
|    value_loss           | 114       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.11e-06   |
| reward_motion           | 6.82e-07   |
| reward_torque           | -3.13      |
| reward_velocity         | 0.794      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.26e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 20         |
|    time_elapsed         | 58         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.21897617 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.0717     |
|    learning_rate        | 0.0003     |
|    loss                 | 145        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.35       |
|    value_loss           | 123        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.01e-06   |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.796      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.28e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 21         |
|    time_elapsed         | 61         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.44544753 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.0266     |
|    learning_rate        | 0.0003     |
|    loss                 | 75.7       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0516    |
|    std                  | 0.349      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.05e-06   |
| reward_motion           | 6.65e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.792      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.3e+03   |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 22         |
|    time_elapsed         | 64         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.09976232 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0835     |
|    learning_rate        | 0.0003     |
|    loss                 | 25.2       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.348      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.5e-07    |
| reward_torque           | -3.17      |
| reward_velocity         | 0.791      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.31e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 23         |
|    time_elapsed         | 67         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.23673081 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0369     |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0577    |
|    std                  | 0.348      |
|    value_loss           | 152        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -2059.00 - Last mean reward per episode: -2307.34
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 2.98e-06  |
| reward_motion           | 6.58e-07  |
| reward_torque           | -3.2      |
| reward_velocity         | 0.785     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.32e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 24        |
|    time_elapsed         | 70        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.8325364 |
|    clip_fraction        | 0.595     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0374   |
|    std                  | 0.347     |
|    value_loss           | 157       |
---------------------------------------
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.95e-06  |
| reward_motion           | 6.54e-07  |
| reward_torque           | -3.19     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.34e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 25        |
|    time_elapsed         | 73        |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.169721  |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.035     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.3      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0566   |
|    std                  | 0.347     |
|    value_loss           | 166       |
---------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.95e-06   |
| reward_motion           | 6.46e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.34e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 26         |
|    time_elapsed         | 76         |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.18979153 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.0987     |
|    learning_rate        | 0.0003     |
|    loss                 | 37.5       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.347      |
|    value_loss           | 120        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.49e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.35e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 27         |
|    time_elapsed         | 79         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.11660235 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | -0.46      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.9       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0506    |
|    std                  | 0.346      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.36e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 28         |
|    time_elapsed         | 82         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11507568 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.0308     |
|    learning_rate        | 0.0003     |
|    loss                 | 28.1       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.346      |
|    value_loss           | 185        |
----------------------------------------
-----------------------------------------
| reward                  | -2.44       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.24e-06    |
| reward_motion           | 6.88e-07    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -2.38e+03   |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 29          |
|    time_elapsed         | 85          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.068516396 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.0659      |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.345       |
|    value_loss           | 198         |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -2059.00 - Last mean reward per episode: -2376.28
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.33e-06   |
| reward_motion           | 7.15e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 30         |
|    time_elapsed         | 88         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.11165166 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0535     |
|    learning_rate        | 0.0003     |
|    loss                 | 86.8       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0514    |
|    std                  | 0.345      |
|    value_loss           | 195        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.75e-06   |
| reward_motion           | 7.84e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 31         |
|    time_elapsed         | 91         |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.16336218 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.0889     |
|    learning_rate        | 0.0003     |
|    loss                 | 182        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.345      |
|    value_loss           | 194        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.74e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.39e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 32         |
|    time_elapsed         | 94         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.20484138 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.0319     |
|    learning_rate        | 0.0003     |
|    loss                 | 41.6       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.344      |
|    value_loss           | 210        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.87e-06   |
| reward_motion           | 8.28e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 33         |
|    time_elapsed         | 97         |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.38690743 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.0346     |
|    learning_rate        | 0.0003     |
|    loss                 | 150        |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0492    |
|    std                  | 0.344      |
|    value_loss           | 228        |
----------------------------------------
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.79e-06   |
| reward_motion           | 8.12e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 34         |
|    time_elapsed         | 100        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.11785532 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0255     |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.71e-06   |
| reward_motion           | 7.97e-07   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 35         |
|    time_elapsed         | 103        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.13725826 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.0618     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0687    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
Num timesteps: 36000
Best mean reward: -2059.00 - Last mean reward per episode: -2416.80
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.64e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 36         |
|    time_elapsed         | 106        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.12666398 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.0903     |
|    learning_rate        | 0.0003     |
|    loss                 | 40         |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0596    |
|    std                  | 0.344      |
|    value_loss           | 225        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-06   |
| reward_motion           | 8.75e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.43e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 37         |
|    time_elapsed         | 109        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.19215783 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0698     |
|    learning_rate        | 0.0003     |
|    loss                 | 104        |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.344      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.1e-06    |
| reward_motion           | 8.6e-07    |
| reward_torque           | -3.22      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 38         |
|    time_elapsed         | 112        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.13736585 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0917     |
|    learning_rate        | 0.0003     |
|    loss                 | 84.8       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.343      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.66e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 39         |
|    time_elapsed         | 115        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.37177145 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0291     |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.343      |
|    value_loss           | 244        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-06  |
| reward_motion           | 8.59e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.44e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 40        |
|    time_elapsed         | 118       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.2808153 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.05      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.063    |
|    std                  | 0.343     |
|    value_loss           | 236       |
---------------------------------------
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.64e-07   |
| reward_torque           | -3.26      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 41         |
|    time_elapsed         | 121        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.17382684 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.0568     |
|    learning_rate        | 0.0003     |
|    loss                 | 131        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0536    |
|    std                  | 0.343      |
|    value_loss           | 235        |
----------------------------------------
Num timesteps: 42000
Best mean reward: -2059.00 - Last mean reward per episode: -2441.78
---------------------------------------
| reward                  | -2.52     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.04e-06  |
| reward_motion           | 8.57e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.45e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 42        |
|    time_elapsed         | 124       |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.1686695 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.6     |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.0672   |
|    std                  | 0.342     |
|    value_loss           | 233       |
---------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-06   |
| reward_motion           | 8.45e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.46e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 43         |
|    time_elapsed         | 127        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.18749896 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0552     |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.342      |
|    value_loss           | 236        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_5
---------------------------------
| reward             | 9.74     |
| reward_contact     | 0        |
| reward_ctrl        | 10.5     |
| reward_motion      | 2.47     |
| reward_torque      | -3.27    |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 468      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | 12.7       |
| reward_contact          | 0          |
| reward_ctrl             | 13.5       |
| reward_motion           | 2.5        |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0349     |
| rollout/                |            |
|    ep_len_mean          | 86.9       |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 382        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.08909861 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.0042    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6e+03    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.368      |
|    value_loss           | 5.3e+03    |
----------------------------------------
----------------------------------------
| reward                  | 12.1       |
| reward_contact          | -0.000819  |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0387     |
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 987        |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.08520674 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 813        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.368      |
|    value_loss           | 3.24e+03   |
----------------------------------------
----------------------------------------
| reward                  | 11.9       |
| reward_contact          | -0.0012    |
| reward_ctrl             | 12.9       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0384     |
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 1.05e+03   |
| time/                   |            |
|    fps                  | 335        |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.06514679 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.0102     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0947    |
|    std                  | 0.368      |
|    value_loss           | 3.44e+03   |
----------------------------------------
----------------------------------------
| reward                  | 12         |
| reward_contact          | -0.00108   |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.38       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0377     |
| rollout/                |            |
|    ep_len_mean          | 127        |
|    ep_rew_mean          | 1.21e+03   |
| time/                   |            |
|    fps                  | 325        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07207671 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.34e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.368      |
|    value_loss           | 3.91e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 878.37
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 11.8        |
| reward_contact          | -0.000959   |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.41        |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0303      |
| rollout/                |             |
|    ep_len_mean          | 86.8        |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 316         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.028427092 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 449         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0339     |
|    std                  | 0.368       |
|    value_loss           | 1.61e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.8       |
| reward_contact          | -0.000907  |
| reward_ctrl             | 12.8       |
| reward_motion           | 2.44       |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0302     |
| rollout/                |            |
|    ep_len_mean          | 94.9       |
|    ep_rew_mean          | 921        |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.04576248 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0659     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.368      |
|    value_loss           | 3.73e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.7        |
| reward_contact          | -0.001      |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.51        |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 86.1        |
|    ep_rew_mean          | 841         |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.055026103 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 332         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.00103   |
| reward_ctrl             | 12.5       |
| reward_motion           | 2.48       |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0311     |
| rollout/                |            |
|    ep_len_mean          | 95.2       |
|    ep_rew_mean          | 917        |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.04045736 |
|    clip_fraction        | 0.0592     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0003     |
|    loss                 | 536        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0443    |
|    std                  | 0.368      |
|    value_loss           | 1.82e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.5        |
| reward_contact          | -0.000958   |
| reward_ctrl             | 12.4        |
| reward_motion           | 2.52        |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0294      |
| rollout/                |             |
|    ep_len_mean          | 91.9        |
|    ep_rew_mean          | 889         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 10          |
|    time_elapsed         | 33          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.032536104 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 502         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0337     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.3        |
| reward_contact          | -0.00123    |
| reward_ctrl             | 12.2        |
| reward_motion           | 2.62        |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 73.1        |
|    ep_rew_mean          | 724         |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 11          |
|    time_elapsed         | 37          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.049803257 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.82e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 878.37 - Last mean reward per episode: 723.79
----------------------------------------
| reward                  | 11.4       |
| reward_contact          | -0.00136   |
| reward_ctrl             | 12.3       |
| reward_motion           | 2.62       |
| reward_torque           | -3.5       |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 708        |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.05735998 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.0003     |
|    loss                 | 853        |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.368      |
|    value_loss           | 2.88e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.0018     |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.67        |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 70.8        |
|    ep_rew_mean          | 695         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.040806033 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0498     |
|    std                  | 0.368       |
|    value_loss           | 1.88e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.00219    |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.72        |
| reward_torque           | -3.61       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 80.6        |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 14          |
|    time_elapsed         | 47          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.046216436 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 762         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 2.02e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.4        |
| reward_contact          | -0.0023     |
| reward_ctrl             | 12.3        |
| reward_motion           | 2.69        |
| reward_torque           | -3.6        |
| reward_velocity         | 0.0234      |
| rollout/                |             |
|    ep_len_mean          | 79.3        |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.038783964 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 867         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.0025    |
| reward_ctrl             | 12.4       |
| reward_motion           | 2.69       |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 86.8       |
|    ep_rew_mean          | 831        |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 16         |
|    time_elapsed         | 54         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.04771261 |
|    clip_fraction        | 0.0803     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.218      |
|    learning_rate        | 0.0003     |
|    loss                 | 663        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0486    |
|    std                  | 0.368      |
|    value_loss           | 1.92e+03   |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_6
---------------------------------
| reward             | 10.6     |
| reward_contact     | 0        |
| reward_ctrl        | 14.2     |
| reward_motion      | -0.1     |
| reward_torque      | -3.52    |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.00277    |
| reward_ctrl             | 13          |
| reward_motion           | -0.0681     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0126      |
| rollout/                |             |
|    ep_len_mean          | 68          |
|    ep_rew_mean          | 643         |
| time/                   |             |
|    fps                  | 387         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 2048        |
| train/                  |             |
|    approx_kl            | 0.055133946 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | -1.6e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 988         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.108      |
|    std                  | 0.368       |
|    value_loss           | 4.26e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.62        |
| reward_contact          | -0.00201    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0768     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0131      |
| rollout/                |             |
|    ep_len_mean          | 85.3        |
|    ep_rew_mean          | 782         |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 3           |
|    time_elapsed         | 8           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.053772744 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0647     |
|    std                  | 0.368       |
|    value_loss           | 2.46e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.48        |
| reward_contact          | -0.00138    |
| reward_ctrl             | 12.8        |
| reward_motion           | -0.0712     |
| reward_torque           | -3.3        |
| reward_velocity         | 0.0167      |
| rollout/                |             |
|    ep_len_mean          | 69.3        |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.071887545 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0692     |
|    std                  | 0.368       |
|    value_loss           | 2.36e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.17       |
| reward_contact          | -0.00164   |
| reward_ctrl             | 12.5       |
| reward_motion           | -0.047     |
| reward_torque           | -3.33      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 84.5       |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 324        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.06008743 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 740        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0612    |
|    std                  | 0.368      |
|    value_loss           | 2.18e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 916.73
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.11        |
| reward_contact          | -0.00199    |
| reward_ctrl             | 12.5        |
| reward_motion           | -0.0374     |
| reward_torque           | -3.33       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 917         |
| time/                   |             |
|    fps                  | 318         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.069795854 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.0429      |
|    learning_rate        | 0.0003      |
|    loss                 | 623         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0794     |
|    std                  | 0.368       |
|    value_loss           | 2.51e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 8.98       |
| reward_contact          | -0.00185   |
| reward_ctrl             | 12.3       |
| reward_motion           | -0.022     |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 906        |
| time/                   |            |
|    fps                  | 314        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.06844729 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.173      |
|    learning_rate        | 0.0003     |
|    loss                 | 594        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.368      |
|    value_loss           | 2.67e+03   |
----------------------------------------
---------------------------------------
| reward                  | 9.47      |
| reward_contact          | -0.00174  |
| reward_ctrl             | 12.8      |
| reward_motion           | -0.0261   |
| reward_torque           | -3.34     |
| reward_velocity         | 0.0231    |
| rollout/                |           |
|    ep_len_mean          | 90.6      |
|    ep_rew_mean          | 819       |
| time/                   |           |
|    fps                  | 311       |
|    iterations           | 8         |
|    time_elapsed         | 26        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0293882 |
|    clip_fraction        | 0.0242    |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78e+03  |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0362   |
|    std                  | 0.368     |
|    value_loss           | 3.16e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.27        |
| reward_contact          | -0.00184    |
| reward_ctrl             | 12.6        |
| reward_motion           | -0.0261     |
| reward_torque           | -3.34       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 884         |
| time/                   |             |
|    fps                  | 309         |
|    iterations           | 9           |
|    time_elapsed         | 29          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.042493284 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+03     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0387     |
|    std                  | 0.368       |
|    value_loss           | 2.83e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.5        |
| reward_contact          | -0.00224   |
| reward_ctrl             | 12.8       |
| reward_motion           | -0.026     |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0236     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 910        |
| time/                   |            |
|    fps                  | 307        |
|    iterations           | 10         |
|    time_elapsed         | 33         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.04812345 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03e+03   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.368      |
|    value_loss           | 2.64e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.82        |
| reward_contact          | -0.00158    |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0231     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0273      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | 766         |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 11          |
|    time_elapsed         | 36          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.063712046 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.0356      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.368       |
|    value_loss           | 3.19e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 916.73 - Last mean reward per episode: 693.01
---------------------------------------
| reward                  | 9.64      |
| reward_contact          | -0.00117  |
| reward_ctrl             | 13.1      |
| reward_motion           | -0.0518   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0213    |
| rollout/                |           |
|    ep_len_mean          | 56        |
|    ep_rew_mean          | 530       |
| time/                   |           |
|    fps                  | 305       |
|    iterations           | 12        |
|    time_elapsed         | 40        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0654025 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.198     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0656   |
|    std                  | 0.368     |
|    value_loss           | 3.05e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.61        |
| reward_contact          | -0.00121    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0626     |
| reward_torque           | -3.4        |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 57.5        |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 13          |
|    time_elapsed         | 43          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.035146132 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | 912         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0486     |
|    std                  | 0.368       |
|    value_loss           | 2.72e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.87       |
| reward_contact          | -0.000842  |
| reward_ctrl             | 13.4       |
| reward_motion           | -0.0653    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 52.4       |
|    ep_rew_mean          | 500        |
| time/                   |            |
|    fps                  | 302        |
|    iterations           | 14         |
|    time_elapsed         | 47         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.03921909 |
|    clip_fraction        | 0.0476     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0164     |
|    learning_rate        | 0.0003     |
|    loss                 | 469        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.368      |
|    value_loss           | 1.97e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0172      |
| rollout/                |             |
|    ep_len_mean          | 54.1        |
|    ep_rew_mean          | 510         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.046162732 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 385         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0633     |
|    std                  | 0.368       |
|    value_loss           | 1.41e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.56        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.017       |
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 16          |
|    time_elapsed         | 54          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.058286734 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | -0.0672     |
|    learning_rate        | 0.0003      |
|    loss                 | 397         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.062      |
|    std                  | 0.368       |
|    value_loss           | 1.15e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.76        |
| reward_contact          | -0.00024    |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0653     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0163      |
| rollout/                |             |
|    ep_len_mean          | 68.4        |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.037595566 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | 576         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0411     |
|    std                  | 0.368       |
|    value_loss           | 2.15e+03    |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 916.73 - Last mean reward per episode: 674.37
----------------------------------------
| reward                  | 9.71       |
| reward_contact          | -0.00024   |
| reward_ctrl             | 13.2       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 73.7       |
|    ep_rew_mean          | 674        |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 18         |
|    time_elapsed         | 61         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.03569829 |
|    clip_fraction        | 0.0447     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.127     |
|    learning_rate        | 0.0003     |
|    loss                 | 903        |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0406    |
|    std                  | 0.368      |
|    value_loss           | 2.46e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.7         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0198      |
| rollout/                |             |
|    ep_len_mean          | 80.7        |
|    ep_rew_mean          | 731         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 19          |
|    time_elapsed         | 65          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.055611745 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | -0.0154     |
|    learning_rate        | 0.0003      |
|    loss                 | 392         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0574     |
|    std                  | 0.368       |
|    value_loss           | 1.72e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.6         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 90.4        |
|    ep_rew_mean          | 813         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 20          |
|    time_elapsed         | 68          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.030617608 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.0102      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.32e+03    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0396     |
|    std                  | 0.368       |
|    value_loss           | 4.55e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.69        |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0647     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0224      |
| rollout/                |             |
|    ep_len_mean          | 89.7        |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 21          |
|    time_elapsed         | 72          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.042898893 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 495         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.05       |
|    std                  | 0.368       |
|    value_loss           | 1.98e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.89        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.39       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 929         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 22          |
|    time_elapsed         | 76          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.019014295 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 768         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0197     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 10.2        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.7        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0238      |
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 953         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 23          |
|    time_elapsed         | 79          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.050864376 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 961         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0516     |
|    std                  | 0.368       |
|    value_loss           | 2.86e+03    |
-----------------------------------------
Num timesteps: 24000
Best mean reward: 916.73 - Last mean reward per episode: 952.60
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.99        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.5        |
| reward_motion           | -0.0586     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0246      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 927         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 24          |
|    time_elapsed         | 82          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.046912353 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+03    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.368       |
|    value_loss           | 3.29e+03    |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_7
---------------------------------
| reward             | -3.82    |
| reward_contact     | 0        |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.64    |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | -94.4    |
| time/              |          |
|    fps             | 484      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0709    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 57.2       |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 377        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.12333509 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.00606    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.88       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.367      |
|    value_loss           | 330        |
----------------------------------------
---------------------------------------
| reward                  | -3.52     |
| reward_contact          | -0.00137  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0442   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0221    |
| rollout/                |           |
|    ep_len_mean          | 84.1      |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 3         |
|    time_elapsed         | 9         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.1174901 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.343     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.1      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.367     |
|    value_loss           | 142       |
---------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | -0.00112    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0364     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 75.3        |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 324         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.054415382 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.367       |
|    value_loss           | 155         |
-----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00096   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0242    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 90.5       |
|    ep_rew_mean          | -283       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 5          |
|    time_elapsed         | 16         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07612029 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.3       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.367      |
|    value_loss           | 131        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -297.27
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.000828  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0346    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 94.6       |
|    ep_rew_mean          | -297       |
| time/                   |            |
|    fps                  | 309        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.08958479 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.367      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000866  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0467    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 95.1       |
|    ep_rew_mean          | -300       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.07138507 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 39.7       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0889    |
|    std                  | 0.367      |
|    value_loss           | 176        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000843  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0371    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0264     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -333       |
| time/                   |            |
|    fps                  | 303        |
|    iterations           | 8          |
|    time_elapsed         | 26         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.09170306 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 280        |
----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 88.8       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.08530077 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.3       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0976    |
|    std                  | 0.367      |
|    value_loss           | 140        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.000614   |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0549     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0212      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | -266        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 10          |
|    time_elapsed         | 34          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.091505446 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.367       |
|    value_loss           | 513         |
-----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.000614  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0549    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 85.5       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08441191 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.0003     |
|    loss                 | 60         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0764    |
|    std                  | 0.366      |
|    value_loss           | 382        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -297.27 - Last mean reward per episode: -308.44
-----------------------------------------
| reward                  | -3.65       |
| reward_contact          | -0.00101    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0577     |
| reward_torque           | -3.52       |
| reward_velocity         | 0.0205      |
| rollout/                |             |
|    ep_len_mean          | 97.5        |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 12          |
|    time_elapsed         | 41          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.058747333 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.6        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.366       |
|    value_loss           | 237         |
-----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 89.3       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 13         |
|    time_elapsed         | 44         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06798034 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0833    |
|    std                  | 0.366      |
|    value_loss           | 209        |
----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 81.3       |
|    ep_rew_mean          | -260       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.13578144 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.446      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 201        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.000958  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0694    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 83.3       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 15         |
|    time_elapsed         | 52         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.07831302 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.8       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.366      |
|    value_loss           | 299        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00129   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0676    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0195     |
| rollout/                |            |
|    ep_len_mean          | 81.6       |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 292        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05408106 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 42         |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.366      |
|    value_loss           | 247        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00132    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.076      |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0193      |
| rollout/                |             |
|    ep_len_mean          | 81.5        |
|    ep_rew_mean          | -262        |
| time/                   |             |
|    fps                  | 292         |
|    iterations           | 17          |
|    time_elapsed         | 59          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.098303586 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.366       |
|    value_loss           | 272         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -297.27 - Last mean reward per episode: -261.67
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 87.5       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 18         |
|    time_elapsed         | 63         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08449566 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.6       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.366      |
|    value_loss           | 286        |
----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0831    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 88.6       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 19         |
|    time_elapsed         | 66         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.09105019 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.85       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.366      |
|    value_loss           | 111        |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.00158    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.077      |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0214      |
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 20          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.076098844 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.89        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0995     |
|    std                  | 0.366       |
|    value_loss           | 131         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00141    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 21          |
|    time_elapsed         | 73          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.116045415 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.49        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.366       |
|    value_loss           | 79.1        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00189    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.068719216 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0901     |
|    std                  | 0.366       |
|    value_loss           | 231         |
-----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00205   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 96.4       |
|    ep_rew_mean          | -310       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 23         |
|    time_elapsed         | 81         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.08666834 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 23         |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0861    |
|    std                  | 0.366      |
|    value_loss           | 348        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_8
---------------------------------
| reward             | -3.58    |
| reward_contact     | -0.00667 |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.39    |
| reward_velocity    | 0.0173   |
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00168   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 71.1       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 378        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10417083 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.0168     |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.367      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 69.6       |
|    ep_rew_mean          | -220       |
| time/                   |            |
|    fps                  | 343        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.11021443 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.334     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.367      |
|    value_loss           | 99.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.0021     |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0882     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0174      |
| rollout/                |             |
|    ep_len_mean          | 58.7        |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.091333374 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | -0.0565     |
|    learning_rate        | 0.0003      |
|    loss                 | 36.8        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.367       |
|    value_loss           | 162         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.58       |
| reward_contact          | -0.00234    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.084      |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0183      |
| rollout/                |             |
|    ep_len_mean          | 55.3        |
|    ep_rew_mean          | -176        |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 5           |
|    time_elapsed         | 15          |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.067936674 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0818     |
|    std                  | 0.367       |
|    value_loss           | 237         |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -138.63
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00219   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0743    |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 42.8       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.06220818 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0003     |
|    loss                 | 88.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.367      |
|    value_loss           | 254        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | -0.00223   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0638    |
| reward_torque           | -3.35      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09764336 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0257     |
|    learning_rate        | 0.0003     |
|    loss                 | 59         |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 201        |
----------------------------------------
-----------------------------------------
| reward                  | -3.51       |
| reward_contact          | -0.00208    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0554     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 48.5        |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.096551545 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.367       |
|    value_loss           | 132         |
-----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.38      |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 58.3       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07261966 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 248        |
----------------------------------------
---------------------------------------
| reward                  | -3.53     |
| reward_contact          | -0.00183  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0591   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0267    |
| rollout/                |           |
|    ep_len_mean          | 68        |
|    ep_rew_mean          | -217      |
| time/                   |           |
|    fps                  | 303       |
|    iterations           | 10        |
|    time_elapsed         | 33        |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.0808554 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 47        |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0864   |
|    std                  | 0.367     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 71         |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.12363265 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.133      |
|    learning_rate        | 0.0003     |
|    loss                 | 76.2       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.367      |
|    value_loss           | 263        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -138.63 - Last mean reward per episode: -231.23
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00204   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 73.6       |
|    ep_rew_mean          | -236       |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07578746 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.8       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 206        |
----------------------------------------
-----------------------------------------
| reward                  | -3.59       |
| reward_contact          | -0.00237    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0516     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.027       |
| rollout/                |             |
|    ep_len_mean          | 63.5        |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.062469497 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0867     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00237   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0621    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -204       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07475457 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0003     |
|    loss                 | 126        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.366      |
|    value_loss           | 506        |
----------------------------------------
