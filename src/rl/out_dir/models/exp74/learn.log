running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 308      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0461    |
| reward_ctrl             | 6.04e-05   |
| reward_motion           | 1.47e-06   |
| reward_position         | 0.00022    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.677      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 392        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.15049204 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | -7.55e-05  |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.368      |
|    value_loss           | 1.18e+03   |
----------------------------------------
----------------------------------------
| reward                  | -2.23      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.645      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 3          |
|    time_elapsed         | 13         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.10099134 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.369      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0474    |
| reward_ctrl             | 3.22e-05   |
| reward_motion           | 8.05e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.706      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 4          |
|    time_elapsed         | 18         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.15403327 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.2      |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.368      |
|    value_loss           | 672        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 2.92e-05   |
| reward_motion           | 7.39e-07   |
| reward_position         | 0.000111   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 5          |
|    time_elapsed         | 23         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.13035062 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.1      |
|    explained_variance   | 0.408      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.5       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.368      |
|    value_loss           | 684        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 630.72
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.047      |
| reward_ctrl             | 2.97e-05    |
| reward_motion           | 6.88e-07    |
| reward_position         | 0.000103    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 6           |
|    time_elapsed         | 28          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.052000042 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.7        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.368       |
|    value_loss           | 745         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.03       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 2.66e-05    |
| reward_motion           | 6.1e-07     |
| reward_position         | 9.15e-05    |
| reward_torque           | -2.75       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 550         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 7           |
|    time_elapsed         | 34          |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.037831835 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.3       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.6        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 572         |
-----------------------------------------
----------------------------------------
| reward                  | -2.04      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 2.74e-05   |
| reward_motion           | 6.05e-07   |
| reward_position         | 9.08e-05   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 8          |
|    time_elapsed         | 39         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04968562 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 62.2       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.368      |
|    value_loss           | 608        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 2.63e-05   |
| reward_motion           | 5.87e-07   |
| reward_position         | 8.8e-05    |
| reward_torque           | -2.66      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 618        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 9          |
|    time_elapsed         | 44         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.10000177 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 53.1       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.368      |
|    value_loss           | 664        |
----------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.54e-05    |
| reward_motion           | 5.8e-07     |
| reward_position         | 8.71e-05    |
| reward_torque           | -2.72       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 659         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 10          |
|    time_elapsed         | 49          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.045438074 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0477     |
|    std                  | 0.368       |
|    value_loss           | 853         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.87       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.56e-05    |
| reward_motion           | 5.89e-07    |
| reward_position         | 8.84e-05    |
| reward_torque           | -2.59       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 11          |
|    time_elapsed         | 54          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.087017946 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.4        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.368       |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 630.72 - Last mean reward per episode: 678.77
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.5e-05     |
| reward_motion           | 5.78e-07    |
| reward_position         | 8.67e-05    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.786       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 666         |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 12          |
|    time_elapsed         | 59          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.037214965 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.8       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0488     |
|    std                  | 0.368       |
|    value_loss           | 700         |
-----------------------------------------
----------------------------------------
| reward                  | -1.95      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.45e-05   |
| reward_motion           | 7.29e-07   |
| reward_position         | 0.000109   |
| reward_torque           | -2.69      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 13         |
|    time_elapsed         | 65         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.03277895 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 91.2       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0453    |
|    std                  | 0.368      |
|    value_loss           | 757        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.49e-05   |
| reward_motion           | 7.56e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -2.67      |
| reward_velocity         | 0.782      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 730        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 14         |
|    time_elapsed         | 70         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09893784 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14        |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 88         |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.368      |
|    value_loss           | 739        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 7.95e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 15          |
|    time_elapsed         | 75          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.059402347 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16         |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0687     |
|    std                  | 0.368       |
|    value_loss           | 501         |
-----------------------------------------
----------------------------------------
| reward                  | -1.92      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.68      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 772        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 16         |
|    time_elapsed         | 80         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05526676 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0708    |
|    std                  | 0.368      |
|    value_loss           | 819        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.65e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -2.68       |
| reward_velocity         | 0.803       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 792         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 17          |
|    time_elapsed         | 85          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.072550446 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0741     |
|    std                  | 0.368       |
|    value_loss           | 811         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 678.77 - Last mean reward per episode: 791.68
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.88       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.31e-05    |
| reward_motion           | 9e-07       |
| reward_position         | 0.000135    |
| reward_torque           | -2.64       |
| reward_velocity         | 0.809       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 18          |
|    time_elapsed         | 91          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.050225984 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 94          |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.368       |
|    value_loss           | 709         |
-----------------------------------------
----------------------------------------
| reward                  | -1.88      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.82       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 19         |
|    time_elapsed         | 96         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.04830476 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 161        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.368      |
|    value_loss           | 733        |
----------------------------------------
----------------------------------------
| reward                  | -1.89      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.814      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 20         |
|    time_elapsed         | 101        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.06929466 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.368      |
|    value_loss           | 760        |
----------------------------------------
-----------------------------------------
| reward                  | -1.91       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.11e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.67       |
| reward_velocity         | 0.808       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 21          |
|    time_elapsed         | 106         |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.062169574 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27e+03    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.368       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 3.99e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.806       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 771         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 22          |
|    time_elapsed         | 111         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.046546943 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.1       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 722         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0547     |
|    std                  | 0.368       |
|    value_loss           | 815         |
-----------------------------------------
----------------------------------------
| reward                  | -1.98      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.12e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.74      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 792        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 23         |
|    time_elapsed         | 116        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.03915607 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 146        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0404    |
|    std                  | 0.368      |
|    value_loss           | 648        |
----------------------------------------
Num timesteps: 24000
Best mean reward: 791.68 - Last mean reward per episode: 791.71
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2.04       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.804       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 765         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 24          |
|    time_elapsed         | 121         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.039605044 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 219         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0567     |
|    std                  | 0.368       |
|    value_loss           | 903         |
-----------------------------------------
---------------------------------------
| reward                  | -2.02     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 4.09e-05  |
| reward_motion           | 8.53e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.797     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 25        |
|    time_elapsed         | 126       |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.0371536 |
|    clip_fraction        | 0.0811    |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.7     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 354       |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0502   |
|    std                  | 0.368     |
|    value_loss           | 939       |
---------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.38e-07    |
| reward_position         | 0.000126    |
| reward_torque           | -2.74       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 768         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 26          |
|    time_elapsed         | 131         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.063086435 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.9        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0678     |
|    std                  | 0.368       |
|    value_loss           | 771         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.99       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.27e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.73       |
| reward_velocity         | 0.788       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 769         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 27          |
|    time_elapsed         | 136         |
|    total_timesteps      | 27648       |
| train/                  |             |
|    approx_kl            | 0.055129383 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1e+03     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0717     |
|    std                  | 0.368       |
|    value_loss           | 819         |
-----------------------------------------
---------------------------------------
| reward                  | -2.03     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.95e-05  |
| reward_motion           | 8.25e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.783     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 744       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 28        |
|    time_elapsed         | 141       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0534485 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.964     |
|    learning_rate        | 0.0003    |
|    loss                 | 203       |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.0704   |
|    std                  | 0.368     |
|    value_loss           | 804       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.01e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -2.81      |
| reward_velocity         | 0.786      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 723        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 29         |
|    time_elapsed         | 146        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.05150444 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27e+03   |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0435    |
|    std                  | 0.368      |
|    value_loss           | 829        |
----------------------------------------
Num timesteps: 30000
Best mean reward: 791.71 - Last mean reward per episode: 722.75
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -2.84      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 30         |
|    time_elapsed         | 151        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06510179 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35e+03   |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.368      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.55e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 31          |
|    time_elapsed         | 156         |
|    total_timesteps      | 31744       |
| train/                  |             |
|    approx_kl            | 0.051414184 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0474     |
|    std                  | 0.368       |
|    value_loss           | 672         |
-----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.57e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 32         |
|    time_elapsed         | 161        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07030794 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 680        |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0665    |
|    std                  | 0.368      |
|    value_loss           | 708        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.1e-05    |
| reward_motion           | 8.48e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 728        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 33         |
|    time_elapsed         | 166        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.07115512 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.368      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.23e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 735        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 34         |
|    time_elapsed         | 171        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05076515 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.367      |
|    value_loss           | 843        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.14e-05    |
| reward_motion           | 8.46e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.82       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 720         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 35          |
|    time_elapsed         | 177         |
|    total_timesteps      | 35840       |
| train/                  |             |
|    approx_kl            | 0.095722534 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0874     |
|    std                  | 0.367       |
|    value_loss           | 699         |
-----------------------------------------
Num timesteps: 36000
Best mean reward: 791.71 - Last mean reward per episode: 720.11
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 709         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 36          |
|    time_elapsed         | 182         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.056049634 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0406     |
|    std                  | 0.367       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 718         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 37          |
|    time_elapsed         | 187         |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.044259906 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 88.2        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0577     |
|    std                  | 0.367       |
|    value_loss           | 972         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 38         |
|    time_elapsed         | 192        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06741989 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 185        |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.5e-07    |
| reward_position         | 0.000143   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 720        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 39         |
|    time_elapsed         | 197        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.05378554 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 598        |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.056     |
|    std                  | 0.367      |
|    value_loss           | 841        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.42e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 706        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 40         |
|    time_elapsed         | 202        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.05185401 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0418    |
|    std                  | 0.367      |
|    value_loss           | 594        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 705        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 41         |
|    time_elapsed         | 207        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.08426472 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0562    |
|    std                  | 0.367      |
|    value_loss           | 873        |
----------------------------------------
Num timesteps: 42000
Best mean reward: 791.71 - Last mean reward per episode: 705.46
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 42         |
|    time_elapsed         | 212        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04333145 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0615    |
|    std                  | 0.367      |
|    value_loss           | 876        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 709        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 43         |
|    time_elapsed         | 217        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.06598474 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.2       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.367      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 706         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 44          |
|    time_elapsed         | 223         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.065347165 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.367       |
|    value_loss           | 765         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.59e-05    |
| reward_motion           | 9.43e-07    |
| reward_position         | 0.000141    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 703         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 45          |
|    time_elapsed         | 228         |
|    total_timesteps      | 46080       |
| train/                  |             |
|    approx_kl            | 0.054258935 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.3       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 200         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0616     |
|    std                  | 0.367       |
|    value_loss           | 722         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 717        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 46         |
|    time_elapsed         | 233        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06634159 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.367      |
|    value_loss           | 761        |
----------------------------------------
Num timesteps: 48000
Best mean reward: 791.71 - Last mean reward per episode: 716.92
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.87e-05   |
| reward_motion           | 9.85e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 712        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 47         |
|    time_elapsed         | 238        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.05172252 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.2      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.367      |
|    value_loss           | 678        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.97e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000153    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 713         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 48          |
|    time_elapsed         | 243         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.035609327 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0352     |
|    std                  | 0.367       |
|    value_loss           | 728         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.93e-05   |
| reward_motion           | 1.02e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 697        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 49         |
|    time_elapsed         | 248        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.05193235 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 342        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.367      |
|    value_loss           | 686        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.98e-05    |
| reward_motion           | 1.03e-06    |
| reward_position         | 0.000155    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 687         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 50          |
|    time_elapsed         | 253         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.053019665 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43e+03    |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0614     |
|    std                  | 0.367       |
|    value_loss           | 727         |
-----------------------------------------
---------------------------------------
| reward                  | -2.08     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.91e-05  |
| reward_motion           | 1.02e-06  |
| reward_position         | 0.000153  |
| reward_torque           | -2.8      |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 671       |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 51        |
|    time_elapsed         | 259       |
|    total_timesteps      | 52224     |
| train/                  |           |
|    approx_kl            | 0.0752333 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 125       |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0765   |
|    std                  | 0.367     |
|    value_loss           | 665       |
---------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.96e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000154    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 669         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 52          |
|    time_elapsed         | 264         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.042688258 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0545     |
|    std                  | 0.367       |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 54000
Best mean reward: 791.71 - Last mean reward per episode: 669.18
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.93e-05    |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000152    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 667         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 53          |
|    time_elapsed         | 269         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.032322936 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.04       |
|    std                  | 0.367       |
|    value_loss           | 814         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.88e-05   |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000151   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 673        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 54         |
|    time_elapsed         | 274        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06756666 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 138        |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.367      |
|    value_loss           | 715        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.84e-05    |
| reward_motion           | 9.95e-07    |
| reward_position         | 0.000149    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.756       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 676         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 55          |
|    time_elapsed         | 280         |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.044751495 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.367       |
|    value_loss           | 824         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.9e-05    |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 681        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 56         |
|    time_elapsed         | 285        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08345644 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 158        |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.367      |
|    value_loss           | 586        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.9e-05     |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000151    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 57          |
|    time_elapsed         | 290         |
|    total_timesteps      | 58368       |
| train/                  |             |
|    approx_kl            | 0.043336414 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.6        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0627     |
|    std                  | 0.367       |
|    value_loss           | 733         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.86e-05    |
| reward_motion           | 9.99e-07    |
| reward_position         | 0.00015     |
| reward_torque           | -2.77       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 672         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 58          |
|    time_elapsed         | 295         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.057172395 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 0.367       |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 60000
Best mean reward: 791.71 - Last mean reward per episode: 672.44
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.79e-05   |
| reward_motion           | 9.88e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 676        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 59         |
|    time_elapsed         | 300        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.07305443 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 160        |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0693    |
|    std                  | 0.367      |
|    value_loss           | 658        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.83e-05    |
| reward_motion           | 9.9e-07     |
| reward_position         | 0.000148    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 675         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 60          |
|    time_elapsed         | 305         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.048278354 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 562         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0548     |
|    std                  | 0.367       |
|    value_loss           | 745         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.78e-05   |
| reward_motion           | 9.8e-07    |
| reward_position         | 0.000147   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 672        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 61         |
|    time_elapsed         | 310        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.06619827 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 646        |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 901        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.69e-07   |
| reward_position         | 0.000145   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 666        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 62         |
|    time_elapsed         | 315        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.07789113 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0805    |
|    std                  | 0.367      |
|    value_loss           | 870        |
----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.68e-05    |
| reward_motion           | 9.63e-07    |
| reward_position         | 0.000144    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 664         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 63          |
|    time_elapsed         | 320         |
|    total_timesteps      | 64512       |
| train/                  |             |
|    approx_kl            | 0.051378116 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.8        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0606     |
|    std                  | 0.367       |
|    value_loss           | 646         |
-----------------------------------------
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.63e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 64         |
|    time_elapsed         | 325        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.07146649 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
Num timesteps: 66000
Best mean reward: 791.71 - Last mean reward per episode: 656.82
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.62e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 65         |
|    time_elapsed         | 329        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.06033282 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.9       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0711    |
|    std                  | 0.367      |
|    value_loss           | 651        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.54e-07    |
| reward_position         | 0.000143    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 657         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 66          |
|    time_elapsed         | 333         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.047285896 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 277         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.367       |
|    value_loss           | 624         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 9.48e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 67         |
|    time_elapsed         | 337        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.07419628 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 111        |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.367      |
|    value_loss           | 697        |
----------------------------------------
---------------------------------------
| reward                  | -2.07     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 9.41e-07  |
| reward_position         | 0.000141  |
| reward_torque           | -2.78     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 654       |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 68        |
|    time_elapsed         | 341       |
|    total_timesteps      | 69632     |
| train/                  |           |
|    approx_kl            | 0.0687863 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 348       |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0664   |
|    std                  | 0.367     |
|    value_loss           | 631       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 653        |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 69         |
|    time_elapsed         | 344        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.06904027 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.4       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.367      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.67e-05   |
| reward_motion           | 9.54e-07   |
| reward_position         | 0.000143   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 651        |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 70         |
|    time_elapsed         | 347        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.08205227 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 159        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.367      |
|    value_loss           | 587        |
----------------------------------------
Num timesteps: 72000
Best mean reward: 791.71 - Last mean reward per episode: 650.60
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 646         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 71          |
|    time_elapsed         | 351         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.059771188 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.367       |
|    value_loss           | 594         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 645        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 72         |
|    time_elapsed         | 355        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.04455894 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0585    |
|    std                  | 0.367      |
|    value_loss           | 499        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.53e-05    |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 73          |
|    time_elapsed         | 360         |
|    total_timesteps      | 74752       |
| train/                  |             |
|    approx_kl            | 0.051120788 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 659         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.055      |
|    std                  | 0.367       |
|    value_loss           | 591         |
-----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 642        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 74         |
|    time_elapsed         | 364        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.07270974 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 657        |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 553        |
----------------------------------------
----------------------------------------
| reward                  | -2.08      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.26e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 639        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 75         |
|    time_elapsed         | 368        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.08604793 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 810        |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0809    |
|    std                  | 0.367      |
|    value_loss           | 433        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.25e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 76         |
|    time_elapsed         | 373        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07724598 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 512        |
----------------------------------------
Num timesteps: 78000
Best mean reward: 791.71 - Last mean reward per episode: 640.18
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 77         |
|    time_elapsed         | 377        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.06728281 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 617        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.46e-05    |
| reward_motion           | 9.16e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 633         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 78          |
|    time_elapsed         | 381         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.052631572 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0682     |
|    std                  | 0.367       |
|    value_loss           | 420         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.65e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 638         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 79          |
|    time_elapsed         | 386         |
|    total_timesteps      | 80896       |
| train/                  |             |
|    approx_kl            | 0.091547176 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.4        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0856     |
|    std                  | 0.367       |
|    value_loss           | 432         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.47e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 636        |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 80         |
|    time_elapsed         | 390        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.07504402 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 103        |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.367      |
|    value_loss           | 492        |
----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.39e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 81         |
|    time_elapsed         | 394        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.05894314 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.3       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.367      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.34e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -2.82       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 82          |
|    time_elapsed         | 399         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.037166715 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0523     |
|    std                  | 0.367       |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 791.71 - Last mean reward per episode: 631.66
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 83         |
|    time_elapsed         | 403        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.12127335 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.3       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0993    |
|    std                  | 0.367      |
|    value_loss           | 340        |
----------------------------------------
---------------------------------------
| reward                  | -2.1      |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.52e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -2.82     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 634       |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 84        |
|    time_elapsed         | 408       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.0711868 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 228       |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.0729   |
|    std                  | 0.367     |
|    value_loss           | 634       |
---------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.51e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 631         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 85          |
|    time_elapsed         | 412         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.059059467 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 472         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0646     |
|    std                  | 0.367       |
|    value_loss           | 605         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.18e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 637        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 86         |
|    time_elapsed         | 416        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10163675 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 487        |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.367      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 9.15e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 87          |
|    time_elapsed         | 420         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.051552266 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0379     |
|    std                  | 0.367       |
|    value_loss           | 485         |
-----------------------------------------
Num timesteps: 90000
Best mean reward: 791.71 - Last mean reward per episode: 630.24
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 88         |
|    time_elapsed         | 425        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.04710511 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 201        |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0552    |
|    std                  | 0.367      |
|    value_loss           | 560        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.44e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 625        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 89         |
|    time_elapsed         | 429        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.05163174 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 78         |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 0.367      |
|    value_loss           | 513        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 621         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 90          |
|    time_elapsed         | 434         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.062439263 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 524         |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.048      |
|    std                  | 0.367       |
|    value_loss           | 502         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.43e-05   |
| reward_motion           | 9.06e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 91         |
|    time_elapsed         | 438        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.07532377 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.367      |
|    value_loss           | 565        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.06e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 611         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 92          |
|    time_elapsed         | 442         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.038080342 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.1        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0483     |
|    std                  | 0.367       |
|    value_loss           | 469         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 8.99e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 612         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 93          |
|    time_elapsed         | 447         |
|    total_timesteps      | 95232       |
| train/                  |             |
|    approx_kl            | 0.057437897 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.367       |
|    value_loss           | 542         |
-----------------------------------------
Num timesteps: 96000
Best mean reward: 791.71 - Last mean reward per episode: 612.12
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 94         |
|    time_elapsed         | 451        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.06996028 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 107        |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.367      |
|    value_loss           | 526        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.34e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 606         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 95          |
|    time_elapsed         | 456         |
|    total_timesteps      | 97280       |
| train/                  |             |
|    approx_kl            | 0.056031484 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.5       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.057      |
|    std                  | 0.367       |
|    value_loss           | 495         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 96         |
|    time_elapsed         | 460        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.07183945 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 175        |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0801    |
|    std                  | 0.367      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.97e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 603        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 97         |
|    time_elapsed         | 464        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.08660442 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0682    |
|    std                  | 0.367      |
|    value_loss           | 556        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.02e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 608         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 98          |
|    time_elapsed         | 469         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.036415048 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.367       |
|    value_loss           | 499         |
-----------------------------------------
---------------------------------------
| reward                  | -2.15     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 9.02e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -2.87     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 606       |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 99        |
|    time_elapsed         | 473       |
|    total_timesteps      | 101376    |
| train/                  |           |
|    approx_kl            | 0.0719479 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.8     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 133       |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0645   |
|    std                  | 0.367     |
|    value_loss           | 479       |
---------------------------------------
Num timesteps: 102000
Best mean reward: 791.71 - Last mean reward per episode: 605.58
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.98e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 602         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 100         |
|    time_elapsed         | 477         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.052123062 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 488         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0443     |
|    std                  | 0.367       |
|    value_loss           | 509         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.96e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 600         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 101         |
|    time_elapsed         | 482         |
|    total_timesteps      | 103424      |
| train/                  |             |
|    approx_kl            | 0.060172085 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0666     |
|    std                  | 0.367       |
|    value_loss           | 449         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 599        |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 102        |
|    time_elapsed         | 485        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07174431 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 79.1       |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.367      |
|    value_loss           | 373        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 595         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 103         |
|    time_elapsed         | 489         |
|    total_timesteps      | 105472      |
| train/                  |             |
|    approx_kl            | 0.061719283 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0671     |
|    std                  | 0.366       |
|    value_loss           | 384         |
-----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.2e-07    |
| reward_position         | 0.000138   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 588        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 104        |
|    time_elapsed         | 492        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.09381586 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.366      |
|    value_loss           | 346        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.24e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 588         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 105         |
|    time_elapsed         | 496         |
|    total_timesteps      | 107520      |
| train/                  |             |
|    approx_kl            | 0.056681145 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 169         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.366       |
|    value_loss           | 358         |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 791.71 - Last mean reward per episode: 587.83
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 597        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 106        |
|    time_elapsed         | 500        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.06581798 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0689    |
|    std                  | 0.366      |
|    value_loss           | 276        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 596        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 107        |
|    time_elapsed         | 504        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.09682576 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 391        |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0893    |
|    std                  | 0.366      |
|    value_loss           | 469        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 585        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 108        |
|    time_elapsed         | 508        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.08974074 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 234        |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.366      |
|    value_loss           | 507        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 583        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 109        |
|    time_elapsed         | 513        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.07364488 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 216        |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0762    |
|    std                  | 0.366      |
|    value_loss           | 431        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.5e-05     |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 571         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 110         |
|    time_elapsed         | 517         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.060291555 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0624     |
|    std                  | 0.366       |
|    value_loss           | 496         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.26e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 111         |
|    time_elapsed         | 521         |
|    total_timesteps      | 113664      |
| train/                  |             |
|    approx_kl            | 0.054138087 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0621     |
|    std                  | 0.366       |
|    value_loss           | 498         |
-----------------------------------------
Num timesteps: 114000
Best mean reward: 791.71 - Last mean reward per episode: 560.01
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 560        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 112        |
|    time_elapsed         | 526        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.06798768 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.366      |
|    value_loss           | 359        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.88       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 546         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 113         |
|    time_elapsed         | 530         |
|    total_timesteps      | 115712      |
| train/                  |             |
|    approx_kl            | 0.046133902 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 431         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.366       |
|    value_loss           | 599         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.18       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.08e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.89       |
| reward_velocity         | 0.757       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 537         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 114         |
|    time_elapsed         | 534         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.068029776 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0635     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 531        |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 115        |
|    time_elapsed         | 539        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.09138459 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0855    |
|    std                  | 0.366      |
|    value_loss           | 305        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.33e-05    |
| reward_motion           | 8.97e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.9        |
| reward_velocity         | 0.754       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 522         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 116         |
|    time_elapsed         | 542         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.084602855 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.5       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0706     |
|    std                  | 0.366       |
|    value_loss           | 458         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.3e-05    |
| reward_motion           | 8.93e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 517        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 117        |
|    time_elapsed         | 546        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.07564285 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 410        |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.366      |
|    value_loss           | 468        |
----------------------------------------
Num timesteps: 120000
Best mean reward: 791.71 - Last mean reward per episode: 516.74
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 505         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 118         |
|    time_elapsed         | 549         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.093632296 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.1        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0891     |
|    std                  | 0.366       |
|    value_loss           | 297         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.19e-05    |
| reward_motion           | 8.75e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 495         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 119         |
|    time_elapsed         | 553         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.042702064 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 156         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0479     |
|    std                  | 0.366       |
|    value_loss           | 528         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.17e-05    |
| reward_motion           | 8.74e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 496         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 120         |
|    time_elapsed         | 557         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.053215243 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 384         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 491         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 121         |
|    time_elapsed         | 561         |
|    total_timesteps      | 123904      |
| train/                  |             |
|    approx_kl            | 0.079917565 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.4        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0821     |
|    std                  | 0.366       |
|    value_loss           | 598         |
-----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.21e-05   |
| reward_motion           | 8.84e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 485        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 122        |
|    time_elapsed         | 565        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.09118663 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 265        |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.069     |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.85e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 477        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 123        |
|    time_elapsed         | 570        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.03758951 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0571    |
|    std                  | 0.366      |
|    value_loss           | 473        |
----------------------------------------
Num timesteps: 126000
Best mean reward: 791.71 - Last mean reward per episode: 476.83
---------------------------------------
| reward                  | -2.21     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.22e-05  |
| reward_motion           | 8.79e-07  |
| reward_position         | 0.000132  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.748     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 478       |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 124       |
|    time_elapsed         | 574       |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.0718819 |
|    clip_fraction        | 0.186     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 352       |
|    n_updates            | 2460      |
|    policy_gradient_loss | -0.069    |
|    std                  | 0.366     |
|    value_loss           | 473       |
---------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.95e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.751      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 475        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 125        |
|    time_elapsed         | 579        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.05870478 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 343        |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0582    |
|    std                  | 0.366      |
|    value_loss           | 510        |
----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.93      |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 467        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 126        |
|    time_elapsed         | 583        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.09303458 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0826    |
|    std                  | 0.366      |
|    value_loss           | 333        |
----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.27e-05    |
| reward_motion           | 8.94e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 457         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 127         |
|    time_elapsed         | 588         |
|    total_timesteps      | 130048      |
| train/                  |             |
|    approx_kl            | 0.075280145 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.064      |
|    std                  | 0.366       |
|    value_loss           | 423         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.25e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.752       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 466         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 128         |
|    time_elapsed         | 592         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.089382544 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.366       |
|    value_loss           | 334         |
-----------------------------------------
Num timesteps: 132000
Best mean reward: 791.71 - Last mean reward per episode: 466.43
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.753      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 469        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 129        |
|    time_elapsed         | 596        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.07682991 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 221        |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.366      |
|    value_loss           | 405        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 473        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 130        |
|    time_elapsed         | 601        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.07019861 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 169        |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.366      |
|    value_loss           | 317        |
----------------------------------------
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.2e-05    |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 464        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 131        |
|    time_elapsed         | 605        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.07635625 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.366      |
|    value_loss           | 448        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.22e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 452        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 132        |
|    time_elapsed         | 610        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.06735976 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.3       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0696    |
|    std                  | 0.366      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 448        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 133        |
|    time_elapsed         | 614        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.07325928 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0698    |
|    std                  | 0.366      |
|    value_loss           | 398        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 443        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 134        |
|    time_elapsed         | 618        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.07183257 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 311        |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
Num timesteps: 138000
Best mean reward: 791.71 - Last mean reward per episode: 443.06
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 444        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 135        |
|    time_elapsed         | 623        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.09175141 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 168        |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.366      |
|    value_loss           | 425        |
----------------------------------------
---------------------------------------
| reward                  | -2.18     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.26e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -2.89     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 445       |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 136       |
|    time_elapsed         | 626       |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.0662691 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 152       |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.0719   |
|    std                  | 0.366     |
|    value_loss           | 433       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.98e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 434        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 137        |
|    time_elapsed         | 630        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.05629784 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.8       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.366      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.13e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 430         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 138         |
|    time_elapsed         | 633         |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.063325554 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 362         |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0723     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
---------------------------------------
| reward                  | -2.19     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.98e-05  |
| reward_motion           | 8.44e-07  |
| reward_position         | 0.000127  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 424       |
| time/                   |           |
|    fps                  | 223       |
|    iterations           | 139       |
|    time_elapsed         | 636       |
|    total_timesteps      | 142336    |
| train/                  |           |
|    approx_kl            | 0.1082619 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 39.6      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.104    |
|    std                  | 0.366     |
|    value_loss           | 306       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.43e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.91      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 424        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 140        |
|    time_elapsed         | 640        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.07125464 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 132        |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.366      |
|    value_loss           | 340        |
----------------------------------------
Num timesteps: 144000
Best mean reward: 791.71 - Last mean reward per episode: 424.38
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.41e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 420        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 141        |
|    time_elapsed         | 644        |
|    total_timesteps      | 144384     |
| train/                  |            |
|    approx_kl            | 0.08280164 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 347        |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0521    |
|    std                  | 0.366      |
|    value_loss           | 378        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.47e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 415        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 142        |
|    time_elapsed         | 649        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.09441495 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 56.9       |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.366      |
|    value_loss           | 307        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.49e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 143         |
|    time_elapsed         | 654         |
|    total_timesteps      | 146432      |
| train/                  |             |
|    approx_kl            | 0.084700994 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 189         |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0941     |
|    std                  | 0.366       |
|    value_loss           | 435         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 144         |
|    time_elapsed         | 658         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.078906074 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0745     |
|    std                  | 0.366       |
|    value_loss           | 474         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.93e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 407         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 145         |
|    time_elapsed         | 663         |
|    total_timesteps      | 148480      |
| train/                  |             |
|    approx_kl            | 0.079785354 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0749     |
|    std                  | 0.366       |
|    value_loss           | 373         |
-----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 399        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 146        |
|    time_elapsed         | 667        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.06602247 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 511        |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.366      |
|    value_loss           | 481        |
----------------------------------------
Num timesteps: 150000
Best mean reward: 791.71 - Last mean reward per episode: 399.17
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 394         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 147         |
|    time_elapsed         | 672         |
|    total_timesteps      | 150528      |
| train/                  |             |
|    approx_kl            | 0.074376225 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0707     |
|    std                  | 0.366       |
|    value_loss           | 408         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.84e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 393         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 148         |
|    time_elapsed         | 676         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.064057305 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.0699     |
|    std                  | 0.366       |
|    value_loss           | 445         |
-----------------------------------------
---------------------------------------
| reward                  | -2.22     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.67e-05  |
| reward_motion           | 7.81e-07  |
| reward_position         | 0.000117  |
| reward_torque           | -2.94     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 393       |
| time/                   |           |
|    fps                  | 224       |
|    iterations           | 149       |
|    time_elapsed         | 680       |
|    total_timesteps      | 152576    |
| train/                  |           |
|    approx_kl            | 0.0948907 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.0826   |
|    std                  | 0.365     |
|    value_loss           | 441       |
---------------------------------------
-----------------------------------------
| reward                  | -2.25       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.67e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -2.96       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 390         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 150         |
|    time_elapsed         | 685         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.051997818 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0506     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.63e-05   |
| reward_motion           | 7.71e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 398        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 151        |
|    time_elapsed         | 689        |
|    total_timesteps      | 154624     |
| train/                  |            |
|    approx_kl            | 0.08013227 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.365      |
|    value_loss           | 373        |
----------------------------------------
----------------------------------------
| reward                  | -2.28      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.83e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 394        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 152        |
|    time_elapsed         | 693        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.05597806 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 156000
Best mean reward: 791.71 - Last mean reward per episode: 393.96
----------------------------------------
| reward                  | -2.27      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 386        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 153        |
|    time_elapsed         | 698        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.06945801 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 203        |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.365      |
|    value_loss           | 537        |
----------------------------------------
-----------------------------------------
| reward                  | -2.28       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.67e-05    |
| reward_motion           | 7.78e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3          |
| reward_velocity         | 0.765       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 378         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 154         |
|    time_elapsed         | 702         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.075821295 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0732     |
|    std                  | 0.365       |
|    value_loss           | 319         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.66e-05    |
| reward_motion           | 7.8e-07     |
| reward_position         | 0.000117    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 372         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 155         |
|    time_elapsed         | 707         |
|    total_timesteps      | 158720      |
| train/                  |             |
|    approx_kl            | 0.065256454 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.4        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.066      |
|    std                  | 0.365       |
|    value_loss           | 472         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.66e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 364         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 156         |
|    time_elapsed         | 711         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.059843626 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.0743     |
|    std                  | 0.365       |
|    value_loss           | 415         |
-----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 157        |
|    time_elapsed         | 716        |
|    total_timesteps      | 160768     |
| train/                  |            |
|    approx_kl            | 0.09143843 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 94.6       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.0821    |
|    std                  | 0.365      |
|    value_loss           | 311        |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.57e-05   |
| reward_motion           | 7.61e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.03      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 158        |
|    time_elapsed         | 720        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.09160215 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 162000
Best mean reward: 791.71 - Last mean reward per episode: 365.98
-----------------------------------------
| reward                  | -2.32       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.58e-05    |
| reward_motion           | 7.65e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.04       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 357         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 159         |
|    time_elapsed         | 725         |
|    total_timesteps      | 162816      |
| train/                  |             |
|    approx_kl            | 0.071382515 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 415         |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 0.365       |
|    value_loss           | 609         |
-----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.53e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.04      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 350        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 160        |
|    time_elapsed         | 729        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.07973179 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 217        |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.365      |
|    value_loss           | 483        |
----------------------------------------
-----------------------------------------
| reward                  | -2.34       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.64e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.05       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 161         |
|    time_elapsed         | 734         |
|    total_timesteps      | 164864      |
| train/                  |             |
|    approx_kl            | 0.062193085 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.8        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0636     |
|    std                  | 0.365       |
|    value_loss           | 465         |
-----------------------------------------
----------------------------------------
| reward                  | -2.35      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.67e-07   |
| reward_position         | 0.000115   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 344        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 162        |
|    time_elapsed         | 738        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.08855179 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 277        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.365      |
|    value_loss           | 418        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000114    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 342         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 163         |
|    time_elapsed         | 742         |
|    total_timesteps      | 166912      |
| train/                  |             |
|    approx_kl            | 0.090915054 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 447         |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.365       |
|    value_loss           | 441         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.53e-05    |
| reward_motion           | 7.55e-07    |
| reward_position         | 0.000113    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 164         |
|    time_elapsed         | 746         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.056085087 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 256         |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0711     |
|    std                  | 0.365       |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 168000
Best mean reward: 791.71 - Last mean reward per episode: 345.20
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.54e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 165        |
|    time_elapsed         | 751        |
|    total_timesteps      | 168960     |
| train/                  |            |
|    approx_kl            | 0.07051596 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.4       |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.365      |
|    value_loss           | 540        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.77e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 335        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 166        |
|    time_elapsed         | 755        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.07950926 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.2       |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 432        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 328        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 167        |
|    time_elapsed         | 759        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.08645288 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 350        |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.0779    |
|    std                  | 0.365      |
|    value_loss           | 459        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.87e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -3.08       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 326         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 168         |
|    time_elapsed         | 764         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.062871024 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 406         |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.0655     |
|    std                  | 0.365       |
|    value_loss           | 491         |
-----------------------------------------
----------------------------------------
| reward                  | -2.37      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.9e-07    |
| reward_position         | 0.000119   |
| reward_torque           | -3.08      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 169        |
|    time_elapsed         | 768        |
|    total_timesteps      | 173056     |
| train/                  |            |
|    approx_kl            | 0.07585441 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 227        |
|    n_updates            | 3360       |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.365      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 174000
Best mean reward: 791.71 - Last mean reward per episode: 318.08
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.61e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 170        |
|    time_elapsed         | 773        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.06525008 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0433    |
|    std                  | 0.365      |
|    value_loss           | 378        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.69e-05  |
| reward_motion           | 7.89e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.09     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 320       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 171       |
|    time_elapsed         | 777       |
|    total_timesteps      | 175104    |
| train/                  |           |
|    approx_kl            | 0.0965157 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 290       |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.0908   |
|    std                  | 0.365     |
|    value_loss           | 494       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.95e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 172        |
|    time_elapsed         | 781        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.06046278 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 172        |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 522        |
----------------------------------------
-----------------------------------------
| reward                  | -2.38       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.74e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.1        |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 317         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 173         |
|    time_elapsed         | 786         |
|    total_timesteps      | 177152      |
| train/                  |             |
|    approx_kl            | 0.076122925 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 284         |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.0637     |
|    std                  | 0.365       |
|    value_loss           | 425         |
-----------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.78e-05   |
| reward_motion           | 8.02e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 174        |
|    time_elapsed         | 790        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.08470478 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.4       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.365      |
|    value_loss           | 270        |
----------------------------------------
-----------------------------------------
| reward                  | -2.4        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.77e-05    |
| reward_motion           | 8.02e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.12       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 175         |
|    time_elapsed         | 795         |
|    total_timesteps      | 179200      |
| train/                  |             |
|    approx_kl            | 0.071893886 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0697     |
|    std                  | 0.365       |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 180000
Best mean reward: 791.71 - Last mean reward per episode: 310.27
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.75e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.12      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 300        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 176        |
|    time_elapsed         | 799        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.08430763 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 346        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.365      |
|    value_loss           | 462        |
----------------------------------------
-----------------------------------------
| reward                  | -2.41       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.73e-05    |
| reward_motion           | 7.94e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.13       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 299         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 177         |
|    time_elapsed         | 803         |
|    total_timesteps      | 181248      |
| train/                  |             |
|    approx_kl            | 0.082156904 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.1        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0705     |
|    std                  | 0.365       |
|    value_loss           | 398         |
-----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.96e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 291        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 178        |
|    time_elapsed         | 808        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.07415432 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 173        |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.365      |
|    value_loss           | 450        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.62e-05   |
| reward_motion           | 7.75e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 283        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 179        |
|    time_elapsed         | 812        |
|    total_timesteps      | 183296     |
| train/                  |            |
|    approx_kl            | 0.08652607 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.365      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.92e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 180        |
|    time_elapsed         | 816        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.09836302 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 271        |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.365      |
|    value_loss           | 437        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000122   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 281        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 181        |
|    time_elapsed         | 821        |
|    total_timesteps      | 185344     |
| train/                  |            |
|    approx_kl            | 0.10941856 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.365      |
|    value_loss           | 280        |
----------------------------------------
Num timesteps: 186000
Best mean reward: 791.71 - Last mean reward per episode: 281.13
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 273        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 182        |
|    time_elapsed         | 825        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.08115048 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.7       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.365      |
|    value_loss           | 420        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.18e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 183        |
|    time_elapsed         | 829        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.06963568 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.365      |
|    value_loss           | 281        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 262        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 184        |
|    time_elapsed         | 834        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.08650813 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.9       |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.365      |
|    value_loss           | 342        |
----------------------------------------
-----------------------------------------
| reward                  | -2.45       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.16       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 261         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 185         |
|    time_elapsed         | 838         |
|    total_timesteps      | 189440      |
| train/                  |             |
|    approx_kl            | 0.059413694 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.0513     |
|    std                  | 0.365       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 186        |
|    time_elapsed         | 842        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.08431653 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 143        |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 379        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.87e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 187        |
|    time_elapsed         | 847        |
|    total_timesteps      | 191488     |
| train/                  |            |
|    approx_kl            | 0.08574592 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.4       |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.0862    |
|    std                  | 0.365      |
|    value_loss           | 314        |
----------------------------------------
Num timesteps: 192000
Best mean reward: 791.71 - Last mean reward per episode: 255.42
-----------------------------------------
| reward                  | -2.46       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.86e-05    |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 188         |
|    time_elapsed         | 851         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.098878115 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.8        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.0883     |
|    std                  | 0.365       |
|    value_loss           | 382         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.47       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.88e-05    |
| reward_motion           | 8.23e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.19       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 245         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 189         |
|    time_elapsed         | 856         |
|    total_timesteps      | 193536      |
| train/                  |             |
|    approx_kl            | 0.031861797 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.0382     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 190        |
|    time_elapsed         | 860        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.05897156 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.365      |
|    value_loss           | 388        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.84e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 191        |
|    time_elapsed         | 865        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.09375177 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0871    |
|    std                  | 0.365      |
|    value_loss           | 395        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.2       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 192        |
|    time_elapsed         | 870        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.09083923 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 300        |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.365      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.95e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 241        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 193        |
|    time_elapsed         | 874        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.08830097 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 80.5       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.0854    |
|    std                  | 0.365      |
|    value_loss           | 393        |
----------------------------------------
Num timesteps: 198000
Best mean reward: 791.71 - Last mean reward per episode: 241.05
-----------------------------------------
| reward                  | -2.49       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.2        |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 238         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 194         |
|    time_elapsed         | 879         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.045449574 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 66          |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.0453     |
|    std                  | 0.365       |
|    value_loss           | 317         |
-----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.28e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 195        |
|    time_elapsed         | 884        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.09413753 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 332        |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.365      |
|    value_loss           | 472        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.22e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 196        |
|    time_elapsed         | 888        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.08775077 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.365      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.01e-05   |
| reward_motion           | 8.35e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 197        |
|    time_elapsed         | 893        |
|    total_timesteps      | 201728     |
| train/                  |            |
|    approx_kl            | 0.08543901 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0776    |
|    std                  | 0.365      |
|    value_loss           | 441        |
----------------------------------------
-----------------------------------------
| reward                  | -2.51       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.23       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 220         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 198         |
|    time_elapsed         | 898         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.101773694 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.093      |
|    std                  | 0.365       |
|    value_loss           | 291         |
-----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.21e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 219        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 199        |
|    time_elapsed         | 902        |
|    total_timesteps      | 203776     |
| train/                  |            |
|    approx_kl            | 0.08246793 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.6       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.0725    |
|    std                  | 0.365      |
|    value_loss           | 366        |
----------------------------------------
Num timesteps: 204000
Best mean reward: 791.71 - Last mean reward per episode: 218.92
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 220        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 200        |
|    time_elapsed         | 907        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.07519507 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.365      |
|    value_loss           | 446        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.29e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 217        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 201        |
|    time_elapsed         | 912        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.07404611 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.365      |
|    value_loss           | 417        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.93e-05  |
| reward_motion           | 8.14e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.24     |
| reward_velocity         | 0.764     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 215       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 202       |
|    time_elapsed         | 916       |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.0682548 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.7     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 275       |
|    n_updates            | 4020      |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.365     |
|    value_loss           | 386       |
---------------------------------------
-----------------------------------------
| reward                  | -2.53       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.85e-05    |
| reward_motion           | 7.96e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.25       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 215         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 203         |
|    time_elapsed         | 921         |
|    total_timesteps      | 207872      |
| train/                  |             |
|    approx_kl            | 0.058709525 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.0601     |
|    std                  | 0.365       |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.84e-05  |
| reward_motion           | 7.97e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.25     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 210       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 204       |
|    time_elapsed         | 925       |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.0821961 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 244       |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.365     |
|    value_loss           | 486       |
---------------------------------------
----------------------------------------
| reward                  | -2.54      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.00012    |
| reward_torque           | -3.26      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 205        |
|    time_elapsed         | 930        |
|    total_timesteps      | 209920     |
| train/                  |            |
|    approx_kl            | 0.09615535 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 118        |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.365      |
|    value_loss           | 424        |
----------------------------------------
Num timesteps: 210000
Best mean reward: 791.71 - Last mean reward per episode: 209.18
----------------------------------------
| reward                  | -2.55      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.04e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.27      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 200        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 206        |
|    time_elapsed         | 935        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.10412278 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.5       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.0897    |
|    std                  | 0.365      |
|    value_loss           | 422        |
----------------------------------------
-----------------------------------------
| reward                  | -2.57       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -3.29       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 197         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 207         |
|    time_elapsed         | 939         |
|    total_timesteps      | 211968      |
| train/                  |             |
|    approx_kl            | 0.078166276 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.0763     |
|    std                  | 0.365       |
|    value_loss           | 390         |
-----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.09e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.3       |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 199        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 208        |
|    time_elapsed         | 944        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.09233539 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.5       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.365      |
|    value_loss           | 307        |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 209        |
|    time_elapsed         | 948        |
|    total_timesteps      | 214016     |
| train/                  |            |
|    approx_kl            | 0.07892518 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.1       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 203        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 210        |
|    time_elapsed         | 952        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.09467095 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.0849    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
Num timesteps: 216000
Best mean reward: 791.71 - Last mean reward per episode: 203.38
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 211        |
|    time_elapsed         | 957        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.09579381 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 510        |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.365      |
|    value_loss           | 494        |
----------------------------------------
-----------------------------------------
| reward                  | -2.6        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.32       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 202         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 212         |
|    time_elapsed         | 961         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.118106246 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0861     |
|    std                  | 0.364       |
|    value_loss           | 381         |
-----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 213        |
|    time_elapsed         | 966        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.08759506 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 97.6       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.364      |
|    value_loss           | 310        |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.14e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 214        |
|    time_elapsed         | 970        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.08177506 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 250        |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.364      |
|    value_loss           | 445        |
----------------------------------------
---------------------------------------
| reward                  | -2.59     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.92e-05  |
| reward_motion           | 8.03e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.32     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 200       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 215       |
|    time_elapsed         | 975       |
|    total_timesteps      | 220160    |
| train/                  |           |
|    approx_kl            | 0.0946173 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | 43.8      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.0792   |
|    std                  | 0.364     |
|    value_loss           | 362       |
---------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000121   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 216        |
|    time_elapsed         | 979        |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.06670367 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 222000
Best mean reward: 791.71 - Last mean reward per episode: 197.53
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.1e-07     |
| reward_position         | 0.000122    |
| reward_torque           | -3.34       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 190         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 217         |
|    time_elapsed         | 984         |
|    total_timesteps      | 222208      |
| train/                  |             |
|    approx_kl            | 0.061054613 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.364       |
|    value_loss           | 424         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 190        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 218        |
|    time_elapsed         | 988        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.10701846 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.364      |
|    value_loss           | 423        |
----------------------------------------
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.98e-05    |
| reward_motion           | 8.14e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.35       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 219         |
|    time_elapsed         | 992         |
|    total_timesteps      | 224256      |
| train/                  |             |
|    approx_kl            | 0.083152995 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.364       |
|    value_loss           | 344         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.34      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 184        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 220        |
|    time_elapsed         | 997        |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.07975471 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.1       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 461        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.02e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 221        |
|    time_elapsed         | 1001       |
|    total_timesteps      | 226304     |
| train/                  |            |
|    approx_kl            | 0.06731045 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 434        |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.364      |
|    value_loss           | 550        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.38e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 222        |
|    time_elapsed         | 1005       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.06061294 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 151        |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 555        |
----------------------------------------
Num timesteps: 228000
Best mean reward: 791.71 - Last mean reward per episode: 181.53
-----------------------------------------
| reward                  | -2.64       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.36       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 223         |
|    time_elapsed         | 1010        |
|    total_timesteps      | 228352      |
| train/                  |             |
|    approx_kl            | 0.090596244 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0788     |
|    std                  | 0.364       |
|    value_loss           | 346         |
-----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.06e-05   |
| reward_motion           | 8.27e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.37      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 224        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.07922794 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 574        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 225        |
|    time_elapsed         | 1018       |
|    total_timesteps      | 230400     |
| train/                  |            |
|    approx_kl            | 0.08420156 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 226        |
|    time_elapsed         | 1023       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.11329693 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.0819    |
|    std                  | 0.364      |
|    value_loss           | 496        |
----------------------------------------
----------------------------------------
| reward                  | -2.66      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.39      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 176        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 227        |
|    time_elapsed         | 1027       |
|    total_timesteps      | 232448     |
| train/                  |            |
|    approx_kl            | 0.08218557 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.8       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.67      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.05e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.4       |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 168        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 228        |
|    time_elapsed         | 1031       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09102081 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0842    |
|    std                  | 0.364      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 234000
Best mean reward: 791.71 - Last mean reward per episode: 167.55
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 159        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 229        |
|    time_elapsed         | 1036       |
|    total_timesteps      | 234496     |
| train/                  |            |
|    approx_kl            | 0.06711652 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.364      |
|    value_loss           | 313        |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.03e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 230        |
|    time_elapsed         | 1040       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.08161515 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 247        |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.0668    |
|    std                  | 0.364      |
|    value_loss           | 621        |
----------------------------------------
---------------------------------------
| reward                  | -2.7      |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.96e-05  |
| reward_motion           | 8.01e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.42     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 155       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 231       |
|    time_elapsed         | 1044      |
|    total_timesteps      | 236544    |
| train/                  |           |
|    approx_kl            | 0.0906602 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.8     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 103       |
|    n_updates            | 4600      |
|    policy_gradient_loss | -0.0795   |
|    std                  | 0.364     |
|    value_loss           | 404       |
---------------------------------------
----------------------------------------
| reward                  | -2.71      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-05   |
| reward_motion           | 8.26e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.43      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 232        |
|    time_elapsed         | 1049       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.08881818 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 223        |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.364      |
|    value_loss           | 533        |
----------------------------------------
---------------------------------------
| reward                  | -2.72     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-05  |
| reward_motion           | 8.24e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -3.44     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 233       |
|    time_elapsed         | 1053      |
|    total_timesteps      | 238592    |
| train/                  |           |
|    approx_kl            | 0.0750297 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 224       |
|    n_updates            | 4640      |
|    policy_gradient_loss | -0.0521   |
|    std                  | 0.364     |
|    value_loss           | 468       |
---------------------------------------
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.07e-05   |
| reward_motion           | 8.23e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 141        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 234        |
|    time_elapsed         | 1057       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.07345635 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 215        |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.364      |
|    value_loss           | 490        |
----------------------------------------
Num timesteps: 240000
Best mean reward: 791.71 - Last mean reward per episode: 141.28
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.34e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 143        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 235        |
|    time_elapsed         | 1062       |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.06890334 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.4       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.0613    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.53e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 236        |
|    time_elapsed         | 1066       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.07793336 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.2       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.364      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.52e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 142        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 237        |
|    time_elapsed         | 1071       |
|    total_timesteps      | 242688     |
| train/                  |            |
|    approx_kl            | 0.08569087 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 81.3       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.364      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.76      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.55e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.48      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 238        |
|    time_elapsed         | 1075       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.09417659 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 378        |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.364      |
|    value_loss           | 548        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000129    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 239         |
|    time_elapsed         | 1079        |
|    total_timesteps      | 244736      |
| train/                  |             |
|    approx_kl            | 0.076811194 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.3        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.0673     |
|    std                  | 0.364       |
|    value_loss           | 519         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.49      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 240        |
|    time_elapsed         | 1084       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.06444687 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.364      |
|    value_loss           | 506        |
----------------------------------------
Num timesteps: 246000
Best mean reward: 791.71 - Last mean reward per episode: 127.14
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 123        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 241        |
|    time_elapsed         | 1088       |
|    total_timesteps      | 246784     |
| train/                  |            |
|    approx_kl            | 0.07253139 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 113        |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0679    |
|    std                  | 0.364      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.768       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 242         |
|    time_elapsed         | 1092        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.081109695 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.364       |
|    value_loss           | 514         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 243        |
|    time_elapsed         | 1097       |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.07144873 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.0479    |
|    std                  | 0.364      |
|    value_loss           | 353        |
----------------------------------------
---------------------------------------
| reward                  | -2.77     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.38e-05  |
| reward_motion           | 8.66e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.49     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 105       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 244       |
|    time_elapsed         | 1102      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0925653 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 230       |
|    n_updates            | 4860      |
|    policy_gradient_loss | -0.0768   |
|    std                  | 0.364     |
|    value_loss           | 403       |
---------------------------------------
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.36e-05  |
| reward_motion           | 8.63e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.5      |
| reward_velocity         | 0.769     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 99.4      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 245       |
|    time_elapsed         | 1106      |
|    total_timesteps      | 250880    |
| train/                  |           |
|    approx_kl            | 0.0684265 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.7     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 381       |
|    n_updates            | 4880      |
|    policy_gradient_loss | -0.0741   |
|    std                  | 0.364     |
|    value_loss           | 536       |
---------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.37e-05    |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -3.5        |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 97.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 246         |
|    time_elapsed         | 1111        |
|    total_timesteps      | 251904      |
| train/                  |             |
|    approx_kl            | 0.057845965 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0739     |
|    std                  | 0.364       |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 252000
Best mean reward: 791.71 - Last mean reward per episode: 97.37
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 247        |
|    time_elapsed         | 1116       |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.06736165 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 564        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 93.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 248        |
|    time_elapsed         | 1120       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.08807652 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 580        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 90.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 249        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 254976     |
| train/                  |            |
|    approx_kl            | 0.06194989 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 112        |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.364      |
|    value_loss           | 427        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 250        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.06726232 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.56e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 91.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 251        |
|    time_elapsed         | 1134       |
|    total_timesteps      | 257024     |
| train/                  |            |
|    approx_kl            | 0.09167318 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 295        |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.364      |
|    value_loss           | 558        |
----------------------------------------
Num timesteps: 258000
Best mean reward: 791.71 - Last mean reward per episode: 91.10
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.75e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 252        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.08047421 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 205        |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.364      |
|    value_loss           | 417        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.77e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 84.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 253        |
|    time_elapsed         | 1143       |
|    total_timesteps      | 259072     |
| train/                  |            |
|    approx_kl            | 0.06514632 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.364      |
|    value_loss           | 485        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 254        |
|    time_elapsed         | 1148       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.07208417 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 284        |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.0785    |
|    std                  | 0.364      |
|    value_loss           | 568        |
----------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.52e-05    |
| reward_motion           | 8.89e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.5        |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 78.3        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 255         |
|    time_elapsed         | 1152        |
|    total_timesteps      | 261120      |
| train/                  |             |
|    approx_kl            | 0.068733916 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.3        |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.364       |
|    value_loss           | 279         |
-----------------------------------------
---------------------------------------
| reward                  | -2.79     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.51     |
| reward_velocity         | 0.768     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 73.8      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 256       |
|    time_elapsed         | 1157      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0644877 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.5     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 312       |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.0623   |
|    std                  | 0.364     |
|    value_loss           | 532       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 69.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 257        |
|    time_elapsed         | 1162       |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.09269175 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 208        |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 264000
Best mean reward: 791.71 - Last mean reward per episode: 69.46
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.61e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 71         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 258        |
|    time_elapsed         | 1166       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.08261158 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 84.6       |
|    n_updates            | 5140       |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.364      |
|    value_loss           | 444        |
----------------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 73         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 259        |
|    time_elapsed         | 1171       |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.15402758 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 48         |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.364      |
|    value_loss           | 459        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 68.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 260        |
|    time_elapsed         | 1175       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.09338033 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 225        |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.363      |
|    value_loss           | 488        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 65.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 261        |
|    time_elapsed         | 1180       |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.07121593 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.363      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.14e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 63.4       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 262        |
|    time_elapsed         | 1185       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.08495103 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 5220       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.363      |
|    value_loss           | 465        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 66.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 263        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 269312     |
| train/                  |            |
|    approx_kl            | 0.11406556 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.363      |
|    value_loss           | 663        |
----------------------------------------
Num timesteps: 270000
Best mean reward: 791.71 - Last mean reward per episode: 66.95
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.69e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -3.5      |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 58.3      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 264       |
|    time_elapsed         | 1194      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0768143 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 58.3      |
|    n_updates            | 5260      |
|    policy_gradient_loss | -0.0761   |
|    std                  | 0.363     |
|    value_loss           | 505       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.21e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.52      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 59.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 265        |
|    time_elapsed         | 1199       |
|    total_timesteps      | 271360     |
| train/                  |            |
|    approx_kl            | 0.08713607 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.363      |
|    value_loss           | 467        |
----------------------------------------
-----------------------------------------
| reward                  | -2.8        |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.62e-05    |
| reward_motion           | 9.17e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -3.53       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 266         |
|    time_elapsed         | 1203        |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.088511154 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 327         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0809     |
|    std                  | 0.363       |
|    value_loss           | 423         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 67.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 267        |
|    time_elapsed         | 1208       |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.08538949 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.1       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.363      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 62.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 268        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.10191356 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 124        |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.0903    |
|    std                  | 0.363      |
|    value_loss           | 570        |
----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 64.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 269        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 275456     |
| train/                  |            |
|    approx_kl            | 0.06791703 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.0707    |
|    std                  | 0.363      |
|    value_loss           | 403        |
----------------------------------------
Num timesteps: 276000
Best mean reward: 791.71 - Last mean reward per episode: 64.63
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.23e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 56.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 270        |
|    time_elapsed         | 1221       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.09127189 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 48.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 271         |
|    time_elapsed         | 1225        |
|    total_timesteps      | 277504      |
| train/                  |             |
|    approx_kl            | 0.053911544 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.363       |
|    value_loss           | 436         |
-----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.12e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 45.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 272        |
|    time_elapsed         | 1230       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.08128683 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 72         |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.363      |
|    value_loss           | 369        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 41         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 273        |
|    time_elapsed         | 1234       |
|    total_timesteps      | 279552     |
| train/                  |            |
|    approx_kl            | 0.07408503 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 269        |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.363      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 34.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 274        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.06615147 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 213        |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 275        |
|    time_elapsed         | 1243       |
|    total_timesteps      | 281600     |
| train/                  |            |
|    approx_kl            | 0.07603255 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.363      |
|    value_loss           | 395        |
----------------------------------------
Num timesteps: 282000
Best mean reward: 791.71 - Last mean reward per episode: 36.90
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 35.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 276        |
|    time_elapsed         | 1247       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.08953834 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 277        |
|    time_elapsed         | 1251       |
|    total_timesteps      | 283648     |
| train/                  |            |
|    approx_kl            | 0.09077023 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 81.7       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 37.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 278         |
|    time_elapsed         | 1255        |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.098774806 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 441         |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.363       |
|    value_loss           | 492         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 29.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 279        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 285696     |
| train/                  |            |
|    approx_kl            | 0.10155187 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 123        |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.363      |
|    value_loss           | 481        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.82e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 280        |
|    time_elapsed         | 1264       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.07861796 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.363      |
|    value_loss           | 411        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.64e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.5        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 281         |
|    time_elapsed         | 1268        |
|    total_timesteps      | 287744      |
| train/                  |             |
|    approx_kl            | 0.121506184 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.363       |
|    value_loss           | 327         |
-----------------------------------------
Num timesteps: 288000
Best mean reward: 791.71 - Last mean reward per episode: 19.47
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.29e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 26.8       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 282        |
|    time_elapsed         | 1273       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.06990592 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0652    |
|    std                  | 0.363      |
|    value_loss           | 336        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.6e-07     |
| reward_position         | 0.000131    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.78        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 283         |
|    time_elapsed         | 1277        |
|    total_timesteps      | 289792      |
| train/                  |             |
|    approx_kl            | 0.100373924 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.099      |
|    std                  | 0.363       |
|    value_loss           | 367         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 284         |
|    time_elapsed         | 1281        |
|    total_timesteps      | 290816      |
| train/                  |             |
|    approx_kl            | 0.100580186 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.363       |
|    value_loss           | 416         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 285        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.09491367 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 177        |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.74e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 6.95       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 286        |
|    time_elapsed         | 1290       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.05984002 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 144        |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.0675    |
|    std                  | 0.363      |
|    value_loss           | 372        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 2.41       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 287        |
|    time_elapsed         | 1295       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.10393738 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.363      |
|    value_loss           | 446        |
----------------------------------------
Num timesteps: 294000
Best mean reward: 791.71 - Last mean reward per episode: 2.41
---------------------------------------
| reward                  | -2.83     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.41e-05  |
| reward_motion           | 8.84e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.56     |
| reward_velocity         | 0.777     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 4.24      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 288       |
|    time_elapsed         | 1299      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0791928 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 197       |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.0775   |
|    std                  | 0.363     |
|    value_loss           | 445       |
---------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 289        |
|    time_elapsed         | 1303       |
|    total_timesteps      | 295936     |
| train/                  |            |
|    approx_kl            | 0.11338747 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 102        |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0832    |
|    std                  | 0.363      |
|    value_loss           | 453        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 4.88        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 290         |
|    time_elapsed         | 1308        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.085854016 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0722     |
|    std                  | 0.363       |
|    value_loss           | 467         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.45e-05    |
| reward_motion           | 8.82e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 5.61        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 291         |
|    time_elapsed         | 1312        |
|    total_timesteps      | 297984      |
| train/                  |             |
|    approx_kl            | 0.122409046 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.0768     |
|    std                  | 0.363       |
|    value_loss           | 324         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.01e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.49       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 292         |
|    time_elapsed         | 1316        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.106792346 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.4        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.0909     |
|    std                  | 0.363       |
|    value_loss           | 537         |
-----------------------------------------
Num timesteps: 300000
Best mean reward: 791.71 - Last mean reward per episode: -4.49
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 8.85e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 293         |
|    time_elapsed         | 1321        |
|    total_timesteps      | 300032      |
| train/                  |             |
|    approx_kl            | 0.081342936 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 87.6        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.363       |
|    value_loss           | 442         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -5.69      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 294        |
|    time_elapsed         | 1325       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.08617334 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 110        |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 528        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.89e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -4.42      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 295        |
|    time_elapsed         | 1330       |
|    total_timesteps      | 302080     |
| train/                  |            |
|    approx_kl            | 0.05853964 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 181        |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.363      |
|    value_loss           | 525        |
----------------------------------------
----------------------------------------
| reward                  | -2.85      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.58      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -9.75      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 296        |
|    time_elapsed         | 1334       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.08167342 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.6       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.363      |
|    value_loss           | 267        |
----------------------------------------
-----------------------------------------
| reward                  | -2.85       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.68e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.24       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 297         |
|    time_elapsed         | 1339        |
|    total_timesteps      | 304128      |
| train/                  |             |
|    approx_kl            | 0.079787806 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.363       |
|    value_loss           | 554         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -8.79      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 298        |
|    time_elapsed         | 1343       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.10891131 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 320        |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0834    |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
Num timesteps: 306000
Best mean reward: 791.71 - Last mean reward per episode: -8.79
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.45e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -11.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 299        |
|    time_elapsed         | 1348       |
|    total_timesteps      | 306176     |
| train/                  |            |
|    approx_kl            | 0.09185974 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 243        |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.363      |
|    value_loss           | 389        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -17.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 300        |
|    time_elapsed         | 1352       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.07487345 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.363      |
|    value_loss           | 362        |
----------------------------------------
---------------------------------------
| reward                  | -2.85     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 8.75e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.58     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -22.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 301       |
|    time_elapsed         | 1357      |
|    total_timesteps      | 308224    |
| train/                  |           |
|    approx_kl            | 0.0672723 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.0003    |
|    loss                 | 94.2      |
|    n_updates            | 6000      |
|    policy_gradient_loss | -0.0552   |
|    std                  | 0.363     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.4e-05    |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -22.7      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 302        |
|    time_elapsed         | 1361       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.07330081 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.3       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.87       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.76e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -20.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 303         |
|    time_elapsed         | 1365        |
|    total_timesteps      | 310272      |
| train/                  |             |
|    approx_kl            | 0.060410976 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 290         |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0609     |
|    std                  | 0.363       |
|    value_loss           | 535         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -19        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 304        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.09256604 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0847    |
|    std                  | 0.363      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 312000
Best mean reward: 791.71 - Last mean reward per episode: -18.98
---------------------------------------
| reward                  | -2.86     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.42e-05  |
| reward_motion           | 8.81e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.59     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -28.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 305       |
|    time_elapsed         | 1374      |
|    total_timesteps      | 312320    |
| train/                  |           |
|    approx_kl            | 0.0849249 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 33.1      |
|    n_updates            | 6080      |
|    policy_gradient_loss | -0.0641   |
|    std                  | 0.362     |
|    value_loss           | 464       |
---------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -31.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 306         |
|    time_elapsed         | 1378        |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.056493938 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 223         |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.362       |
|    value_loss           | 455         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -29.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 307        |
|    time_elapsed         | 1383       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.09777212 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.0864    |
|    std                  | 0.362      |
|    value_loss           | 448        |
----------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 308         |
|    time_elapsed         | 1387        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.090832666 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 323         |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.0817     |
|    std                  | 0.362       |
|    value_loss           | 542         |
-----------------------------------------
---------------------------------------
| reward                  | -2.87     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.31e-05  |
| reward_motion           | 8.68e-07  |
| reward_position         | 0.000133  |
| reward_torque           | -3.6      |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -46.8     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 309       |
|    time_elapsed         | 1392      |
|    total_timesteps      | 316416    |
| train/                  |           |
|    approx_kl            | 0.0758764 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | 92.8      |
|    n_updates            | 6160      |
|    policy_gradient_loss | -0.0626   |
|    std                  | 0.362     |
|    value_loss           | 457       |
---------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.3e-05     |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.6        |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -49.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 310         |
|    time_elapsed         | 1396        |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.074833825 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.362       |
|    value_loss           | 283         |
-----------------------------------------
Num timesteps: 318000
Best mean reward: 791.71 - Last mean reward per episode: -49.09
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.6       |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -52.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 311        |
|    time_elapsed         | 1400       |
|    total_timesteps      | 318464     |
| train/                  |            |
|    approx_kl            | 0.07202639 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.362      |
|    value_loss           | 443        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -49.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 312        |
|    time_elapsed         | 1405       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.07316707 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 176        |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.362      |
|    value_loss           | 487        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 313        |
|    time_elapsed         | 1409       |
|    total_timesteps      | 320512     |
| train/                  |            |
|    approx_kl            | 0.08623876 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.6       |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 419        |
----------------------------------------
-----------------------------------------
| reward                  | -2.89       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 314         |
|    time_elapsed         | 1414        |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.054401636 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 6260        |
|    policy_gradient_loss | -0.0629     |
|    std                  | 0.362       |
|    value_loss           | 473         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.9        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.84e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.62       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 315         |
|    time_elapsed         | 1419        |
|    total_timesteps      | 322560      |
| train/                  |             |
|    approx_kl            | 0.097741075 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 174         |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.0815     |
|    std                  | 0.362       |
|    value_loss           | 487         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -60        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 316        |
|    time_elapsed         | 1423       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.08981447 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 178        |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.0756    |
|    std                  | 0.362      |
|    value_loss           | 421        |
----------------------------------------
Num timesteps: 324000
Best mean reward: 791.71 - Last mean reward per episode: -59.96
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -64.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 317        |
|    time_elapsed         | 1428       |
|    total_timesteps      | 324608     |
| train/                  |            |
|    approx_kl            | 0.07262496 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.362      |
|    value_loss           | 495        |
----------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -58.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 318        |
|    time_elapsed         | 1433       |
|    total_timesteps      | 325632     |
| train/                  |            |
|    approx_kl            | 0.08142903 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6340       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.362      |
|    value_loss           | 393        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.78e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 319         |
|    time_elapsed         | 1437        |
|    total_timesteps      | 326656      |
| train/                  |             |
|    approx_kl            | 0.057203952 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 149         |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.0562     |
|    std                  | 0.362       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -61.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 320        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.07968338 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.0626    |
|    std                  | 0.362      |
|    value_loss           | 355        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -63.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 321        |
|    time_elapsed         | 1447       |
|    total_timesteps      | 328704     |
| train/                  |            |
|    approx_kl            | 0.07895845 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.362      |
|    value_loss           | 415        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.2e-05     |
| reward_motion           | 8.5e-07     |
| reward_position         | 0.00013     |
| reward_torque           | -3.62       |
| reward_velocity         | 0.779       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -62.5       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 322         |
|    time_elapsed         | 1451        |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.082525745 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.1        |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.362       |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 330000
Best mean reward: 791.71 - Last mean reward per episode: -62.45
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.56e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -66.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 323        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.08526311 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.362      |
|    value_loss           | 503        |
----------------------------------------
---------------------------------------
| reward                  | -2.88     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.24e-05  |
| reward_motion           | 8.57e-07  |
| reward_position         | 0.000131  |
| reward_torque           | -3.61     |
| reward_velocity         | 0.776     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 324       |
|    time_elapsed         | 1460      |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0881749 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | 281       |
|    n_updates            | 6460      |
|    policy_gradient_loss | -0.0783   |
|    std                  | 0.362     |
|    value_loss           | 405       |
---------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 325        |
|    time_elapsed         | 1465       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.08563089 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.2       |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 0.362      |
|    value_loss           | 386        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 326        |
|    time_elapsed         | 1469       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.06256071 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 6500       |
|    policy_gradient_loss | -0.0566    |
|    std                  | 0.362      |
|    value_loss           | 392        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 327        |
|    time_elapsed         | 1473       |
|    total_timesteps      | 334848     |
| train/                  |            |
|    approx_kl            | 0.08924619 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.8       |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.362      |
|    value_loss           | 366        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 328        |
|    time_elapsed         | 1478       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.09193449 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 96.4       |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.0824    |
|    std                  | 0.362      |
|    value_loss           | 319        |
----------------------------------------
Num timesteps: 336000
Best mean reward: 791.71 - Last mean reward per episode: -74.94
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 329        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.08618444 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.5       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.362      |
|    value_loss           | 396        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -67.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 330        |
|    time_elapsed         | 1486       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.05716681 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 6580       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.362      |
|    value_loss           | 581        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -70.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 331        |
|    time_elapsed         | 1491       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.09605715 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 164        |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.0876    |
|    std                  | 0.362      |
|    value_loss           | 537        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.39e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 332        |
|    time_elapsed         | 1495       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.08115013 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.071     |
|    std                  | 0.362      |
|    value_loss           | 578        |
----------------------------------------
---------------------------------------
| reward                  | -2.91     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.17e-05  |
| reward_motion           | 8.39e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -3.63     |
| reward_velocity         | 0.767     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -70.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 333       |
|    time_elapsed         | 1500      |
|    total_timesteps      | 340992    |
| train/                  |           |
|    approx_kl            | 0.0719579 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 6640      |
|    policy_gradient_loss | -0.0657   |
|    std                  | 0.362     |
|    value_loss           | 449       |
---------------------------------------
Num timesteps: 342000
Best mean reward: 791.71 - Last mean reward per episode: -70.20
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.44e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 334        |
|    time_elapsed         | 1504       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.07477051 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.0692    |
|    std                  | 0.362      |
|    value_loss           | 357        |
----------------------------------------
-----------------------------------------
| reward                  | -2.91       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.05e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -3.63       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -82.2       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 335         |
|    time_elapsed         | 1509        |
|    total_timesteps      | 343040      |
| train/                  |             |
|    approx_kl            | 0.038849957 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.0445     |
|    std                  | 0.362       |
|    value_loss           | 342         |
-----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -83.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 336        |
|    time_elapsed         | 1513       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.08295447 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 358        |
|    n_updates            | 6700       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.362      |
|    value_loss           | 511        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 337        |
|    time_elapsed         | 1518       |
|    total_timesteps      | 345088     |
| train/                  |            |
|    approx_kl            | 0.10497835 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 6720       |
|    policy_gradient_loss | -0.0865    |
|    std                  | 0.362      |
|    value_loss           | 490        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -89.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 338        |
|    time_elapsed         | 1522       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.11712283 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.1       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -81.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 339        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.10852885 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.362      |
|    value_loss           | 504        |
----------------------------------------
Num timesteps: 348000
Best mean reward: 791.71 - Last mean reward per episode: -81.56
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 7.89e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -75.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 340        |
|    time_elapsed         | 1531       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.06358122 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 376        |
|    n_updates            | 6780       |
|    policy_gradient_loss | -0.0604    |
|    std                  | 0.362      |
|    value_loss           | 440        |
----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 341        |
|    time_elapsed         | 1536       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.08850199 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.362      |
|    value_loss           | 465        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.9e-05     |
| reward_motion           | 7.97e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -74.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 342         |
|    time_elapsed         | 1540        |
|    total_timesteps      | 350208      |
| train/                  |             |
|    approx_kl            | 0.098224916 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 6820        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.362       |
|    value_loss           | 438         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -77.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 343        |
|    time_elapsed         | 1544       |
|    total_timesteps      | 351232     |
| train/                  |            |
|    approx_kl            | 0.10913843 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 98         |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.0914    |
|    std                  | 0.362      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -75.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 344         |
|    time_elapsed         | 1549        |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.086836606 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 214         |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.362       |
|    value_loss           | 413         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.99e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -69.6       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 345         |
|    time_elapsed         | 1553        |
|    total_timesteps      | 353280      |
| train/                  |             |
|    approx_kl            | 0.060340073 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.9        |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.0478     |
|    std                  | 0.362       |
|    value_loss           | 399         |
-----------------------------------------
Num timesteps: 354000
Best mean reward: 791.71 - Last mean reward per episode: -69.57
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.94e-05    |
| reward_motion           | 8.01e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -76.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 346         |
|    time_elapsed         | 1557        |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.095809594 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 6900        |
|    policy_gradient_loss | -0.0825     |
|    std                  | 0.362       |
|    value_loss           | 411         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.66       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -77.8       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 347         |
|    time_elapsed         | 1562        |
|    total_timesteps      | 355328      |
| train/                  |             |
|    approx_kl            | 0.097013265 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 6920        |
|    policy_gradient_loss | -0.0865     |
|    std                  | 0.362       |
|    value_loss           | 374         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 348        |
|    time_elapsed         | 1566       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.06848779 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 94.4       |
|    n_updates            | 6940       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.362      |
|    value_loss           | 434        |
----------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.97e-05  |
| reward_motion           | 8.1e-07   |
| reward_position         | 0.000124  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -85       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 349       |
|    time_elapsed         | 1571      |
|    total_timesteps      | 357376    |
| train/                  |           |
|    approx_kl            | 0.0612828 |
|    clip_fraction        | 0.178     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 118       |
|    n_updates            | 6960      |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.362     |
|    value_loss           | 382       |
---------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.9e-05   |
| reward_motion           | 7.98e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.775     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -90.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 350       |
|    time_elapsed         | 1575      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0758909 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 124       |
|    n_updates            | 6980      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.362     |
|    value_loss           | 488       |
---------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.8e-05     |
| reward_motion           | 7.85e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.66       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -94.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 351         |
|    time_elapsed         | 1579        |
|    total_timesteps      | 359424      |
| train/                  |             |
|    approx_kl            | 0.101735264 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 7000        |
|    policy_gradient_loss | -0.0952     |
|    std                  | 0.362       |
|    value_loss           | 491         |
-----------------------------------------
Num timesteps: 360000
Best mean reward: 791.71 - Last mean reward per episode: -94.37
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.77e-05  |
| reward_motion           | 7.8e-07   |
| reward_position         | 0.000119  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -89.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 352       |
|    time_elapsed         | 1584      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.1101183 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 89.5      |
|    n_updates            | 7020      |
|    policy_gradient_loss | -0.0816   |
|    std                  | 0.362     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.77e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -91.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 353        |
|    time_elapsed         | 1588       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.07463523 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.8       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.362      |
|    value_loss           | 331        |
----------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.66e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.783      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 354        |
|    time_elapsed         | 1593       |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.09484478 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 41.7       |
|    n_updates            | 7060       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.69e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -96        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 355        |
|    time_elapsed         | 1598       |
|    total_timesteps      | 363520     |
| train/                  |            |
|    approx_kl            | 0.07948056 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 230        |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.362      |
|    value_loss           | 342        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.68e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 356        |
|    time_elapsed         | 1602       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.09544092 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.2       |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.362      |
|    value_loss           | 335        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -97.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 357        |
|    time_elapsed         | 1607       |
|    total_timesteps      | 365568     |
| train/                  |            |
|    approx_kl            | 0.07863753 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.6       |
|    n_updates            | 7120       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.362      |
|    value_loss           | 478        |
----------------------------------------
Num timesteps: 366000
Best mean reward: 791.71 - Last mean reward per episode: -97.89
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.57e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.68      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 358        |
|    time_elapsed         | 1611       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.09423084 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 226        |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.0858    |
|    std                  | 0.362      |
|    value_loss           | 437        |
----------------------------------------
---------------------------------------
| reward                  | -2.94     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.63e-05  |
| reward_motion           | 7.54e-07  |
| reward_position         | 0.000116  |
| reward_torque           | -3.67     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 359       |
|    time_elapsed         | 1616      |
|    total_timesteps      | 367616    |
| train/                  |           |
|    approx_kl            | 0.1212948 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 97.7      |
|    n_updates            | 7160      |
|    policy_gradient_loss | -0.0898   |
|    std                  | 0.362     |
|    value_loss           | 416       |
---------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.65e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -99        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 360        |
|    time_elapsed         | 1620       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.10194968 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0818    |
|    std                  | 0.362      |
|    value_loss           | 470        |
----------------------------------------
-----------------------------------------
| reward                  | -2.95       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.7e-05     |
| reward_motion           | 7.61e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.781       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 361         |
|    time_elapsed         | 1624        |
|    total_timesteps      | 369664      |
| train/                  |             |
|    approx_kl            | 0.096999794 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 7200        |
|    policy_gradient_loss | -0.0912     |
|    std                  | 0.362       |
|    value_loss           | 318         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.94       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.53e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 362         |
|    time_elapsed         | 1629        |
|    total_timesteps      | 370688      |
| train/                  |             |
|    approx_kl            | 0.081590526 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 51          |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.362       |
|    value_loss           | 373         |
-----------------------------------------
---------------------------------------
| reward                  | -2.95     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.76e-05  |
| reward_motion           | 7.68e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.68     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 363       |
|    time_elapsed         | 1633      |
|    total_timesteps      | 371712    |
| train/                  |           |
|    approx_kl            | 0.0668627 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 44.1      |
|    n_updates            | 7240      |
|    policy_gradient_loss | -0.0582   |
|    std                  | 0.362     |
|    value_loss           | 240       |
---------------------------------------
Num timesteps: 372000
Best mean reward: 791.71 - Last mean reward per episode: -112.40
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.72e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 364         |
|    time_elapsed         | 1637        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.094104856 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 7260        |
|    policy_gradient_loss | -0.0816     |
|    std                  | 0.361       |
|    value_loss           | 385         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.59e-07    |
| reward_position         | 0.000116    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 365         |
|    time_elapsed         | 1642        |
|    total_timesteps      | 373760      |
| train/                  |             |
|    approx_kl            | 0.049591344 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.0509     |
|    std                  | 0.361       |
|    value_loss           | 347         |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Traceback (most recent call last):
  File "ddpg.py", line 224, in <module>
    'info_keywords' : info_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 977, in __init__
    super(AntEnvV8, self).__init__('ant.xml')
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 467, in __init__
    super(AntEnvV4, self).__init__(path)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 84, in __init__
    mujoco_env.MujocoEnv.__init__(self, path, 5)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py", line 64, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 1086, in step
    reward = info['reward_ctrl'] + info['reward_torque'] + info['reward_motion'] + info['reward_position'] + info['reward_velocity'] + info['reward_contact']
KeyError: 'reward_position'
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_4
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    fps             | 501       |
|    iterations      | 1         |
|    time_elapsed    | 2         |
|    total_timesteps | 1024      |
----------------------------------
----------------------------------------
| reward                  | -1.59      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 1.48e-06   |
| reward_motion           | 3.8e-07    |
| reward_torque           | -2.33      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -1.68e+03  |
| time/                   |            |
|    fps                  | 408        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.36854783 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.6      |
|    explained_variance   | -0.00194   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.364      |
|    value_loss           | 27.3       |
----------------------------------------
---------------------------------------
| reward                  | -1.72     |
| reward_contact          | -0.0474   |
| reward_ctrl             | 5.05e-06  |
| reward_motion           | 1.15e-06  |
| reward_torque           | -2.54     |
| reward_velocity         | 0.869     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -1.97e+03 |
| time/                   |           |
|    fps                  | 384       |
|    iterations           | 3         |
|    time_elapsed         | 7         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.3725308 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.8     |
|    explained_variance   | 0.0732    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0945   |
|    std                  | 0.362     |
|    value_loss           | 27.4      |
---------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.95e-06  |
| reward_motion           | 9.14e-07  |
| reward_torque           | -3.03     |
| reward_velocity         | 0.814     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.03e+03 |
| time/                   |           |
|    fps                  | 373       |
|    iterations           | 4         |
|    time_elapsed         | 10        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.133129  |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19       |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.01      |
|    n_updates            | 60        |
|    policy_gradient_loss | -2.98e-05 |
|    std                  | 0.362     |
|    value_loss           | 126       |
---------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.5e-06    |
| reward_motion           | 8.15e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.839      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.06e+03  |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 5          |
|    time_elapsed         | 13         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.20465791 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.361      |
|    value_loss           | 66.6       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -2059.00
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.93e-06   |
| reward_motion           | 1.06e-06   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.849      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.13e+03  |
| time/                   |            |
|    fps                  | 362        |
|    iterations           | 6          |
|    time_elapsed         | 16         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.11940461 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.92       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0845    |
|    std                  | 0.36       |
|    value_loss           | 29.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.4e-06    |
| reward_motion           | 9.54e-07   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.837      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 359        |
|    iterations           | 7          |
|    time_elapsed         | 19         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09918121 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.359      |
|    value_loss           | 40.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.0466    |
| reward_ctrl             | 3.99e-06   |
| reward_motion           | 8.61e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 357        |
|    iterations           | 8          |
|    time_elapsed         | 22         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04906732 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | -1.03      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0508    |
|    std                  | 0.359      |
|    value_loss           | 50.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0465    |
| reward_ctrl             | 3.78e-06   |
| reward_motion           | 8.16e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.815      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 356        |
|    iterations           | 9          |
|    time_elapsed         | 25         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.09477489 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.31       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0674    |
|    std                  | 0.358      |
|    value_loss           | 39.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 3.46e-06   |
| reward_motion           | 7.53e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.18e+03  |
| time/                   |            |
|    fps                  | 354        |
|    iterations           | 10         |
|    time_elapsed         | 28         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.64199543 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.0344     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0534    |
|    std                  | 0.357      |
|    value_loss           | 47.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.24      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.2e-06    |
| reward_motion           | 7.02e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 11         |
|    time_elapsed         | 31         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.22036704 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.164      |
|    learning_rate        | 0.0003     |
|    loss                 | 35         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.355      |
|    value_loss           | 61.6       |
----------------------------------------
Num timesteps: 12000
Best mean reward: -2059.00 - Last mean reward per episode: -2203.10
----------------------------------------
| reward                  | -2.34      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.88e-07   |
| reward_torque           | -3.11      |
| reward_velocity         | 0.812      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 12         |
|    time_elapsed         | 34         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16806449 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.0587     |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.355      |
|    value_loss           | 56.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.33      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.13e-06   |
| reward_motion           | 7.12e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 352        |
|    iterations           | 13         |
|    time_elapsed         | 37         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.22779572 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | -0.00966   |
|    learning_rate        | 0.0003     |
|    loss                 | 30         |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.355      |
|    value_loss           | 70.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.98e-06   |
| reward_motion           | 6.79e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.813      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 14         |
|    time_elapsed         | 40         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.25915498 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.0405     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.39       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.043     |
|    std                  | 0.354      |
|    value_loss           | 75.4       |
----------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.97e-06  |
| reward_motion           | 6.71e-07  |
| reward_torque           | -3.04     |
| reward_velocity         | 0.82      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.2e+03  |
| time/                   |           |
|    fps                  | 351       |
|    iterations           | 15        |
|    time_elapsed         | 43        |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.2859522 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.6     |
|    explained_variance   | -0.0266   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0304   |
|    std                  | 0.353     |
|    value_loss           | 74.5      |
---------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.85e-06   |
| reward_motion           | 6.45e-07   |
| reward_torque           | -3.05      |
| reward_velocity         | 0.809      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 16         |
|    time_elapsed         | 46         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.18760654 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | -0.237     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.8       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.352      |
|    value_loss           | 92.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.8e-06    |
| reward_motion           | 6.31e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 17         |
|    time_elapsed         | 49         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.29463354 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.0503     |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.352      |
|    value_loss           | 102        |
----------------------------------------
Num timesteps: 18000
Best mean reward: -2059.00 - Last mean reward per episode: -2224.99
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.81e-06   |
| reward_motion           | 6.25e-07   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 18         |
|    time_elapsed         | 52         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.36696294 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.032      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.1       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.351      |
|    value_loss           | 107        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.16e-06  |
| reward_motion           | 6.89e-07  |
| reward_torque           | -3.12     |
| reward_velocity         | 0.8       |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.25e+03 |
| time/                   |           |
|    fps                  | 349       |
|    iterations           | 19        |
|    time_elapsed         | 55        |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.0930747 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | -0.0214   |
|    learning_rate        | 0.0003    |
|    loss                 | 75.1      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0604   |
|    std                  | 0.351     |
|    value_loss           | 114       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.11e-06   |
| reward_motion           | 6.82e-07   |
| reward_torque           | -3.13      |
| reward_velocity         | 0.794      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.26e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 20         |
|    time_elapsed         | 58         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.21897617 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.0717     |
|    learning_rate        | 0.0003     |
|    loss                 | 145        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.35       |
|    value_loss           | 123        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.01e-06   |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.796      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.28e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 21         |
|    time_elapsed         | 61         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.44544753 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.0266     |
|    learning_rate        | 0.0003     |
|    loss                 | 75.7       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0516    |
|    std                  | 0.349      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.05e-06   |
| reward_motion           | 6.65e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.792      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.3e+03   |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 22         |
|    time_elapsed         | 64         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.09976232 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0835     |
|    learning_rate        | 0.0003     |
|    loss                 | 25.2       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.348      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.5e-07    |
| reward_torque           | -3.17      |
| reward_velocity         | 0.791      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.31e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 23         |
|    time_elapsed         | 67         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.23673081 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0369     |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0577    |
|    std                  | 0.348      |
|    value_loss           | 152        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -2059.00 - Last mean reward per episode: -2307.34
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 2.98e-06  |
| reward_motion           | 6.58e-07  |
| reward_torque           | -3.2      |
| reward_velocity         | 0.785     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.32e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 24        |
|    time_elapsed         | 70        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.8325364 |
|    clip_fraction        | 0.595     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0374   |
|    std                  | 0.347     |
|    value_loss           | 157       |
---------------------------------------
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.95e-06  |
| reward_motion           | 6.54e-07  |
| reward_torque           | -3.19     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.34e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 25        |
|    time_elapsed         | 73        |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.169721  |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.035     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.3      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0566   |
|    std                  | 0.347     |
|    value_loss           | 166       |
---------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.95e-06   |
| reward_motion           | 6.46e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.34e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 26         |
|    time_elapsed         | 76         |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.18979153 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.0987     |
|    learning_rate        | 0.0003     |
|    loss                 | 37.5       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.347      |
|    value_loss           | 120        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.49e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.35e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 27         |
|    time_elapsed         | 79         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.11660235 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | -0.46      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.9       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0506    |
|    std                  | 0.346      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.36e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 28         |
|    time_elapsed         | 82         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11507568 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.0308     |
|    learning_rate        | 0.0003     |
|    loss                 | 28.1       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.346      |
|    value_loss           | 185        |
----------------------------------------
-----------------------------------------
| reward                  | -2.44       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.24e-06    |
| reward_motion           | 6.88e-07    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -2.38e+03   |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 29          |
|    time_elapsed         | 85          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.068516396 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.0659      |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.345       |
|    value_loss           | 198         |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -2059.00 - Last mean reward per episode: -2376.28
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.33e-06   |
| reward_motion           | 7.15e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 30         |
|    time_elapsed         | 88         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.11165166 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0535     |
|    learning_rate        | 0.0003     |
|    loss                 | 86.8       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0514    |
|    std                  | 0.345      |
|    value_loss           | 195        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.75e-06   |
| reward_motion           | 7.84e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 31         |
|    time_elapsed         | 91         |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.16336218 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.0889     |
|    learning_rate        | 0.0003     |
|    loss                 | 182        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.345      |
|    value_loss           | 194        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.74e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.39e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 32         |
|    time_elapsed         | 94         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.20484138 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.0319     |
|    learning_rate        | 0.0003     |
|    loss                 | 41.6       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.344      |
|    value_loss           | 210        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.87e-06   |
| reward_motion           | 8.28e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 33         |
|    time_elapsed         | 97         |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.38690743 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.0346     |
|    learning_rate        | 0.0003     |
|    loss                 | 150        |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0492    |
|    std                  | 0.344      |
|    value_loss           | 228        |
----------------------------------------
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.79e-06   |
| reward_motion           | 8.12e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 34         |
|    time_elapsed         | 100        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.11785532 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0255     |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.71e-06   |
| reward_motion           | 7.97e-07   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 35         |
|    time_elapsed         | 103        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.13725826 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.0618     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0687    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
Num timesteps: 36000
Best mean reward: -2059.00 - Last mean reward per episode: -2416.80
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.64e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 36         |
|    time_elapsed         | 106        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.12666398 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.0903     |
|    learning_rate        | 0.0003     |
|    loss                 | 40         |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0596    |
|    std                  | 0.344      |
|    value_loss           | 225        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-06   |
| reward_motion           | 8.75e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.43e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 37         |
|    time_elapsed         | 109        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.19215783 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0698     |
|    learning_rate        | 0.0003     |
|    loss                 | 104        |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.344      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.1e-06    |
| reward_motion           | 8.6e-07    |
| reward_torque           | -3.22      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 38         |
|    time_elapsed         | 112        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.13736585 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0917     |
|    learning_rate        | 0.0003     |
|    loss                 | 84.8       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.343      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.66e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 39         |
|    time_elapsed         | 115        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.37177145 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0291     |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.343      |
|    value_loss           | 244        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-06  |
| reward_motion           | 8.59e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.44e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 40        |
|    time_elapsed         | 118       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.2808153 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.05      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.063    |
|    std                  | 0.343     |
|    value_loss           | 236       |
---------------------------------------
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.64e-07   |
| reward_torque           | -3.26      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 41         |
|    time_elapsed         | 121        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.17382684 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.0568     |
|    learning_rate        | 0.0003     |
|    loss                 | 131        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0536    |
|    std                  | 0.343      |
|    value_loss           | 235        |
----------------------------------------
Num timesteps: 42000
Best mean reward: -2059.00 - Last mean reward per episode: -2441.78
---------------------------------------
| reward                  | -2.52     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.04e-06  |
| reward_motion           | 8.57e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.45e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 42        |
|    time_elapsed         | 124       |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.1686695 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.6     |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.0672   |
|    std                  | 0.342     |
|    value_loss           | 233       |
---------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-06   |
| reward_motion           | 8.45e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.46e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 43         |
|    time_elapsed         | 127        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.18749896 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0552     |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.342      |
|    value_loss           | 236        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_5
---------------------------------
| reward             | 9.74     |
| reward_contact     | 0        |
| reward_ctrl        | 10.5     |
| reward_motion      | 2.47     |
| reward_torque      | -3.27    |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 468      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | 12.7       |
| reward_contact          | 0          |
| reward_ctrl             | 13.5       |
| reward_motion           | 2.5        |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0349     |
| rollout/                |            |
|    ep_len_mean          | 86.9       |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 382        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.08909861 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.0042    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6e+03    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.368      |
|    value_loss           | 5.3e+03    |
----------------------------------------
----------------------------------------
| reward                  | 12.1       |
| reward_contact          | -0.000819  |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0387     |
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 987        |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.08520674 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 813        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.368      |
|    value_loss           | 3.24e+03   |
----------------------------------------
----------------------------------------
| reward                  | 11.9       |
| reward_contact          | -0.0012    |
| reward_ctrl             | 12.9       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0384     |
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 1.05e+03   |
| time/                   |            |
|    fps                  | 335        |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.06514679 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.0102     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0947    |
|    std                  | 0.368      |
|    value_loss           | 3.44e+03   |
----------------------------------------
----------------------------------------
| reward                  | 12         |
| reward_contact          | -0.00108   |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.38       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0377     |
| rollout/                |            |
|    ep_len_mean          | 127        |
|    ep_rew_mean          | 1.21e+03   |
| time/                   |            |
|    fps                  | 325        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07207671 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.34e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.368      |
|    value_loss           | 3.91e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 878.37
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 11.8        |
| reward_contact          | -0.000959   |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.41        |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0303      |
| rollout/                |             |
|    ep_len_mean          | 86.8        |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 316         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.028427092 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 449         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0339     |
|    std                  | 0.368       |
|    value_loss           | 1.61e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.8       |
| reward_contact          | -0.000907  |
| reward_ctrl             | 12.8       |
| reward_motion           | 2.44       |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0302     |
| rollout/                |            |
|    ep_len_mean          | 94.9       |
|    ep_rew_mean          | 921        |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.04576248 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0659     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.368      |
|    value_loss           | 3.73e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.7        |
| reward_contact          | -0.001      |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.51        |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 86.1        |
|    ep_rew_mean          | 841         |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.055026103 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 332         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.00103   |
| reward_ctrl             | 12.5       |
| reward_motion           | 2.48       |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0311     |
| rollout/                |            |
|    ep_len_mean          | 95.2       |
|    ep_rew_mean          | 917        |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.04045736 |
|    clip_fraction        | 0.0592     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0003     |
|    loss                 | 536        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0443    |
|    std                  | 0.368      |
|    value_loss           | 1.82e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.5        |
| reward_contact          | -0.000958   |
| reward_ctrl             | 12.4        |
| reward_motion           | 2.52        |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0294      |
| rollout/                |             |
|    ep_len_mean          | 91.9        |
|    ep_rew_mean          | 889         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 10          |
|    time_elapsed         | 33          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.032536104 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 502         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0337     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.3        |
| reward_contact          | -0.00123    |
| reward_ctrl             | 12.2        |
| reward_motion           | 2.62        |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 73.1        |
|    ep_rew_mean          | 724         |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 11          |
|    time_elapsed         | 37          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.049803257 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.82e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 878.37 - Last mean reward per episode: 723.79
----------------------------------------
| reward                  | 11.4       |
| reward_contact          | -0.00136   |
| reward_ctrl             | 12.3       |
| reward_motion           | 2.62       |
| reward_torque           | -3.5       |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 708        |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.05735998 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.0003     |
|    loss                 | 853        |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.368      |
|    value_loss           | 2.88e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.0018     |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.67        |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 70.8        |
|    ep_rew_mean          | 695         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.040806033 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0498     |
|    std                  | 0.368       |
|    value_loss           | 1.88e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.00219    |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.72        |
| reward_torque           | -3.61       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 80.6        |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 14          |
|    time_elapsed         | 47          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.046216436 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 762         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 2.02e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.4        |
| reward_contact          | -0.0023     |
| reward_ctrl             | 12.3        |
| reward_motion           | 2.69        |
| reward_torque           | -3.6        |
| reward_velocity         | 0.0234      |
| rollout/                |             |
|    ep_len_mean          | 79.3        |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.038783964 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 867         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.0025    |
| reward_ctrl             | 12.4       |
| reward_motion           | 2.69       |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 86.8       |
|    ep_rew_mean          | 831        |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 16         |
|    time_elapsed         | 54         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.04771261 |
|    clip_fraction        | 0.0803     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.218      |
|    learning_rate        | 0.0003     |
|    loss                 | 663        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0486    |
|    std                  | 0.368      |
|    value_loss           | 1.92e+03   |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_6
---------------------------------
| reward             | 10.6     |
| reward_contact     | 0        |
| reward_ctrl        | 14.2     |
| reward_motion      | -0.1     |
| reward_torque      | -3.52    |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.00277    |
| reward_ctrl             | 13          |
| reward_motion           | -0.0681     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0126      |
| rollout/                |             |
|    ep_len_mean          | 68          |
|    ep_rew_mean          | 643         |
| time/                   |             |
|    fps                  | 387         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 2048        |
| train/                  |             |
|    approx_kl            | 0.055133946 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | -1.6e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 988         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.108      |
|    std                  | 0.368       |
|    value_loss           | 4.26e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.62        |
| reward_contact          | -0.00201    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0768     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0131      |
| rollout/                |             |
|    ep_len_mean          | 85.3        |
|    ep_rew_mean          | 782         |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 3           |
|    time_elapsed         | 8           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.053772744 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0647     |
|    std                  | 0.368       |
|    value_loss           | 2.46e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.48        |
| reward_contact          | -0.00138    |
| reward_ctrl             | 12.8        |
| reward_motion           | -0.0712     |
| reward_torque           | -3.3        |
| reward_velocity         | 0.0167      |
| rollout/                |             |
|    ep_len_mean          | 69.3        |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.071887545 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0692     |
|    std                  | 0.368       |
|    value_loss           | 2.36e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.17       |
| reward_contact          | -0.00164   |
| reward_ctrl             | 12.5       |
| reward_motion           | -0.047     |
| reward_torque           | -3.33      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 84.5       |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 324        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.06008743 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 740        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0612    |
|    std                  | 0.368      |
|    value_loss           | 2.18e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 916.73
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.11        |
| reward_contact          | -0.00199    |
| reward_ctrl             | 12.5        |
| reward_motion           | -0.0374     |
| reward_torque           | -3.33       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 917         |
| time/                   |             |
|    fps                  | 318         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.069795854 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.0429      |
|    learning_rate        | 0.0003      |
|    loss                 | 623         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0794     |
|    std                  | 0.368       |
|    value_loss           | 2.51e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 8.98       |
| reward_contact          | -0.00185   |
| reward_ctrl             | 12.3       |
| reward_motion           | -0.022     |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 906        |
| time/                   |            |
|    fps                  | 314        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.06844729 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.173      |
|    learning_rate        | 0.0003     |
|    loss                 | 594        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.368      |
|    value_loss           | 2.67e+03   |
----------------------------------------
---------------------------------------
| reward                  | 9.47      |
| reward_contact          | -0.00174  |
| reward_ctrl             | 12.8      |
| reward_motion           | -0.0261   |
| reward_torque           | -3.34     |
| reward_velocity         | 0.0231    |
| rollout/                |           |
|    ep_len_mean          | 90.6      |
|    ep_rew_mean          | 819       |
| time/                   |           |
|    fps                  | 311       |
|    iterations           | 8         |
|    time_elapsed         | 26        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0293882 |
|    clip_fraction        | 0.0242    |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78e+03  |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0362   |
|    std                  | 0.368     |
|    value_loss           | 3.16e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.27        |
| reward_contact          | -0.00184    |
| reward_ctrl             | 12.6        |
| reward_motion           | -0.0261     |
| reward_torque           | -3.34       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 884         |
| time/                   |             |
|    fps                  | 309         |
|    iterations           | 9           |
|    time_elapsed         | 29          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.042493284 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+03     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0387     |
|    std                  | 0.368       |
|    value_loss           | 2.83e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.5        |
| reward_contact          | -0.00224   |
| reward_ctrl             | 12.8       |
| reward_motion           | -0.026     |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0236     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 910        |
| time/                   |            |
|    fps                  | 307        |
|    iterations           | 10         |
|    time_elapsed         | 33         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.04812345 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03e+03   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.368      |
|    value_loss           | 2.64e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.82        |
| reward_contact          | -0.00158    |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0231     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0273      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | 766         |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 11          |
|    time_elapsed         | 36          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.063712046 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.0356      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.368       |
|    value_loss           | 3.19e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 916.73 - Last mean reward per episode: 693.01
---------------------------------------
| reward                  | 9.64      |
| reward_contact          | -0.00117  |
| reward_ctrl             | 13.1      |
| reward_motion           | -0.0518   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0213    |
| rollout/                |           |
|    ep_len_mean          | 56        |
|    ep_rew_mean          | 530       |
| time/                   |           |
|    fps                  | 305       |
|    iterations           | 12        |
|    time_elapsed         | 40        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0654025 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.198     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0656   |
|    std                  | 0.368     |
|    value_loss           | 3.05e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.61        |
| reward_contact          | -0.00121    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0626     |
| reward_torque           | -3.4        |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 57.5        |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 13          |
|    time_elapsed         | 43          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.035146132 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | 912         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0486     |
|    std                  | 0.368       |
|    value_loss           | 2.72e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.87       |
| reward_contact          | -0.000842  |
| reward_ctrl             | 13.4       |
| reward_motion           | -0.0653    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 52.4       |
|    ep_rew_mean          | 500        |
| time/                   |            |
|    fps                  | 302        |
|    iterations           | 14         |
|    time_elapsed         | 47         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.03921909 |
|    clip_fraction        | 0.0476     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0164     |
|    learning_rate        | 0.0003     |
|    loss                 | 469        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.368      |
|    value_loss           | 1.97e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0172      |
| rollout/                |             |
|    ep_len_mean          | 54.1        |
|    ep_rew_mean          | 510         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.046162732 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 385         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0633     |
|    std                  | 0.368       |
|    value_loss           | 1.41e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.56        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.017       |
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 16          |
|    time_elapsed         | 54          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.058286734 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | -0.0672     |
|    learning_rate        | 0.0003      |
|    loss                 | 397         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.062      |
|    std                  | 0.368       |
|    value_loss           | 1.15e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.76        |
| reward_contact          | -0.00024    |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0653     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0163      |
| rollout/                |             |
|    ep_len_mean          | 68.4        |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.037595566 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | 576         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0411     |
|    std                  | 0.368       |
|    value_loss           | 2.15e+03    |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 916.73 - Last mean reward per episode: 674.37
----------------------------------------
| reward                  | 9.71       |
| reward_contact          | -0.00024   |
| reward_ctrl             | 13.2       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 73.7       |
|    ep_rew_mean          | 674        |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 18         |
|    time_elapsed         | 61         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.03569829 |
|    clip_fraction        | 0.0447     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.127     |
|    learning_rate        | 0.0003     |
|    loss                 | 903        |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0406    |
|    std                  | 0.368      |
|    value_loss           | 2.46e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.7         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0198      |
| rollout/                |             |
|    ep_len_mean          | 80.7        |
|    ep_rew_mean          | 731         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 19          |
|    time_elapsed         | 65          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.055611745 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | -0.0154     |
|    learning_rate        | 0.0003      |
|    loss                 | 392         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0574     |
|    std                  | 0.368       |
|    value_loss           | 1.72e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.6         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 90.4        |
|    ep_rew_mean          | 813         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 20          |
|    time_elapsed         | 68          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.030617608 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.0102      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.32e+03    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0396     |
|    std                  | 0.368       |
|    value_loss           | 4.55e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.69        |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0647     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0224      |
| rollout/                |             |
|    ep_len_mean          | 89.7        |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 21          |
|    time_elapsed         | 72          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.042898893 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 495         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.05       |
|    std                  | 0.368       |
|    value_loss           | 1.98e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.89        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.39       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 929         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 22          |
|    time_elapsed         | 76          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.019014295 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 768         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0197     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 10.2        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.7        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0238      |
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 953         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 23          |
|    time_elapsed         | 79          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.050864376 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 961         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0516     |
|    std                  | 0.368       |
|    value_loss           | 2.86e+03    |
-----------------------------------------
Num timesteps: 24000
Best mean reward: 916.73 - Last mean reward per episode: 952.60
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.99        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.5        |
| reward_motion           | -0.0586     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0246      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 927         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 24          |
|    time_elapsed         | 82          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.046912353 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+03    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.368       |
|    value_loss           | 3.29e+03    |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_7
---------------------------------
| reward             | -3.82    |
| reward_contact     | 0        |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.64    |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | -94.4    |
| time/              |          |
|    fps             | 484      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0709    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 57.2       |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 377        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.12333509 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.00606    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.88       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.367      |
|    value_loss           | 330        |
----------------------------------------
---------------------------------------
| reward                  | -3.52     |
| reward_contact          | -0.00137  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0442   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0221    |
| rollout/                |           |
|    ep_len_mean          | 84.1      |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 3         |
|    time_elapsed         | 9         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.1174901 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.343     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.1      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.367     |
|    value_loss           | 142       |
---------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | -0.00112    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0364     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 75.3        |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 324         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.054415382 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.367       |
|    value_loss           | 155         |
-----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00096   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0242    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 90.5       |
|    ep_rew_mean          | -283       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 5          |
|    time_elapsed         | 16         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07612029 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.3       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.367      |
|    value_loss           | 131        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -297.27
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.000828  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0346    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 94.6       |
|    ep_rew_mean          | -297       |
| time/                   |            |
|    fps                  | 309        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.08958479 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.367      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000866  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0467    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 95.1       |
|    ep_rew_mean          | -300       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.07138507 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 39.7       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0889    |
|    std                  | 0.367      |
|    value_loss           | 176        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000843  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0371    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0264     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -333       |
| time/                   |            |
|    fps                  | 303        |
|    iterations           | 8          |
|    time_elapsed         | 26         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.09170306 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 280        |
----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 88.8       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.08530077 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.3       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0976    |
|    std                  | 0.367      |
|    value_loss           | 140        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.000614   |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0549     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0212      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | -266        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 10          |
|    time_elapsed         | 34          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.091505446 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.367       |
|    value_loss           | 513         |
-----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.000614  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0549    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 85.5       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08441191 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.0003     |
|    loss                 | 60         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0764    |
|    std                  | 0.366      |
|    value_loss           | 382        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -297.27 - Last mean reward per episode: -308.44
-----------------------------------------
| reward                  | -3.65       |
| reward_contact          | -0.00101    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0577     |
| reward_torque           | -3.52       |
| reward_velocity         | 0.0205      |
| rollout/                |             |
|    ep_len_mean          | 97.5        |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 12          |
|    time_elapsed         | 41          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.058747333 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.6        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.366       |
|    value_loss           | 237         |
-----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 89.3       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 13         |
|    time_elapsed         | 44         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06798034 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0833    |
|    std                  | 0.366      |
|    value_loss           | 209        |
----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 81.3       |
|    ep_rew_mean          | -260       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.13578144 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.446      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 201        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.000958  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0694    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 83.3       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 15         |
|    time_elapsed         | 52         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.07831302 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.8       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.366      |
|    value_loss           | 299        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00129   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0676    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0195     |
| rollout/                |            |
|    ep_len_mean          | 81.6       |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 292        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05408106 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 42         |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.366      |
|    value_loss           | 247        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00132    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.076      |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0193      |
| rollout/                |             |
|    ep_len_mean          | 81.5        |
|    ep_rew_mean          | -262        |
| time/                   |             |
|    fps                  | 292         |
|    iterations           | 17          |
|    time_elapsed         | 59          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.098303586 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.366       |
|    value_loss           | 272         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -297.27 - Last mean reward per episode: -261.67
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 87.5       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 18         |
|    time_elapsed         | 63         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08449566 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.6       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.366      |
|    value_loss           | 286        |
----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0831    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 88.6       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 19         |
|    time_elapsed         | 66         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.09105019 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.85       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.366      |
|    value_loss           | 111        |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.00158    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.077      |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0214      |
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 20          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.076098844 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.89        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0995     |
|    std                  | 0.366       |
|    value_loss           | 131         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00141    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 21          |
|    time_elapsed         | 73          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.116045415 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.49        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.366       |
|    value_loss           | 79.1        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00189    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.068719216 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0901     |
|    std                  | 0.366       |
|    value_loss           | 231         |
-----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00205   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 96.4       |
|    ep_rew_mean          | -310       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 23         |
|    time_elapsed         | 81         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.08666834 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 23         |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0861    |
|    std                  | 0.366      |
|    value_loss           | 348        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_8
---------------------------------
| reward             | -3.58    |
| reward_contact     | -0.00667 |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.39    |
| reward_velocity    | 0.0173   |
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00168   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 71.1       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 378        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10417083 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.0168     |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.367      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 69.6       |
|    ep_rew_mean          | -220       |
| time/                   |            |
|    fps                  | 343        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.11021443 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.334     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.367      |
|    value_loss           | 99.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.0021     |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0882     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0174      |
| rollout/                |             |
|    ep_len_mean          | 58.7        |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.091333374 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | -0.0565     |
|    learning_rate        | 0.0003      |
|    loss                 | 36.8        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.367       |
|    value_loss           | 162         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.58       |
| reward_contact          | -0.00234    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.084      |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0183      |
| rollout/                |             |
|    ep_len_mean          | 55.3        |
|    ep_rew_mean          | -176        |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 5           |
|    time_elapsed         | 15          |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.067936674 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0818     |
|    std                  | 0.367       |
|    value_loss           | 237         |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -138.63
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00219   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0743    |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 42.8       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.06220818 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0003     |
|    loss                 | 88.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.367      |
|    value_loss           | 254        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | -0.00223   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0638    |
| reward_torque           | -3.35      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09764336 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0257     |
|    learning_rate        | 0.0003     |
|    loss                 | 59         |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 201        |
----------------------------------------
-----------------------------------------
| reward                  | -3.51       |
| reward_contact          | -0.00208    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0554     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 48.5        |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.096551545 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.367       |
|    value_loss           | 132         |
-----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.38      |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 58.3       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07261966 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 248        |
----------------------------------------
---------------------------------------
| reward                  | -3.53     |
| reward_contact          | -0.00183  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0591   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0267    |
| rollout/                |           |
|    ep_len_mean          | 68        |
|    ep_rew_mean          | -217      |
| time/                   |           |
|    fps                  | 303       |
|    iterations           | 10        |
|    time_elapsed         | 33        |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.0808554 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 47        |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0864   |
|    std                  | 0.367     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 71         |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.12363265 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.133      |
|    learning_rate        | 0.0003     |
|    loss                 | 76.2       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.367      |
|    value_loss           | 263        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -138.63 - Last mean reward per episode: -231.23
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00204   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 73.6       |
|    ep_rew_mean          | -236       |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07578746 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.8       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 206        |
----------------------------------------
-----------------------------------------
| reward                  | -3.59       |
| reward_contact          | -0.00237    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0516     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.027       |
| rollout/                |             |
|    ep_len_mean          | 63.5        |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.062469497 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0867     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00237   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0621    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -204       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07475457 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0003     |
|    loss                 | 126        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.366      |
|    value_loss           | 506        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | -0.00254   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0674    |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0317     |
| rollout/                |            |
|    ep_len_mean          | 72.3       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 15         |
|    time_elapsed         | 51         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.09738049 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.92       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 90.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | -0.00225   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0669    |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 60.5       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.07088742 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 64.8       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0858    |
|    std                  | 0.366      |
|    value_loss           | 371        |
----------------------------------------
-----------------------------------------
| reward                  | -3.54       |
| reward_contact          | -0.00205    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0669     |
| reward_torque           | -3.4        |
| reward_velocity         | 0.0292      |
| rollout/                |             |
|    ep_len_mean          | 52.5        |
|    ep_rew_mean          | -170        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.074232996 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0949     |
|    std                  | 0.366       |
|    value_loss           | 230         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -138.63 - Last mean reward per episode: -186.61
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.031      |
| rollout/                |            |
|    ep_len_mean          | 57.3       |
|    ep_rew_mean          | -185       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 18         |
|    time_elapsed         | 62         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06691042 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0853    |
|    std                  | 0.366      |
|    value_loss           | 376        |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.39      |
| reward_velocity         | 0.0312     |
| rollout/                |            |
|    ep_len_mean          | 58.5       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 19         |
|    time_elapsed         | 66         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.08361464 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0003     |
|    loss                 | 52.1       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.366      |
|    value_loss           | 293        |
----------------------------------------
-----------------------------------------
| reward                  | -3.57       |
| reward_contact          | -0.00168    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0789     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0275      |
| rollout/                |             |
|    ep_len_mean          | 59          |
|    ep_rew_mean          | -192        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 20          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.068159856 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.3        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.366       |
|    value_loss           | 229         |
-----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | -0.00259   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0735    |
| reward_torque           | -3.42      |
| reward_velocity         | 0.03       |
| rollout/                |            |
|    ep_len_mean          | 68.6       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 21         |
|    time_elapsed         | 73         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.07058899 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.4       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.094     |
|    std                  | 0.366      |
|    value_loss           | 168        |
----------------------------------------
-----------------------------------------
| reward                  | -3.58       |
| reward_contact          | -0.00308    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0683     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 58.2        |
|    ep_rew_mean          | -189        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.072769254 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0982     |
|    std                  | 0.366       |
|    value_loss           | 91.2        |
-----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00332   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0683    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 52.8       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 23         |
|    time_elapsed         | 80         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.06702521 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0003     |
|    loss                 | 50.8       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.366      |
|    value_loss           | 342        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -138.63 - Last mean reward per episode: -172.74
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00318   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 62         |
|    ep_rew_mean          | -202       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 24         |
|    time_elapsed         | 84         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08230947 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.1       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.366      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00277   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 67.5       |
|    ep_rew_mean          | -218       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 25         |
|    time_elapsed         | 88         |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.06597649 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.5       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0866    |
|    std                  | 0.366      |
|    value_loss           | 260        |
----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.00275    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0541     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 67.4        |
|    ep_rew_mean          | -218        |
| time/                   |             |
|    fps                  | 289         |
|    iterations           | 26          |
|    time_elapsed         | 92          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.072470814 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.104      |
|    std                  | 0.366       |
|    value_loss           | 158         |
-----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00205   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0632    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 55.9       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 27         |
|    time_elapsed         | 95         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.08116076 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.8       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.088     |
|    std                  | 0.366      |
|    value_loss           | 177        |
----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.00184    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0526     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0199      |
| rollout/                |             |
|    ep_len_mean          | 66.3        |
|    ep_rew_mean          | -215        |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 28          |
|    time_elapsed         | 99          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.069119915 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0947     |
|    std                  | 0.366       |
|    value_loss           | 126         |
-----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0526    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 76.5       |
|    ep_rew_mean          | -247       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 29         |
|    time_elapsed         | 103        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.07737486 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.366      |
|    value_loss           | 203        |
----------------------------------------
Num timesteps: 30000
Best mean reward: -138.63 - Last mean reward per episode: -253.93
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00161   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 79.2       |
|    ep_rew_mean          | -256       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 30         |
|    time_elapsed         | 107        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06954172 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.366      |
|    value_loss           | 72.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0228     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -283       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 31         |
|    time_elapsed         | 110        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.10670468 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.0102    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.05       |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0477    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0226     |
| rollout/                |            |
|    ep_len_mean          | 97.9       |
|    ep_rew_mean          | -316       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 32         |
|    time_elapsed         | 114        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07300088 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.197      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.67       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.366      |
|    value_loss           | 84.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00136    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0477     |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0231      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -333        |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 33          |
|    time_elapsed         | 118         |
|    total_timesteps      | 33792       |
| train/                  |             |
|    approx_kl            | 0.088946536 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.6       |
|    explained_variance   | -1.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.7         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 73.2        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.0014     |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0455     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -334        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 34          |
|    time_elapsed         | 121         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.097367436 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.5         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.366       |
|    value_loss           | 139         |
-----------------------------------------
----------------------------------------
| reward                  | -3.61      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0492    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -365       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 35         |
|    time_elapsed         | 125        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.09249616 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.08       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 160        |
----------------------------------------
Num timesteps: 36000
Best mean reward: -138.63 - Last mean reward per episode: -346.75
-----------------------------------------
| reward                  | -3.66       |
| reward_contact          | -0.00169    |
| reward_ctrl             | -0.0909     |
| reward_motion           | -0.0639     |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | -311        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 36          |
|    time_elapsed         | 129         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.070547566 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.101      |
|    std                  | 0.366       |
|    value_loss           | 77.4        |
-----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0722    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 93.8       |
|    ep_rew_mean          | -304       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 37         |
|    time_elapsed         | 132        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.06791668 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.4       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 388        |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00227    |
| reward_ctrl             | -0.0909     |
| reward_motion           | -0.0752     |
| reward_torque           | -3.55       |
| reward_velocity         | 0.0225      |
| rollout/                |             |
|    ep_len_mean          | 59.6        |
|    ep_rew_mean          | -196        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 38          |
|    time_elapsed         | 136         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.099047005 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.61        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 147         |
-----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00228   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0647    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 57.5       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 39         |
|    time_elapsed         | 139        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.07516223 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.7       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.7       |
| reward_contact          | -0.0023    |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0739    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 47.3       |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 40         |
|    time_elapsed         | 143        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.10898656 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.14       |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -3.7       |
| reward_contact          | -0.00206   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0658    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0246     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 41         |
|    time_elapsed         | 146        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.11880714 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.366      |
|    value_loss           | 220        |
----------------------------------------
Num timesteps: 42000
Best mean reward: -138.63 - Last mean reward per episode: -187.23
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0608    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 52         |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 42         |
|    time_elapsed         | 150        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.10621195 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.87       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.366      |
|    value_loss           | 75.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0487    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0213     |
| rollout/                |            |
|    ep_len_mean          | 51.9       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 43         |
|    time_elapsed         | 153        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.07928381 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.056     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.3       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 233        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0697    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 27         |
|    ep_rew_mean          | -93.1      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 44         |
|    time_elapsed         | 157        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.07353075 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.095     |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 273        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0697    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0137     |
| rollout/                |            |
|    ep_len_mean          | 27.8       |
|    ep_rew_mean          | -95.7      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 45         |
|    time_elapsed         | 161        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.07727442 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0922     |
|    learning_rate        | 0.0003     |
|    loss                 | 25.5       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.365      |
|    value_loss           | 242        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00214   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 46         |
|    time_elapsed         | 164        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.09110732 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.372      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.78       |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.365      |
|    value_loss           | 118        |
----------------------------------------
Num timesteps: 48000
Best mean reward: -138.63 - Last mean reward per episode: -156.28
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00254   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0528    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 47         |
|    time_elapsed         | 168        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.11572909 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.65       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 134        |
----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 57.2       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 48         |
|    time_elapsed         | 172        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07793903 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.54       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.365      |
|    value_loss           | 96.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 57.4       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 49         |
|    time_elapsed         | 176        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.15945438 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.15       |
|    learning_rate        | 0.0003     |
|    loss                 | 10.9       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.365      |
|    value_loss           | 169        |
----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0636    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 74.6       |
|    ep_rew_mean          | -245       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 50         |
|    time_elapsed         | 179        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.09482075 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.406      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.365      |
|    value_loss           | 54.8       |
----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00192    |
| reward_ctrl             | -0.0963     |
| reward_motion           | -0.0586     |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0178      |
| rollout/                |             |
|    ep_len_mean          | 84.8        |
|    ep_rew_mean          | -278        |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 51          |
|    time_elapsed         | 183         |
|    total_timesteps      | 52224       |
| train/                  |             |
|    approx_kl            | 0.058142394 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0863     |
|    std                  | 0.365       |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------------
| reward                  | -3.61     |
| reward_contact          | -0.00192  |
| reward_ctrl             | -0.0963   |
| reward_motion           | -0.0478   |
| reward_torque           | -3.49     |
| reward_velocity         | 0.0197    |
| rollout/                |           |
|    ep_len_mean          | 94.8      |
|    ep_rew_mean          | -309      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 52        |
|    time_elapsed         | 186       |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.1113736 |
|    clip_fraction        | 0.189     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.5      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -0.108    |
|    std                  | 0.365     |
|    value_loss           | 98.1      |
---------------------------------------
Num timesteps: 54000
Best mean reward: -138.63 - Last mean reward per episode: -311.77
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00216   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0511    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0193     |
| rollout/                |            |
|    ep_len_mean          | 95.6       |
|    ep_rew_mean          | -312       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 53         |
|    time_elapsed         | 190        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.09085918 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.2       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.365      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00204   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0465    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | -343       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 54         |
|    time_elapsed         | 193        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.08517785 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.16       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.365      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.0018    |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0411    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | -371       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 55         |
|    time_elapsed         | 197        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.07932863 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.409      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.1       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.365      |
|    value_loss           | 302        |
----------------------------------------
---------------------------------------
| reward                  | -3.68     |
| reward_contact          | -0.0018   |
| reward_ctrl             | -0.0963   |
| reward_motion           | -0.0411   |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0233    |
| rollout/                |           |
|    ep_len_mean          | 114       |
|    ep_rew_mean          | -370      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 56        |
|    time_elapsed         | 201       |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0811523 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 18.3      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.0949   |
|    std                  | 0.365     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00155   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0464    |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 91.9       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 57         |
|    time_elapsed         | 204        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.09470156 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.53       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.365      |
|    value_loss           | 80.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00155    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0429     |
| reward_torque           | -3.58       |
| reward_velocity         | 0.0238      |
| rollout/                |             |
|    ep_len_mean          | 91.2        |
|    ep_rew_mean          | -296        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 58          |
|    time_elapsed         | 208         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.069616064 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | -0.0606     |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.104      |
|    std                  | 0.365       |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -138.63 - Last mean reward per episode: -208.97
-----------------------------------------
| reward                  | -3.71       |
| reward_contact          | -0.00148    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0577     |
| reward_torque           | -3.58       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 63.8        |
|    ep_rew_mean          | -209        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 59          |
|    time_elapsed         | 211         |
|    total_timesteps      | 60416       |
| train/                  |             |
|    approx_kl            | 0.077760346 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.3         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.365       |
|    value_loss           | 116         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.68       |
| reward_contact          | -0.00124    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0521     |
| reward_torque           | -3.56       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 74.7        |
|    ep_rew_mean          | -244        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 60          |
|    time_elapsed         | 215         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.092641875 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.35        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.116      |
|    std                  | 0.365       |
|    value_loss           | 114         |
-----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00119   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0496    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 75.4       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 61         |
|    time_elapsed         | 218        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.09082776 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.34       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.365      |
|    value_loss           | 162        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0572    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -184       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 62         |
|    time_elapsed         | 222        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.08267109 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.087      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.365      |
|    value_loss           | 247        |
----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00165   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0572    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0228     |
| rollout/                |            |
|    ep_len_mean          | 55.1       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 63         |
|    time_elapsed         | 225        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.06325209 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0251     |
|    learning_rate        | 0.0003     |
|    loss                 | 19.8       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.365      |
|    value_loss           | 336        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.0801    |
| reward_motion           | -0.0461    |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 66.3       |
|    ep_rew_mean          | -218       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 64         |
|    time_elapsed         | 229        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.09174758 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.26       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.365      |
|    value_loss           | 67.7       |
----------------------------------------
Num timesteps: 66000
Best mean reward: -138.63 - Last mean reward per episode: -217.63
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0697    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 43.9       |
|    ep_rew_mean          | -146       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 65         |
|    time_elapsed         | 233        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.09963308 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0697    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 53.6       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 66         |
|    time_elapsed         | 236        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.11569247 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.64       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 211        |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0736    |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0272     |
| rollout/                |            |
|    ep_len_mean          | 53.3       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 67         |
|    time_elapsed         | 240        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.09667668 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.86       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.365      |
|    value_loss           | 111        |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00152    |
| reward_ctrl             | -0.0902     |
| reward_motion           | -0.0636     |
| reward_torque           | -3.56       |
| reward_velocity         | 0.028       |
| rollout/                |             |
|    ep_len_mean          | 63.4        |
|    ep_rew_mean          | -208        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 68          |
|    time_elapsed         | 244         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.121750504 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.34        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.138      |
|    std                  | 0.365       |
|    value_loss           | 142         |
-----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0686    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -148       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 69         |
|    time_elapsed         | 247        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.18296453 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.88       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.365      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0492    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 54         |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 70         |
|    time_elapsed         | 251        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.11368002 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.91       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.365      |
|    value_loss           | 166        |
----------------------------------------
Num timesteps: 72000
Best mean reward: -138.63 - Last mean reward per episode: -182.54
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0371    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0248     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 71         |
|    time_elapsed         | 254        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.16698757 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.12       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.365      |
|    value_loss           | 113        |
----------------------------------------
-----------------------------------------
| reward                  | -3.73       |
| reward_contact          | -0.00209    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0445     |
| reward_torque           | -3.6        |
| reward_velocity         | 0.0237      |
| rollout/                |             |
|    ep_len_mean          | 38.4        |
|    ep_rew_mean          | -131        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 72          |
|    time_elapsed         | 258         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.085742936 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.0433      |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.365       |
|    value_loss           | 213         |
-----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.0022    |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 73         |
|    time_elapsed         | 261        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.12684867 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 137        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 36.2       |
|    ep_rew_mean          | -123       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 74         |
|    time_elapsed         | 265        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.18529704 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.06       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 191        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0729    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 45.2       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 75         |
|    time_elapsed         | 268        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.17445703 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.446      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.09       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0668    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 76         |
|    time_elapsed         | 272        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.13276449 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.05       |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 156        |
----------------------------------------
Num timesteps: 78000
Best mean reward: -138.63 - Last mean reward per episode: -182.60
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.0825    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0304     |
| rollout/                |            |
|    ep_len_mean          | 64.5       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 77         |
|    time_elapsed         | 275        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.10273191 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.12       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.365      |
|    value_loss           | 129        |
----------------------------------------
---------------------------------------
| reward                  | -3.74     |
| reward_contact          | -0.00101  |
| reward_ctrl             | -0.0909   |
| reward_motion           | -0.0549   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0287    |
| rollout/                |           |
|    ep_len_mean          | 52.5      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 78        |
|    time_elapsed         | 279       |
|    total_timesteps      | 79872     |
| train/                  |           |
|    approx_kl            | 0.1510182 |
|    clip_fraction        | 0.303     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.778     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.46      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.142    |
|    std                  | 0.365     |
|    value_loss           | 130       |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00125   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0549    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0285     |
| rollout/                |            |
|    ep_len_mean          | 53.6       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 79         |
|    time_elapsed         | 283        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.11147174 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0869     |
|    learning_rate        | 0.0003     |
|    loss                 | 8.01       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.365      |
|    value_loss           | 149        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0579    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 63.6       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 80         |
|    time_elapsed         | 286        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.14892924 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.52       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.365      |
|    value_loss           | 73.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0726    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 43         |
|    ep_rew_mean          | -143       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 81         |
|    time_elapsed         | 290        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.15100369 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 76.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0771    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 42.7       |
|    ep_rew_mean          | -142       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 82         |
|    time_elapsed         | 293        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.16122808 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 10         |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.365      |
|    value_loss           | 144        |
----------------------------------------
Num timesteps: 84000
Best mean reward: -138.63 - Last mean reward per episode: -142.50
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.061     |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 52.9       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 83         |
|    time_elapsed         | 297        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.15519834 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.365      |
|    value_loss           | 90.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00249  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0714   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0155    |
| rollout/                |           |
|    ep_len_mean          | 52.3      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 84        |
|    time_elapsed         | 301       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.2233521 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.25      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.364     |
|    value_loss           | 136       |
---------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00242   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0735    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 51.7       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 85         |
|    time_elapsed         | 305        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.13412023 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.5        |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.364      |
|    value_loss           | 77.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0561    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 53         |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 86         |
|    time_elapsed         | 308        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.12134208 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.364      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0561    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 53         |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 87         |
|    time_elapsed         | 312        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.18956733 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0476    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.31       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.364      |
|    value_loss           | 99.3       |
----------------------------------------
Num timesteps: 90000
Best mean reward: -138.63 - Last mean reward per episode: -208.26
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00226  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0498   |
| reward_torque           | -3.67     |
| reward_velocity         | 0.0193    |
| rollout/                |           |
|    ep_len_mean          | 63.3      |
|    ep_rew_mean          | -208      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 88        |
|    time_elapsed         | 316       |
|    total_timesteps      | 90112     |
| train/                  |           |
|    approx_kl            | 0.1579457 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.482     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.95      |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.364     |
|    value_loss           | 52.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0529    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0205     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 89         |
|    time_elapsed         | 320        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.12793954 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.88       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.364      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0539    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 63.7       |
|    ep_rew_mean          | -210       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 90         |
|    time_elapsed         | 323        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.18851484 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.0106     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.62       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.364      |
|    value_loss           | 182        |
----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00103   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 52.9       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 91         |
|    time_elapsed         | 327        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.21907353 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.364      |
|    value_loss           | 78.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 62.4       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 92         |
|    time_elapsed         | 331        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.23029736 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.364      |
|    value_loss           | 80.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00101   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0663    |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | -239       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 93         |
|    time_elapsed         | 334        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.18609498 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.29       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.364      |
|    value_loss           | 56.4       |
----------------------------------------
Num timesteps: 96000
Best mean reward: -138.63 - Last mean reward per episode: -175.89
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.000854  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 52.3       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 94         |
|    time_elapsed         | 338        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.28788555 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.364      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0599    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 61.3       |
|    ep_rew_mean          | -205       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 95         |
|    time_elapsed         | 342        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.17428435 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.01       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.364      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0696    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 62.2       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 96         |
|    time_elapsed         | 345        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.18998115 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.148     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.74       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0254     |
| rollout/                |            |
|    ep_len_mean          | 72.3       |
|    ep_rew_mean          | -240       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 97         |
|    time_elapsed         | 349        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.17720614 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 75.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.76     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0629   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0242    |
| rollout/                |           |
|    ep_len_mean          | 53.8      |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 98        |
|    time_elapsed         | 353       |
|    total_timesteps      | 100352    |
| train/                  |           |
|    approx_kl            | 0.1779699 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.74      |
|    n_updates            | 1940      |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.364     |
|    value_loss           | 57.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.00148  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0599   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.023     |
| rollout/                |           |
|    ep_len_mean          | 54.2      |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 99        |
|    time_elapsed         | 356       |
|    total_timesteps      | 101376    |
| train/                  |           |
|    approx_kl            | 0.1428752 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | -0.112    |
|    learning_rate        | 0.0003    |
|    loss                 | 11.7      |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.364     |
|    value_loss           | 177       |
---------------------------------------
Num timesteps: 102000
Best mean reward: -138.63 - Last mean reward per episode: -162.43
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000756  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0196     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 100        |
|    time_elapsed         | 360        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.22451073 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.17       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.35       |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.364      |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.000605  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0785    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 33.8       |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 101        |
|    time_elapsed         | 364        |
|    total_timesteps      | 103424     |
| train/                  |            |
|    approx_kl            | 0.19149505 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.364      |
|    value_loss           | 123        |
----------------------------------------
---------------------------------------
| reward                  | -3.81     |
| reward_contact          | -0.000605 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0834   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 43.1      |
|    ep_rew_mean          | -144      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 102       |
|    time_elapsed         | 367       |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.2511599 |
|    clip_fraction        | 0.416     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.494     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.66      |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.364     |
|    value_loss           | 100       |
---------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.000401  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0834    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 42.5       |
|    ep_rew_mean          | -142       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 103        |
|    time_elapsed         | 371        |
|    total_timesteps      | 105472     |
| train/                  |            |
|    approx_kl            | 0.17901304 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.364      |
|    value_loss           | 56.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000554  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0768    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 52.5       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 104        |
|    time_elapsed         | 374        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.11949587 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.42       |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.364      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.000518  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0665    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 105        |
|    time_elapsed         | 378        |
|    total_timesteps      | 107520     |
| train/                  |            |
|    approx_kl            | 0.23475204 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.57       |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.364      |
|    value_loss           | 43.1       |
----------------------------------------
Num timesteps: 108000
Best mean reward: -138.63 - Last mean reward per episode: -240.00
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.000518 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0665   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0219    |
| rollout/                |           |
|    ep_len_mean          | 73.4      |
|    ep_rew_mean          | -240      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 106       |
|    time_elapsed         | 381       |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.2534588 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.215     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.09      |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.363     |
|    value_loss           | 137       |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.000922  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0556    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 83.5       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 107        |
|    time_elapsed         | 385        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.18999796 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.363      |
|    value_loss           | 62.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 93.9       |
|    ep_rew_mean          | -305       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 108        |
|    time_elapsed         | 389        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.12192519 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.04       |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.363      |
|    value_loss           | 67.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00127   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0292     |
| rollout/                |            |
|    ep_len_mean          | 99.9       |
|    ep_rew_mean          | -324       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 109        |
|    time_elapsed         | 392        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.23648953 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.47       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.363      |
|    value_loss           | 98.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0583    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0305     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -343       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 110        |
|    time_elapsed         | 396        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.15398324 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.21       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.363      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0583    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0344     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -342       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 111        |
|    time_elapsed         | 399        |
|    total_timesteps      | 113664     |
| train/                  |            |
|    approx_kl            | 0.15456894 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 92.2       |
----------------------------------------
Num timesteps: 114000
Best mean reward: -138.63 - Last mean reward per episode: -342.33
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0496    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.035      |
| rollout/                |            |
|    ep_len_mean          | 116        |
|    ep_rew_mean          | -376       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 112        |
|    time_elapsed         | 403        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.25120765 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.363      |
|    value_loss           | 94.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00198   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0465    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0358     |
| rollout/                |            |
|    ep_len_mean          | 120        |
|    ep_rew_mean          | -388       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 113        |
|    time_elapsed         | 407        |
|    total_timesteps      | 115712     |
| train/                  |            |
|    approx_kl            | 0.19363809 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.73       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.363      |
|    value_loss           | 68         |
----------------------------------------
--------------------------------------
| reward                  | -3.74    |
| reward_contact          | -0.00191 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.052   |
| reward_torque           | -3.62    |
| reward_velocity         | 0.0326   |
| rollout/                |          |
|    ep_len_mean          | 96.7     |
|    ep_rew_mean          | -314     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 114      |
|    time_elapsed         | 410      |
|    total_timesteps      | 116736   |
| train/                  |          |
|    approx_kl            | 0.138787 |
|    clip_fraction        | 0.363    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.758    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.72     |
|    n_updates            | 2260     |
|    policy_gradient_loss | -0.145   |
|    std                  | 0.363    |
|    value_loss           | 108      |
--------------------------------------
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0568    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0319     |
| rollout/                |            |
|    ep_len_mean          | 97.1       |
|    ep_rew_mean          | -315       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 115        |
|    time_elapsed         | 414        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.22771211 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.93       |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.363      |
|    value_loss           | 144        |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0433    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0385     |
| rollout/                |            |
|    ep_len_mean          | 87.4       |
|    ep_rew_mean          | -284       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 116        |
|    time_elapsed         | 418        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.14404258 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.518      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.32       |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0517    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0333     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 117        |
|    time_elapsed         | 422        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.15724035 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.7        |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.363      |
|    value_loss           | 137        |
----------------------------------------
Num timesteps: 120000
Best mean reward: -138.63 - Last mean reward per episode: -282.19
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0526    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0357     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -256       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 118        |
|    time_elapsed         | 425        |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.18062328 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.2        |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.137     |
|    std                  | 0.363      |
|    value_loss           | 52.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00216   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0455    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0341     |
| rollout/                |            |
|    ep_len_mean          | 87.9       |
|    ep_rew_mean          | -286       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 119        |
|    time_elapsed         | 429        |
|    total_timesteps      | 121856     |
| train/                  |            |
|    approx_kl            | 0.14713405 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.42       |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.363      |
|    value_loss           | 253        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00252   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0519    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0292     |
| rollout/                |            |
|    ep_len_mean          | 63.2       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 120        |
|    time_elapsed         | 433        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.16059622 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.16       |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.363      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.0025    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0482    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0262     |
| rollout/                |            |
|    ep_len_mean          | 52.4       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 121        |
|    time_elapsed         | 436        |
|    total_timesteps      | 123904     |
| train/                  |            |
|    approx_kl            | 0.19410208 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.363      |
|    value_loss           | 111        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0581    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 45         |
|    ep_rew_mean          | -151       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 122        |
|    time_elapsed         | 440        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.36102858 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.78       |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.363      |
|    value_loss           | 146        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0533    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 123        |
|    time_elapsed         | 444        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.20007198 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.49       |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.363      |
|    value_loss           | 170        |
----------------------------------------
Num timesteps: 126000
Best mean reward: -138.63 - Last mean reward per episode: -182.91
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00269   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 64.7       |
|    ep_rew_mean          | -213       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 124        |
|    time_elapsed         | 447        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.13550422 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.86       |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.363      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00269   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 74.6       |
|    ep_rew_mean          | -244       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 125        |
|    time_elapsed         | 451        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.13427624 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.495      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.46       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 166        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00244   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0556    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 64.9       |
|    ep_rew_mean          | -214       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 126        |
|    time_elapsed         | 455        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.37402228 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.79       |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.363      |
|    value_loss           | 42.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0553    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0218     |
| rollout/                |            |
|    ep_len_mean          | 73.8       |
|    ep_rew_mean          | -242       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 127        |
|    time_elapsed         | 459        |
|    total_timesteps      | 130048     |
| train/                  |            |
|    approx_kl            | 0.29079568 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.41       |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.363      |
|    value_loss           | 120        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0514    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 75.4       |
|    ep_rew_mean          | -247       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 128        |
|    time_elapsed         | 462        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.18076289 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.3        |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 202        |
----------------------------------------
Num timesteps: 132000
Best mean reward: -138.63 - Last mean reward per episode: -246.14
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0514    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 75         |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 129        |
|    time_elapsed         | 466        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.19317493 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.363      |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00168   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0733    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 50.1       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 130        |
|    time_elapsed         | 469        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.11982493 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.15       |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.363      |
|    value_loss           | 158        |
----------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00169  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0706   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0141    |
| rollout/                |           |
|    ep_len_mean          | 30.4      |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 131       |
|    time_elapsed         | 473       |
|    total_timesteps      | 134144    |
| train/                  |           |
|    approx_kl            | 0.1396409 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.2       |
|    learning_rate        | 0.0003    |
|    loss                 | 22.2      |
|    n_updates            | 2600      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.363     |
|    value_loss           | 263       |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0593    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 29.3       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 132        |
|    time_elapsed         | 476        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.21981317 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00164   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 29.4       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 133        |
|    time_elapsed         | 480        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.25822717 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0146     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.02       |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.363      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 40.1       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 134        |
|    time_elapsed         | 483        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.25418472 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.363      |
|    value_loss           | 71.5       |
----------------------------------------
Num timesteps: 138000
Best mean reward: -138.63 - Last mean reward per episode: -162.64
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00136  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0579   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.017     |
| rollout/                |           |
|    ep_len_mean          | 48.5      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 135       |
|    time_elapsed         | 487       |
|    total_timesteps      | 138240    |
| train/                  |           |
|    approx_kl            | 0.2237775 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 96.1      |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00108   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0479    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 59.8       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 136        |
|    time_elapsed         | 490        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.21458992 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.363      |
|    value_loss           | 122        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0638    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 38.8       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 137        |
|    time_elapsed         | 494        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.26278722 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.29       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.363      |
|    value_loss           | 95.5       |
----------------------------------------
---------------------------------------
| reward                  | -3.73     |
| reward_contact          | -0.000831 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0136    |
| rollout/                |           |
|    ep_len_mean          | 30.1      |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 138       |
|    time_elapsed         | 498       |
|    total_timesteps      | 141312    |
| train/                  |           |
|    approx_kl            | 0.2073907 |
|    clip_fraction        | 0.368     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.0882    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.59      |
|    n_updates            | 2740      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.363     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0834    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 27.6       |
|    ep_rew_mean          | -96.4      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 139        |
|    time_elapsed         | 501        |
|    total_timesteps      | 142336     |
| train/                  |            |
|    approx_kl            | 0.23781297 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.363      |
|    value_loss           | 77.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.78     |
| reward_contact          | -0.00136  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0694   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.00981   |
| rollout/                |           |
|    ep_len_mean          | 28.4      |
|    ep_rew_mean          | -99.3     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 140       |
|    time_elapsed         | 505       |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.3014912 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.71      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 2780      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.362     |
|    value_loss           | 98.4      |
---------------------------------------
Num timesteps: 144000
Best mean reward: -138.63 - Last mean reward per episode: -132.07
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0601    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 40.3       |
|    ep_rew_mean          | -138       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 141        |
|    time_elapsed         | 508        |
|    total_timesteps      | 144384     |
| train/                  |            |
|    approx_kl            | 0.32839063 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.362      |
|    value_loss           | 83.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0601   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0111    |
| rollout/                |           |
|    ep_len_mean          | 40.4      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 142       |
|    time_elapsed         | 512       |
|    total_timesteps      | 145408    |
| train/                  |           |
|    approx_kl            | 0.4918242 |
|    clip_fraction        | 0.527     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 2820      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.362     |
|    value_loss           | 67.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00171   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0572    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 40.6       |
|    ep_rew_mean          | -138       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 143        |
|    time_elapsed         | 515        |
|    total_timesteps      | 146432     |
| train/                  |            |
|    approx_kl            | 0.32793552 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.77       |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.362      |
|    value_loss           | 73         |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0536    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 51.6       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 144        |
|    time_elapsed         | 519        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.27646038 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | -0.107     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.49       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.362      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00221   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0601    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 145        |
|    time_elapsed         | 522        |
|    total_timesteps      | 148480     |
| train/                  |            |
|    approx_kl            | 0.24255568 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.96       |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.362      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -3.84      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -160       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 146        |
|    time_elapsed         | 526        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.33325788 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | -0.394     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.362      |
|    value_loss           | 89.4       |
----------------------------------------
Num timesteps: 150000
Best mean reward: -132.07 - Last mean reward per episode: -187.75
----------------------------------------
| reward                  | -3.84      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 56.3       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 147        |
|    time_elapsed         | 529        |
|    total_timesteps      | 150528     |
| train/                  |            |
|    approx_kl            | 0.31835955 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.362      |
|    value_loss           | 70.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0699    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0151     |
| rollout/                |            |
|    ep_len_mean          | 56.8       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 148        |
|    time_elapsed         | 533        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.32852384 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.362      |
|    value_loss           | 75         |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0507    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 149        |
|    time_elapsed         | 536        |
|    total_timesteps      | 152576     |
| train/                  |            |
|    approx_kl            | 0.45275003 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.362      |
|    value_loss           | 78.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0432    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 150        |
|    time_elapsed         | 540        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.29522333 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.66       |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.362      |
|    value_loss           | 86.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00219   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0382    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 57.4       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 151        |
|    time_elapsed         | 544        |
|    total_timesteps      | 154624     |
| train/                  |            |
|    approx_kl            | 0.34428954 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.362      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.046     |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 59.4       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 152        |
|    time_elapsed         | 547        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.20169242 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.2       |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.362      |
|    value_loss           | 125        |
----------------------------------------
Num timesteps: 156000
Best mean reward: -132.07 - Last mean reward per episode: -165.63
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0703   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0203    |
| rollout/                |           |
|    ep_len_mean          | 37.9      |
|    ep_rew_mean          | -129      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 153       |
|    time_elapsed         | 551       |
|    total_timesteps      | 156672    |
| train/                  |           |
|    approx_kl            | 0.1484077 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.564     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 3040      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.362     |
|    value_loss           | 149       |
---------------------------------------
---------------------------------------
| reward                  | -3.75     |
| reward_contact          | -0.00154  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0355   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0214    |
| rollout/                |           |
|    ep_len_mean          | 48.2      |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 154       |
|    time_elapsed         | 554       |
|    total_timesteps      | 157696    |
| train/                  |           |
|    approx_kl            | 0.3438017 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 3060      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.362     |
|    value_loss           | 97.8      |
---------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00164   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0427    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0234     |
| rollout/                |            |
|    ep_len_mean          | 58.1       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 155        |
|    time_elapsed         | 558        |
|    total_timesteps      | 158720     |
| train/                  |            |
|    approx_kl            | 0.34817958 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.362      |
|    value_loss           | 92.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00151   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0355    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 156        |
|    time_elapsed         | 562        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.25933456 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.362      |
|    value_loss           | 64         |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0786    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | -73.3      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 157        |
|    time_elapsed         | 565        |
|    total_timesteps      | 160768     |
| train/                  |            |
|    approx_kl            | 0.20894763 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.224     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.62       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.362      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000868  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0759    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -72.3      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 158        |
|    time_elapsed         | 569        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.17032357 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.133      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.54       |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.362      |
|    value_loss           | 162        |
----------------------------------------
Num timesteps: 162000
Best mean reward: -132.07 - Last mean reward per episode: -72.31
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000868  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.066     |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 159        |
|    time_elapsed         | 572        |
|    total_timesteps      | 162816     |
| train/                  |            |
|    approx_kl            | 0.23768029 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.361      |
|    value_loss           | 134        |
----------------------------------------
---------------------------------------
| reward                  | -3.86     |
| reward_contact          | -0.0002   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0722   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.0171    |
| rollout/                |           |
|    ep_len_mean          | 29.3      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 160       |
|    time_elapsed         | 576       |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.3199792 |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.749     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 3180      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.361     |
|    value_loss           | 97.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | 0          |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0722    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 161        |
|    time_elapsed         | 579        |
|    total_timesteps      | 164864     |
| train/                  |            |
|    approx_kl            | 0.23922631 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.0291    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.36       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.361      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -8.68e-05  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.067     |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 40.9       |
|    ep_rew_mean          | -140       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 162        |
|    time_elapsed         | 583        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.31808227 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.4        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.361      |
|    value_loss           | 61.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0631    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 51.3       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 163        |
|    time_elapsed         | 586        |
|    total_timesteps      | 166912     |
| train/                  |            |
|    approx_kl            | 0.26486766 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.361      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0717    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 51.5       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 164        |
|    time_elapsed         | 590        |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.13627161 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 3260       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.361      |
|    value_loss           | 148        |
----------------------------------------
Num timesteps: 168000
Best mean reward: -72.31 - Last mean reward per episode: -173.58
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0717    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 61.7       |
|    ep_rew_mean          | -206       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 165        |
|    time_elapsed         | 593        |
|    total_timesteps      | 168960     |
| train/                  |            |
|    approx_kl            | 0.15843682 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.23       |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.361      |
|    value_loss           | 131        |
----------------------------------------
---------------------------------------
| reward                  | -3.85     |
| reward_contact          | -0.0012   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0695   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.022     |
| rollout/                |           |
|    ep_len_mean          | 60.8      |
|    ep_rew_mean          | -202      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 166       |
|    time_elapsed         | 597       |
|    total_timesteps      | 169984    |
| train/                  |           |
|    approx_kl            | 0.5230099 |
|    clip_fraction        | 0.533     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 3300      |
|    policy_gradient_loss | -0.141    |
|    std                  | 0.361     |
|    value_loss           | 63.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 167        |
|    time_elapsed         | 600        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.45631516 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.361      |
|    value_loss           | 182        |
----------------------------------------
---------------------------------------
| reward                  | -3.82     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0826   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 25.5      |
|    ep_rew_mean          | -90.5     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 168       |
|    time_elapsed         | 604       |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.1938377 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 3340      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.361     |
|    value_loss           | 175       |
---------------------------------------
---------------------------------------
| reward                  | -3.82     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0826   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0119    |
| rollout/                |           |
|    ep_len_mean          | 35.1      |
|    ep_rew_mean          | -121      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 169       |
|    time_elapsed         | 607       |
|    total_timesteps      | 173056    |
| train/                  |           |
|    approx_kl            | 0.3627219 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.32      |
|    n_updates            | 3360      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.361     |
|    value_loss           | 144       |
---------------------------------------
Num timesteps: 174000
Best mean reward: -72.31 - Last mean reward per episode: -152.75
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0806    |
| reward_torque           | -3.67      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 45         |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 170        |
|    time_elapsed         | 611        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.50357187 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.361      |
|    value_loss           | 84.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0711    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 171        |
|    time_elapsed         | 615        |
|    total_timesteps      | 175104     |
| train/                  |            |
|    approx_kl            | 0.13590036 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.3       |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.361      |
|    value_loss           | 324        |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0817    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 49.3       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 172        |
|    time_elapsed         | 618        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.17257623 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.25       |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.361      |
|    value_loss           | 156        |
----------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00181  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 60.5      |
|    ep_rew_mean          | -203      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 173       |
|    time_elapsed         | 622       |
|    total_timesteps      | 177152    |
| train/                  |           |
|    approx_kl            | 0.2123828 |
|    clip_fraction        | 0.407     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.237     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.57      |
|    n_updates            | 3440      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.361     |
|    value_loss           | 204       |
---------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00181  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0231    |
| rollout/                |           |
|    ep_len_mean          | 70.5      |
|    ep_rew_mean          | -234      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 174       |
|    time_elapsed         | 625       |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.3664076 |
|    clip_fraction        | 0.492     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.673     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.34      |
|    n_updates            | 3460      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.361     |
|    value_loss           | 86        |
---------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00181   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0714    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 69.2       |
|    ep_rew_mean          | -229       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 175        |
|    time_elapsed         | 629        |
|    total_timesteps      | 179200     |
| train/                  |            |
|    approx_kl            | 0.21481404 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.71       |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.361      |
|    value_loss           | 78.4       |
----------------------------------------
Num timesteps: 180000
Best mean reward: -72.31 - Last mean reward per episode: -261.22
---------------------------------------
| reward                  | -3.75     |
| reward_contact          | -0.0017   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0717   |
| reward_torque           | -3.6      |
| reward_velocity         | 0.0255    |
| rollout/                |           |
|    ep_len_mean          | 79.3      |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 176       |
|    time_elapsed         | 633       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.2458967 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.685     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.47      |
|    n_updates            | 3500      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.361     |
|    value_loss           | 96.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0694    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 69.1       |
|    ep_rew_mean          | -229       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 177        |
|    time_elapsed         | 636        |
|    total_timesteps      | 181248     |
| train/                  |            |
|    approx_kl            | 0.20583606 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.37       |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.361      |
|    value_loss           | 118        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 68.7       |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 178        |
|    time_elapsed         | 640        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.23189592 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.66       |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.361      |
|    value_loss           | 115        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.0013    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0653    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 78.7       |
|    ep_rew_mean          | -259       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 179        |
|    time_elapsed         | 643        |
|    total_timesteps      | 183296     |
| train/                  |            |
|    approx_kl            | 0.16957897 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.27       |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.361      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0713    |
| reward_torque           | -3.69      |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 48.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 180        |
|    time_elapsed         | 647        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.34666207 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.06       |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.361      |
|    value_loss           | 76.3       |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0816   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.00964   |
| rollout/                |           |
|    ep_len_mean          | 29.1      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 181       |
|    time_elapsed         | 650       |
|    total_timesteps      | 185344    |
| train/                  |           |
|    approx_kl            | 0.2027188 |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.00364   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.55      |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.361     |
|    value_loss           | 189       |
---------------------------------------
Num timesteps: 186000
Best mean reward: -72.31 - Last mean reward per episode: -102.37
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0104     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 182        |
|    time_elapsed         | 654        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.25482127 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.361      |
|    value_loss           | 174        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.00971    |
| rollout/                |            |
|    ep_len_mean          | 38.9       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 183        |
|    time_elapsed         | 658        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.34795552 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.93       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.361      |
|    value_loss           | 125        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00181   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 184        |
|    time_elapsed         | 661        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.39554232 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.361      |
|    value_loss           | 53.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0939    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 185        |
|    time_elapsed         | 665        |
|    total_timesteps      | 189440     |
| train/                  |            |
|    approx_kl            | 0.22385332 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | -0.00799   |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.361      |
|    value_loss           | 172        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0869    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 30.8       |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 186        |
|    time_elapsed         | 668        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.25057673 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.88       |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.361      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00234   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0931    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.011      |
| rollout/                |            |
|    ep_len_mean          | 31.2       |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 187        |
|    time_elapsed         | 672        |
|    total_timesteps      | 191488     |
| train/                  |            |
|    approx_kl            | 0.24711326 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.62       |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.36       |
|    value_loss           | 118        |
----------------------------------------
Num timesteps: 192000
Best mean reward: -72.31 - Last mean reward per episode: -136.37
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00174  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0902   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0126    |
| rollout/                |           |
|    ep_len_mean          | 36.9      |
|    ep_rew_mean          | -127      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 188       |
|    time_elapsed         | 675       |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.5058031 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 3740      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.36      |
|    value_loss           | 56.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00174   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0902    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 36.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 189        |
|    time_elapsed         | 679        |
|    total_timesteps      | 193536     |
| train/                  |            |
|    approx_kl            | 0.31118467 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.36       |
|    value_loss           | 131        |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0764   |
| reward_torque           | -3.73     |
| reward_velocity         | 0.0174    |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -160      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 190       |
|    time_elapsed         | 683       |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.3108564 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 3780      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.36      |
|    value_loss           | 53.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0633    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 36.7       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 191        |
|    time_elapsed         | 686        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.27092522 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.07       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.36       |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0598    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 192        |
|    time_elapsed         | 690        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.23524715 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.42       |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.36       |
|    value_loss           | 132        |
----------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00304   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 39.4       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 193        |
|    time_elapsed         | 693        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.24884942 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.38       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.36       |
|    value_loss           | 99.3       |
----------------------------------------
Num timesteps: 198000
Best mean reward: -72.31 - Last mean reward per episode: -135.46
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.0028    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0531    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48.7       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 194        |
|    time_elapsed         | 697        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.20364907 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.63       |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.36       |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00302   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0531    |
| reward_torque           | -3.69      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 195        |
|    time_elapsed         | 701        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.22442748 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.8       |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.36       |
|    value_loss           | 84.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00277   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0884    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 40.2       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 196        |
|    time_elapsed         | 704        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.24759886 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.36       |
|    value_loss           | 83.9       |
----------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0798   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 50.2      |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 197       |
|    time_elapsed         | 708       |
|    total_timesteps      | 201728    |
| train/                  |           |
|    approx_kl            | 0.3088709 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 3920      |
|    policy_gradient_loss | -0.213    |
|    std                  | 0.36      |
|    value_loss           | 179       |
---------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0798   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 60.2      |
|    ep_rew_mean          | -203      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 198       |
|    time_elapsed         | 711       |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.3405066 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.6       |
|    n_updates            | 3940      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.36      |
|    value_loss           | 86.8      |
---------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00187  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0698   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 38.4      |
|    ep_rew_mean          | -132      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 199       |
|    time_elapsed         | 715       |
|    total_timesteps      | 203776    |
| train/                  |           |
|    approx_kl            | 0.2285565 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.803     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.4      |
|    n_updates            | 3960      |
|    policy_gradient_loss | -0.166    |
|    std                  | 0.36      |
|    value_loss           | 106       |
---------------------------------------
Num timesteps: 204000
Best mean reward: -72.31 - Last mean reward per episode: -132.32
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 200        |
|    time_elapsed         | 719        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.21328148 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.03       |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.36       |
|    value_loss           | 197        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0594    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 55.6       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 201        |
|    time_elapsed         | 722        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.30242336 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.36       |
|    value_loss           | 157        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00197   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0119     |
| rollout/                |            |
|    ep_len_mean          | 36.8       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 202        |
|    time_elapsed         | 726        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.17908296 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.57       |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.36       |
|    value_loss           | 79.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 203        |
|    time_elapsed         | 729        |
|    total_timesteps      | 207872     |
| train/                  |            |
|    approx_kl            | 0.25434375 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.45       |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.36       |
|    value_loss           | 220        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00185  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0724   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0173    |
| rollout/                |           |
|    ep_len_mean          | 57.2      |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 204       |
|    time_elapsed         | 733       |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.2696988 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.36      |
|    value_loss           | 117       |
---------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00161 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0643  |
| reward_torque           | -3.81    |
| reward_velocity         | 0.0178   |
| rollout/                |          |
|    ep_len_mean          | 67.4     |
|    ep_rew_mean          | -228     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 205      |
|    time_elapsed         | 737      |
|    total_timesteps      | 209920   |
| train/                  |          |
|    approx_kl            | 0.296696 |
|    clip_fraction        | 0.479    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.4    |
|    explained_variance   | 0.725    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.71     |
|    n_updates            | 4080     |
|    policy_gradient_loss | -0.164   |
|    std                  | 0.36     |
|    value_loss           | 63.4     |
--------------------------------------
Num timesteps: 210000
Best mean reward: -72.31 - Last mean reward per episode: -227.87
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0597    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 50.2       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 206        |
|    time_elapsed         | 740        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.25137722 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.36       |
|    value_loss           | 99.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0606    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | -69.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 207        |
|    time_elapsed         | 744        |
|    total_timesteps      | 211968     |
| train/                  |            |
|    approx_kl            | 0.18408452 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.36       |
|    value_loss           | 281        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0601    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0122     |
| rollout/                |            |
|    ep_len_mean          | 18.1       |
|    ep_rew_mean          | -67        |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 208        |
|    time_elapsed         | 748        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.18281552 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.143      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.08       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.36       |
|    value_loss           | 215        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.066     |
| reward_torque           | -3.72      |
| reward_velocity         | 0.011      |
| rollout/                |            |
|    ep_len_mean          | 27.8       |
|    ep_rew_mean          | -97.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 209        |
|    time_elapsed         | 751        |
|    total_timesteps      | 214016     |
| train/                  |            |
|    approx_kl            | 0.27804562 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.36       |
|    value_loss           | 107        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 27.3       |
|    ep_rew_mean          | -95.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 210        |
|    time_elapsed         | 755        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.28606674 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.6        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.36       |
|    value_loss           | 207        |
----------------------------------------
Num timesteps: 216000
Best mean reward: -72.31 - Last mean reward per episode: -129.44
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0761    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.00994    |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 211        |
|    time_elapsed         | 758        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.28486478 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.3        |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.36       |
|    value_loss           | 137        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00157   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0687    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.00989    |
| rollout/                |            |
|    ep_len_mean          | 37.8       |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 212        |
|    time_elapsed         | 762        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.31958354 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.36       |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0731    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.00742    |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 213        |
|    time_elapsed         | 766        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.30686766 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.57       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.36       |
|    value_loss           | 95.5       |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00158  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0672   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.00805   |
| rollout/                |           |
|    ep_len_mean          | 46.3      |
|    ep_rew_mean          | -158      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 214       |
|    time_elapsed         | 769       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.3711713 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.8       |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.36      |
|    value_loss           | 71.3      |
---------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00134  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0626   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0122    |
| rollout/                |           |
|    ep_len_mean          | 56.2      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 215       |
|    time_elapsed         | 773       |
|    total_timesteps      | 220160    |
| train/                  |           |
|    approx_kl            | 0.5033884 |
|    clip_fraction        | 0.561     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.799     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.48      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.36      |
|    value_loss           | 82.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.0012   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0654   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0147    |
| rollout/                |           |
|    ep_len_mean          | 66.1      |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 216       |
|    time_elapsed         | 776       |
|    total_timesteps      | 221184    |
| train/                  |           |
|    approx_kl            | 0.2191947 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.8     |
|    explained_variance   | 0.197     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 4300      |
|    policy_gradient_loss | -0.129    |
|    std                  | 0.36      |
|    value_loss           | 84.3      |
---------------------------------------
Num timesteps: 222000
Best mean reward: -72.31 - Last mean reward per episode: -256.36
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0557    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 67.6       |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 217        |
|    time_elapsed         | 780        |
|    total_timesteps      | 222208     |
| train/                  |            |
|    approx_kl            | 0.36121154 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.36       |
|    value_loss           | 71.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.078     |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 218        |
|    time_elapsed         | 783        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.32178152 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.89       |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.36       |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0736    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 219        |
|    time_elapsed         | 787        |
|    total_timesteps      | 224256     |
| train/                  |            |
|    approx_kl            | 0.27824563 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.18       |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.36       |
|    value_loss           | 243        |
----------------------------------------
--------------------------------------
| reward                  | -3.89    |
| reward_contact          | -0.00169 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0781  |
| reward_torque           | -3.72    |
| reward_velocity         | 0.0101   |
| rollout/                |          |
|    ep_len_mean          | 38.3     |
|    ep_rew_mean          | -132     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 220      |
|    time_elapsed         | 791      |
|    total_timesteps      | 225280   |
| train/                  |          |
|    approx_kl            | 0.314102 |
|    clip_fraction        | 0.492    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.614    |
|    learning_rate        | 0.0003   |
|    loss                 | 7.01     |
|    n_updates            | 4380     |
|    policy_gradient_loss | -0.182   |
|    std                  | 0.36     |
|    value_loss           | 93.8     |
--------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0921    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 38.6       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 221        |
|    time_elapsed         | 794        |
|    total_timesteps      | 226304     |
| train/                  |            |
|    approx_kl            | 0.22075033 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.52       |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.36       |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48.2       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 222        |
|    time_elapsed         | 798        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.27911896 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.09       |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.36       |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 228000
Best mean reward: -72.31 - Last mean reward per episode: -164.31
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 223        |
|    time_elapsed         | 802        |
|    total_timesteps      | 228352     |
| train/                  |            |
|    approx_kl            | 0.45910394 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.36       |
|    value_loss           | 86.2       |
----------------------------------------
--------------------------------------
| reward                  | -3.93    |
| reward_contact          | -0.00165 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0837  |
| reward_torque           | -3.76    |
| reward_velocity         | 0.0148   |
| rollout/                |          |
|    ep_len_mean          | 51.8     |
|    ep_rew_mean          | -177     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 224      |
|    time_elapsed         | 805      |
|    total_timesteps      | 229376   |
| train/                  |          |
|    approx_kl            | 0.622482 |
|    clip_fraction        | 0.552    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.888    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.33     |
|    n_updates            | 4460     |
|    policy_gradient_loss | -0.163   |
|    std                  | 0.36     |
|    value_loss           | 42.4     |
--------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00197   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0872    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 41.6       |
|    ep_rew_mean          | -144       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 225        |
|    time_elapsed         | 809        |
|    total_timesteps      | 230400     |
| train/                  |            |
|    approx_kl            | 0.40062478 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.36       |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0846    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 52.2       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 226        |
|    time_elapsed         | 813        |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.27791643 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.24       |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.359      |
|    value_loss           | 113        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0881    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 51.5       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 227        |
|    time_elapsed         | 816        |
|    total_timesteps      | 232448     |
| train/                  |            |
|    approx_kl            | 0.34172493 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.26       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.359      |
|    value_loss           | 121        |
----------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0825   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 38.5      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 228       |
|    time_elapsed         | 820       |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.4931413 |
|    clip_fraction        | 0.566     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.345     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.7       |
|    n_updates            | 4540      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.359     |
|    value_loss           | 98.2      |
---------------------------------------
Num timesteps: 234000
Best mean reward: -72.31 - Last mean reward per episode: -101.59
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.000744  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0851    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 28.7       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 229        |
|    time_elapsed         | 823        |
|    total_timesteps      | 234496     |
| train/                  |            |
|    approx_kl            | 0.27011216 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.249     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.73       |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.359      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000761  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0848    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 29         |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 230        |
|    time_elapsed         | 827        |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.39494225 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.359      |
|    value_loss           | 84.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 38.8       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 231        |
|    time_elapsed         | 830        |
|    total_timesteps      | 236544     |
| train/                  |            |
|    approx_kl            | 0.29833746 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0003     |
|    loss                 | 4          |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.359      |
|    value_loss           | 158        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.000896  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.083     |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 38.6       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 232        |
|    time_elapsed         | 834        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.34127718 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.77       |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.359      |
|    value_loss           | 166        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00116   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0738    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 49         |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 233        |
|    time_elapsed         | 838        |
|    total_timesteps      | 238592     |
| train/                  |            |
|    approx_kl            | 0.37395412 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.359      |
|    value_loss           | 87.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.0016   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0762   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 28.8      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 234       |
|    time_elapsed         | 841       |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.3462485 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 4660      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.359     |
|    value_loss           | 124       |
---------------------------------------
Num timesteps: 240000
Best mean reward: -72.31 - Last mean reward per episode: -102.14
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.0016    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0762    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 28.9       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 235        |
|    time_elapsed         | 845        |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.27733186 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.0923     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.13       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.359      |
|    value_loss           | 136        |
----------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00172  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0922   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0174    |
| rollout/                |           |
|    ep_len_mean          | 27.8      |
|    ep_rew_mean          | -98.1     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 236       |
|    time_elapsed         | 849       |
|    total_timesteps      | 241664    |
| train/                  |           |
|    approx_kl            | 0.3557347 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.15      |
|    n_updates            | 4700      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.359     |
|    value_loss           | 82.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.87     |
| reward_contact          | -0.00118  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0874   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 34.7      |
|    ep_rew_mean          | -120      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 237       |
|    time_elapsed         | 852       |
|    total_timesteps      | 242688    |
| train/                  |           |
|    approx_kl            | 0.5407515 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 4720      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.359     |
|    value_loss           | 118       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00118   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0807    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0186     |
| rollout/                |            |
|    ep_len_mean          | 44.6       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 238        |
|    time_elapsed         | 856        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.34348375 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.91       |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.359      |
|    value_loss           | 240        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00114   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0778    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0123     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 239        |
|    time_elapsed         | 860        |
|    total_timesteps      | 244736     |
| train/                  |            |
|    approx_kl            | 0.27531302 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.8        |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.359      |
|    value_loss           | 99.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00096   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | -67.9      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 240        |
|    time_elapsed         | 864        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.19908765 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.0101     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.64       |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.359      |
|    value_loss           | 210        |
----------------------------------------
Num timesteps: 246000
Best mean reward: -72.31 - Last mean reward per episode: -68.51
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0111    |
| rollout/                |           |
|    ep_len_mean          | 18.8      |
|    ep_rew_mean          | -69.7     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 241       |
|    time_elapsed         | 868       |
|    total_timesteps      | 246784    |
| train/                  |           |
|    approx_kl            | 0.4209903 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.95      |
|    n_updates            | 4800      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.359     |
|    value_loss           | 158       |
---------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0988    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 31.9       |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 242        |
|    time_elapsed         | 871        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.38348752 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.81       |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.359      |
|    value_loss           | 97.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0988    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 32.3       |
|    ep_rew_mean          | -113       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 243        |
|    time_elapsed         | 875        |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.50151694 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.24       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.359      |
|    value_loss           | 101        |
----------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00109 |
| reward_ctrl             | -0.0944  |
| reward_motion           | -0.0926  |
| reward_torque           | -3.78    |
| reward_velocity         | 0.0173   |
| rollout/                |          |
|    ep_len_mean          | 42.4     |
|    ep_rew_mean          | -145     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 244      |
|    time_elapsed         | 879      |
|    total_timesteps      | 249856   |
| train/                  |          |
|    approx_kl            | 0.754702 |
|    clip_fraction        | 0.567    |
|    clip_range           | 0.4      |
|    entropy_loss         | -20.8    |
|    explained_variance   | 0.784    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.86     |
|    n_updates            | 4860     |
|    policy_gradient_loss | -0.179   |
|    std                  | 0.359    |
|    value_loss           | 61.2     |
--------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00127   |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.089     |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 52.2       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 245        |
|    time_elapsed         | 883        |
|    total_timesteps      | 250880     |
| train/                  |            |
|    approx_kl            | 0.33218917 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.359      |
|    value_loss           | 56.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00151  |
| reward_ctrl             | -0.0944   |
| reward_motion           | -0.089    |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0191    |
| rollout/                |           |
|    ep_len_mean          | 61.2      |
|    ep_rew_mean          | -204      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 246       |
|    time_elapsed         | 886       |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.2645834 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.52      |
|    n_updates            | 4900      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.359     |
|    value_loss           | 125       |
---------------------------------------
Num timesteps: 252000
Best mean reward: -68.51 - Last mean reward per episode: -204.44
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0774    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 69.6       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 247        |
|    time_elapsed         | 890        |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.46001226 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62       |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.359      |
|    value_loss           | 107        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.0022    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0682    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 248        |
|    time_elapsed         | 894        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.53318405 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.359      |
|    value_loss           | 124        |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00242  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0744   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 35.7      |
|    ep_rew_mean          | -123      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 249       |
|    time_elapsed         | 897       |
|    total_timesteps      | 254976    |
| train/                  |           |
|    approx_kl            | 0.4965455 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.805     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 4960      |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.359     |
|    value_loss           | 93.6      |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00209  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0774   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0115    |
| rollout/                |           |
|    ep_len_mean          | 33.3      |
|    ep_rew_mean          | -116      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 250       |
|    time_elapsed         | 901       |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.2736624 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.2       |
|    n_updates            | 4980      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.359     |
|    value_loss           | 194       |
---------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00191  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0728   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0122    |
| rollout/                |           |
|    ep_len_mean          | 43.4      |
|    ep_rew_mean          | -148      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 251       |
|    time_elapsed         | 905       |
|    total_timesteps      | 257024    |
| train/                  |           |
|    approx_kl            | 0.4548053 |
|    clip_fraction        | 0.524     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.792     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.16      |
|    n_updates            | 5000      |
|    policy_gradient_loss | -0.165    |
|    std                  | 0.359     |
|    value_loss           | 73.2      |
---------------------------------------
Num timesteps: 258000
Best mean reward: -68.51 - Last mean reward per episode: -148.24
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00208  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0694   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 53.5      |
|    ep_rew_mean          | -180      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 252       |
|    time_elapsed         | 908       |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.1721398 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.4      |
|    n_updates            | 5020      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.359     |
|    value_loss           | 160       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00139   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0816    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 253        |
|    time_elapsed         | 912        |
|    total_timesteps      | 259072     |
| train/                  |            |
|    approx_kl            | 0.30751303 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.438      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.24       |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.359      |
|    value_loss           | 77.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00067   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0718    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 31.7       |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 254        |
|    time_elapsed         | 916        |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.33187163 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.69       |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.359      |
|    value_loss           | 202        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.0021    |
| reward_ctrl             | -0.0894    |
| reward_motion           | -0.0801    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | -70        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 255        |
|    time_elapsed         | 920        |
|    total_timesteps      | 261120     |
| train/                  |            |
|    approx_kl            | 0.23440772 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.16       |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00188   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0829    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 15.2       |
|    ep_rew_mean          | -57.1      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 256        |
|    time_elapsed         | 924        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.22768447 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.0481    |
|    learning_rate        | 0.0003     |
|    loss                 | 6.87       |
|    n_updates            | 5100       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.358      |
|    value_loss           | 226        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00971    |
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | -63        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 257        |
|    time_elapsed         | 927        |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.26715848 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | -0.0852    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.21       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 144        |
----------------------------------------
Num timesteps: 264000
Best mean reward: -68.51 - Last mean reward per episode: -63.38
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00224  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0748   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.01      |
| rollout/                |           |
|    ep_len_mean          | 17        |
|    ep_rew_mean          | -63.4     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 258       |
|    time_elapsed         | 930       |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.3342979 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 5140      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.358     |
|    value_loss           | 128       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0622    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 26.9       |
|    ep_rew_mean          | -95.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 259        |
|    time_elapsed         | 933        |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.42672575 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.358      |
|    value_loss           | 113        |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00203  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0562   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0163    |
| rollout/                |           |
|    ep_len_mean          | 27        |
|    ep_rew_mean          | -95.8     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 260       |
|    time_elapsed         | 936       |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.2800152 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.71      |
|    n_updates            | 5180      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.358     |
|    value_loss           | 129       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0562    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 27         |
|    ep_rew_mean          | -95.7      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 261        |
|    time_elapsed         | 939        |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.27896422 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.048      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.42       |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.358      |
|    value_loss           | 132        |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00206  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0644   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 27.4      |
|    ep_rew_mean          | -97.4     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 262       |
|    time_elapsed         | 942       |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.3802692 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.01      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.358     |
|    value_loss           | 96.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0727    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 27.7       |
|    ep_rew_mean          | -98.6      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 263        |
|    time_elapsed         | 945        |
|    total_timesteps      | 269312     |
| train/                  |            |
|    approx_kl            | 0.23873353 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.28       |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.358      |
|    value_loss           | 168        |
----------------------------------------
Num timesteps: 270000
Best mean reward: -63.38 - Last mean reward per episode: -132.70
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00199   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0121     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 264        |
|    time_elapsed         | 948        |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.41716772 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.32       |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.358      |
|    value_loss           | 61.9       |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0593   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0116    |
| rollout/                |           |
|    ep_len_mean          | 48        |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 265       |
|    time_elapsed         | 952       |
|    total_timesteps      | 271360    |
| train/                  |           |
|    approx_kl            | 0.5198351 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.662     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.42      |
|    n_updates            | 5280      |
|    policy_gradient_loss | -0.191    |
|    std                  | 0.358     |
|    value_loss           | 107       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0593    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 58.7       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 266        |
|    time_elapsed         | 955        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.58499473 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.229     |
|    std                  | 0.358      |
|    value_loss           | 113        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00101   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.00926    |
| rollout/                |            |
|    ep_len_mean          | 48.3       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 267        |
|    time_elapsed         | 958        |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.22189789 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.9       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.358      |
|    value_loss           | 161        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.000798  |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0582    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0117     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 268        |
|    time_elapsed         | 961        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.34347674 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.45       |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.358      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.000798  |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0582    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0119     |
| rollout/                |            |
|    ep_len_mean          | 57.6       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 269        |
|    time_elapsed         | 964        |
|    total_timesteps      | 275456     |
| train/                  |            |
|    approx_kl            | 0.31875604 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.66       |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.358      |
|    value_loss           | 174        |
----------------------------------------
Num timesteps: 276000
Best mean reward: -63.38 - Last mean reward per episode: -161.01
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0723    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 270        |
|    time_elapsed         | 967        |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.23767808 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.358      |
|    value_loss           | 101        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0653    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 37.2       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 271        |
|    time_elapsed         | 970        |
|    total_timesteps      | 277504     |
| train/                  |            |
|    approx_kl            | 0.49354082 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 5400       |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.358      |
|    value_loss           | 78.9       |
----------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00189 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0808  |
| reward_torque           | -3.78    |
| reward_velocity         | 0.0126   |
| rollout/                |          |
|    ep_len_mean          | 15.9     |
|    ep_rew_mean          | -59.5    |
| time/                   |          |
|    fps                  | 286      |
|    iterations           | 272      |
|    time_elapsed         | 973      |
|    total_timesteps      | 278528   |
| train/                  |          |
|    approx_kl            | 0.680837 |
|    clip_fraction        | 0.608    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.6    |
|    explained_variance   | 0.769    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.952    |
|    n_updates            | 5420     |
|    policy_gradient_loss | -0.223   |
|    std                  | 0.358    |
|    value_loss           | 97.4     |
--------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00179   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 17.4       |
|    ep_rew_mean          | -64.9      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 273        |
|    time_elapsed         | 976        |
|    total_timesteps      | 279552     |
| train/                  |            |
|    approx_kl            | 0.29229861 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.0217     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.19       |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.358      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00898    |
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | -69.1      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 274        |
|    time_elapsed         | 979        |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.26341864 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.358      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.00861    |
| rollout/                |            |
|    ep_len_mean          | 24.1       |
|    ep_rew_mean          | -86.8      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 275        |
|    time_elapsed         | 982        |
|    total_timesteps      | 281600     |
| train/                  |            |
|    approx_kl            | 0.48075184 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.303      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.358      |
|    value_loss           | 117        |
----------------------------------------
Num timesteps: 282000
Best mean reward: -63.38 - Last mean reward per episode: -86.50
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.00867    |
| rollout/                |            |
|    ep_len_mean          | 24         |
|    ep_rew_mean          | -86.5      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 276        |
|    time_elapsed         | 985        |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.57373464 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.358      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000824  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0861    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.00921    |
| rollout/                |            |
|    ep_len_mean          | 33.5       |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 277        |
|    time_elapsed         | 988        |
|    total_timesteps      | 283648     |
| train/                  |            |
|    approx_kl            | 0.52857876 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.358      |
|    value_loss           | 56.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.000824  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.08      |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0093     |
| rollout/                |            |
|    ep_len_mean          | 43.6       |
|    ep_rew_mean          | -151       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 278        |
|    time_elapsed         | 992        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.39017332 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.358      |
|    value_loss           | 110        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 53.3       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 279        |
|    time_elapsed         | 995        |
|    total_timesteps      | 285696     |
| train/                  |            |
|    approx_kl            | 0.43813702 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.15       |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.358      |
|    value_loss           | 73.1       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00123  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0682   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 62.5      |
|    ep_rew_mean          | -213      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 280       |
|    time_elapsed         | 998       |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.5980768 |
|    clip_fraction        | 0.626     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.66      |
|    n_updates            | 5580      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.358     |
|    value_loss           | 130       |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0657    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 72.6       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 281        |
|    time_elapsed         | 1001       |
|    total_timesteps      | 287744     |
| train/                  |            |
|    approx_kl            | 0.32057574 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.61       |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 189        |
----------------------------------------
Num timesteps: 288000
Best mean reward: -63.38 - Last mean reward per episode: -248.01
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0657   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0183    |
| rollout/                |           |
|    ep_len_mean          | 74        |
|    ep_rew_mean          | -251      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 282       |
|    time_elapsed         | 1004      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.9688711 |
|    clip_fraction        | 0.622     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.857     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.94      |
|    n_updates            | 5620      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.358     |
|    value_loss           | 65.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0512    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 283        |
|    time_elapsed         | 1007       |
|    total_timesteps      | 289792     |
| train/                  |            |
|    approx_kl            | 0.34954274 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.358      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0546    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 284        |
|    time_elapsed         | 1010       |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.23329642 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.61       |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00175   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0661    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 56.1       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 285        |
|    time_elapsed         | 1013       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.28268188 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.357      |
|    value_loss           | 87         |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00164  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0686   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0157    |
| rollout/                |           |
|    ep_len_mean          | 46.8      |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 286       |
|    time_elapsed         | 1016      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.4226592 |
|    clip_fraction        | 0.509     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 5700      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.357     |
|    value_loss           | 146       |
---------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0633    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 287        |
|    time_elapsed         | 1019       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.15495507 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.7       |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.357      |
|    value_loss           | 257        |
----------------------------------------
Num timesteps: 294000
Best mean reward: -63.38 - Last mean reward per episode: -194.91
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00167   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.074     |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 55.1       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 288        |
|    time_elapsed         | 1023       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.18868396 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 5740       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00158  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0909   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0149    |
| rollout/                |           |
|    ep_len_mean          | 37.5      |
|    ep_rew_mean          | -132      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 289       |
|    time_elapsed         | 1026      |
|    total_timesteps      | 295936    |
| train/                  |           |
|    approx_kl            | 0.4144636 |
|    clip_fraction        | 0.572     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.07      |
|    n_updates            | 5760      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.357     |
|    value_loss           | 138       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0879    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -104       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 290        |
|    time_elapsed         | 1030       |
|    total_timesteps      | 296960     |
| train/                  |            |
|    approx_kl            | 0.37940833 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.105      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 5780       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
--------------------------------------
| reward                  | -3.93    |
| reward_contact          | -0.00172 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0889  |
| reward_torque           | -3.76    |
| reward_velocity         | 0.0128   |
| rollout/                |          |
|    ep_len_mean          | 28.8     |
|    ep_rew_mean          | -103     |
| time/                   |          |
|    fps                  | 288      |
|    iterations           | 291      |
|    time_elapsed         | 1034     |
|    total_timesteps      | 297984   |
| train/                  |          |
|    approx_kl            | 0.722342 |
|    clip_fraction        | 0.559    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.4    |
|    explained_variance   | 0.886    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.13     |
|    n_updates            | 5800     |
|    policy_gradient_loss | -0.194   |
|    std                  | 0.357    |
|    value_loss           | 82.1     |
--------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00166  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0857   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0169    |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 292       |
|    time_elapsed         | 1038      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.7147992 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.899     |
|    n_updates            | 5820      |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.357     |
|    value_loss           | 49.8      |
---------------------------------------
Num timesteps: 300000
Best mean reward: -63.38 - Last mean reward per episode: -172.00
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00142   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0799    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 49.8       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 293        |
|    time_elapsed         | 1041       |
|    total_timesteps      | 300032     |
| train/                  |            |
|    approx_kl            | 0.30227596 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.357      |
|    value_loss           | 134        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000719  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0882    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 37.2       |
|    ep_rew_mean          | -130       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 294        |
|    time_elapsed         | 1045       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.24325319 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.05       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.357      |
|    value_loss           | 227        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00024  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0838   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0143    |
| rollout/                |           |
|    ep_len_mean          | 27.8      |
|    ep_rew_mean          | -99.4     |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 295       |
|    time_elapsed         | 1049      |
|    total_timesteps      | 302080    |
| train/                  |           |
|    approx_kl            | 0.3370017 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | -0.12     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.6       |
|    n_updates            | 5880      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.357     |
|    value_loss           | 198       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000649  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0796    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0117     |
| rollout/                |            |
|    ep_len_mean          | 26.1       |
|    ep_rew_mean          | -92.8      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 296        |
|    time_elapsed         | 1052       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.24127942 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.36       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.357      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00105   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0756    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 36.6       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 297        |
|    time_elapsed         | 1056       |
|    total_timesteps      | 304128     |
| train/                  |            |
|    approx_kl            | 0.34115908 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.99       |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.357      |
|    value_loss           | 177        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0776    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 25.9       |
|    ep_rew_mean          | -92.2      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 298        |
|    time_elapsed         | 1060       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.19175299 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.93       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.357      |
|    value_loss           | 136        |
----------------------------------------
Num timesteps: 306000
Best mean reward: -63.38 - Last mean reward per episode: -125.45
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0745   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0144    |
| rollout/                |           |
|    ep_len_mean          | 36        |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 299       |
|    time_elapsed         | 1063      |
|    total_timesteps      | 306176    |
| train/                  |           |
|    approx_kl            | 0.3205521 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 5960      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.357     |
|    value_loss           | 140       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 300        |
|    time_elapsed         | 1067       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.28683978 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.357      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00153   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0738    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 38         |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 301        |
|    time_elapsed         | 1071       |
|    total_timesteps      | 308224     |
| train/                  |            |
|    approx_kl            | 0.21886161 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 6000       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.357      |
|    value_loss           | 164        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.0015   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0705   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 48.2      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 302       |
|    time_elapsed         | 1075      |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.5459168 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.718     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 6020      |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.357     |
|    value_loss           | 164       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00245   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0675    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 303        |
|    time_elapsed         | 1078       |
|    total_timesteps      | 310272     |
| train/                  |            |
|    approx_kl            | 0.75299984 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.357      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00215  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0724   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 304       |
|    time_elapsed         | 1082      |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.7593733 |
|    clip_fraction        | 0.612     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 6060      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.357     |
|    value_loss           | 119       |
---------------------------------------
Num timesteps: 312000
Best mean reward: -63.38 - Last mean reward per episode: -135.54
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0724    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 48.5       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 305        |
|    time_elapsed         | 1086       |
|    total_timesteps      | 312320     |
| train/                  |            |
|    approx_kl            | 0.98770946 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 6080       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.357      |
|    value_loss           | 95.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00195   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0724    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 306        |
|    time_elapsed         | 1089       |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.26876247 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.48       |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.357      |
|    value_loss           | 72.3       |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00161   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0174     |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 307        |
|    time_elapsed         | 1093       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.55357516 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.675      |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.357      |
|    value_loss           | 39         |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00223  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0779   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0219    |
| rollout/                |           |
|    ep_len_mean          | 58.1      |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 308       |
|    time_elapsed         | 1096      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.3940823 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.955     |
|    n_updates            | 6140      |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.357     |
|    value_loss           | 205       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00195   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.07      |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 68.2       |
|    ep_rew_mean          | -232       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 309        |
|    time_elapsed         | 1100       |
|    total_timesteps      | 316416     |
| train/                  |            |
|    approx_kl            | 0.22938171 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.357      |
|    value_loss           | 161        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00207   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0837    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 310        |
|    time_elapsed         | 1104       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.20780717 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.357      |
|    value_loss           | 112        |
----------------------------------------
Num timesteps: 318000
Best mean reward: -63.38 - Last mean reward per episode: -230.59
--------------------------------------
| reward                  | -3.94    |
| reward_contact          | -0.00228 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0656  |
| reward_torque           | -3.79    |
| reward_velocity         | 0.0162   |
| rollout/                |          |
|    ep_len_mean          | 57.7     |
|    ep_rew_mean          | -198     |
| time/                   |          |
|    fps                  | 287      |
|    iterations           | 311      |
|    time_elapsed         | 1107     |
|    total_timesteps      | 318464   |
| train/                  |          |
|    approx_kl            | 0.817657 |
|    clip_fraction        | 0.556    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22      |
|    explained_variance   | 0.865    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.08     |
|    n_updates            | 6200     |
|    policy_gradient_loss | -0.214   |
|    std                  | 0.357    |
|    value_loss           | 98.7     |
--------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00172   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 60.1       |
|    ep_rew_mean          | -206       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 312        |
|    time_elapsed         | 1111       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.30783203 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.357      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 60.6       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 313        |
|    time_elapsed         | 1114       |
|    total_timesteps      | 320512     |
| train/                  |            |
|    approx_kl            | 0.37986365 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.5        |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.357      |
|    value_loss           | 120        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00171  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0712   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 49.4      |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 314       |
|    time_elapsed         | 1118      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.6377945 |
|    clip_fraction        | 0.562     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.888     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 6260      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.357     |
|    value_loss           | 51.4      |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0712    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 51         |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 315        |
|    time_elapsed         | 1121       |
|    total_timesteps      | 322560     |
| train/                  |            |
|    approx_kl            | 0.37177432 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.7       |
|    n_updates            | 6280       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.357      |
|    value_loss           | 202        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00145   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0785    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 51         |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 316        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.29948264 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.12       |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.357      |
|    value_loss           | 108        |
----------------------------------------
Num timesteps: 324000
Best mean reward: -63.38 - Last mean reward per episode: -176.31
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0753   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0149    |
| rollout/                |           |
|    ep_len_mean          | 61.1      |
|    ep_rew_mean          | -209      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 317       |
|    time_elapsed         | 1129      |
|    total_timesteps      | 324608    |
| train/                  |           |
|    approx_kl            | 0.2741406 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.679     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.49      |
|    n_updates            | 6320      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.357     |
|    value_loss           | 147       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00165  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0705   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0186    |
| rollout/                |           |
|    ep_len_mean          | 71.1      |
|    ep_rew_mean          | -242      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 318       |
|    time_elapsed         | 1132      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.3353169 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.33      |
|    n_updates            | 6340      |
|    policy_gradient_loss | -0.166    |
|    std                  | 0.357     |
|    value_loss           | 90.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 319        |
|    time_elapsed         | 1136       |
|    total_timesteps      | 326656     |
| train/                  |            |
|    approx_kl            | 0.44978076 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.27       |
|    n_updates            | 6360       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.357      |
|    value_loss           | 82.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 58         |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 320        |
|    time_elapsed         | 1140       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.73575515 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.34       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.357      |
|    value_loss           | 177        |
----------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00332  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0714   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.016     |
| rollout/                |           |
|    ep_len_mean          | 56.2      |
|    ep_rew_mean          | -192      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 321       |
|    time_elapsed         | 1143      |
|    total_timesteps      | 328704    |
| train/                  |           |
|    approx_kl            | 0.5643609 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.33      |
|    n_updates            | 6400      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.356     |
|    value_loss           | 133       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00374   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0581    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 56.4       |
|    ep_rew_mean          | -193       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 322        |
|    time_elapsed         | 1147       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.53742754 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 6420       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.356      |
|    value_loss           | 150        |
----------------------------------------
Num timesteps: 330000
Best mean reward: -63.38 - Last mean reward per episode: -127.61
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00318   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.082     |
| reward_torque           | -3.77      |
| reward_velocity         | 0.012      |
| rollout/                |            |
|    ep_len_mean          | 36.6       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 323        |
|    time_elapsed         | 1151       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.21094002 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.07       |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.356      |
|    value_loss           | 158        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00334   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0752    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 46.5       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 324        |
|    time_elapsed         | 1154       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.54393625 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 6460       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.356      |
|    value_loss           | 94.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00374   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0752    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0136     |
| rollout/                |            |
|    ep_len_mean          | 57.3       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 325        |
|    time_elapsed         | 1158       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.30211753 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.49       |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.356      |
|    value_loss           | 78.8       |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00308  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0764   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.013     |
| rollout/                |           |
|    ep_len_mean          | 57.2      |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 326       |
|    time_elapsed         | 1162      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.2800269 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.516     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.5      |
|    n_updates            | 6500      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.356     |
|    value_loss           | 347       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0136     |
| rollout/                |            |
|    ep_len_mean          | 58.7       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 327        |
|    time_elapsed         | 1165       |
|    total_timesteps      | 334848     |
| train/                  |            |
|    approx_kl            | 0.34173077 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.356      |
|    value_loss           | 108        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 68.8       |
|    ep_rew_mean          | -234       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 328        |
|    time_elapsed         | 1169       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.34884518 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.53       |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.356      |
|    value_loss           | 107        |
----------------------------------------
Num timesteps: 336000
Best mean reward: -63.38 - Last mean reward per episode: -234.16
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 329        |
|    time_elapsed         | 1172       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.39822656 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.356      |
|    value_loss           | 68.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.73     |
| reward_velocity         | 0.0145    |
| rollout/                |           |
|    ep_len_mean          | 47        |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 330       |
|    time_elapsed         | 1176      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.3280568 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.4       |
|    n_updates            | 6580      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.356     |
|    value_loss           | 316       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0875    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 57.6       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 331        |
|    time_elapsed         | 1180       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.24517041 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.15       |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.356      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000919  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0831    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 332        |
|    time_elapsed         | 1183       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.26216576 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.33       |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.356      |
|    value_loss           | 96.2       |
----------------------------------------
--------------------------------------
| reward                  | -3.87    |
| reward_contact          | -0.0014  |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0831  |
| reward_torque           | -3.71    |
| reward_velocity         | 0.0165   |
| rollout/                |          |
|    ep_len_mean          | 38.8     |
|    ep_rew_mean          | -135     |
| time/                   |          |
|    fps                  | 287      |
|    iterations           | 333      |
|    time_elapsed         | 1187     |
|    total_timesteps      | 340992   |
| train/                  |          |
|    approx_kl            | 0.31723  |
|    clip_fraction        | 0.484    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22      |
|    explained_variance   | 0.729    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.35     |
|    n_updates            | 6640     |
|    policy_gradient_loss | -0.211   |
|    std                  | 0.356    |
|    value_loss           | 172      |
--------------------------------------
Num timesteps: 342000
Best mean reward: -63.38 - Last mean reward per episode: -101.22
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00122   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0769    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 28.4       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 334        |
|    time_elapsed         | 1191       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.57439095 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.356      |
|    value_loss           | 124        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00214   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0868    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 27.5       |
|    ep_rew_mean          | -98.1      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 335        |
|    time_elapsed         | 1194       |
|    total_timesteps      | 343040     |
| train/                  |            |
|    approx_kl            | 0.41933316 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.67       |
|    n_updates            | 6680       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.356      |
|    value_loss           | 106        |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00224  |
| reward_ctrl             | -0.0988   |
| reward_motion           | -0.083    |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 37.4      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 336       |
|    time_elapsed         | 1198      |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.7729536 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.845     |
|    n_updates            | 6700      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.356     |
|    value_loss           | 74.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00241   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0788    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0185     |
| rollout/                |            |
|    ep_len_mean          | 47.3       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 337        |
|    time_elapsed         | 1202       |
|    total_timesteps      | 345088     |
| train/                  |            |
|    approx_kl            | 0.42307824 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 6720       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0698    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 49.1       |
|    ep_rew_mean          | -170       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 338        |
|    time_elapsed         | 1205       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.29145288 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.29       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.356      |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0606    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 60.5       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 339        |
|    time_elapsed         | 1209       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.32728803 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.98       |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.356      |
|    value_loss           | 106        |
----------------------------------------
Num timesteps: 348000
Best mean reward: -63.38 - Last mean reward per episode: -207.64
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00119  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0181    |
| rollout/                |           |
|    ep_len_mean          | 60        |
|    ep_rew_mean          | -207      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 340       |
|    time_elapsed         | 1212      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.3496292 |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.65      |
|    n_updates            | 6780      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.356     |
|    value_loss           | 133       |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0714    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 69.3       |
|    ep_rew_mean          | -237       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 341        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.55597585 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.66       |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.356      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00037   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0658    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 60.3       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 342        |
|    time_elapsed         | 1220       |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.39476937 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.356      |
|    value_loss           | 95.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.000344 |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0855   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0187    |
| rollout/                |           |
|    ep_len_mean          | 47.4      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 343       |
|    time_elapsed         | 1224      |
|    total_timesteps      | 351232    |
| train/                  |           |
|    approx_kl            | 0.5752926 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 6840      |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.356     |
|    value_loss           | 121       |
---------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00044  |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0777   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 57.7      |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 344       |
|    time_elapsed         | 1227      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.2618421 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.825     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.23      |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.356     |
|    value_loss           | 217       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00092   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0777    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 345        |
|    time_elapsed         | 1231       |
|    total_timesteps      | 353280     |
| train/                  |            |
|    approx_kl            | 0.36548486 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 43.8       |
|    n_updates            | 6880       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.356      |
|    value_loss           | 122        |
----------------------------------------
Num timesteps: 354000
Best mean reward: -63.38 - Last mean reward per episode: -165.12
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 346        |
|    time_elapsed         | 1235       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.44719616 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.57       |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.356      |
|    value_loss           | 99.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00149   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 57.5       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 347        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 355328     |
| train/                  |            |
|    approx_kl            | 0.54739803 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.87       |
|    n_updates            | 6920       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.356      |
|    value_loss           | 127        |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00148  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0604   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0226    |
| rollout/                |           |
|    ep_len_mean          | 67        |
|    ep_rew_mean          | -229      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 348       |
|    time_elapsed         | 1242      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 43.801178 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.61      |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.356     |
|    value_loss           | 124       |
---------------------------------------
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0804   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0188    |
| rollout/                |           |
|    ep_len_mean          | 48        |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 349       |
|    time_elapsed         | 1246      |
|    total_timesteps      | 357376    |
| train/                  |           |
|    approx_kl            | 0.8180683 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.26      |
|    n_updates            | 6960      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.356     |
|    value_loss           | 77.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00145   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.072     |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 58.3       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 350        |
|    time_elapsed         | 1249       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.47330943 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.356      |
|    value_loss           | 182        |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00187  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0778   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0201    |
| rollout/                |           |
|    ep_len_mean          | 57.8      |
|    ep_rew_mean          | -200      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 351       |
|    time_elapsed         | 1253      |
|    total_timesteps      | 359424    |
| train/                  |           |
|    approx_kl            | 1.1113491 |
|    clip_fraction        | 0.552     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 7000      |
|    policy_gradient_loss | -0.112    |
|    std                  | 0.356     |
|    value_loss           | 75.9      |
---------------------------------------
Num timesteps: 360000
Best mean reward: -63.38 - Last mean reward per episode: -135.13
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0808    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 38.3       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 352        |
|    time_elapsed         | 1257       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.32027358 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.43       |
|    n_updates            | 7020       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.356      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0767    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 48.3       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 353        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.38496023 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.87       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.356      |
|    value_loss           | 142        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00291  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0584   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0159    |
| rollout/                |           |
|    ep_len_mean          | 58.8      |
|    ep_rew_mean          | -204      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 354       |
|    time_elapsed         | 1264      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.3748232 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 7060      |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.356     |
|    value_loss           | 63.3      |
---------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00315 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0584  |
| reward_torque           | -3.85    |
| reward_velocity         | 0.0178   |
| rollout/                |          |
|    ep_len_mean          | 59.9     |
|    ep_rew_mean          | -208     |
| time/                   |          |
|    fps                  | 286      |
|    iterations           | 355      |
|    time_elapsed         | 1267     |
|    total_timesteps      | 363520   |
| train/                  |          |
|    approx_kl            | 0.549195 |
|    clip_fraction        | 0.457    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.2    |
|    explained_variance   | 0.762    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.49     |
|    n_updates            | 7080     |
|    policy_gradient_loss | -0.182   |
|    std                  | 0.356    |
|    value_loss           | 266      |
--------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00244   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0518    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0137     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 356        |
|    time_elapsed         | 1271       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.12948388 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.2       |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 226        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0569    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 36.7       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 357        |
|    time_elapsed         | 1275       |
|    total_timesteps      | 365568     |
| train/                  |            |
|    approx_kl            | 0.37696522 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.28       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 7120       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.356      |
|    value_loss           | 234        |
----------------------------------------
Num timesteps: 366000
Best mean reward: -63.38 - Last mean reward per episode: -132.75
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00174   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 37.9       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 358        |
|    time_elapsed         | 1278       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.18941039 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.97       |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0441    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 47.8       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 359        |
|    time_elapsed         | 1282       |
|    total_timesteps      | 367616     |
| train/                  |            |
|    approx_kl            | 0.33528614 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.35       |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.356      |
|    value_loss           | 190        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.0012    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0482    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 360        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.29301536 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.5        |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.356      |
|    value_loss           | 156        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0449    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0229     |
| rollout/                |            |
|    ep_len_mean          | 57.9       |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 361        |
|    time_elapsed         | 1289       |
|    total_timesteps      | 369664     |
| train/                  |            |
|    approx_kl            | 0.24608983 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 7200       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.356      |
|    value_loss           | 180        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0449    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 57.8       |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 362        |
|    time_elapsed         | 1293       |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.51944435 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.81       |
|    n_updates            | 7220       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.356      |
|    value_loss           | 96.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.055     |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -203       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 363        |
|    time_elapsed         | 1297       |
|    total_timesteps      | 371712     |
| train/                  |            |
|    approx_kl            | 0.23502257 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.03       |
|    n_updates            | 7240       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.356      |
|    value_loss           | 101        |
----------------------------------------
Num timesteps: 372000
Best mean reward: -63.38 - Last mean reward per episode: -203.43
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.055     |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 69.1       |
|    ep_rew_mean          | -238       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 364        |
|    time_elapsed         | 1300       |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.25955117 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    n_updates            | 7260       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.356      |
|    value_loss           | 425        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.000825  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0878    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 29         |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 365        |
|    time_elapsed         | 1304       |
|    total_timesteps      | 373760     |
| train/                  |            |
|    approx_kl            | 0.36993116 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 7280       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.356      |
|    value_loss           | 69.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.000825 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0878   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0162    |
| rollout/                |           |
|    ep_len_mean          | 39.1      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 366       |
|    time_elapsed         | 1307      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.4815482 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | -0.195    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.6       |
|    n_updates            | 7300      |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.356     |
|    value_loss           | 231       |
---------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.000827 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0931   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0195    |
| rollout/                |           |
|    ep_len_mean          | 38.3      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 367       |
|    time_elapsed         | 1311      |
|    total_timesteps      | 375808    |
| train/                  |           |
|    approx_kl            | 0.4816535 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.35      |
|    n_updates            | 7320      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.356     |
|    value_loss           | 110       |
---------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00107  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0884   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0194    |
| rollout/                |           |
|    ep_len_mean          | 47.2      |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 368       |
|    time_elapsed         | 1315      |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.6146321 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.848     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.57      |
|    n_updates            | 7340      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.355     |
|    value_loss           | 177       |
---------------------------------------
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.000859 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.015     |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 369       |
|    time_elapsed         | 1318      |
|    total_timesteps      | 377856    |
| train/                  |           |
|    approx_kl            | 0.2647634 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.66      |
|    n_updates            | 7360      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.355     |
|    value_loss           | 116       |
---------------------------------------
Num timesteps: 378000
Best mean reward: -63.38 - Last mean reward per episode: -163.50
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.000859 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0151    |
| rollout/                |           |
|    ep_len_mean          | 57.1      |
|    ep_rew_mean          | -197      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 370       |
|    time_elapsed         | 1322      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.2758844 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.94      |
|    n_updates            | 7380      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.355     |
|    value_loss           | 254       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.0011   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0682   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0195    |
| rollout/                |           |
|    ep_len_mean          | 57.3      |
|    ep_rew_mean          | -198      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 371       |
|    time_elapsed         | 1326      |
|    total_timesteps      | 379904    |
| train/                  |           |
|    approx_kl            | 0.2308224 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.697     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.23      |
|    n_updates            | 7400      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.355     |
|    value_loss           | 107       |
---------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00122   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0642    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 67.5       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 372        |
|    time_elapsed         | 1329       |
|    total_timesteps      | 380928     |
| train/                  |            |
|    approx_kl            | 0.65539503 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.27       |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.355      |
|    value_loss           | 33.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0384    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 67.1       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 373        |
|    time_elapsed         | 1333       |
|    total_timesteps      | 381952     |
| train/                  |            |
|    approx_kl            | 0.34901255 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.68       |
|    n_updates            | 7440       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.355      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0384    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 77.1       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 374        |
|    time_elapsed         | 1337       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.30204695 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.64       |
|    n_updates            | 7460       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.355      |
|    value_loss           | 288        |
----------------------------------------
Num timesteps: 384000
Best mean reward: -63.38 - Last mean reward per episode: -194.90
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00153  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0442   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0154    |
| rollout/                |           |
|    ep_len_mean          | 56.5      |
|    ep_rew_mean          | -195      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 375       |
|    time_elapsed         | 1340      |
|    total_timesteps      | 384000    |
| train/                  |           |
|    approx_kl            | 0.4172854 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.1       |
|    n_updates            | 7480      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.355     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0701    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 34.4       |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 376        |
|    time_elapsed         | 1344       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.67804694 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7500       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.355      |
|    value_loss           | 52.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.0017    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 45.4       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 377        |
|    time_elapsed         | 1348       |
|    total_timesteps      | 386048     |
| train/                  |            |
|    approx_kl            | 0.41826862 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 7520       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 176        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 46.4       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 378        |
|    time_elapsed         | 1352       |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.18567243 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.5       |
|    n_updates            | 7540       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 261        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0881    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 379        |
|    time_elapsed         | 1355       |
|    total_timesteps      | 388096     |
| train/                  |            |
|    approx_kl            | 0.42830938 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.58       |
|    n_updates            | 7560       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.355      |
|    value_loss           | 115        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0917    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 47.2       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 380        |
|    time_elapsed         | 1359       |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.26628798 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.14       |
|    n_updates            | 7580       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.355      |
|    value_loss           | 289        |
----------------------------------------
Num timesteps: 390000
Best mean reward: -63.38 - Last mean reward per episode: -130.86
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0976   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.00747   |
| rollout/                |           |
|    ep_len_mean          | 37.2      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 381       |
|    time_elapsed         | 1363      |
|    total_timesteps      | 390144    |
| train/                  |           |
|    approx_kl            | 0.2895919 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.24      |
|    n_updates            | 7600      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.355     |
|    value_loss           | 171       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00116  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.8      |
| reward_velocity         | 0.00909   |
| rollout/                |           |
|    ep_len_mean          | 35.6      |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 382       |
|    time_elapsed         | 1367      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.5427918 |
|    clip_fraction        | 0.525     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 7620      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.355     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00928    |
| rollout/                |            |
|    ep_len_mean          | 25.7       |
|    ep_rew_mean          | -92.6      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 383        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 392192     |
| train/                  |            |
|    approx_kl            | 0.28788298 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.3       |
|    n_updates            | 7640       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.355      |
|    value_loss           | 210        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 384        |
|    time_elapsed         | 1374       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.25013903 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.85       |
|    n_updates            | 7660       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.355      |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0904    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 47.4       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 385        |
|    time_elapsed         | 1378       |
|    total_timesteps      | 394240     |
| train/                  |            |
|    approx_kl            | 0.31216162 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.98       |
|    n_updates            | 7680       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.355      |
|    value_loss           | 149        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00095  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0766   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.00873   |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 386       |
|    time_elapsed         | 1382      |
|    total_timesteps      | 395264    |
| train/                  |           |
|    approx_kl            | 0.4776835 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 7700      |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.355     |
|    value_loss           | 127       |
---------------------------------------
Num timesteps: 396000
Best mean reward: -63.38 - Last mean reward per episode: -163.86
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00095   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0714    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00935    |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 387        |
|    time_elapsed         | 1386       |
|    total_timesteps      | 396288     |
| train/                  |            |
|    approx_kl            | 0.49872696 |
|    clip_fraction        | 0.603      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 7720       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.355      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0711    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00899    |
| rollout/                |            |
|    ep_len_mean          | 31.2       |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 388        |
|    time_elapsed         | 1390       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.33789963 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.07       |
|    n_updates            | 7740       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.355      |
|    value_loss           | 135        |
----------------------------------------
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.00048  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0626   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 40.5      |
|    ep_rew_mean          | -143      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 389       |
|    time_elapsed         | 1393      |
|    total_timesteps      | 398336    |
| train/                  |           |
|    approx_kl            | 0.5179801 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78      |
|    n_updates            | 7760      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.355     |
|    value_loss           | 172       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0704    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 41         |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 390        |
|    time_elapsed         | 1397       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.30704182 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.82       |
|    n_updates            | 7780       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.355      |
|    value_loss           | 145        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0632   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 51.1      |
|    ep_rew_mean          | -178      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 391       |
|    time_elapsed         | 1401      |
|    total_timesteps      | 400384    |
| train/                  |           |
|    approx_kl            | 1.3394313 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.84      |
|    n_updates            | 7800      |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.355     |
|    value_loss           | 47        |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0529    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 61.1       |
|    ep_rew_mean          | -211       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 392        |
|    time_elapsed         | 1404       |
|    total_timesteps      | 401408     |
| train/                  |            |
|    approx_kl            | 0.23714311 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.62       |
|    n_updates            | 7820       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.355      |
|    value_loss           | 120        |
----------------------------------------
Num timesteps: 402000
Best mean reward: -63.38 - Last mean reward per episode: -246.33
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000796  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0506    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 71.4       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 393        |
|    time_elapsed         | 1408       |
|    total_timesteps      | 402432     |
| train/                  |            |
|    approx_kl            | 0.40642262 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.65       |
|    n_updates            | 7840       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.355      |
|    value_loss           | 97.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0367    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 65.8       |
|    ep_rew_mean          | -226       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 394        |
|    time_elapsed         | 1412       |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.37478182 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.02       |
|    n_updates            | 7860       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.355      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0336    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 75.7       |
|    ep_rew_mean          | -259       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 395        |
|    time_elapsed         | 1416       |
|    total_timesteps      | 404480     |
| train/                  |            |
|    approx_kl            | 0.52354705 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.17       |
|    n_updates            | 7880       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.355      |
|    value_loss           | 305        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0525    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 24.6       |
|    ep_rew_mean          | -89        |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 396        |
|    time_elapsed         | 1419       |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.60849965 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 7900       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.355      |
|    value_loss           | 106        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000815  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0525    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0111     |
| rollout/                |            |
|    ep_len_mean          | 35         |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 397        |
|    time_elapsed         | 1423       |
|    total_timesteps      | 406528     |
| train/                  |            |
|    approx_kl            | 0.44664246 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53       |
|    n_updates            | 7920       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.355      |
|    value_loss           | 172        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000335 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0749   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.00905   |
| rollout/                |           |
|    ep_len_mean          | 26.9      |
|    ep_rew_mean          | -97.4     |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 398       |
|    time_elapsed         | 1427      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.3157359 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.878     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.02      |
|    n_updates            | 7940      |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.355     |
|    value_loss           | 116       |
---------------------------------------
Num timesteps: 408000
Best mean reward: -63.38 - Last mean reward per episode: -97.40
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000335 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0748   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00922   |
| rollout/                |           |
|    ep_len_mean          | 36.8      |
|    ep_rew_mean          | -130      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 399       |
|    time_elapsed         | 1430      |
|    total_timesteps      | 408576    |
| train/                  |           |
|    approx_kl            | 0.6823025 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.897     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.02      |
|    n_updates            | 7960      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.355     |
|    value_loss           | 156       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000863 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.09     |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00965   |
| rollout/                |           |
|    ep_len_mean          | 26.6      |
|    ep_rew_mean          | -96       |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 400       |
|    time_elapsed         | 1434      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.3025665 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | 28        |
|    n_updates            | 7980      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.355     |
|    value_loss           | 262       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0857    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.012      |
| rollout/                |            |
|    ep_len_mean          | 37         |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 401        |
|    time_elapsed         | 1438       |
|    total_timesteps      | 410624     |
| train/                  |            |
|    approx_kl            | 0.34966886 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.43       |
|    n_updates            | 8000       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.355      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0828    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 402        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 411648     |
| train/                  |            |
|    approx_kl            | 0.27043265 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.24       |
|    n_updates            | 8020       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 206        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.0013    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0754    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 36.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 403        |
|    time_elapsed         | 1445       |
|    total_timesteps      | 412672     |
| train/                  |            |
|    approx_kl            | 0.42986935 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.21       |
|    n_updates            | 8040       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.355      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0825    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | -66.5      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 404        |
|    time_elapsed         | 1449       |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.48211375 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.85       |
|    n_updates            | 8060       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.355      |
|    value_loss           | 173        |
----------------------------------------
Num timesteps: 414000
Best mean reward: -63.38 - Last mean reward per episode: -66.49
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.000844 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0616   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0151    |
| rollout/                |           |
|    ep_len_mean          | 27.4      |
|    ep_rew_mean          | -99.1     |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 405       |
|    time_elapsed         | 1453      |
|    total_timesteps      | 414720    |
| train/                  |           |
|    approx_kl            | 1.3719039 |
|    clip_fraction        | 0.543     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.464     |
|    n_updates            | 8080      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.355     |
|    value_loss           | 113       |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000956  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0577    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0185     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -130       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 406        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 415744     |
| train/                  |            |
|    approx_kl            | 0.42250845 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.78       |
|    n_updates            | 8100       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.355      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.000956 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0668   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0187    |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -134      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 407       |
|    time_elapsed         | 1460      |
|    total_timesteps      | 416768    |
| train/                  |           |
|    approx_kl            | 0.4643223 |
|    clip_fraction        | 0.534     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.864     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.94      |
|    n_updates            | 8120      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.355     |
|    value_loss           | 104       |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.0012    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0604    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 408        |
|    time_elapsed         | 1464       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.54839766 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.83       |
|    n_updates            | 8140       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.355      |
|    value_loss           | 83.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0615    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 56.7       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 409        |
|    time_elapsed         | 1467       |
|    total_timesteps      | 418816     |
| train/                  |            |
|    approx_kl            | 0.43524492 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 8160       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.354      |
|    value_loss           | 112        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00202  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0543   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 67.3      |
|    ep_rew_mean          | -231      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 410       |
|    time_elapsed         | 1471      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.3953567 |
|    clip_fraction        | 0.601     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.7      |
|    n_updates            | 8180      |
|    policy_gradient_loss | -0.182    |
|    std                  | 0.354     |
|    value_loss           | 262       |
---------------------------------------
Num timesteps: 420000
Best mean reward: -63.38 - Last mean reward per episode: -228.74
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00189   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0586    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 56.8       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 411        |
|    time_elapsed         | 1475       |
|    total_timesteps      | 420864     |
| train/                  |            |
|    approx_kl            | 0.42520824 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.57       |
|    n_updates            | 8200       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.354      |
|    value_loss           | 83.6       |
----------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00244 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0791  |
| reward_torque           | -3.82    |
| reward_velocity         | 0.0143   |
| rollout/                |          |
|    ep_len_mean          | 56.7     |
|    ep_rew_mean          | -196     |
| time/                   |          |
|    fps                  | 285      |
|    iterations           | 412      |
|    time_elapsed         | 1478     |
|    total_timesteps      | 421888   |
| train/                  |          |
|    approx_kl            | 0.950876 |
|    clip_fraction        | 0.617    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.8    |
|    explained_variance   | 0.915    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.12     |
|    n_updates            | 8220     |
|    policy_gradient_loss | -0.19    |
|    std                  | 0.354    |
|    value_loss           | 64.8     |
--------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00257   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0906    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 44.4       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 413        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 422912     |
| train/                  |            |
|    approx_kl            | 0.40403545 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.5       |
|    n_updates            | 8240       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.354      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0902    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 35.4       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 414        |
|    time_elapsed         | 1486       |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.37594998 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.88       |
|    n_updates            | 8260       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.354      |
|    value_loss           | 201        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00238   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0858    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 45.6       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 415        |
|    time_elapsed         | 1490       |
|    total_timesteps      | 424960     |
| train/                  |            |
|    approx_kl            | 0.71219885 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.45       |
|    n_updates            | 8280       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.354      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0842    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 36.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 416        |
|    time_elapsed         | 1493       |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.47726566 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.56       |
|    n_updates            | 8300       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.354      |
|    value_loss           | 88.4       |
----------------------------------------
Num timesteps: 426000
Best mean reward: -63.38 - Last mean reward per episode: -129.87
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00179   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0842    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 36.4       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 417        |
|    time_elapsed         | 1497       |
|    total_timesteps      | 427008     |
| train/                  |            |
|    approx_kl            | 0.40723807 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 8320       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.354      |
|    value_loss           | 313        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00159  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0739   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 45.6      |
|    ep_rew_mean          | -158      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 418       |
|    time_elapsed         | 1501      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.7864481 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.2       |
|    n_updates            | 8340      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.354     |
|    value_loss           | 67.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0647    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 35.2       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 419        |
|    time_elapsed         | 1505       |
|    total_timesteps      | 429056     |
| train/                  |            |
|    approx_kl            | 0.27581483 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.02       |
|    n_updates            | 8360       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.354      |
|    value_loss           | 297        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.000952  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0827    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 25         |
|    ep_rew_mean          | -90.4      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 420        |
|    time_elapsed         | 1508       |
|    total_timesteps      | 430080     |
| train/                  |            |
|    approx_kl            | 0.20145088 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.6       |
|    n_updates            | 8380       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.354      |
|    value_loss           | 138        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 34.5      |
|    ep_rew_mean          | -122      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 421       |
|    time_elapsed         | 1512      |
|    total_timesteps      | 431104    |
| train/                  |           |
|    approx_kl            | 0.5858379 |
|    clip_fraction        | 0.528     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.54      |
|    n_updates            | 8400      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.354     |
|    value_loss           | 128       |
---------------------------------------
Num timesteps: 432000
Best mean reward: -63.38 - Last mean reward per episode: -122.33
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000906  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0765    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 34.5       |
|    ep_rew_mean          | -122       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 422        |
|    time_elapsed         | 1515       |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.26940846 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.5       |
|    n_updates            | 8420       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.354      |
|    value_loss           | 289        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0841    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -123       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 423        |
|    time_elapsed         | 1519       |
|    total_timesteps      | 433152     |
| train/                  |            |
|    approx_kl            | 0.26015857 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.26       |
|    n_updates            | 8440       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.354      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 424        |
|    time_elapsed         | 1523       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.27896038 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 8460       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.354      |
|    value_loss           | 143        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0852    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 425        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 435200     |
| train/                  |            |
|    approx_kl            | 0.38560873 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 8480       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.354      |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0863    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00776    |
| rollout/                |            |
|    ep_len_mean          | 26.7       |
|    ep_rew_mean          | -97.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 426        |
|    time_elapsed         | 1530       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.29351357 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 8500       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.354      |
|    value_loss           | 246        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00174  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.00803   |
| rollout/                |           |
|    ep_len_mean          | 36.1      |
|    ep_rew_mean          | -128      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 427       |
|    time_elapsed         | 1534      |
|    total_timesteps      | 437248    |
| train/                  |           |
|    approx_kl            | 0.3907338 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.486     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.88      |
|    n_updates            | 8520      |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.354     |
|    value_loss           | 215       |
---------------------------------------
Num timesteps: 438000
Best mean reward: -63.38 - Last mean reward per episode: -128.37
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00102   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.085     |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00887    |
| rollout/                |            |
|    ep_len_mean          | 36.3       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 428        |
|    time_elapsed         | 1538       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.39633846 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.25       |
|    n_updates            | 8540       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.354      |
|    value_loss           | 76.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00121   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 429        |
|    time_elapsed         | 1541       |
|    total_timesteps      | 439296     |
| train/                  |            |
|    approx_kl            | 0.80881566 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 8560       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.354      |
|    value_loss           | 149        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00116  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.081    |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 45.2      |
|    ep_rew_mean          | -157      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 430       |
|    time_elapsed         | 1545      |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.2752297 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | 30.9      |
|    n_updates            | 8580      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.354     |
|    value_loss           | 175       |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00146  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0152    |
| rollout/                |           |
|    ep_len_mean          | 55.5      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 431       |
|    time_elapsed         | 1549      |
|    total_timesteps      | 441344    |
| train/                  |           |
|    approx_kl            | 0.2689147 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 8600      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.354     |
|    value_loss           | 181       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00108  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0695   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.016     |
| rollout/                |           |
|    ep_len_mean          | 44.6      |
|    ep_rew_mean          | -155      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 432       |
|    time_elapsed         | 1553      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.3003227 |
|    clip_fraction        | 0.524     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.69      |
|    n_updates            | 8620      |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.354     |
|    value_loss           | 169       |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0656   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0192    |
| rollout/                |           |
|    ep_len_mean          | 55.1      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 433       |
|    time_elapsed         | 1557      |
|    total_timesteps      | 443392    |
| train/                  |           |
|    approx_kl            | 1.2538021 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.2       |
|    n_updates            | 8640      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.354     |
|    value_loss           | 86.7      |
---------------------------------------
Num timesteps: 444000
Best mean reward: -63.38 - Last mean reward per episode: -222.11
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0616   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 64.9      |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 434       |
|    time_elapsed         | 1560      |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 1.5972466 |
|    clip_fraction        | 0.636     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.61      |
|    n_updates            | 8660      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.354     |
|    value_loss           | 106       |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.0941    |
| reward_motion           | -0.064     |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 65.5       |
|    ep_rew_mean          | -225       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 435        |
|    time_elapsed         | 1564       |
|    total_timesteps      | 445440     |
| train/                  |            |
|    approx_kl            | 0.57625556 |
|    clip_fraction        | 0.678      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.12       |
|    n_updates            | 8680       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.354      |
|    value_loss           | 97.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00129   |
| reward_ctrl             | -0.0941    |
| reward_motion           | -0.0703    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0244     |
| rollout/                |            |
|    ep_len_mean          | 44.9       |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 436        |
|    time_elapsed         | 1568       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.23225716 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.9       |
|    n_updates            | 8700       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.354      |
|    value_loss           | 249        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0783    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0258     |
| rollout/                |            |
|    ep_len_mean          | 54.4       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 437        |
|    time_elapsed         | 1572       |
|    total_timesteps      | 447488     |
| train/                  |            |
|    approx_kl            | 0.88879985 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 8720       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.354      |
|    value_loss           | 84.9       |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000911  |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0921    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 52         |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 438        |
|    time_elapsed         | 1575       |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.29066724 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.28       |
|    n_updates            | 8740       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.354      |
|    value_loss           | 174        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000911 |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0829   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0218    |
| rollout/                |           |
|    ep_len_mean          | 62.1      |
|    ep_rew_mean          | -214      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 439       |
|    time_elapsed         | 1579      |
|    total_timesteps      | 449536    |
| train/                  |           |
|    approx_kl            | 0.4085124 |
|    clip_fraction        | 0.474     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.847     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 8760      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.354     |
|    value_loss           | 209       |
---------------------------------------
Num timesteps: 450000
Best mean reward: -63.38 - Last mean reward per episode: -248.53
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000708 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0731   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0103    |
| rollout/                |           |
|    ep_len_mean          | 53.2      |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 440       |
|    time_elapsed         | 1583      |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.3786428 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.7       |
|    n_updates            | 8780      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.354     |
|    value_loss           | 121       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000708  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0731    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 63.2       |
|    ep_rew_mean          | -219       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 441        |
|    time_elapsed         | 1586       |
|    total_timesteps      | 451584     |
| train/                  |            |
|    approx_kl            | 0.22186717 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 8800       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.354      |
|    value_loss           | 355        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000764  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00836    |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 442        |
|    time_elapsed         | 1590       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.27236608 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 8820       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.354      |
|    value_loss           | 162        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000524  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00844    |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 443        |
|    time_elapsed         | 1594       |
|    total_timesteps      | 453632     |
| train/                  |            |
|    approx_kl            | 0.33655977 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | -0.159     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.93       |
|    n_updates            | 8840       |
|    policy_gradient_loss | -0.207     |
|    std                  | 0.354      |
|    value_loss           | 279        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000401  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0802    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00789    |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 444        |
|    time_elapsed         | 1597       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.56706333 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.86       |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.354      |
|    value_loss           | 65.1       |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0961    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 37.4       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 445        |
|    time_elapsed         | 1601       |
|    total_timesteps      | 455680     |
| train/                  |            |
|    approx_kl            | 0.26140085 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 9.39       |
|    n_updates            | 8880       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.354      |
|    value_loss           | 193        |
----------------------------------------
Num timesteps: 456000
Best mean reward: -63.38 - Last mean reward per episode: -133.47
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.000846 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.082    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 446       |
|    time_elapsed         | 1605      |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 0.7970977 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.711     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 8900      |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.354     |
|    value_loss           | 177       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000982  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 57.1       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 447        |
|    time_elapsed         | 1608       |
|    total_timesteps      | 457728     |
| train/                  |            |
|    approx_kl            | 0.35103822 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.15       |
|    n_updates            | 8920       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.354      |
|    value_loss           | 145        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0705    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 66.8       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 448        |
|    time_elapsed         | 1612       |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.49459758 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.02       |
|    n_updates            | 8940       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.354      |
|    value_loss           | 74.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0705    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 76.5       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 449        |
|    time_elapsed         | 1616       |
|    total_timesteps      | 459776     |
| train/                  |            |
|    approx_kl            | 0.45320842 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.3        |
|    n_updates            | 8960       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.354      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00121   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 86.7       |
|    ep_rew_mean          | -297       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 450        |
|    time_elapsed         | 1619       |
|    total_timesteps      | 460800     |
| train/                  |            |
|    approx_kl            | 0.38039494 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 8980       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.354      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00173  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0645   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 67.2      |
|    ep_rew_mean          | -231      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 451       |
|    time_elapsed         | 1623      |
|    total_timesteps      | 461824    |
| train/                  |           |
|    approx_kl            | 0.7007009 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.396     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.33      |
|    n_updates            | 9000      |
|    policy_gradient_loss | -0.115    |
|    std                  | 0.354     |
|    value_loss           | 131       |
---------------------------------------
Num timesteps: 462000
Best mean reward: -63.38 - Last mean reward per episode: -233.30
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0645    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 67.7       |
|    ep_rew_mean          | -233       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 452        |
|    time_elapsed         | 1627       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.25675064 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.299      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.6       |
|    n_updates            | 9020       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.354      |
|    value_loss           | 661        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00233   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0592    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 77.5       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 453        |
|    time_elapsed         | 1630       |
|    total_timesteps      | 463872     |
| train/                  |            |
|    approx_kl            | 0.29339504 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.9        |
|    n_updates            | 9040       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.354      |
|    value_loss           | 189        |
----------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00242  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0693   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 78        |
|    ep_rew_mean          | -269      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 454       |
|    time_elapsed         | 1634      |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.5218841 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.889     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.34      |
|    n_updates            | 9060      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.354     |
|    value_loss           | 75.1      |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 455        |
|    time_elapsed         | 1638       |
|    total_timesteps      | 465920     |
| train/                  |            |
|    approx_kl            | 0.39825416 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 72.2       |
|    n_updates            | 9080       |
|    policy_gradient_loss | -0.188     |
|    std                  | 0.354      |
|    value_loss           | 263        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.0017    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 456        |
|    time_elapsed         | 1641       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.29533693 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 9100       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.354      |
|    value_loss           | 167        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.0015   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0158    |
| rollout/                |           |
|    ep_len_mean          | 54.7      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 457       |
|    time_elapsed         | 1645      |
|    total_timesteps      | 467968    |
| train/                  |           |
|    approx_kl            | 0.9043795 |
|    clip_fraction        | 0.629     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.889     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.84      |
|    n_updates            | 9120      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.353     |
|    value_loss           | 150       |
---------------------------------------
Num timesteps: 468000
Best mean reward: -63.38 - Last mean reward per episode: -189.98
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00129  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0948   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.00936   |
| rollout/                |           |
|    ep_len_mean          | 34.8      |
|    ep_rew_mean          | -124      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 458       |
|    time_elapsed         | 1649      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 1.3862952 |
|    clip_fraction        | 0.676     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.684     |
|    n_updates            | 9140      |
|    policy_gradient_loss | -0.165    |
|    std                  | 0.353     |
|    value_loss           | 45.7      |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0948    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.00804    |
| rollout/                |            |
|    ep_len_mean          | 25.7       |
|    ep_rew_mean          | -93.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 459        |
|    time_elapsed         | 1652       |
|    total_timesteps      | 470016     |
| train/                  |            |
|    approx_kl            | 0.26756197 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.93       |
|    n_updates            | 9160       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.353      |
|    value_loss           | 382        |
----------------------------------------
--------------------------------------
| reward                  | -4.02    |
| reward_contact          | -0.00107 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0948  |
| reward_torque           | -3.83    |
| reward_velocity         | 0.0105   |
| rollout/                |          |
|    ep_len_mean          | 37.1     |
|    ep_rew_mean          | -132     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 460      |
|    time_elapsed         | 1656     |
|    total_timesteps      | 471040   |
| train/                  |          |
|    approx_kl            | 0.722155 |
|    clip_fraction        | 0.561    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.8    |
|    explained_variance   | 0.918    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.44     |
|    n_updates            | 9180     |
|    policy_gradient_loss | -0.179   |
|    std                  | 0.353    |
|    value_loss           | 65.6     |
--------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00154   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0912    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 38.1       |
|    ep_rew_mean          | -136       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 461        |
|    time_elapsed         | 1659       |
|    total_timesteps      | 472064     |
| train/                  |            |
|    approx_kl            | 0.45894516 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 9200       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.353      |
|    value_loss           | 170        |
----------------------------------------
---------------------------------------
| reward                  | -4.07     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0912   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.00987   |
| rollout/                |           |
|    ep_len_mean          | 37.6      |
|    ep_rew_mean          | -134      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 462       |
|    time_elapsed         | 1663      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.6468083 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.39      |
|    n_updates            | 9220      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 257       |
---------------------------------------
Num timesteps: 474000
Best mean reward: -63.38 - Last mean reward per episode: -181.78
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0122     |
| rollout/                |            |
|    ep_len_mean          | 51.4       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 463        |
|    time_elapsed         | 1667       |
|    total_timesteps      | 474112     |
| train/                  |            |
|    approx_kl            | 0.54084027 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 9240       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.353      |
|    value_loss           | 76         |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0113     |
| rollout/                |            |
|    ep_len_mean          | 61.3       |
|    ep_rew_mean          | -215       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 464        |
|    time_elapsed         | 1671       |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.42845312 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 9260       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.353      |
|    value_loss           | 238        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00116   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0926    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 39.3       |
|    ep_rew_mean          | -141       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 465        |
|    time_elapsed         | 1674       |
|    total_timesteps      | 476160     |
| train/                  |            |
|    approx_kl            | 0.36577994 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 9280       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.353      |
|    value_loss           | 188        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.00141  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00908   |
| rollout/                |           |
|    ep_len_mean          | 37.4      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 466       |
|    time_elapsed         | 1678      |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 2.5015154 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.78      |
|    n_updates            | 9300      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.353     |
|    value_loss           | 183       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00141  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00973   |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 467       |
|    time_elapsed         | 1682      |
|    total_timesteps      | 478208    |
| train/                  |           |
|    approx_kl            | 0.5588834 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.56      |
|    n_updates            | 9320      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.353     |
|    value_loss           | 167       |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 48.2       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 468        |
|    time_elapsed         | 1685       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.88191164 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.832      |
|    n_updates            | 9340       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.353      |
|    value_loss           | 43.5       |
----------------------------------------
Num timesteps: 480000
Best mean reward: -63.38 - Last mean reward per episode: -165.75
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0644    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 469        |
|    time_elapsed         | 1689       |
|    total_timesteps      | 480256     |
| train/                  |            |
|    approx_kl            | 0.22767824 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 9360       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.353      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000701  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00627    |
| rollout/                |            |
|    ep_len_mean          | 15.8       |
|    ep_rew_mean          | -60.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 470        |
|    time_elapsed         | 1692       |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.29996347 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.37       |
|    n_updates            | 9380       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.353      |
|    value_loss           | 278        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000873  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0904    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.00983    |
| rollout/                |            |
|    ep_len_mean          | 15.1       |
|    ep_rew_mean          | -57.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 471        |
|    time_elapsed         | 1696       |
|    total_timesteps      | 482304     |
| train/                  |            |
|    approx_kl            | 0.30862546 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | -0.068     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.88       |
|    n_updates            | 9400       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.353      |
|    value_loss           | 241        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000873 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0811   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0121    |
| rollout/                |           |
|    ep_len_mean          | 27.1      |
|    ep_rew_mean          | -98.7     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 472       |
|    time_elapsed         | 1700      |
|    total_timesteps      | 483328    |
| train/                  |           |
|    approx_kl            | 0.6673261 |
|    clip_fraction        | 0.511     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.867     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 9420      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 132       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0811    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 27.2       |
|    ep_rew_mean          | -99.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 473        |
|    time_elapsed         | 1703       |
|    total_timesteps      | 484352     |
| train/                  |            |
|    approx_kl            | 0.69099355 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.5       |
|    n_updates            | 9440       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.353      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000643  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 474        |
|    time_elapsed         | 1707       |
|    total_timesteps      | 485376     |
| train/                  |            |
|    approx_kl            | 0.30361605 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.36       |
|    n_updates            | 9460       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.353      |
|    value_loss           | 120        |
----------------------------------------
Num timesteps: 486000
Best mean reward: -63.38 - Last mean reward per episode: -92.43
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.000695  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.87      |
| reward_velocity         | 0.00879    |
| rollout/                |            |
|    ep_len_mean          | 14.1       |
|    ep_rew_mean          | -54.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 475        |
|    time_elapsed         | 1711       |
|    total_timesteps      | 486400     |
| train/                  |            |
|    approx_kl            | 0.29325378 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.13       |
|    n_updates            | 9480       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.353      |
|    value_loss           | 234        |
----------------------------------------
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.000695 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0938   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0101    |
| rollout/                |           |
|    ep_len_mean          | 24.3      |
|    ep_rew_mean          | -88.2     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 476       |
|    time_elapsed         | 1715      |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.6818845 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | 0.613     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 9500      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 168       |
---------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.000695  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0938    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 34.4       |
|    ep_rew_mean          | -122       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 477        |
|    time_elapsed         | 1718       |
|    total_timesteps      | 488448     |
| train/                  |            |
|    approx_kl            | 0.59273034 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 9520       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.353      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.079     |
| reward_torque           | -3.86      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 36.3       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 478        |
|    time_elapsed         | 1722       |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.31759673 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.8        |
|    n_updates            | 9540       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.353      |
|    value_loss           | 101        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00143  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.072    |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0133    |
| rollout/                |           |
|    ep_len_mean          | 47        |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 479       |
|    time_elapsed         | 1726      |
|    total_timesteps      | 490496    |
| train/                  |           |
|    approx_kl            | 0.8690135 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.6     |
|    explained_variance   | 0.414     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.5       |
|    n_updates            | 9560      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.353     |
|    value_loss           | 164       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0685    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 56.7       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 480        |
|    time_elapsed         | 1730       |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.22620931 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 9580       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.353      |
|    value_loss           | 202        |
----------------------------------------
Num timesteps: 492000
Best mean reward: -63.38 - Last mean reward per episode: -199.45
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00119  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0134    |
| rollout/                |           |
|    ep_len_mean          | 57        |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 481       |
|    time_elapsed         | 1733      |
|    total_timesteps      | 492544    |
| train/                  |           |
|    approx_kl            | 0.3660801 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.775     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.05      |
|    n_updates            | 9600      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.353     |
|    value_loss           | 104       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00071  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0605   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0125    |
| rollout/                |           |
|    ep_len_mean          | 46.9      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 482       |
|    time_elapsed         | 1737      |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 1.0251296 |
|    clip_fraction        | 0.624     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.92      |
|    n_updates            | 9620      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.353     |
|    value_loss           | 86.5      |
---------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.0002   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0642   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 45.2      |
|    ep_rew_mean          | -159      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 483       |
|    time_elapsed         | 1741      |
|    total_timesteps      | 494592    |
| train/                  |           |
|    approx_kl            | 0.7006027 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.816     |
|    n_updates            | 9640      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.353     |
|    value_loss           | 104       |
---------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00057  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0646   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00807   |
| rollout/                |           |
|    ep_len_mean          | 34.6      |
|    ep_rew_mean          | -123      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 484       |
|    time_elapsed         | 1744      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.6338114 |
|    clip_fraction        | 0.619     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.98      |
|    n_updates            | 9660      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.353     |
|    value_loss           | 107       |
---------------------------------------
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00133  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0739   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0107    |
| rollout/                |           |
|    ep_len_mean          | 23.9      |
|    ep_rew_mean          | -86.9     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 485       |
|    time_elapsed         | 1748      |
|    total_timesteps      | 496640    |
| train/                  |           |
|    approx_kl            | 0.8359202 |
|    clip_fraction        | 0.609     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.4     |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.73      |
|    n_updates            | 9680      |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.353     |
|    value_loss           | 86.4      |
---------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.89      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 26.3       |
|    ep_rew_mean          | -94.6      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 486        |
|    time_elapsed         | 1752       |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.42835408 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 9700       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.353      |
|    value_loss           | 135        |
----------------------------------------
Num timesteps: 498000
Best mean reward: -63.38 - Last mean reward per episode: -63.05
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00164  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0799   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0106    |
| rollout/                |           |
|    ep_len_mean          | 16.8      |
|    ep_rew_mean          | -63.1     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 487       |
|    time_elapsed         | 1755      |
|    total_timesteps      | 498688    |
| train/                  |           |
|    approx_kl            | 0.5428996 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 9720      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.353     |
|    value_loss           | 154       |
---------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0847    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0087     |
| rollout/                |            |
|    ep_len_mean          | 28.1       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 488        |
|    time_elapsed         | 1759       |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.99617803 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 9740       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.352      |
|    value_loss           | 66.3       |
----------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.000909  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.87      |
| reward_velocity         | 0.00672    |
| rollout/                |            |
|    ep_len_mean          | 14         |
|    ep_rew_mean          | -54        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 489        |
|    time_elapsed         | 1763       |
|    total_timesteps      | 500736     |
| train/                  |            |
|    approx_kl            | 0.29248554 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.73       |
|    n_updates            | 9760       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.352      |
|    value_loss           | 231        |
----------------------------------------
---------------------------------------
| reward                  | -4.09     |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.09     |
| reward_torque           | -3.91     |
| reward_velocity         | 0.00941   |
| rollout/                |           |
|    ep_len_mean          | 11.9      |
|    ep_rew_mean          | -46.1     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 490       |
|    time_elapsed         | 1766      |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.6285322 |
|    clip_fraction        | 0.508     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.3     |
|    explained_variance   | 0.0428    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.23      |
|    n_updates            | 9780      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.352     |
|    value_loss           | 95.9      |
---------------------------------------
----------------------------------------
| reward                  | -4.07      |
| reward_contact          | -0.00155   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0769    |
| reward_torque           | -3.9       |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | -82.5      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 491        |
|    time_elapsed         | 1770       |
|    total_timesteps      | 502784     |
| train/                  |            |
|    approx_kl            | 0.60537595 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.206     |
|    std                  | 0.352      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -4.07      |
| reward_contact          | -0.00131   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0769    |
| reward_torque           | -3.9       |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | -80        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 492        |
|    time_elapsed         | 1774       |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.25271356 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.352      |
|    value_loss           | 163        |
----------------------------------------
Num timesteps: 504000
Best mean reward: -63.05 - Last mean reward per episode: -80.00
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00151  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0788   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.012     |
| rollout/                |           |
|    ep_len_mean          | 32.5      |
|    ep_rew_mean          | -115      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 493       |
|    time_elapsed         | 1778      |
|    total_timesteps      | 504832    |
| train/                  |           |
|    approx_kl            | 0.9998336 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.948     |
|    n_updates            | 9840      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.352     |
|    value_loss           | 55.1      |
---------------------------------------
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00173  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.075    |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0133    |
| rollout/                |           |
|    ep_len_mean          | 43.4      |
|    ep_rew_mean          | -152      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 494       |
|    time_elapsed         | 1781      |
|    total_timesteps      | 505856    |
| train/                  |           |
|    approx_kl            | 0.3880131 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.04      |
|    n_updates            | 9860      |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.352     |
|    value_loss           | 216       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00149   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0693    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 44         |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 495        |
|    time_elapsed         | 1785       |
|    total_timesteps      | 506880     |
| train/                  |            |
|    approx_kl            | 0.34061325 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.352      |
|    value_loss           | 264        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0943    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 23.9       |
|    ep_rew_mean          | -87        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 496        |
|    time_elapsed         | 1788       |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.17631595 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.6       |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.352      |
|    value_loss           | 211        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00044   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0898    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 33.9       |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 497        |
|    time_elapsed         | 1792       |
|    total_timesteps      | 508928     |
| train/                  |            |
|    approx_kl            | 0.42041576 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.3      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.555      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.352      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0868    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0058     |
| rollout/                |            |
|    ep_len_mean          | 22.4       |
|    ep_rew_mean          | -81.2      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 498        |
|    time_elapsed         | 1796       |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.47609308 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.29       |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.352      |
|    value_loss           | 148        |
----------------------------------------
Num timesteps: 510000
Best mean reward: -63.05 - Last mean reward per episode: -81.17
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0821    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0086     |
| rollout/                |            |
|    ep_len_mean          | 32.3       |
|    ep_rew_mean          | -114       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 499        |
|    time_elapsed         | 1799       |
|    total_timesteps      | 510976     |
| train/                  |            |
|    approx_kl            | 0.40394205 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.352      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0755    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | -81.4      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 500        |
|    time_elapsed         | 1803       |
|    total_timesteps      | 512000     |
| train/                  |            |
|    approx_kl            | 0.46649462 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 9980       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.352      |
|    value_loss           | 173        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0755    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | -82.6      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 501        |
|    time_elapsed         | 1807       |
|    total_timesteps      | 513024     |
| train/                  |            |
|    approx_kl            | 0.59766114 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.3      |
|    explained_variance   | -0.0232    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 10000      |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.352      |
|    value_loss           | 81.2       |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00102  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.098    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.014     |
| rollout/                |           |
|    ep_len_mean          | 24.4      |
|    ep_rew_mean          | -88.7     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 502       |
|    time_elapsed         | 1810      |
|    total_timesteps      | 514048    |
| train/                  |           |
|    approx_kl            | 1.0312245 |
|    clip_fraction        | 0.631     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 10020     |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.352     |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.000466 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0101    |
| rollout/                |           |
|    ep_len_mean          | 17.5      |
|    ep_rew_mean          | -66.9     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 503       |
|    time_elapsed         | 1814      |
|    total_timesteps      | 515072    |
| train/                  |           |
|    approx_kl            | 0.2969876 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23       |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 10040     |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.352     |
|    value_loss           | 288       |
---------------------------------------
Num timesteps: 516000
Best mean reward: -63.05 - Last mean reward per episode: -66.93
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000466  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 27.7       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 504        |
|    time_elapsed         | 1818       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.33233848 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.53       |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.352      |
|    value_loss           | 286        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00102   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0888    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 505        |
|    time_elapsed         | 1821       |
|    total_timesteps      | 517120     |
| train/                  |            |
|    approx_kl            | 0.16740772 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.8       |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.352      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0888    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00966    |
| rollout/                |            |
|    ep_len_mean          | 42.3       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 506        |
|    time_elapsed         | 1825       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.31997707 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.352      |
|    value_loss           | 163        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.0018   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0888   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 44        |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 507       |
|    time_elapsed         | 1829      |
|    total_timesteps      | 519168    |
| train/                  |           |
|    approx_kl            | 0.6466951 |
|    clip_fraction        | 0.494     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 10120     |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.352     |
|    value_loss           | 216       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0854   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0157    |
| rollout/                |           |
|    ep_len_mean          | 53.6      |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 508       |
|    time_elapsed         | 1832      |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 0.3536955 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.8     |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.28      |
|    n_updates            | 10140     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.352     |
|    value_loss           | 209       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 509        |
|    time_elapsed         | 1836       |
|    total_timesteps      | 521216     |
| train/                  |            |
|    approx_kl            | 0.29888153 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.08       |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.352      |
|    value_loss           | 174        |
----------------------------------------
Num timesteps: 522000
Best mean reward: -63.05 - Last mean reward per episode: -135.32
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00143  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0966   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0134    |
| rollout/                |           |
|    ep_len_mean          | 37.7      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 510       |
|    time_elapsed         | 1840      |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 0.3722145 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.68      |
|    n_updates            | 10180     |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.352     |
|    value_loss           | 122       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0898    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 48.1       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 511        |
|    time_elapsed         | 1843       |
|    total_timesteps      | 523264     |
| train/                  |            |
|    approx_kl            | 0.45173562 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.25       |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.352      |
|    value_loss           | 185        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.087     |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -204       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 512        |
|    time_elapsed         | 1847       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.33306018 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.94       |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.352      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.0842    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 66.2       |
|    ep_rew_mean          | -230       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 513        |
|    time_elapsed         | 1851       |
|    total_timesteps      | 525312     |
| train/                  |            |
|    approx_kl            | 0.23584895 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.3       |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.352      |
|    value_loss           | 245        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.0842    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 66.3       |
|    ep_rew_mean          | -230       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 514        |
|    time_elapsed         | 1855       |
|    total_timesteps      | 526336     |
| train/                  |            |
|    approx_kl            | 0.36110473 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.215     |
|    std                  | 0.352      |
|    value_loss           | 241        |
----------------------------------------
---------------------------------------
| reward                  | -4.06     |
| reward_contact          | -0.000569 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0902   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0109    |
| rollout/                |           |
|    ep_len_mean          | 24.7      |
|    ep_rew_mean          | -89.9     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 515       |
|    time_elapsed         | 1858      |
|    total_timesteps      | 527360    |
| train/                  |           |
|    approx_kl            | 1.1207352 |
|    clip_fraction        | 0.582     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.837     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 10280     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.352     |
|    value_loss           | 56        |
---------------------------------------
Num timesteps: 528000
Best mean reward: -63.05 - Last mean reward per episode: -89.11
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00024  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.093    |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0107    |
| rollout/                |           |
|    ep_len_mean          | 24.5      |
|    ep_rew_mean          | -89.1     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 516       |
|    time_elapsed         | 1862      |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 0.5009669 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | -0.352    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.45      |
|    n_updates            | 10300     |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.352     |
|    value_loss           | 402       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.08      |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 24.9       |
|    ep_rew_mean          | -90.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 517        |
|    time_elapsed         | 1866       |
|    total_timesteps      | 529408     |
| train/                  |            |
|    approx_kl            | 0.77265966 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.352      |
|    value_loss           | 77.3       |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0104     |
| rollout/                |            |
|    ep_len_mean          | 35.6       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 518        |
|    time_elapsed         | 1870       |
|    total_timesteps      | 530432     |
| train/                  |            |
|    approx_kl            | 0.32941997 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.44       |
|    n_updates            | 10340      |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.351      |
|    value_loss           | 186        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00135  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0698   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0112    |
| rollout/                |           |
|    ep_len_mean          | 35.9      |
|    ep_rew_mean          | -128      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 519       |
|    time_elapsed         | 1873      |
|    total_timesteps      | 531456    |
| train/                  |           |
|    approx_kl            | 0.7109261 |
|    clip_fraction        | 0.531     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.865     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.41      |
|    n_updates            | 10360     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 160       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 45.1       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 520        |
|    time_elapsed         | 1877       |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.51359904 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.47       |
|    n_updates            | 10380      |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.351      |
|    value_loss           | 95.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00235   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0766    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 46.5       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 521        |
|    time_elapsed         | 1881       |
|    total_timesteps      | 533504     |
| train/                  |            |
|    approx_kl            | 0.34905317 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.52       |
|    n_updates            | 10400      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.351      |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 534000
Best mean reward: -63.05 - Last mean reward per episode: -128.79
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00125  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0867   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0125    |
| rollout/                |           |
|    ep_len_mean          | 40.3      |
|    ep_rew_mean          | -143      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 522       |
|    time_elapsed         | 1885      |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 0.3295213 |
|    clip_fraction        | 0.538     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.14      |
|    n_updates            | 10420     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 167       |
---------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.00114   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00852    |
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | -78.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 523        |
|    time_elapsed         | 1889       |
|    total_timesteps      | 535552     |
| train/                  |            |
|    approx_kl            | 0.65951025 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 10440      |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.351      |
|    value_loss           | 145        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000901  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 31.8       |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 524        |
|    time_elapsed         | 1892       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.92099726 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.351      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.000682  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0102     |
| rollout/                |            |
|    ep_len_mean          | 33         |
|    ep_rew_mean          | -119       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 525        |
|    time_elapsed         | 1896       |
|    total_timesteps      | 537600     |
| train/                  |            |
|    approx_kl            | 0.41897923 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.77       |
|    n_updates            | 10480      |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.351      |
|    value_loss           | 166        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000802 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0824   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0109    |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 526       |
|    time_elapsed         | 1900      |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.9505757 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.04      |
|    n_updates            | 10500     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 82        |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000667  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0787    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 527        |
|    time_elapsed         | 1903       |
|    total_timesteps      | 539648     |
| train/                  |            |
|    approx_kl            | 0.56767243 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.1        |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.351      |
|    value_loss           | 216        |
----------------------------------------
Num timesteps: 540000
Best mean reward: -63.05 - Last mean reward per episode: -130.31
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.083     |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00979    |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 528        |
|    time_elapsed         | 1907       |
|    total_timesteps      | 540672     |
| train/                  |            |
|    approx_kl            | 0.28936315 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.5        |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 174        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00137  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0804   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0126    |
| rollout/                |           |
|    ep_len_mean          | 44.4      |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 529       |
|    time_elapsed         | 1911      |
|    total_timesteps      | 541696    |
| train/                  |           |
|    approx_kl            | 0.6286762 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.8     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.14      |
|    n_updates            | 10560     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.351     |
|    value_loss           | 140       |
---------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00138   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0782    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 530        |
|    time_elapsed         | 1914       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.35119474 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 48         |
|    n_updates            | 10580      |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.351      |
|    value_loss           | 285        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0818    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 531        |
|    time_elapsed         | 1918       |
|    total_timesteps      | 543744     |
| train/                  |            |
|    approx_kl            | 0.84457564 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 10600      |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.351      |
|    value_loss           | 174        |
----------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00149 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.09    |
| reward_torque           | -3.81    |
| reward_velocity         | 0.0162   |
| rollout/                |          |
|    ep_len_mean          | 45.3     |
|    ep_rew_mean          | -159     |
| time/                   |          |
|    fps                  | 283      |
|    iterations           | 532      |
|    time_elapsed         | 1922     |
|    total_timesteps      | 544768   |
| train/                  |          |
|    approx_kl            | 0.574094 |
|    clip_fraction        | 0.617    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.4    |
|    explained_variance   | 0.913    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.58     |
|    n_updates            | 10620    |
|    policy_gradient_loss | -0.196   |
|    std                  | 0.351    |
|    value_loss           | 106      |
--------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 55.8       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 533        |
|    time_elapsed         | 1925       |
|    total_timesteps      | 545792     |
| train/                  |            |
|    approx_kl            | 0.62303174 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 10640      |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.351      |
|    value_loss           | 235        |
----------------------------------------
Num timesteps: 546000
Best mean reward: -63.05 - Last mean reward per episode: -200.38
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0864    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 534        |
|    time_elapsed         | 1929       |
|    total_timesteps      | 546816     |
| train/                  |            |
|    approx_kl            | 0.26566616 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.4       |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.351      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.000993  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0897    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 535        |
|    time_elapsed         | 1932       |
|    total_timesteps      | 547840     |
| train/                  |            |
|    approx_kl            | 0.59053326 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.351      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.081     |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 536        |
|    time_elapsed         | 1936       |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.57912946 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.35       |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.351      |
|    value_loss           | 85.8       |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000352 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 38.3      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 537       |
|    time_elapsed         | 1940      |
|    total_timesteps      | 549888    |
| train/                  |           |
|    approx_kl            | 1.1979327 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.6     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 10720     |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.351     |
|    value_loss           | 141       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000473 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0833   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.014     |
| rollout/                |           |
|    ep_len_mean          | 48.1      |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 538       |
|    time_elapsed         | 1943      |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 1.0421927 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.928     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.902     |
|    n_updates            | 10740     |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.351     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000338  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 539        |
|    time_elapsed         | 1947       |
|    total_timesteps      | 551936     |
| train/                  |            |
|    approx_kl            | 0.34594598 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.351      |
|    value_loss           | 170        |
----------------------------------------
Num timesteps: 552000
Best mean reward: -63.05 - Last mean reward per episode: -128.74
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000338  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 540        |
|    time_elapsed         | 1951       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.85380745 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.9        |
|    n_updates            | 10780      |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.351      |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.000948  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0187     |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 541        |
|    time_elapsed         | 1954       |
|    total_timesteps      | 553984     |
| train/                  |            |
|    approx_kl            | 0.68285906 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.351      |
|    value_loss           | 53.2       |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0871    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 33.9       |
|    ep_rew_mean          | -120       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 542        |
|    time_elapsed         | 1958       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.79760426 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.6        |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 220        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.001    |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0827   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0114    |
| rollout/                |           |
|    ep_len_mean          | 24.7      |
|    ep_rew_mean          | -89.3     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 543       |
|    time_elapsed         | 1962      |
|    total_timesteps      | 556032    |
| train/                  |           |
|    approx_kl            | 0.3138877 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.931     |
|    learning_rate        | 0.0003    |
|    loss                 | 28.7      |
|    n_updates            | 10840     |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.351     |
|    value_loss           | 210       |
---------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0872   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0148    |
| rollout/                |           |
|    ep_len_mean          | 35.2      |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 544       |
|    time_elapsed         | 1966      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.4367705 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.84      |
|    n_updates            | 10860     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 198       |
---------------------------------------
Num timesteps: 558000
Best mean reward: -63.05 - Last mean reward per episode: -124.75
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0872    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 45.4       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 545        |
|    time_elapsed         | 1969       |
|    total_timesteps      | 558080     |
| train/                  |            |
|    approx_kl            | 0.31359047 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | 23         |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.351      |
|    value_loss           | 135        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.000586 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.079    |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 36.3      |
|    ep_rew_mean          | -129      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 546       |
|    time_elapsed         | 1973      |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 1.2112174 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | -0.0695   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.97      |
|    n_updates            | 10900     |
|    policy_gradient_loss | -0.139    |
|    std                  | 0.351     |
|    value_loss           | 85.8      |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000972  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0766    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 46.2       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 547        |
|    time_elapsed         | 1977       |
|    total_timesteps      | 560128     |
| train/                  |            |
|    approx_kl            | 0.44535783 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.17       |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 175        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00115  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0954   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0103    |
| rollout/                |           |
|    ep_len_mean          | 15.6      |
|    ep_rew_mean          | -59.7     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 548       |
|    time_elapsed         | 1980      |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.3666895 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.25      |
|    n_updates            | 10940     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 273       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0102     |
| rollout/                |            |
|    ep_len_mean          | 15.7       |
|    ep_rew_mean          | -60        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 549        |
|    time_elapsed         | 1984       |
|    total_timesteps      | 562176     |
| train/                  |            |
|    approx_kl            | 0.63639164 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.0406     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.12       |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.212     |
|    std                  | 0.351      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00963    |
| rollout/                |            |
|    ep_len_mean          | 26.4       |
|    ep_rew_mean          | -96.2      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 550        |
|    time_elapsed         | 1988       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.26546812 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.9       |
|    n_updates            | 10980      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.351      |
|    value_loss           | 140        |
----------------------------------------
Num timesteps: 564000
Best mean reward: -63.05 - Last mean reward per episode: -125.46
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0942   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0113    |
| rollout/                |           |
|    ep_len_mean          | 35.6      |
|    ep_rew_mean          | -127      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 551       |
|    time_elapsed         | 1991      |
|    total_timesteps      | 564224    |
| train/                  |           |
|    approx_kl            | 0.4346162 |
|    clip_fraction        | 0.51      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.21      |
|    n_updates            | 11000     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 145       |
---------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000746  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0588    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 14.4       |
|    ep_rew_mean          | -54.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 552        |
|    time_elapsed         | 1995       |
|    total_timesteps      | 565248     |
| train/                  |            |
|    approx_kl            | 0.40083927 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.9      |
|    explained_variance   | -0.285     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 11020      |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.351      |
|    value_loss           | 225        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.000853 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0539   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0166    |
| rollout/                |           |
|    ep_len_mean          | 25        |
|    ep_rew_mean          | -90.8     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 553       |
|    time_elapsed         | 1999      |
|    total_timesteps      | 566272    |
| train/                  |           |
|    approx_kl            | 0.5287496 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.708     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.48      |
|    n_updates            | 11040     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.351     |
|    value_loss           | 152       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000107  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0852    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 24.1       |
|    ep_rew_mean          | -88.1      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 554        |
|    time_elapsed         | 2002       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.46956775 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.09       |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.351      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000221  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00996    |
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | -80.6      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 555        |
|    time_elapsed         | 2006       |
|    total_timesteps      | 568320     |
| train/                  |            |
|    approx_kl            | 0.54161096 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.69       |
|    n_updates            | 11080      |
|    policy_gradient_loss | -0.227     |
|    std                  | 0.351      |
|    value_loss           | 132        |
----------------------------------------
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00118  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0822   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.00746   |
| rollout/                |           |
|    ep_len_mean          | 22.3      |
|    ep_rew_mean          | -82.2     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 556       |
|    time_elapsed         | 2010      |
|    total_timesteps      | 569344    |
| train/                  |           |
|    approx_kl            | 0.6290469 |
|    clip_fraction        | 0.579     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.876     |
|    n_updates            | 11100     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 90.3      |
---------------------------------------
Num timesteps: 570000
Best mean reward: -63.05 - Last mean reward per episode: -82.16
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0768   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.00836   |
| rollout/                |           |
|    ep_len_mean          | 32.3      |
|    ep_rew_mean          | -116      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 557       |
|    time_elapsed         | 2014      |
|    total_timesteps      | 570368    |
| train/                  |           |
|    approx_kl            | 0.6175133 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 11120     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.35      |
|    value_loss           | 158       |
---------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0692   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.00987   |
| rollout/                |           |
|    ep_len_mean          | 42.5      |
|    ep_rew_mean          | -150      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 558       |
|    time_elapsed         | 2017      |
|    total_timesteps      | 571392    |
| train/                  |           |
|    approx_kl            | 0.4204683 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.05      |
|    n_updates            | 11140     |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.35      |
|    value_loss           | 121       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00108   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.087     |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 35.5       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 559        |
|    time_elapsed         | 2021       |
|    total_timesteps      | 572416     |
| train/                  |            |
|    approx_kl            | 0.36268115 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.46       |
|    n_updates            | 11160      |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.35       |
|    value_loss           | 90.6       |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000951 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0906   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00828   |
| rollout/                |           |
|    ep_len_mean          | 16.8      |
|    ep_rew_mean          | -64       |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 560       |
|    time_elapsed         | 2025      |
|    total_timesteps      | 573440    |
| train/                  |           |
|    approx_kl            | 1.5559456 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | -0.0884   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.83      |
|    n_updates            | 11180     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.35      |
|    value_loss           | 118       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000951 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0906   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00845   |
| rollout/                |           |
|    ep_len_mean          | 26.4      |
|    ep_rew_mean          | -96.4     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 561       |
|    time_elapsed         | 2029      |
|    total_timesteps      | 574464    |
| train/                  |           |
|    approx_kl            | 1.0972482 |
|    clip_fraction        | 0.569     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.841     |
|    n_updates            | 11200     |
|    policy_gradient_loss | -0.225    |
|    std                  | 0.35      |
|    value_loss           | 137       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.001    |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00858   |
| rollout/                |           |
|    ep_len_mean          | 36.8      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 562       |
|    time_elapsed         | 2032      |
|    total_timesteps      | 575488    |
| train/                  |           |
|    approx_kl            | 1.5520428 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.848     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.99      |
|    n_updates            | 11220     |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.35      |
|    value_loss           | 101       |
---------------------------------------
Num timesteps: 576000
Best mean reward: -63.05 - Last mean reward per episode: -130.79
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00893   |
| rollout/                |           |
|    ep_len_mean          | 46.9      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 563       |
|    time_elapsed         | 2036      |
|    total_timesteps      | 576512    |
| train/                  |           |
|    approx_kl            | 1.0211638 |
|    clip_fraction        | 0.637     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.47      |
|    n_updates            | 11240     |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.35      |
|    value_loss           | 78.1      |
---------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00686   |
| rollout/                |           |
|    ep_len_mean          | 46.4      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 564       |
|    time_elapsed         | 2040      |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.6500547 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.366     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.75      |
|    n_updates            | 11260     |
|    policy_gradient_loss | -0.163    |
|    std                  | 0.35      |
|    value_loss           | 116       |
---------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_9
---------------------------------
| reward             | -3.22    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | 0        |
| reward_torque      | -3.23    |
| reward_velocity    | 0.0152   |
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -75.8    |
| time/              |          |
|    fps             | 282      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.26      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.27      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 81.2       |
|    ep_rew_mean          | -264       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10685694 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.00577    |
|    learning_rate        | 0.0003     |
|    loss                 | 69         |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.367      |
|    value_loss           | 309        |
----------------------------------------
----------------------------------------
| reward                  | -3.33      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.35      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | -407       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 3          |
|    time_elapsed         | 13         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.09222409 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0994    |
|    std                  | 0.367      |
|    value_loss           | 298        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 97.8       |
|    ep_rew_mean          | -321       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 4          |
|    time_elapsed         | 17         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.12846133 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.7       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.367      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | -370       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 5          |
|    time_elapsed         | 22         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.09951311 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0808    |
|    learning_rate        | 0.0003     |
|    loss                 | 40.6       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.367      |
|    value_loss           | 164        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -402.75
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0174     |
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | -403       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 6          |
|    time_elapsed         | 28         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07804628 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.4       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0756    |
|    std                  | 0.367      |
|    value_loss           | 263        |
----------------------------------------
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -424       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 7          |
|    time_elapsed         | 32         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.11590248 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.1       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.367      |
|    value_loss           | 210        |
----------------------------------------
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0237      |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | -352        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 8           |
|    time_elapsed         | 37          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.073391944 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.9        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.077      |
|    std                  | 0.367       |
|    value_loss           | 340         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | -389       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 9          |
|    time_elapsed         | 42         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07272075 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 98.3       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0948    |
|    std                  | 0.367      |
|    value_loss           | 381        |
----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -391        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 10          |
|    time_elapsed         | 47          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.112510115 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.96        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0958     |
|    std                  | 0.367       |
|    value_loss           | 125         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 107        |
|    ep_rew_mean          | -357       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 11         |
|    time_elapsed         | 52         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08184364 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.5       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0951    |
|    std                  | 0.367      |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -402.75 - Last mean reward per episode: -395.19
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -339       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 12         |
|    time_elapsed         | 57         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.08700475 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.306      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.8       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.367      |
|    value_loss           | 220        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0309     |
| rollout/                |            |
|    ep_len_mean          | 69.2       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 13         |
|    time_elapsed         | 62         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06795293 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.1       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.367      |
|    value_loss           | 207        |
----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0232      |
| rollout/                |             |
|    ep_len_mean          | 64.2        |
|    ep_rew_mean          | -215        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 14          |
|    time_elapsed         | 67          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.069691315 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | -0.0594     |
|    learning_rate        | 0.0003      |
|    loss                 | 97.6        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0907     |
|    std                  | 0.367       |
|    value_loss           | 403         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 44.7       |
|    ep_rew_mean          | -149       |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 15         |
|    time_elapsed         | 72         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.06312081 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.1       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0966    |
|    std                  | 0.367      |
|    value_loss           | 262        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 54.7       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 16         |
|    time_elapsed         | 77         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.08332546 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.7       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0938    |
|    std                  | 0.366      |
|    value_loss           | 116        |
----------------------------------------
--------------------------------------
| reward                  | -3.41    |
| reward_contact          | 0        |
| reward_ctrl             | 0        |
| reward_motion           | 0        |
| reward_torque           | -3.43    |
| reward_velocity         | 0.0234   |
| rollout/                |          |
|    ep_len_mean          | 65.5     |
|    ep_rew_mean          | -220     |
| time/                   |          |
|    fps                  | 210      |
|    iterations           | 17       |
|    time_elapsed         | 82       |
|    total_timesteps      | 17408    |
| train/                  |          |
|    approx_kl            | 0.072109 |
|    clip_fraction        | 0.176    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.2    |
|    explained_variance   | 0.765    |
|    learning_rate        | 0.0003   |
|    loss                 | 23.3     |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.0753  |
|    std                  | 0.366    |
|    value_loss           | 152      |
--------------------------------------
Num timesteps: 18000
Best mean reward: -395.19 - Last mean reward per episode: -252.97
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0236      |
| rollout/                |             |
|    ep_len_mean          | 75.4        |
|    ep_rew_mean          | -253        |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 18          |
|    time_elapsed         | 88          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.060054056 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.3       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.9        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0819     |
|    std                  | 0.366       |
|    value_loss           | 219         |
-----------------------------------------
---------------------------------------
| reward                  | -3.41     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.43     |
| reward_velocity         | 0.0253    |
| rollout/                |           |
|    ep_len_mean          | 85.4      |
|    ep_rew_mean          | -287      |
| time/                   |           |
|    fps                  | 208       |
|    iterations           | 19        |
|    time_elapsed         | 93        |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.0652843 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0003    |
|    loss                 | 46.1      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0702   |
|    std                  | 0.366     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0264     |
| rollout/                |            |
|    ep_len_mean          | 95.4       |
|    ep_rew_mean          | -322       |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 20         |
|    time_elapsed         | 98         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10353299 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.366      |
|    value_loss           | 152        |
----------------------------------------
----------------------------------------
| reward                  | -3.42      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0288     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -356       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 21         |
|    time_elapsed         | 103        |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.15301812 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.8       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.366      |
|    value_loss           | 103        |
----------------------------------------
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -350        |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 22          |
|    time_elapsed         | 108         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.048719645 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0751     |
|    std                  | 0.366       |
|    value_loss           | 174         |
-----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0262     |
| rollout/                |            |
|    ep_len_mean          | 108        |
|    ep_rew_mean          | -365       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 23         |
|    time_elapsed         | 113        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.06391521 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.23       |
|    learning_rate        | 0.0003     |
|    loss                 | 38.8       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0748    |
|    std                  | 0.366      |
|    value_loss           | 274        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -252.97 - Last mean reward per episode: -398.95
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | -402       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 24         |
|    time_elapsed         | 118        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.10754076 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.81       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0999    |
|    std                  | 0.366      |
|    value_loss           | 109        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -435       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 25         |
|    time_elapsed         | 123        |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.06762931 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0988    |
|    std                  | 0.366      |
|    value_loss           | 269        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0234     |
| rollout/                |            |
|    ep_len_mean          | 138        |
|    ep_rew_mean          | -466       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 26         |
|    time_elapsed         | 128        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.06698659 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.7       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0869    |
|    std                  | 0.366      |
|    value_loss           | 258        |
----------------------------------------
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | -473       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 27         |
|    time_elapsed         | 133        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.03745903 |
|    clip_fraction        | 0.0746     |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.4       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0618    |
|    std                  | 0.366      |
|    value_loss           | 290        |
----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0324     |
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | -507       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 28         |
|    time_elapsed         | 138        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.09587663 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.182      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.7       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 300        |
----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0342      |
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | -509        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 29          |
|    time_elapsed         | 143         |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.082485154 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0873     |
|    std                  | 0.366       |
|    value_loss           | 235         |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -252.97 - Last mean reward per episode: -543.54
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0395     |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -539       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 30         |
|    time_elapsed         | 148        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.08126942 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.48       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.366      |
|    value_loss           | 79.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0389     |
| rollout/                |            |
|    ep_len_mean          | 140        |
|    ep_rew_mean          | -475       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 31         |
|    time_elapsed         | 153        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.08574103 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 194        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0331     |
| rollout/                |            |
|    ep_len_mean          | 96.1       |
|    ep_rew_mean          | -326       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 32         |
|    time_elapsed         | 158        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.06671844 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.3       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0904    |
|    std                  | 0.366      |
|    value_loss           | 193        |
----------------------------------------
-----------------------------------------
| reward                  | -3.47       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0335      |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | -360        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 33          |
|    time_elapsed         | 163         |
|    total_timesteps      | 33792       |
| train/                  |             |
|    approx_kl            | 0.062012903 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0928     |
|    std                  | 0.366       |
|    value_loss           | 222         |
-----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0324     |
| rollout/                |            |
|    ep_len_mean          | 94.8       |
|    ep_rew_mean          | -322       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 34         |
|    time_elapsed         | 168        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07803719 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.4       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 91.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.47       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0325      |
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | -315        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 35          |
|    time_elapsed         | 173         |
|    total_timesteps      | 35840       |
| train/                  |             |
|    approx_kl            | 0.099081606 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.35        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 140         |
-----------------------------------------
Num timesteps: 36000
Best mean reward: -252.97 - Last mean reward per episode: -315.38
---------------------------------------
| reward                  | -3.46     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.49     |
| reward_velocity         | 0.0323    |
| rollout/                |           |
|    ep_len_mean          | 102       |
|    ep_rew_mean          | -349      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 36        |
|    time_elapsed         | 178       |
|    total_timesteps      | 36864     |
| train/                  |           |
|    approx_kl            | 0.1101397 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.214     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.82      |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.0943   |
|    std                  | 0.366     |
|    value_loss           | 136       |
---------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 78.2       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 37         |
|    time_elapsed         | 183        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.04298848 |
|    clip_fraction        | 0.0799     |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.366      |
|    value_loss           | 157        |
----------------------------------------
-----------------------------------------
| reward                  | -3.45       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0258      |
| rollout/                |             |
|    ep_len_mean          | 87.9        |
|    ep_rew_mean          | -300        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 38          |
|    time_elapsed         | 188         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.067519605 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.0662      |
|    learning_rate        | 0.0003      |
|    loss                 | 42.3        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.096      |
|    std                  | 0.366       |
|    value_loss           | 346         |
-----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.025      |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 39         |
|    time_elapsed         | 193        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.08896073 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.78       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 136        |
----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0226      |
| rollout/                |             |
|    ep_len_mean          | 87.9        |
|    ep_rew_mean          | -300        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 40          |
|    time_elapsed         | 198         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.091612816 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.366       |
|    value_loss           | 68.3        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.43       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 95.5        |
|    ep_rew_mean          | -326        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 41          |
|    time_elapsed         | 203         |
|    total_timesteps      | 41984       |
| train/                  |             |
|    approx_kl            | 0.091683194 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.63        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.366       |
|    value_loss           | 151         |
-----------------------------------------
Num timesteps: 42000
Best mean reward: -252.97 - Last mean reward per episode: -326.50
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 42         |
|    time_elapsed         | 208        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.13295096 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.366      |
|    value_loss           | 224        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 76.4       |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 43         |
|    time_elapsed         | 213        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.08034168 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.64       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.366      |
|    value_loss           | 159        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 80.4       |
|    ep_rew_mean          | -276       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 44         |
|    time_elapsed         | 218        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.09620194 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.19       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 109        |
----------------------------------------
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0276     |
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | -312       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 45         |
|    time_elapsed         | 223        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.08886904 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.53       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 141        |
----------------------------------------
-----------------------------------------
| reward                  | -3.48       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.51       |
| reward_velocity         | 0.0278      |
| rollout/                |             |
|    ep_len_mean          | 90.7        |
|    ep_rew_mean          | -311        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 46          |
|    time_elapsed         | 228         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.105513476 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.3         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.105      |
|    std                  | 0.366       |
|    value_loss           | 118         |
-----------------------------------------
Num timesteps: 48000
Best mean reward: -252.97 - Last mean reward per episode: -348.42
---------------------------------------
| reward                  | -3.49     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.52     |
| reward_velocity         | 0.0278    |
| rollout/                |           |
|    ep_len_mean          | 102       |
|    ep_rew_mean          | -348      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 47        |
|    time_elapsed         | 233       |
|    total_timesteps      | 48128     |
| train/                  |           |
|    approx_kl            | 0.1558862 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.654     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.34      |
|    n_updates            | 920       |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.366     |
|    value_loss           | 171       |
---------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0282     |
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | -390       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 48         |
|    time_elapsed         | 238        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06145618 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.5       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.366      |
|    value_loss           | 190        |
----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0232     |
| rollout/                |            |
|    ep_len_mean          | 93.1       |
|    ep_rew_mean          | -319       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 49         |
|    time_elapsed         | 243        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.07513807 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.093     |
|    std                  | 0.366      |
|    value_loss           | 157        |
----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | -351       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 50         |
|    time_elapsed         | 248        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.13576367 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.366      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | -348       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 51         |
|    time_elapsed         | 253        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.11762303 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.66       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | -383       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 52         |
|    time_elapsed         | 258        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.08652225 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.1       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.366      |
|    value_loss           | 255        |
----------------------------------------
Num timesteps: 54000
Best mean reward: -252.97 - Last mean reward per episode: -417.31
-----------------------------------------
| reward                  | -3.55       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.58       |
| reward_velocity         | 0.029       |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | -419        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 53          |
|    time_elapsed         | 263         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.099555954 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.366       |
|    value_loss           | 171         |
-----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0293     |
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | -454       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 54         |
|    time_elapsed         | 268        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06288915 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.9       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.366      |
|    value_loss           | 192        |
----------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.55       |
| reward_velocity         | 0.03        |
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | -453        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 55          |
|    time_elapsed         | 273         |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.057649896 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.366       |
|    value_loss           | 261         |
-----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.55      |
| reward_velocity         | 0.037      |
| rollout/                |            |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | -488       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 56         |
|    time_elapsed         | 278        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.10201044 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.0003     |
|    loss                 | 18         |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0933    |
|    std                  | 0.366      |
|    value_loss           | 266        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0385     |
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | -419       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 57         |
|    time_elapsed         | 283        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.06344026 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.01       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0913    |
|    std                  | 0.366      |
|    value_loss           | 116        |
----------------------------------------
---------------------------------------
| reward                  | -3.54     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0416    |
| rollout/                |           |
|    ep_len_mean          | 133       |
|    ep_rew_mean          | -454      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 58        |
|    time_elapsed         | 287       |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0898159 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.1      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.106    |
|    std                  | 0.366     |
|    value_loss           | 234       |
---------------------------------------
Num timesteps: 60000
Best mean reward: -252.97 - Last mean reward per episode: -456.03
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0414     |
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | -456       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 59         |
|    time_elapsed         | 292        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.12097858 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.56       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.365      |
|    value_loss           | 89         |
----------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0424     |
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | -415       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 60         |
|    time_elapsed         | 297        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.14020307 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 85         |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0424     |
| rollout/                |            |
|    ep_len_mean          | 132        |
|    ep_rew_mean          | -454       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 61         |
|    time_elapsed         | 302        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.08372341 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 226        |
----------------------------------------
-----------------------------------------
| reward                  | -3.49       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0356      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | -362        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 62          |
|    time_elapsed         | 307         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.080171436 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.71        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.365       |
|    value_loss           | 114         |
-----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0288     |
| rollout/                |            |
|    ep_len_mean          | 84.9       |
|    ep_rew_mean          | -292       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 63         |
|    time_elapsed         | 311        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.08373365 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0373    |
|    learning_rate        | 0.0003     |
|    loss                 | 27.5       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.365      |
|    value_loss           | 333        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 74.1       |
|    ep_rew_mean          | -254       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 64         |
|    time_elapsed         | 316        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.09768624 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.97       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.365      |
|    value_loss           | 225        |
----------------------------------------
Num timesteps: 66000
Best mean reward: -252.97 - Last mean reward per episode: -254.16
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 74.1       |
|    ep_rew_mean          | -254       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 65         |
|    time_elapsed         | 321        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.11038068 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.4       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 186        |
----------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.54       |
| reward_velocity         | 0.0145      |
| rollout/                |             |
|    ep_len_mean          | 64.7        |
|    ep_rew_mean          | -223        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 66          |
|    time_elapsed         | 326         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.109496385 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.86        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.118      |
|    std                  | 0.365       |
|    value_loss           | 93.9        |
-----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 54.8       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 67         |
|    time_elapsed         | 331        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.09371218 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.5       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 299        |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 54.3       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 68         |
|    time_elapsed         | 336        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.09444392 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.302      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.5       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 244        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 53.8       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 69         |
|    time_elapsed         | 340        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.14548701 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.365      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 63.9       |
|    ep_rew_mean          | -220       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 70         |
|    time_elapsed         | 345        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.08314982 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 24.1       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.365      |
|    value_loss           | 279        |
----------------------------------------
Num timesteps: 72000
Best mean reward: -252.97 - Last mean reward per episode: -179.34
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -3.51       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.52       |
| reward_velocity         | 0.0191      |
| rollout/                |             |
|    ep_len_mean          | 51.3        |
|    ep_rew_mean          | -177        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 71          |
|    time_elapsed         | 351         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.096345484 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.127      |
|    std                  | 0.365       |
|    value_loss           | 126         |
-----------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 61.7       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 72         |
|    time_elapsed         | 356        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.15074712 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.71       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.365      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 71.7       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 73         |
|    time_elapsed         | 361        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.13299297 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.73       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 81         |
|    ep_rew_mean          | -278       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 74         |
|    time_elapsed         | 365        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.08469623 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 79.4       |
|    ep_rew_mean          | -273       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 75         |
|    time_elapsed         | 370        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.16653484 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.43       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.365      |
|    value_loss           | 134        |
----------------------------------------
-----------------------------------------
| reward                  | -3.55       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.57       |
| reward_velocity         | 0.014       |
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | -207        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 76          |
|    time_elapsed         | 375         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.090216026 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.0501      |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.127      |
|    std                  | 0.365       |
|    value_loss           | 240         |
-----------------------------------------
Num timesteps: 78000
Best mean reward: -179.34 - Last mean reward per episode: -206.66
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 69.9       |
|    ep_rew_mean          | -241       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 77         |
|    time_elapsed         | 380        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.10045168 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.365      |
|    value_loss           | 208        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 43.2       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 78         |
|    time_elapsed         | 384        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.12553442 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.04       |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.365      |
|    value_loss           | 126        |
----------------------------------------
---------------------------------------
| reward                  | -3.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.54     |
| reward_velocity         | 0.0144    |
| rollout/                |           |
|    ep_len_mean          | 43.2      |
|    ep_rew_mean          | -150      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 79        |
|    time_elapsed         | 389       |
|    total_timesteps      | 80896     |
| train/                  |           |
|    approx_kl            | 0.1187946 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.32      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.137    |
|    std                  | 0.365     |
|    value_loss           | 163       |
---------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.53       |
| reward_velocity         | 0.015       |
| rollout/                |             |
|    ep_len_mean          | 53.1        |
|    ep_rew_mean          | -185        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 80          |
|    time_elapsed         | 394         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.103914715 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.85        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.149      |
|    std                  | 0.365       |
|    value_loss           | 227         |
-----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 53.9       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 81         |
|    time_elapsed         | 399        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.09956674 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.68       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.365      |
|    value_loss           | 70         |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0213     |
| rollout/                |            |
|    ep_len_mean          | 54.5       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 82         |
|    time_elapsed         | 404        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.07202363 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.141     |
|    learning_rate        | 0.0003     |
|    loss                 | 13.3       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.365      |
|    value_loss           | 250        |
----------------------------------------
Num timesteps: 84000
Best mean reward: -179.34 - Last mean reward per episode: -189.54
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 54.4       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 83         |
|    time_elapsed         | 409        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.11812264 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.95       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.365      |
|    value_loss           | 86.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 64.2       |
|    ep_rew_mean          | -223       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 84         |
|    time_elapsed         | 414        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.12485481 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.22       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 68.7       |
|    ep_rew_mean          | -239       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 85         |
|    time_elapsed         | 419        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.12843837 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.5       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.365      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0226     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -272       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 86         |
|    time_elapsed         | 424        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.15686947 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.94       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 86.3       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 87         |
|    time_elapsed         | 429        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.14253086 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 225        |
----------------------------------------
Num timesteps: 90000
Best mean reward: -179.34 - Last mean reward per episode: -265.84
----------------------------------------
| reward                  | -3.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 77.5       |
|    ep_rew_mean          | -269       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 88         |
|    time_elapsed         | 434        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.09138085 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.94       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.365      |
|    value_loss           | 233        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 56.3       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 89         |
|    time_elapsed         | 439        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.09164302 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.227     |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.365      |
|    value_loss           | 297        |
----------------------------------------
---------------------------------------
| reward                  | -3.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0218    |
| rollout/                |           |
|    ep_len_mean          | 46.8      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 90        |
|    time_elapsed         | 444       |
|    total_timesteps      | 92160     |
| train/                  |           |
|    approx_kl            | 0.1434091 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.229     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.2      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.365     |
|    value_loss           | 233       |
---------------------------------------
---------------------------------------
| reward                  | -3.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0235    |
| rollout/                |           |
|    ep_len_mean          | 38.7      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 91        |
|    time_elapsed         | 449       |
|    total_timesteps      | 93184     |
| train/                  |           |
|    approx_kl            | 0.1124434 |
|    clip_fraction        | 0.283     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.06      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.365     |
|    value_loss           | 138       |
---------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 92         |
|    time_elapsed         | 454        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.09183948 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.27       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.365      |
|    value_loss           | 215        |
----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 47.8       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 93         |
|    time_elapsed         | 459        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.14662409 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.02       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.365      |
|    value_loss           | 108        |
----------------------------------------
Num timesteps: 96000
Best mean reward: -179.34 - Last mean reward per episode: -199.31
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0296     |
| rollout/                |            |
|    ep_len_mean          | 56.6       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 94         |
|    time_elapsed         | 463        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.14176369 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.8       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 218        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_10
---------------------------------
| reward             | -3.47    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | -0.1     |
| reward_torque      | -3.38    |
| reward_velocity    | 0.0115   |
| rollout/           |          |
|    ep_len_mean     | 72.6     |
|    ep_rew_mean     | -221     |
| time/              |          |
|    fps             | 341      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.1       |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 68.3       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 2          |
|    time_elapsed         | 7          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.12546113 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | -0.00252   |
|    learning_rate        | 0.0003     |
|    loss                 | 26.3       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.367      |
|    value_loss           | 252        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.1       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 95.9       |
|    ep_rew_mean          | -291       |
| time/                   |            |
|    fps                  | 276        |
|    iterations           | 3          |
|    time_elapsed         | 11         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.09592333 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.367      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.085     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -345       |
| time/                   |            |
|    fps                  | 270        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.11307147 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 93.3       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_11
---------------------------------
| reward             | -3.16    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | -0.1     |
| reward_torque      | -3.1     |
| reward_velocity    | 0.0345   |
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -79.8    |
| time/              |          |
|    fps             | 340      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | -3.33     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.023     |
| reward_torque           | -3.37     |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 65.9      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 290       |
|    iterations           | 2         |
|    time_elapsed         | 7         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 0.1118205 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.000973  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.74      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.367     |
|    value_loss           | 164       |
---------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.00494     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0206      |
| rollout/                |             |
|    ep_len_mean          | 62.2        |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 277         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.075688794 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.0377      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.367       |
|    value_loss           | 122         |
-----------------------------------------
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0133     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 88.3       |
|    ep_rew_mean          | -226       |
| time/                   |            |
|    fps                  | 271        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.08545355 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.367      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.35      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0411     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 5          |
|    time_elapsed         | 19         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.06722508 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.8       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0931    |
|    std                  | 0.367      |
|    value_loss           | 117        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -229.45
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0272     |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -225       |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 6          |
|    time_elapsed         | 23         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07324174 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.4       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.367      |
|    value_loss           | 91.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0236     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0236     |
| rollout/                |            |
|    ep_len_mean          | 71.9       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 7          |
|    time_elapsed         | 27         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.06833801 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.056      |
|    learning_rate        | 0.0003     |
|    loss                 | 70.8       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.367      |
|    value_loss           | 205        |
----------------------------------------
-----------------------------------------
| reward                  | -3.41       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0259      |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0231      |
| rollout/                |             |
|    ep_len_mean          | 61          |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 8           |
|    time_elapsed         | 31          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.062687606 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.9        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.367       |
|    value_loss           | 263         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0141      |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 35.9        |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 9           |
|    time_elapsed         | 35          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.054853775 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0993     |
|    std                  | 0.367       |
|    value_loss           | 283         |
-----------------------------------------
----------------------------------------
| reward                  | -3.42      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0141     |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 41.2       |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 10         |
|    time_elapsed         | 39         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.09887732 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.404      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.367      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.43      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0143     |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 43.1       |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 11         |
|    time_elapsed         | 43         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.13207608 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | -0.000791  |
|    learning_rate        | 0.0003     |
|    loss                 | 38.3       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.366      |
|    value_loss           | 182        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -229.45 - Last mean reward per episode: -127.00
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0396     |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0304     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 12         |
|    time_elapsed         | 47         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07386387 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.1       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.366      |
|    value_loss           | 206        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.00293   |
| reward_torque           | -3.43      |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 45.9       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 13         |
|    time_elapsed         | 51         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06611325 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.291      |
|    learning_rate        | 0.0003     |
|    loss                 | 74.6       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 280        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0148     |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0272     |
| rollout/                |            |
|    ep_len_mean          | 45.9       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 14         |
|    time_elapsed         | 55         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09070608 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.0003     |
|    loss                 | 72.5       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 196        |
----------------------------------------
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0148     |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 15         |
|    time_elapsed         | 59         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.11738958 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.366      |
|    value_loss           | 84         |
----------------------------------------
-----------------------------------------
| reward                  | -3.37       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0325      |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0281      |
| rollout/                |             |
|    ep_len_mean          | 50.2        |
|    ep_rew_mean          | -134        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 16          |
|    time_elapsed         | 63          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.120515704 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.366       |
|    value_loss           | 108         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0321      |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0262      |
| rollout/                |             |
|    ep_len_mean          | 51.5        |
|    ep_rew_mean          | -138        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 17          |
|    time_elapsed         | 67          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.114222154 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.144      |
|    std                  | 0.366       |
|    value_loss           | 129         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -127.00 - Last mean reward per episode: -156.47
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0513      |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0262      |
| rollout/                |             |
|    ep_len_mean          | 59.8        |
|    ep_rew_mean          | -156        |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 18          |
|    time_elapsed         | 71          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.123548895 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.85        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.138      |
|    std                  | 0.366       |
|    value_loss           | 73.4        |
-----------------------------------------
----------------------------------------
| reward                  | -3.43      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0232     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0311     |
| rollout/                |            |
|    ep_len_mean          | 68         |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 19         |
|    time_elapsed         | 75         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.09727553 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.366      |
|    value_loss           | 73.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0449     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0317     |
| rollout/                |            |
|    ep_len_mean          | 73.9       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 20         |
|    time_elapsed         | 79         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10544833 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | 24.5       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 124        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_12
---------------------------------
| reward             | -2.43    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | 1.02     |
| reward_torque      | -3.47    |
| reward_velocity    | 0.0149   |
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 342      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.991      |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 98         |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 2          |
|    time_elapsed         | 7          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10207515 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.0184     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.4       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.367      |
|    value_loss           | 156        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.945      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 3          |
|    time_elapsed         | 11         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.11764982 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | -0.162     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.1        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.367      |
|    value_loss           | 47.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.932      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 73.4       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 272        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.12323756 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | -0.145     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.7       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.367      |
|    value_loss           | 64         |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 64.3       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 268        |
|    iterations           | 5          |
|    time_elapsed         | 19         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.09229784 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.135      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.83       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.367      |
|    value_loss           | 50.8       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -131.76
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 76.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 265        |
|    iterations           | 6          |
|    time_elapsed         | 23         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07450648 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.0812     |
|    learning_rate        | 0.0003     |
|    loss                 | 33.9       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.367      |
|    value_loss           | 89.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 71.5       |
|    ep_rew_mean          | -146       |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 7          |
|    time_elapsed         | 27         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.11284697 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.367      |
|    value_loss           | 28.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.949      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 8          |
|    time_elapsed         | 31         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07660948 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.132     |
|    learning_rate        | 0.0003     |
|    loss                 | 17         |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 78.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.953      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 87.5       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 9          |
|    time_elapsed         | 35         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07484859 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.367      |
|    value_loss           | 80.9       |
----------------------------------------
---------------------------------------
| reward                  | -2.5      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.943     |
| reward_torque           | -3.47     |
| reward_velocity         | 0.027     |
| rollout/                |           |
|    ep_len_mean          | 90.3      |
|    ep_rew_mean          | -183      |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 10        |
|    time_elapsed         | 39        |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.0823557 |
|    clip_fraction        | 0.205     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.101    |
|    std                  | 0.366     |
|    value_loss           | 76.1      |
---------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.944      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 83.7       |
|    ep_rew_mean          | -170       |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 11         |
|    time_elapsed         | 43         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.11638403 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.366      |
|    value_loss           | 70.1       |
----------------------------------------
Num timesteps: 12000
Best mean reward: -131.76 - Last mean reward per episode: -170.43
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.954      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 89.2       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 12         |
|    time_elapsed         | 47         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07480501 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0003     |
|    loss                 | 33.6       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.366      |
|    value_loss           | 159        |
----------------------------------------
---------------------------------------
| reward                  | -2.52     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.954     |
| reward_torque           | -3.5      |
| reward_velocity         | 0.0265    |
| rollout/                |           |
|    ep_len_mean          | 96.4      |
|    ep_rew_mean          | -197      |
| time/                   |           |
|    fps                  | 259       |
|    iterations           | 13        |
|    time_elapsed         | 51        |
|    total_timesteps      | 13312     |
| train/                  |           |
|    approx_kl            | 0.0688664 |
|    clip_fraction        | 0.164     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.11      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0863   |
|    std                  | 0.366     |
|    value_loss           | 85.3      |
---------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.957      |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 90.3       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 14         |
|    time_elapsed         | 55         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.11092051 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.2       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.366      |
|    value_loss           | 78.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.962      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 89.9       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 15         |
|    time_elapsed         | 59         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.11095235 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.6       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.366      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.956      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0284     |
| rollout/                |            |
|    ep_len_mean          | 87         |
|    ep_rew_mean          | -181       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 16         |
|    time_elapsed         | 63         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.11190578 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.38       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.366      |
|    value_loss           | 67.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.961      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0276     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 17         |
|    time_elapsed         | 67         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.10090847 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.68       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.366      |
|    value_loss           | 50.7       |
----------------------------------------
Num timesteps: 18000
Best mean reward: -131.76 - Last mean reward per episode: -162.58
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.961      |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 76.7       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 18         |
|    time_elapsed         | 71         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.11514368 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.92       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.366      |
|    value_loss           | 56         |
----------------------------------------
-----------------------------------------
| reward                  | -2.49       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.964       |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 83.5        |
|    ep_rew_mean          | -176        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 19          |
|    time_elapsed         | 75          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.100146875 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.366       |
|    value_loss           | 71.2        |
-----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.957      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 82.6       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 20         |
|    time_elapsed         | 79         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10226883 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.01       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 58.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 21         |
|    time_elapsed         | 84         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.14570352 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.69       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.365      |
|    value_loss           | 48.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.931      |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 68.1       |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 22         |
|    time_elapsed         | 89         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.08297723 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | -0.133     |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 86.1       |
----------------------------------------
-----------------------------------------
| reward                  | -2.51       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.923       |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0242      |
| rollout/                |             |
|    ep_len_mean          | 74.5        |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 23          |
|    time_elapsed         | 94          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.107540056 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.13        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.365       |
|    value_loss           | 72          |
-----------------------------------------
Num timesteps: 24000
Best mean reward: -131.76 - Last mean reward per episode: -143.16
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.926      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0219     |
| rollout/                |            |
|    ep_len_mean          | 67.8       |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 24         |
|    time_elapsed         | 99         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.14310622 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.33       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.365      |
|    value_loss           | 54.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.47      |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 67         |
|    ep_rew_mean          | -144       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 25         |
|    time_elapsed         | 104        |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.11249768 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.26       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.365      |
|    value_loss           | 70.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 60.4       |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 26         |
|    time_elapsed         | 109        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.18206921 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.46       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.365      |
|    value_loss           | 72.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.943      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 61         |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 27         |
|    time_elapsed         | 113        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.17379266 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.0549    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.01       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.365      |
|    value_loss           | 70.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.95       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 71.2       |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 28         |
|    time_elapsed         | 118        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13648833 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.365      |
|    value_loss           | 38.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.95       |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 29         |
|    time_elapsed         | 123        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.10979601 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 31.5       |
----------------------------------------
Num timesteps: 30000
Best mean reward: -131.76 - Last mean reward per episode: -188.92
---------------------------------------
| reward                  | -2.5      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.982     |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0255    |
| rollout/                |           |
|    ep_len_mean          | 79.4      |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 240       |
|    iterations           | 30        |
|    time_elapsed         | 127       |
|    total_timesteps      | 30720     |
| train/                  |           |
|    approx_kl            | 0.0853755 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.91      |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.111    |
|    std                  | 0.365     |
|    value_loss           | 49.5      |
---------------------------------------
---------------------------------------
| reward                  | -2.54     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.96      |
| reward_torque           | -3.52     |
| reward_velocity         | 0.022     |
| rollout/                |           |
|    ep_len_mean          | 63.1      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 239       |
|    iterations           | 31        |
|    time_elapsed         | 132       |
|    total_timesteps      | 31744     |
| train/                  |           |
|    approx_kl            | 0.0878329 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.0003    |
|    loss                 | 11.7      |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.365     |
|    value_loss           | 117       |
---------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 62         |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 32         |
|    time_elapsed         | 137        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.21021704 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.365      |
|    value_loss           | 64.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 36.2       |
|    ep_rew_mean          | -82.1      |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 33         |
|    time_elapsed         | 142        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.09699252 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | -0.0219    |
|    learning_rate        | 0.0003     |
|    loss                 | 8.91       |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 94.7       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.928     |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 44.5      |
|    ep_rew_mean          | -99.2     |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 34        |
|    time_elapsed         | 147       |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.1271945 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.0598    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.57      |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.145    |
|    std                  | 0.364     |
|    value_loss           | 83.9      |
---------------------------------------
---------------------------------------
| reward                  | -2.65     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.918     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 39.1      |
|    ep_rew_mean          | -87.8     |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 35        |
|    time_elapsed         | 152       |
|    total_timesteps      | 35840     |
| train/                  |           |
|    approx_kl            | 0.1630308 |
|    clip_fraction        | 0.347     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | -0.283    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.58      |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.364     |
|    value_loss           | 66.1      |
---------------------------------------
Num timesteps: 36000
Best mean reward: -131.76 - Last mean reward per episode: -87.77
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 48.9       |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 36         |
|    time_elapsed         | 157        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.20378548 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 37.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0152     |
| rollout/                |            |
|    ep_len_mean          | 48.9       |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 37         |
|    time_elapsed         | 162        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.15472117 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.62       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.364      |
|    value_loss           | 79.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 57.9       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 38         |
|    time_elapsed         | 167        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.08284654 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.31       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.364      |
|    value_loss           | 64.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.975      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 65.7       |
|    ep_rew_mean          | -141       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 39         |
|    time_elapsed         | 172        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.16583937 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.364      |
|    value_loss           | 53.5       |
----------------------------------------
---------------------------------------
| reward                  | -2.6      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.982     |
| reward_torque           | -3.6      |
| reward_velocity         | 0.0172    |
| rollout/                |           |
|    ep_len_mean          | 76.1      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 40        |
|    time_elapsed         | 177       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.1525042 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.26      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.364     |
|    value_loss           | 84.2      |
---------------------------------------
----------------------------------------
| reward                  | -2.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.963      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 57.1       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 41         |
|    time_elapsed         | 182        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.34836543 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.82       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.364      |
|    value_loss           | 28.8       |
----------------------------------------
Num timesteps: 42000
Best mean reward: -87.77 - Last mean reward per episode: -124.21
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 48         |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 42         |
|    time_elapsed         | 187        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.11095698 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0388     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.94       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.364      |
|    value_loss           | 122        |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.943      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -86.4      |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 43         |
|    time_elapsed         | 192        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.13068032 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.199      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.72       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.364      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -2.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.941      |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 47.4       |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 44         |
|    time_elapsed         | 197        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.15542763 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.364      |
|    value_loss           | 31.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.92       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 45.8       |
|    ep_rew_mean          | -100       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 45         |
|    time_elapsed         | 201        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.17269671 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.47       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.364      |
|    value_loss           | 47.2       |
----------------------------------------
---------------------------------------
| reward                  | -2.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.93      |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0194    |
| rollout/                |           |
|    ep_len_mean          | 43.1      |
|    ep_rew_mean          | -94.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 46        |
|    time_elapsed         | 206       |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.2158814 |
|    clip_fraction        | 0.421     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.49      |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.364     |
|    value_loss           | 70.5      |
---------------------------------------
Num timesteps: 48000
Best mean reward: -87.77 - Last mean reward per episode: -92.36
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 42.3       |
|    ep_rew_mean          | -92.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 47         |
|    time_elapsed         | 211        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.11759651 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | -0.115     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.29       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.364      |
|    value_loss           | 108        |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.936      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 32         |
|    ep_rew_mean          | -72.4      |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 48         |
|    time_elapsed         | 216        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.13015762 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.364      |
|    value_loss           | 43.9       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.941     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.02      |
| rollout/                |           |
|    ep_len_mean          | 43        |
|    ep_rew_mean          | -95.7     |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 49        |
|    time_elapsed         | 221       |
|    total_timesteps      | 50176     |
| train/                  |           |
|    approx_kl            | 0.1601876 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.43      |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.364     |
|    value_loss           | 75        |
---------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.941      |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 43.5       |
|    ep_rew_mean          | -96.6      |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 50         |
|    time_elapsed         | 226        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.33444732 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.364      |
|    value_loss           | 72.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 53.7       |
|    ep_rew_mean          | -118       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 51         |
|    time_elapsed         | 231        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.16293636 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.81       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.364      |
|    value_loss           | 72         |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 63.8       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 52         |
|    time_elapsed         | 237        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.19210586 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.39       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.364      |
|    value_loss           | 81         |
----------------------------------------
Num timesteps: 54000
Best mean reward: -87.77 - Last mean reward per episode: -159.14
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 73.5       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 53         |
|    time_elapsed         | 242        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.14484899 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.363      |
|    value_loss           | 45.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 83.7       |
|    ep_rew_mean          | -179       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 54         |
|    time_elapsed         | 246        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.15225911 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.363      |
|    value_loss           | 69.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.911      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 73.8       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 55         |
|    time_elapsed         | 251        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.29744816 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.363      |
|    value_loss           | 48.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.913      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 73         |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 56         |
|    time_elapsed         | 256        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.15328516 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.233      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.02       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.363      |
|    value_loss           | 88.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.913      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 82.5       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 57         |
|    time_elapsed         | 261        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.18133706 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.36       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.37       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 81.5       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.932      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 83.6       |
|    ep_rew_mean          | -179       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 58         |
|    time_elapsed         | 266        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.14764038 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.12       |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 40.8       |
----------------------------------------
Num timesteps: 60000
Best mean reward: -87.77 - Last mean reward per episode: -183.77
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 85.2       |
|    ep_rew_mean          | -184       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 59         |
|    time_elapsed         | 271        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.13345909 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.57       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 67.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.953      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 75         |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 60         |
|    time_elapsed         | 276        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.20082566 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 81.8       |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.961     |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0191    |
| rollout/                |           |
|    ep_len_mean          | 84.5      |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 61        |
|    time_elapsed         | 280       |
|    total_timesteps      | 62464     |
| train/                  |           |
|    approx_kl            | 0.1963744 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.173     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.363     |
|    value_loss           | 70.1      |
---------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.971     |
| reward_torque           | -3.52     |
| reward_velocity         | 0.0211    |
| rollout/                |           |
|    ep_len_mean          | 75.9      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 62        |
|    time_elapsed         | 285       |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.2019859 |
|    clip_fraction        | 0.349     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.61      |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.363     |
|    value_loss           | 68.7      |
---------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.971      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 81.5       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 63         |
|    time_elapsed         | 290        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.20065655 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.83       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 88.5       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.982      |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 91.9       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 64         |
|    time_elapsed         | 295        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.17107004 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 49.7       |
----------------------------------------
Num timesteps: 66000
Best mean reward: -87.77 - Last mean reward per episode: -199.10
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.98       |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 91.5       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 65         |
|    time_elapsed         | 300        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.20050028 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.81       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.363      |
|    value_loss           | 37.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.973      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 66         |
|    time_elapsed         | 304        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.28185272 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.0769     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.32       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 51.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 80.8       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 67         |
|    time_elapsed         | 309        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.30631936 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.028      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.363      |
|    value_loss           | 52.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.956      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0258     |
| rollout/                |            |
|    ep_len_mean          | 90.7       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 68         |
|    time_elapsed         | 314        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.33374453 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.39       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.363      |
|    value_loss           | 44.8       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.936     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0241    |
| rollout/                |           |
|    ep_len_mean          | 79.9      |
|    ep_rew_mean          | -172      |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 69        |
|    time_elapsed         | 319       |
|    total_timesteps      | 70656     |
| train/                  |           |
|    approx_kl            | 0.1711858 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.609     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.58      |
|    n_updates            | 1360      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 39.8      |
---------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.936     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0222    |
| rollout/                |           |
|    ep_len_mean          | 89.4      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 70        |
|    time_elapsed         | 324       |
|    total_timesteps      | 71680     |
| train/                  |           |
|    approx_kl            | 0.1989198 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.418     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.96      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.363     |
|    value_loss           | 80.3      |
---------------------------------------
Num timesteps: 72000
Best mean reward: -87.77 - Last mean reward per episode: -191.68
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.926      |
| reward_torque           | -3.6       |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 89.7       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 71         |
|    time_elapsed         | 328        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.21040803 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.49       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 43.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.935      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 98.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 72         |
|    time_elapsed         | 334        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.23942284 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.363      |
|    value_loss           | 28.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.929      |
| reward_torque           | -3.57      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 92.5       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 73         |
|    time_elapsed         | 338        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.16479537 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.1       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 78.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 74         |
|    time_elapsed         | 344        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.20323634 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.45       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.363      |
|    value_loss           | 63.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 88.9       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 75         |
|    time_elapsed         | 349        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.53468585 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 46.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 99.2       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 76         |
|    time_elapsed         | 354        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.16609463 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.3        |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.363      |
|    value_loss           | 67.6       |
----------------------------------------
Num timesteps: 78000
Best mean reward: -87.77 - Last mean reward per episode: -211.87
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 88.7       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 77         |
|    time_elapsed         | 359        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.21269989 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.67       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 58.6       |
----------------------------------------
---------------------------------------
| reward                  | -2.62     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.951     |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0131    |
| rollout/                |           |
|    ep_len_mean          | 89.3      |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 219       |
|    iterations           | 78        |
|    time_elapsed         | 364       |
|    total_timesteps      | 79872     |
| train/                  |           |
|    approx_kl            | 0.3063129 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.45      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.362     |
|    value_loss           | 69.7      |
---------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 88.8       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 79         |
|    time_elapsed         | 369        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.20394605 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.483      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.362      |
|    value_loss           | 74.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.964      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0151     |
| rollout/                |            |
|    ep_len_mean          | 88.4       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 80         |
|    time_elapsed         | 374        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.21989313 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.362      |
|    value_loss           | 58.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.965      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 81         |
|    time_elapsed         | 379        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.44319522 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.77       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.362      |
|    value_loss           | 52.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.955      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 81.8       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 82         |
|    time_elapsed         | 384        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.19507873 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.19       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.362      |
|    value_loss           | 57.1       |
----------------------------------------
Num timesteps: 84000
Best mean reward: -87.77 - Last mean reward per episode: -177.34
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0187     |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 83         |
|    time_elapsed         | 389        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.13268054 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.56       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.362      |
|    value_loss           | 76.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 84         |
|    time_elapsed         | 394        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.19481435 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.05       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.362      |
|    value_loss           | 88.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.955      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 85.7       |
|    ep_rew_mean          | -185       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 85         |
|    time_elapsed         | 399        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.24213727 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.64       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.362      |
|    value_loss           | 25.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.944      |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0248     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 86         |
|    time_elapsed         | 404        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.28687358 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.31       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.362      |
|    value_loss           | 28.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0271     |
| rollout/                |            |
|    ep_len_mean          | 98.1       |
|    ep_rew_mean          | -210       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 87         |
|    time_elapsed         | 409        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.17261931 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.362      |
|    value_loss           | 104        |
----------------------------------------
Num timesteps: 90000
Best mean reward: -87.77 - Last mean reward per episode: -192.07
----------------------------------------
| reward                  | -2.66      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0291     |
| rollout/                |            |
|    ep_len_mean          | 89.6       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 88         |
|    time_elapsed         | 413        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.17289877 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.32       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.362      |
|    value_loss           | 67.4       |
----------------------------------------
---------------------------------------
| reward                  | -2.65     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.943     |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0276    |
| rollout/                |           |
|    ep_len_mean          | 88        |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 217       |
|    iterations           | 89        |
|    time_elapsed         | 418       |
|    total_timesteps      | 91136     |
| train/                  |           |
|    approx_kl            | 0.2088778 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.687     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.98      |
|    n_updates            | 1760      |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.362     |
|    value_loss           | 38.2      |
---------------------------------------
---------------------------------------
| reward                  | -2.68     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.927     |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0249    |
| rollout/                |           |
|    ep_len_mean          | 88.9      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 217       |
|    iterations           | 90        |
|    time_elapsed         | 423       |
|    total_timesteps      | 92160     |
| train/                  |           |
|    approx_kl            | 0.2544246 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.96      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.362     |
|    value_loss           | 53.3      |
---------------------------------------
----------------------------------------
| reward                  | -2.68      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.929      |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0303     |
| rollout/                |            |
|    ep_len_mean          | 98.8       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 91         |
|    time_elapsed         | 428        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.27079284 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.362      |
|    value_loss           | 36.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.935      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0312     |
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | -237       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 92         |
|    time_elapsed         | 433        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.16997279 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.07       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.362      |
|    value_loss           | 37.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 78.5       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 93         |
|    time_elapsed         | 439        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.14407443 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.7       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.362      |
|    value_loss           | 106        |
----------------------------------------
Num timesteps: 96000
Best mean reward: -87.77 - Last mean reward per episode: -170.84
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 88.5       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 94         |
|    time_elapsed         | 444        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.34288812 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.362      |
|    value_loss           | 93.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.71      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.925      |
| reward_torque           | -3.66      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 67.4       |
|    ep_rew_mean          | -149       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 95         |
|    time_elapsed         | 449        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.15484408 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.64       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.362      |
|    value_loss           | 68.6       |
----------------------------------------
usage: ddpg.py [-h] [--experiment EXPERIMENT] [--out_path OUT_PATH]
               [--env ENV] [--env_version ENV_VERSION] [--model_dir MODEL_DIR]
               [--test_env [TEST_ENV]] [--her [HER]] [--ppo [PPO]]
               [--sac [SAC]] [--a2c [A2C]] [--standard [STANDARD]]
               [--evaluate [EVALUATE]]
ddpg.py: error: argument --experiment: invalid int value: '74^C-out_path'
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
[DDPG] Using HER
Using cuda device
Logging to rl/out_dir/models/exp74/SAC_1
----------------------------------
| reward             | -1.16     |
| reward_contact     | -0.0297   |
| reward_motion      | 0.869     |
| reward_torque      | -0.423    |
| reward_velocity    | -1.57     |
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.93e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 981       |
|    time_elapsed    | 4         |
|    total timesteps | 4096      |
| train/             |           |
|    std             | 0.0498    |
----------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -1925.16
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.39     |
| reward_contact     | -0.0224   |
| reward_motion      | 1.21      |
| reward_torque      | -0.448    |
| reward_velocity    | -2.13     |
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.92e+03 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 880       |
|    time_elapsed    | 9         |
|    total timesteps | 8192      |
| train/             |           |
|    std             | 0.0498    |
----------------------------------
---------------------------------
| reward             | -1.29    |
| reward_contact     | -0.0203  |
| reward_motion      | 1.18     |
| reward_torque      | -0.383   |
| reward_velocity    | -2.07    |
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | -1.8e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 237      |
|    time_elapsed    | 49       |
|    total timesteps | 11874    |
| train/             |          |
|    actor_loss      | 582      |
|    critic_loss     | 738      |
|    ent_coef        | 0.755    |
|    ent_coef_loss   | -0.464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1856     |
|    std             | 0.0498   |
---------------------------------
Num timesteps: 12000
Best mean reward: -1925.16 - Last mean reward per episode: -1800.38
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.27     |
| reward_contact     | -0.0259   |
| reward_motion      | 0.891     |
| reward_torque      | -0.292    |
| reward_velocity    | -1.84     |
| rollout/           |           |
|    ep_len_mean     | 998       |
|    ep_rew_mean     | -1.71e+03 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 120       |
|    time_elapsed    | 132       |
|    total timesteps | 15970     |
| train/             |           |
|    actor_loss      | 1.55e+03  |
|    critic_loss     | 609       |
|    ent_coef        | 0.27      |
|    ent_coef_loss   | -5.55     |
|    learning_rate   | 0.0003    |
|    n_updates       | 5952      |
|    std             | 0.0497    |
----------------------------------
Num timesteps: 18000
Best mean reward: -1800.38 - Last mean reward per episode: -1707.11
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.26     |
| reward_contact     | -0.0268   |
| reward_motion      | 0.78      |
| reward_torque      | -0.249    |
| reward_velocity    | -1.77     |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 93        |
|    time_elapsed    | 215       |
|    total timesteps | 20066     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 602       |
|    ent_coef        | 0.207     |
|    ent_coef_loss   | 0.334     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10048     |
|    std             | 0.0491    |
----------------------------------
Num timesteps: 24000
Best mean reward: -1707.11 - Last mean reward per episode: -1690.20
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.23     |
| reward_contact     | -0.0282   |
| reward_motion      | 0.696     |
| reward_torque      | -0.217    |
| reward_velocity    | -1.68     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.69e+03 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 81        |
|    time_elapsed    | 297       |
|    total timesteps | 24162     |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 693       |
|    ent_coef        | 0.212     |
|    ent_coef_loss   | -0.429    |
|    learning_rate   | 0.0003    |
|    n_updates       | 14144     |
|    std             | 0.0486    |
----------------------------------
----------------------------------
| reward             | -1.31     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.647     |
| reward_torque      | -0.199    |
| reward_velocity    | -1.73     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.67e+03 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 74        |
|    time_elapsed    | 381       |
|    total timesteps | 28258     |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 678       |
|    ent_coef        | 0.154     |
|    ent_coef_loss   | -1.18     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18240     |
|    std             | 0.0484    |
----------------------------------
Num timesteps: 30000
Best mean reward: -1690.20 - Last mean reward per episode: -1668.32
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.34     |
| reward_contact     | -0.0297   |
| reward_motion      | 0.643     |
| reward_torque      | -0.195    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.67e+03 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 69        |
|    time_elapsed    | 464       |
|    total timesteps | 32354     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 656       |
|    ent_coef        | 0.141     |
|    ent_coef_loss   | -0.192    |
|    learning_rate   | 0.0003    |
|    n_updates       | 22336     |
|    std             | 0.0482    |
----------------------------------
----------------------------------
| reward             | -1.42     |
| reward_contact     | -0.0308   |
| reward_motion      | 0.596     |
| reward_torque      | -0.185    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 36        |
|    fps             | 67        |
|    time_elapsed    | 537       |
|    total timesteps | 35997     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 405       |
|    ent_coef        | 0.0992    |
|    ent_coef_loss   | -0.699    |
|    learning_rate   | 0.0003    |
|    n_updates       | 25984     |
|    std             | 0.048     |
----------------------------------
Num timesteps: 36000
Best mean reward: -1668.32 - Last mean reward per episode: -1653.08
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.42     |
| reward_contact     | -0.0299   |
| reward_motion      | 0.586     |
| reward_torque      | -0.182    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 40        |
|    fps             | 64        |
|    time_elapsed    | 619       |
|    total timesteps | 40093     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 541       |
|    ent_coef        | 0.0747    |
|    ent_coef_loss   | -1.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 30080     |
|    std             | 0.0475    |
----------------------------------
Num timesteps: 42000
Best mean reward: -1653.08 - Last mean reward per episode: -1651.14
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.43     |
| reward_contact     | -0.031    |
| reward_motion      | 0.536     |
| reward_torque      | -0.171    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 62        |
|    time_elapsed    | 702       |
|    total timesteps | 44189     |
| train/             |           |
|    actor_loss      | 1.68e+03  |
|    critic_loss     | 418       |
|    ent_coef        | 0.0831    |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 0.0003    |
|    n_updates       | 34176     |
|    std             | 0.0471    |
----------------------------------
Num timesteps: 48000
Best mean reward: -1651.14 - Last mean reward per episode: -1646.53
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.44     |
| reward_contact     | -0.0304   |
| reward_motion      | 0.524     |
| reward_torque      | -0.168    |
| reward_velocity    | -1.77     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 61        |
|    time_elapsed    | 785       |
|    total timesteps | 48285     |
| train/             |           |
|    actor_loss      | 1.69e+03  |
|    critic_loss     | 947       |
|    ent_coef        | 0.0683    |
|    ent_coef_loss   | -0.652    |
|    learning_rate   | 0.0003    |
|    n_updates       | 38272     |
|    std             | 0.0468    |
----------------------------------
----------------------------------
| reward             | -1.46     |
| reward_contact     | -0.0314   |
| reward_motion      | 0.499     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 60        |
|    time_elapsed    | 868       |
|    total timesteps | 52381     |
| train/             |           |
|    actor_loss      | 1.7e+03   |
|    critic_loss     | 2.27e+03  |
|    ent_coef        | 0.0672    |
|    ent_coef_loss   | 2.05      |
|    learning_rate   | 0.0003    |
|    n_updates       | 42368     |
|    std             | 0.0465    |
----------------------------------
Num timesteps: 54000
Best mean reward: -1646.53 - Last mean reward per episode: -1640.94
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0324   |
| reward_motion      | 0.482     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.74     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 59        |
|    time_elapsed    | 950       |
|    total timesteps | 56477     |
| train/             |           |
|    actor_loss      | 1.75e+03  |
|    critic_loss     | 535       |
|    ent_coef        | 0.0666    |
|    ent_coef_loss   | -0.558    |
|    learning_rate   | 0.0003    |
|    n_updates       | 46464     |
|    std             | 0.0462    |
----------------------------------
Num timesteps: 60000
Best mean reward: -1640.94 - Last mean reward per episode: -1629.86
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0315   |
| reward_motion      | 0.505     |
| reward_torque      | -0.169    |
| reward_velocity    | -1.75     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 58        |
|    time_elapsed    | 1027      |
|    total timesteps | 60301     |
| train/             |           |
|    actor_loss      | 1.79e+03  |
|    critic_loss     | 1.07e+03  |
|    ent_coef        | 0.0558    |
|    ent_coef_loss   | -0.522    |
|    learning_rate   | 0.0003    |
|    n_updates       | 50304     |
|    std             | 0.0459    |
----------------------------------
----------------------------------
| reward             | -1.48     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.519     |
| reward_torque      | -0.171    |
| reward_velocity    | -1.79     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 64        |
|    fps             | 58        |
|    time_elapsed    | 1110      |
|    total timesteps | 64397     |
| train/             |           |
|    actor_loss      | 1.82e+03  |
|    critic_loss     | 790       |
|    ent_coef        | 0.0543    |
|    ent_coef_loss   | -0.724    |
|    learning_rate   | 0.0003    |
|    n_updates       | 54400     |
|    std             | 0.0455    |
----------------------------------
Num timesteps: 66000
Best mean reward: -1629.86 - Last mean reward per episode: -1636.83
----------------------------------
| reward             | -1.49     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.509     |
| reward_torque      | -0.164    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 57        |
|    time_elapsed    | 1192      |
|    total timesteps | 68493     |
| train/             |           |
|    actor_loss      | 1.82e+03  |
|    critic_loss     | 921       |
|    ent_coef        | 0.0479    |
|    ent_coef_loss   | -0.744    |
|    learning_rate   | 0.0003    |
|    n_updates       | 58496     |
|    std             | 0.0451    |
----------------------------------
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0296   |
| reward_motion      | 0.559     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.82     |
| rollout/           |           |
|    ep_len_mean     | 997       |
|    ep_rew_mean     | -1.62e+03 |
| time/              |           |
|    episodes        | 72        |
|    fps             | 57        |
|    time_elapsed    | 1259      |
|    total timesteps | 71801     |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 642       |
|    ent_coef        | 0.0454    |
|    ent_coef_loss   | 1.17      |
|    learning_rate   | 0.0003    |
|    n_updates       | 61760     |
|    std             | 0.0447    |
----------------------------------
Num timesteps: 72000
Best mean reward: -1629.86 - Last mean reward per episode: -1595.16
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------
| reward             | -1.45    |
| reward_contact     | -0.029   |
| reward_motion      | 0.563    |
| reward_torque      | -0.16    |
| reward_velocity    | -1.82    |
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 56       |
|    time_elapsed    | 1323     |
|    total timesteps | 74961    |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 603      |
|    ent_coef        | 0.0412   |
|    ent_coef_loss   | 0.318    |
|    learning_rate   | 0.0003   |
|    n_updates       | 64960    |
|    std             | 0.0443   |
---------------------------------
Num timesteps: 78000
Best mean reward: -1595.16 - Last mean reward per episode: -1598.11
---------------------------------
| reward             | -1.48    |
| reward_contact     | -0.03    |
| reward_motion      | 0.552    |
| reward_torque      | -0.16    |
| reward_velocity    | -1.84    |
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 56       |
|    time_elapsed    | 1405     |
|    total timesteps | 79057    |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 636      |
|    ent_coef        | 0.0427   |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.0003   |
|    n_updates       | 69056    |
|    std             | 0.0439   |
---------------------------------
---------------------------------
| reward             | -1.49    |
| reward_contact     | -0.0302  |
| reward_motion      | 0.553    |
| reward_torque      | -0.164   |
| reward_velocity    | -1.85    |
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 55       |
|    time_elapsed    | 1487     |
|    total timesteps | 83153    |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 762      |
|    ent_coef        | 0.0353   |
|    ent_coef_loss   | -0.806   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73152    |
|    std             | 0.0434   |
---------------------------------
Num timesteps: 84000
Best mean reward: -1595.16 - Last mean reward per episode: -1594.25
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.46     |
| reward_contact     | -0.0295   |
| reward_motion      | 0.557     |
| reward_torque      | -0.166    |
| reward_velocity    | -1.82     |
| rollout/           |           |
|    ep_len_mean     | 987       |
|    ep_rew_mean     | -1.59e+03 |
| time/              |           |
|    episodes        | 88        |
|    fps             | 55        |
|    time_elapsed    | 1560      |
|    total timesteps | 86825     |
| train/             |           |
|    actor_loss      | 1.91e+03  |
|    critic_loss     | 686       |
|    ent_coef        | 0.04      |
|    ent_coef_loss   | 0.938     |
|    learning_rate   | 0.0003    |
|    n_updates       | 76800     |
|    std             | 0.043     |
----------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_32
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -2.68e+03 |
| time/              |           |
|    fps             | 16        |
|    iterations      | 1         |
|    time_elapsed    | 63        |
|    total_timesteps | 1024      |
----------------------------------
---------------------------------------
| reward                  | -2.08     |
| reward_contact          | -0.337    |
| reward_motion           | 0         |
| reward_torque           | -1.74     |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -1.96e+03 |
| time/                   |           |
|    fps                  | 16        |
|    iterations           | 2         |
|    time_elapsed         | 127       |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 0.1806148 |
|    clip_fraction        | 0.499     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 5.67e-05  |
|    learning_rate        | 0.0003    |
|    loss                 | 59.1      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.151    |
|    std                  | 0.368     |
|    value_loss           | 275       |
---------------------------------------
---------------------------------------
| reward                  | -1.87     |
| reward_contact          | -0.168    |
| reward_motion           | 0         |
| reward_torque           | -0.96     |
| reward_velocity         | -0.746    |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -1.63e+03 |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 3         |
|    time_elapsed         | 192       |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 2.233634  |
|    clip_fraction        | 0.707     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | -0.0156   |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0807   |
|    std                  | 0.37      |
|    value_loss           | 39.2      |
---------------------------------------
---------------------------------------
| reward                  | -0.949    |
| reward_contact          | -0.0841   |
| reward_motion           | 0.25      |
| reward_torque           | -0.742    |
| reward_velocity         | -0.373    |
| rollout/                |           |
|    ep_len_mean          | 792       |
|    ep_rew_mean          | -1.23e+03 |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 4         |
|    time_elapsed         | 256       |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.743238  |
|    clip_fraction        | 0.569     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.00142   |
|    learning_rate        | 0.0003    |
|    loss                 | 17.5      |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0893   |
|    std                  | 0.37      |
|    value_loss           | 35.7      |
---------------------------------------
----------------------------------------
| reward                  | -0.87      |
| reward_contact          | -0.0673    |
| reward_motion           | 0.2        |
| reward_torque           | -0.64      |
| reward_velocity         | -0.363     |
| rollout/                |            |
|    ep_len_mean          | 838        |
|    ep_rew_mean          | -1.11e+03  |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 5          |
|    time_elapsed         | 322        |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.21903056 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.0114     |
|    learning_rate        | 0.0003     |
|    loss                 | 16.2       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0905    |
|    std                  | 0.369      |
|    value_loss           | 34.8       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -971.13
Saving new best model to rl/out_dir/models/exp74/best_model.zip
--------------------------------------
| reward                  | -0.725   |
| reward_contact          | -0.0561  |
| reward_motion           | 0.167    |
| reward_torque           | -0.533   |
| reward_velocity         | -0.302   |
| rollout/                |          |
|    ep_len_mean          | 869      |
|    ep_rew_mean          | -971     |
| time/                   |          |
|    fps                  | 15       |
|    iterations           | 6        |
|    time_elapsed         | 387      |
|    total_timesteps      | 6144     |
| train/                  |          |
|    approx_kl            | 0.162121 |
|    clip_fraction        | 0.482    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.135    |
|    learning_rate        | 0.0003   |
|    loss                 | 22.3     |
|    n_updates            | 100      |
|    policy_gradient_loss | -0.102   |
|    std                  | 0.368    |
|    value_loss           | 34.6     |
--------------------------------------
----------------------------------------
| reward                  | -0.637     |
| reward_contact          | -0.0481    |
| reward_motion           | 0.143      |
| reward_torque           | -0.472     |
| reward_velocity         | -0.259     |
| rollout/                |            |
|    ep_len_mean          | 891        |
|    ep_rew_mean          | -857       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 7          |
|    time_elapsed         | 451        |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.35550386 |
|    clip_fraction        | 0.597      |
|    clip_range           | 0.4        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.15       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.82       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.368      |
|    value_loss           | 23.4       |
----------------------------------------
----------------------------------------
| reward                  | -0.557     |
| reward_contact          | -0.0421    |
| reward_motion           | 0.125      |
| reward_torque           | -0.413     |
| reward_velocity         | -0.227     |
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | -773       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 8          |
|    time_elapsed         | 515        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.18209553 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.22       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| reward                  | -0.495     |
| reward_contact          | -0.0374    |
| reward_motion           | 0.111      |
| reward_torque           | -0.367     |
| reward_velocity         | -0.202     |
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | -718       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 9          |
|    time_elapsed         | 580        |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.28194153 |
|    clip_fraction        | 0.679      |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.0811     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.29       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0958    |
|    std                  | 0.366      |
|    value_loss           | 18.5       |
----------------------------------------
--------------------------------------
| reward                  | -0.446   |
| reward_contact          | -0.0337  |
| reward_motion           | 0.1      |
| reward_torque           | -0.331   |
| reward_velocity         | -0.181   |
| rollout/                |          |
|    ep_len_mean          | 931      |
|    ep_rew_mean          | -641     |
| time/                   |          |
|    fps                  | 15       |
|    iterations           | 10       |
|    time_elapsed         | 643      |
|    total_timesteps      | 10240    |
| train/                  |          |
|    approx_kl            | 1.136606 |
|    clip_fraction        | 0.72     |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.615    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.11     |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.1     |
|    std                  | 0.365    |
|    value_loss           | 10.3     |
--------------------------------------
---------------------------------------
| reward                  | -0.405    |
| reward_contact          | -0.0306   |
| reward_motion           | 0.0909    |
| reward_torque           | -0.301    |
| reward_velocity         | -0.165    |
| rollout/                |           |
|    ep_len_mean          | 939       |
|    ep_rew_mean          | -588      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 11        |
|    time_elapsed         | 707       |
|    total_timesteps      | 11264     |
| train/                  |           |
|    approx_kl            | 0.2896827 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.47      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.01      |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.151    |
|    std                  | 0.365     |
|    value_loss           | 5.05      |
---------------------------------------
Num timesteps: 12000
Best mean reward: -971.13 - Last mean reward per episode: -537.74
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -0.371    |
| reward_contact          | -0.028    |
| reward_motion           | 0.0833    |
| reward_torque           | -0.276    |
| reward_velocity         | -0.151    |
| rollout/                |           |
|    ep_len_mean          | 946       |
|    ep_rew_mean          | -538      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 12        |
|    time_elapsed         | 771       |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.4127201 |
|    clip_fraction        | 0.664     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.348     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.24      |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.362     |
|    value_loss           | 4.41      |
---------------------------------------
----------------------------------------
| reward                  | -0.343     |
| reward_contact          | -0.0259    |
| reward_motion           | 0.0769     |
| reward_torque           | -0.254     |
| reward_velocity         | -0.14      |
| rollout/                |            |
|    ep_len_mean          | 952        |
|    ep_rew_mean          | -498       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 13         |
|    time_elapsed         | 836        |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.17545548 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.7        |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.361      |
|    value_loss           | 6.67       |
----------------------------------------
----------------------------------------
| reward                  | -0.318     |
| reward_contact          | -0.024     |
| reward_motion           | 0.0714     |
| reward_torque           | -0.236     |
| reward_velocity         | -0.13      |
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | -439       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 14         |
|    time_elapsed         | 900        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.20172518 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.28       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.361      |
|    value_loss           | 11.1       |
----------------------------------------
---------------------------------------
| reward                  | -0.32     |
| reward_contact          | -0.0451   |
| reward_motion           | 0.0667    |
| reward_torque           | -0.22     |
| reward_velocity         | -0.121    |
| rollout/                |           |
|    ep_len_mean          | 962       |
|    ep_rew_mean          | -404      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 15        |
|    time_elapsed         | 963       |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.5106324 |
|    clip_fraction        | 0.696     |
|    clip_range           | 0.4       |
|    entropy_loss         | -113      |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.96      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.36      |
|    value_loss           | 4.09      |
---------------------------------------
----------------------------------------
| reward                  | -0.3       |
| reward_contact          | -0.0423    |
| reward_motion           | 0.0625     |
| reward_torque           | -0.207     |
| reward_velocity         | -0.113     |
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | -374       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 16         |
|    time_elapsed         | 1026       |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.32043108 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.167      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.611      |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.359      |
|    value_loss           | 1.9        |
----------------------------------------
----------------------------------------
| reward                  | -0.282     |
| reward_contact          | -0.0398    |
| reward_motion           | 0.0588     |
| reward_torque           | -0.195     |
| reward_velocity         | -0.107     |
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | -350       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 17         |
|    time_elapsed         | 1090       |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.21412814 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.14       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.357      |
|    value_loss           | 3.77       |
----------------------------------------
Num timesteps: 18000
Best mean reward: -537.74 - Last mean reward per episode: -322.53
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -0.267     |
| reward_contact          | -0.0376    |
| reward_motion           | 0.0556     |
| reward_torque           | -0.184     |
| reward_velocity         | -0.101     |
| rollout/                |            |
|    ep_len_mean          | 972        |
|    ep_rew_mean          | -323       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 18         |
|    time_elapsed         | 1154       |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.18796721 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.461      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.357      |
|    value_loss           | 1.49       |
----------------------------------------
----------------------------------------
| reward                  | -0.252     |
| reward_contact          | -0.0356    |
| reward_motion           | 0.0526     |
| reward_torque           | -0.174     |
| reward_velocity         | -0.0955    |
| rollout/                |            |
|    ep_len_mean          | 975        |
|    ep_rew_mean          | -298       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 19         |
|    time_elapsed         | 1218       |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.39409107 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.532      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.354      |
|    value_loss           | 2.69       |
----------------------------------------
----------------------------------------
| reward                  | -0.24      |
| reward_contact          | -0.0338    |
| reward_motion           | 0.05       |
| reward_torque           | -0.165     |
| reward_velocity         | -0.0907    |
| rollout/                |            |
|    ep_len_mean          | 978        |
|    ep_rew_mean          | -277       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 20         |
|    time_elapsed         | 1282       |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.14990345 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.986      |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.355      |
|    value_loss           | 1.96       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_33
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 1        |
|    time_elapsed    | 12       |
|    total_timesteps | 200      |
---------------------------------
---------------------------------------
| reward                  | 0.664     |
| reward_contact          | -0.336    |
| reward_motion           | 1         |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -419      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 2         |
|    time_elapsed         | 25        |
|    total_timesteps      | 400       |
| train/                  |           |
|    approx_kl            | 0.4539015 |
|    clip_fraction        | 0.684     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -8.54e-05 |
|    learning_rate        | 0.0003    |
|    loss                 | 208       |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.368     |
|    value_loss           | 555       |
---------------------------------------
---------------------------------------
| reward                  | -1.39     |
| reward_contact          | -0.419    |
| reward_motion           | 0.5       |
| reward_torque           | 0         |
| reward_velocity         | -1.48     |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -375      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 3         |
|    time_elapsed         | 37        |
|    total_timesteps      | 600       |
| train/                  |           |
|    approx_kl            | 0.5292574 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.00273  |
|    learning_rate        | 0.0003    |
|    loss                 | 61.1      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.368     |
|    value_loss           | 157       |
---------------------------------------
----------------------------------------
| reward                  | -0.986     |
| reward_contact          | -0.279     |
| reward_motion           | 0.333      |
| reward_torque           | -0.0561    |
| reward_velocity         | -0.983     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -298       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 4          |
|    time_elapsed         | 50         |
|    total_timesteps      | 800        |
| train/                  |            |
|    approx_kl            | 0.53796005 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.00127    |
|    learning_rate        | 0.0003     |
|    loss                 | 63.1       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.368      |
|    value_loss           | 148        |
----------------------------------------
---------------------------------------
| reward                  | -1.23     |
| reward_contact          | -0.946    |
| reward_motion           | 0.5       |
| reward_torque           | -0.0421   |
| reward_velocity         | -0.738    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -248      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 5         |
|    time_elapsed         | 63        |
|    total_timesteps      | 1000      |
| train/                  |           |
|    approx_kl            | 1.2135609 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | -0.00439  |
|    learning_rate        | 0.0003    |
|    loss                 | 20.1      |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.367     |
|    value_loss           | 38.4      |
---------------------------------------
---------------------------------------
| reward                  | -0.98     |
| reward_contact          | -0.756    |
| reward_motion           | 0.4       |
| reward_torque           | -0.0337   |
| reward_velocity         | -0.59     |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 6         |
|    time_elapsed         | 75        |
|    total_timesteps      | 1200      |
| train/                  |           |
|    approx_kl            | 1.0392833 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.000897  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.49      |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.367     |
|    value_loss           | 18.6      |
---------------------------------------
----------------------------------------
| reward                  | -0.833     |
| reward_contact          | -0.63      |
| reward_motion           | 0.333      |
| reward_torque           | -0.0443    |
| reward_velocity         | -0.492     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -193       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 7          |
|    time_elapsed         | 88         |
|    total_timesteps      | 1400       |
| train/                  |            |
|    approx_kl            | 0.43278924 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.0511     |
|    learning_rate        | 0.0003     |
|    loss                 | 14.8       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.367      |
|    value_loss           | 34.5       |
----------------------------------------
---------------------------------------
| reward                  | -0.714    |
| reward_contact          | -0.54     |
| reward_motion           | 0.286     |
| reward_torque           | -0.038    |
| reward_velocity         | -0.421    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 8         |
|    time_elapsed         | 101       |
|    total_timesteps      | 1600      |
| train/                  |           |
|    approx_kl            | 0.5990219 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.0692   |
|    learning_rate        | 0.0003    |
|    loss                 | 8.55      |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.367     |
|    value_loss           | 18.5      |
---------------------------------------
---------------------------------------
| reward                  | -0.625    |
| reward_contact          | -0.473    |
| reward_motion           | 0.25      |
| reward_torque           | -0.0332   |
| reward_velocity         | -0.369    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 9         |
|    time_elapsed         | 114       |
|    total_timesteps      | 1800      |
| train/                  |           |
|    approx_kl            | 0.5318181 |
|    clip_fraction        | 0.415     |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | 0.0472    |
|    learning_rate        | 0.0003    |
|    loss                 | 9.1       |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.129    |
|    std                  | 0.367     |
|    value_loss           | 19.5      |
---------------------------------------
----------------------------------------
| reward                  | -0.685     |
| reward_contact          | -0.468     |
| reward_motion           | 0.222      |
| reward_torque           | -0.112     |
| reward_velocity         | -0.328     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 10         |
|    time_elapsed         | 126        |
|    total_timesteps      | 2000       |
| train/                  |            |
|    approx_kl            | 0.28169483 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | -0.00679   |
|    learning_rate        | 0.0003     |
|    loss                 | 12.9       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.367      |
|    value_loss           | 23.7       |
----------------------------------------
----------------------------------------
| reward                  | -0.616     |
| reward_contact          | -0.421     |
| reward_motion           | 0.2        |
| reward_torque           | -0.101     |
| reward_velocity         | -0.295     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 11         |
|    time_elapsed         | 139        |
|    total_timesteps      | 2200       |
| train/                  |            |
|    approx_kl            | 0.40759388 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.119      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.66       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.367      |
|    value_loss           | 20.6       |
----------------------------------------
----------------------------------------
| reward                  | -0.56      |
| reward_contact          | -0.383     |
| reward_motion           | 0.182      |
| reward_torque           | -0.0914    |
| reward_velocity         | -0.268     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 12         |
|    time_elapsed         | 152        |
|    total_timesteps      | 2400       |
| train/                  |            |
|    approx_kl            | 0.52947295 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.103      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.9        |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.219     |
|    std                  | 0.367      |
|    value_loss           | 11.3       |
----------------------------------------
----------------------------------------
| reward                  | -0.514     |
| reward_contact          | -0.351     |
| reward_motion           | 0.167      |
| reward_torque           | -0.0838    |
| reward_velocity         | -0.246     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 13         |
|    time_elapsed         | 164        |
|    total_timesteps      | 2600       |
| train/                  |            |
|    approx_kl            | 0.35313648 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | -0.269     |
|    learning_rate        | 0.0003     |
|    loss                 | 11.2       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.366      |
|    value_loss           | 30.5       |
----------------------------------------
----------------------------------------
| reward                  | -0.397     |
| reward_contact          | -0.324     |
| reward_motion           | 0.231      |
| reward_torque           | -0.0773    |
| reward_velocity         | -0.227     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -95.6      |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 14         |
|    time_elapsed         | 177        |
|    total_timesteps      | 2800       |
| train/                  |            |
|    approx_kl            | 0.71353036 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | -0.147     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.7        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.366      |
|    value_loss           | 7.66       |
----------------------------------------
---------------------------------------
| reward                  | -0.297    |
| reward_contact          | -0.301    |
| reward_motion           | 0.286     |
| reward_torque           | -0.0718   |
| reward_velocity         | -0.211    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -93       |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 15        |
|    time_elapsed         | 189       |
|    total_timesteps      | 3000      |
| train/                  |           |
|    approx_kl            | 0.4534725 |
|    clip_fraction        | 0.474     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.0886    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.98      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.366     |
|    value_loss           | 6.63      |
---------------------------------------
---------------------------------------
| reward                  | -0.278    |
| reward_contact          | -0.281    |
| reward_motion           | 0.267     |
| reward_torque           | -0.067    |
| reward_velocity         | -0.197    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -87       |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 16        |
|    time_elapsed         | 202       |
|    total_timesteps      | 3200      |
| train/                  |           |
|    approx_kl            | 0.3376599 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | -0.00114  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.6      |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.142    |
|    std                  | 0.366     |
|    value_loss           | 27        |
---------------------------------------
---------------------------------------
| reward                  | -0.26     |
| reward_contact          | -0.263    |
| reward_motion           | 0.25      |
| reward_torque           | -0.0628   |
| reward_velocity         | -0.184    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -82.5     |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 17        |
|    time_elapsed         | 214       |
|    total_timesteps      | 3400      |
| train/                  |           |
|    approx_kl            | 0.6265061 |
|    clip_fraction        | 0.708     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.038     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.35      |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.365     |
|    value_loss           | 12.4      |
---------------------------------------
---------------------------------------
| reward                  | -0.245    |
| reward_contact          | -0.248    |
| reward_motion           | 0.235     |
| reward_torque           | -0.0591   |
| reward_velocity         | -0.174    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -74       |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 18        |
|    time_elapsed         | 227       |
|    total_timesteps      | 3600      |
| train/                  |           |
|    approx_kl            | 0.4455958 |
|    clip_fraction        | 0.671     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | -0.101    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.25      |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.365     |
|    value_loss           | 15.7      |
---------------------------------------
----------------------------------------
| reward                  | -0.231     |
| reward_contact          | -0.234     |
| reward_motion           | 0.222      |
| reward_torque           | -0.0558    |
| reward_velocity         | -0.164     |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -67.5      |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 19         |
|    time_elapsed         | 240        |
|    total_timesteps      | 3800       |
| train/                  |            |
|    approx_kl            | 0.18275332 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.00253    |
|    learning_rate        | 0.0003     |
|    loss                 | 5.43       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.365      |
|    value_loss           | 12.8       |
----------------------------------------
---------------------------------------
| reward                  | -0.219    |
| reward_contact          | -0.222    |
| reward_motion           | 0.211     |
| reward_torque           | -0.0529   |
| reward_velocity         | -0.155    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -65.9     |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 20        |
|    time_elapsed         | 252       |
|    total_timesteps      | 4000      |
| train/                  |           |
|    approx_kl            | 0.4912749 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.365     |
|    value_loss           | 28.5      |
---------------------------------------
---------------------------------------
| reward                  | -0.208    |
| reward_contact          | -0.21     |
| reward_motion           | 0.2       |
| reward_torque           | -0.0503   |
| reward_velocity         | -0.148    |
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | -61.5     |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 21        |
|    time_elapsed         | 265       |
|    total_timesteps      | 4200      |
| train/                  |           |
|    approx_kl            | 0.3421271 |
|    clip_fraction        | 0.534     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.98      |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.365     |
|    value_loss           | 22.9      |
---------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_34
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 20       |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -24.4    |
| time/                   |          |
|    fps                  | 15       |
|    iterations           | 2        |
|    time_elapsed         | 2        |
|    total_timesteps      | 40       |
| train/                  |          |
|    approx_kl            | 1.794553 |
|    clip_fraction        | 0.775    |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | 0.00239  |
|    learning_rate        | 0.0003   |
|    loss                 | 4.19     |
|    n_updates            | 20       |
|    policy_gradient_loss | -0.281   |
|    std                  | 0.368    |
|    value_loss           | 10.4     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -22.2      |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 3          |
|    time_elapsed         | 3          |
|    total_timesteps      | 60         |
| train/                  |            |
|    approx_kl            | 0.66638166 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.00737    |
|    learning_rate        | 0.0003     |
|    loss                 | 50.4       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.259     |
|    std                  | 0.368      |
|    value_loss           | 121        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -19.6     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 4         |
|    time_elapsed         | 5         |
|    total_timesteps      | 80        |
| train/                  |           |
|    approx_kl            | 2.7483754 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.0613   |
|    learning_rate        | 0.0003    |
|    loss                 | 15        |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.315    |
|    std                  | 0.368     |
|    value_loss           | 32.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -21       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 5         |
|    time_elapsed         | 6         |
|    total_timesteps      | 100       |
| train/                  |           |
|    approx_kl            | 1.5500106 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.00176   |
|    learning_rate        | 0.0003    |
|    loss                 | 2.04      |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 5.84      |
---------------------------------------
---------------------------------------
| reward                  | -0.406    |
| reward_contact          | -0.156    |
| reward_motion           | 0.4       |
| reward_torque           | -0.029    |
| reward_velocity         | -0.62     |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -22.6     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 6         |
|    time_elapsed         | 8         |
|    total_timesteps      | 120       |
| train/                  |           |
|    approx_kl            | 1.1127717 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.0204    |
|    learning_rate        | 0.0003    |
|    loss                 | 30.6      |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.368     |
|    value_loss           | 70.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -21       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 7         |
|    time_elapsed         | 9         |
|    total_timesteps      | 140       |
| train/                  |           |
|    approx_kl            | 0.4224108 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.00204   |
|    learning_rate        | 0.0003    |
|    loss                 | 21        |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.176    |
|    std                  | 0.368     |
|    value_loss           | 53.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -20.2    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 8        |
|    time_elapsed         | 10       |
|    total_timesteps      | 160      |
| train/                  |          |
|    approx_kl            | 1.130426 |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | -0.0253  |
|    learning_rate        | 0.0003   |
|    loss                 | 5.25     |
|    n_updates            | 140      |
|    policy_gradient_loss | -0.184   |
|    std                  | 0.368    |
|    value_loss           | 13       |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -19.2    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 9        |
|    time_elapsed         | 12       |
|    total_timesteps      | 180      |
| train/                  |          |
|    approx_kl            | 4.715111 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.00674  |
|    learning_rate        | 0.0003   |
|    loss                 | 20.3     |
|    n_updates            | 160      |
|    policy_gradient_loss | -0.31    |
|    std                  | 0.368    |
|    value_loss           | 43.4     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -19.2    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 10       |
|    time_elapsed         | 13       |
|    total_timesteps      | 200      |
| train/                  |          |
|    approx_kl            | 15.96641 |
|    clip_fraction        | 0.708    |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | -0.0237  |
|    learning_rate        | 0.0003   |
|    loss                 | 6.56     |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.261   |
|    std                  | 0.368    |
|    value_loss           | 14.5     |
--------------------------------------
--------------------------------------
| reward                  | -0.586   |
| reward_contact          | -0.525   |
| reward_motion           | 0.4      |
| reward_torque           | -0.0313  |
| reward_velocity         | -0.43    |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -21.7    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 11       |
|    time_elapsed         | 14       |
|    total_timesteps      | 220      |
| train/                  |          |
|    approx_kl            | 2.872548 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -117     |
|    explained_variance   | 0.0424   |
|    learning_rate        | 0.0003   |
|    loss                 | 14.9     |
|    n_updates            | 200      |
|    policy_gradient_loss | -0.236   |
|    std                  | 0.368    |
|    value_loss           | 32.9     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -23.8      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 12         |
|    time_elapsed         | 16         |
|    total_timesteps      | 240        |
| train/                  |            |
|    approx_kl            | 0.46971676 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.0421     |
|    learning_rate        | 0.0003     |
|    loss                 | 111        |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.243     |
|    std                  | 0.368      |
|    value_loss           | 246        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -25        |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 13         |
|    time_elapsed         | 17         |
|    total_timesteps      | 260        |
| train/                  |            |
|    approx_kl            | 0.49803218 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | -0.0343    |
|    learning_rate        | 0.0003     |
|    loss                 | 44         |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.368      |
|    value_loss           | 106        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -22.5      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 14         |
|    time_elapsed         | 18         |
|    total_timesteps      | 280        |
| train/                  |            |
|    approx_kl            | 0.63491356 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.0374     |
|    learning_rate        | 0.0003     |
|    loss                 | 32.3       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.255     |
|    std                  | 0.368      |
|    value_loss           | 76.6       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -22.5      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 15         |
|    time_elapsed         | 20         |
|    total_timesteps      | 300        |
| train/                  |            |
|    approx_kl            | 0.24706984 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.00679    |
|    learning_rate        | 0.0003     |
|    loss                 | 36.6       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.368      |
|    value_loss           | 80.7       |
----------------------------------------
---------------------------------------
| reward                  | -0.81     |
| reward_contact          | -0.899    |
| reward_motion           | 0.4       |
| reward_torque           | -0.0247   |
| reward_velocity         | -0.287    |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -22.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 16        |
|    time_elapsed         | 21        |
|    total_timesteps      | 320       |
| train/                  |           |
|    approx_kl            | 30.896864 |
|    clip_fraction        | 0.9       |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.0115   |
|    learning_rate        | 0.0003    |
|    loss                 | 25.8      |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.328    |
|    std                  | 0.368     |
|    value_loss           | 53.6      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -21.9    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 17       |
|    time_elapsed         | 23       |
|    total_timesteps      | 340      |
| train/                  |          |
|    approx_kl            | 1.18335  |
|    clip_fraction        | 0.735    |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.00654  |
|    learning_rate        | 0.0003   |
|    loss                 | 14.9     |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.254   |
|    std                  | 0.368    |
|    value_loss           | 32.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -21       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 18        |
|    time_elapsed         | 24        |
|    total_timesteps      | 360       |
| train/                  |           |
|    approx_kl            | 2.6680193 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -0.0154   |
|    learning_rate        | 0.0003    |
|    loss                 | 10.8      |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.302    |
|    std                  | 0.368     |
|    value_loss           | 24.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -19.6     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 19        |
|    time_elapsed         | 25        |
|    total_timesteps      | 380       |
| train/                  |           |
|    approx_kl            | 1.5799633 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -0.0579   |
|    learning_rate        | 0.0003    |
|    loss                 | 7.62      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.368     |
|    value_loss           | 17.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -20.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 20        |
|    time_elapsed         | 27        |
|    total_timesteps      | 400       |
| train/                  |           |
|    approx_kl            | 0.6594613 |
|    clip_fraction        | 0.675     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -0.222    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.06      |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.368     |
|    value_loss           | 12.5      |
---------------------------------------
----------------------------------------
| reward                  | -0.881     |
| reward_contact          | -1.05      |
| reward_motion           | 0.4        |
| reward_torque           | -0.0185    |
| reward_velocity         | -0.215     |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -20.6      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 21         |
|    time_elapsed         | 28         |
|    total_timesteps      | 420        |
| train/                  |            |
|    approx_kl            | 0.34468746 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | -0.0557    |
|    learning_rate        | 0.0003     |
|    loss                 | 54.5       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.368      |
|    value_loss           | 142        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -20.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 22        |
|    time_elapsed         | 29        |
|    total_timesteps      | 440       |
| train/                  |           |
|    approx_kl            | 1.0421606 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.6      |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.368     |
|    value_loss           | 37.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -19.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 23        |
|    time_elapsed         | 31        |
|    total_timesteps      | 460       |
| train/                  |           |
|    approx_kl            | 0.9572527 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.144     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.9      |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.368     |
|    value_loss           | 28.9      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -18.1      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 24         |
|    time_elapsed         | 32         |
|    total_timesteps      | 480        |
| train/                  |            |
|    approx_kl            | 0.83776647 |
|    clip_fraction        | 0.7        |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | -0.253     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.964      |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.263     |
|    std                  | 0.368      |
|    value_loss           | 6.92       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -18.2     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 25        |
|    time_elapsed         | 33        |
|    total_timesteps      | 500       |
| train/                  |           |
|    approx_kl            | 0.7182998 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -1.71     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.68      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.368     |
|    value_loss           | 22.3      |
---------------------------------------
---------------------------------------
| reward                  | -0.844    |
| reward_contact          | -1.01     |
| reward_motion           | 0.36      |
| reward_torque           | -0.0222   |
| reward_velocity         | -0.172    |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -18.1     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 26        |
|    time_elapsed         | 35        |
|    total_timesteps      | 520       |
| train/                  |           |
|    approx_kl            | 1.3625606 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -0.0573   |
|    learning_rate        | 0.0003    |
|    loss                 | 21.9      |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.368     |
|    value_loss           | 47.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -18       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 27        |
|    time_elapsed         | 36        |
|    total_timesteps      | 540       |
| train/                  |           |
|    approx_kl            | 2.1463783 |
|    clip_fraction        | 0.528     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.0645    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.6       |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.368     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -17.2     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 28        |
|    time_elapsed         | 37        |
|    total_timesteps      | 560       |
| train/                  |           |
|    approx_kl            | 15.391757 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.0608    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.27      |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.368     |
|    value_loss           | 12.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -16.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 29        |
|    time_elapsed         | 39        |
|    total_timesteps      | 580       |
| train/                  |           |
|    approx_kl            | 2.3767922 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.5       |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.368     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -16.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 30        |
|    time_elapsed         | 40        |
|    total_timesteps      | 600       |
| train/                  |           |
|    approx_kl            | 4.4304423 |
|    clip_fraction        | 0.763     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -1.28     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00596   |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.368     |
|    value_loss           | 1.41      |
---------------------------------------
---------------------------------------
| reward                  | -0.874    |
| reward_contact          | -1.05     |
| reward_motion           | 0.367     |
| reward_torque           | -0.0435   |
| reward_velocity         | -0.143    |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -16       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 31        |
|    time_elapsed         | 42        |
|    total_timesteps      | 620       |
| train/                  |           |
|    approx_kl            | 0.6145675 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -0.427    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.36      |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.191    |
|    std                  | 0.368     |
|    value_loss           | 14.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -15.9    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 32       |
|    time_elapsed         | 43       |
|    total_timesteps      | 640      |
| train/                  |          |
|    approx_kl            | 3.608387 |
|    clip_fraction        | 0.85     |
|    clip_range           | 0.4      |
|    entropy_loss         | -122     |
|    explained_variance   | -0.453   |
|    learning_rate        | 0.0003   |
|    loss                 | 2.68     |
|    n_updates            | 620      |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.368    |
|    value_loss           | 7.7      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -15.7      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 33         |
|    time_elapsed         | 44         |
|    total_timesteps      | 660        |
| train/                  |            |
|    approx_kl            | 0.17340982 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.05       |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0456    |
|    std                  | 0.368      |
|    value_loss           | 12.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -15.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 34        |
|    time_elapsed         | 46        |
|    total_timesteps      | 680       |
| train/                  |           |
|    approx_kl            | 12.202824 |
|    clip_fraction        | 0.87      |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0.171     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.07      |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.328    |
|    std                  | 0.368     |
|    value_loss           | 15        |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -15.4    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 35       |
|    time_elapsed         | 47       |
|    total_timesteps      | 700      |
| train/                  |          |
|    approx_kl            | 0.914961 |
|    clip_fraction        | 0.61     |
|    clip_range           | 0.4      |
|    entropy_loss         | -124     |
|    explained_variance   | 0.135    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.67     |
|    n_updates            | 680      |
|    policy_gradient_loss | -0.217   |
|    std                  | 0.367    |
|    value_loss           | 7.21     |
--------------------------------------
--------------------------------------
| reward                  | -0.89    |
| reward_contact          | -1.1     |
| reward_motion           | 0.371    |
| reward_torque           | -0.0376  |
| reward_velocity         | -0.123   |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -15.1    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 36       |
|    time_elapsed         | 48       |
|    total_timesteps      | 720      |
| train/                  |          |
|    approx_kl            | 4.854625 |
|    clip_fraction        | 0.658    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | -0.262   |
|    learning_rate        | 0.0003   |
|    loss                 | 4.52     |
|    n_updates            | 700      |
|    policy_gradient_loss | -0.241   |
|    std                  | 0.367    |
|    value_loss           | 15.9     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -14.8    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 37       |
|    time_elapsed         | 50       |
|    total_timesteps      | 740      |
| train/                  |          |
|    approx_kl            | 8.21444  |
|    clip_fraction        | 0.825    |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | -0.412   |
|    learning_rate        | 0.0003   |
|    loss                 | 10.4     |
|    n_updates            | 720      |
|    policy_gradient_loss | -0.287   |
|    std                  | 0.367    |
|    value_loss           | 24.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -14.1     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 38        |
|    time_elapsed         | 51        |
|    total_timesteps      | 760       |
| train/                  |           |
|    approx_kl            | 4.6245427 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.915    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.7       |
|    n_updates            | 740       |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.367     |
|    value_loss           | 5.02      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -13.9      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 39         |
|    time_elapsed         | 52         |
|    total_timesteps      | 780        |
| train/                  |            |
|    approx_kl            | 0.36678132 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -1.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.8       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.367      |
|    value_loss           | 36.1       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -13.3     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 40        |
|    time_elapsed         | 54        |
|    total_timesteps      | 800       |
| train/                  |           |
|    approx_kl            | 7.3002505 |
|    clip_fraction        | 0.9       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.0919   |
|    learning_rate        | 0.0003    |
|    loss                 | 2.18      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.303    |
|    std                  | 0.367     |
|    value_loss           | 5.19      |
---------------------------------------
----------------------------------------
| reward                  | -0.814     |
| reward_contact          | -1.05      |
| reward_motion           | 0.4        |
| reward_torque           | -0.0529    |
| reward_velocity         | -0.107     |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -12.8      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 41         |
|    time_elapsed         | 55         |
|    total_timesteps      | 820        |
| train/                  |            |
|    approx_kl            | 0.69164497 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.0891    |
|    learning_rate        | 0.0003     |
|    loss                 | 5.87       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.263     |
|    std                  | 0.367      |
|    value_loss           | 15.7       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -12.7      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 42         |
|    time_elapsed         | 56         |
|    total_timesteps      | 840        |
| train/                  |            |
|    approx_kl            | 0.33549654 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.0468    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.36       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.367      |
|    value_loss           | 11.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -12.4     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 43        |
|    time_elapsed         | 58        |
|    total_timesteps      | 860       |
| train/                  |           |
|    approx_kl            | 2.3656611 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.000463 |
|    learning_rate        | 0.0003    |
|    loss                 | 9.58      |
|    n_updates            | 840       |
|    policy_gradient_loss | -0.302    |
|    std                  | 0.367     |
|    value_loss           | 20.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -12.2    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 44       |
|    time_elapsed         | 59       |
|    total_timesteps      | 880      |
| train/                  |          |
|    approx_kl            | 2.338781 |
|    clip_fraction        | 0.65     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.088    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.372    |
|    n_updates            | 860      |
|    policy_gradient_loss | -0.253   |
|    std                  | 0.367    |
|    value_loss           | 1.87     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -11.9     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 45        |
|    time_elapsed         | 61        |
|    total_timesteps      | 900       |
| train/                  |           |
|    approx_kl            | 1.8015374 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.0292   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.94      |
|    n_updates            | 880       |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.367     |
|    value_loss           | 4.74      |
---------------------------------------
---------------------------------------
| reward                  | -0.739    |
| reward_contact          | -0.978    |
| reward_motion           | 0.4       |
| reward_torque           | -0.0646   |
| reward_velocity         | -0.0955   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -11.5     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 46        |
|    time_elapsed         | 62        |
|    total_timesteps      | 920       |
| train/                  |           |
|    approx_kl            | 1.2521963 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.139     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.781     |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.367     |
|    value_loss           | 3.12      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -11       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 47        |
|    time_elapsed         | 63        |
|    total_timesteps      | 940       |
| train/                  |           |
|    approx_kl            | 1.4801915 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.0202    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.57      |
|    n_updates            | 920       |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.367     |
|    value_loss           | 6.42      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -10.6     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 48        |
|    time_elapsed         | 65        |
|    total_timesteps      | 960       |
| train/                  |           |
|    approx_kl            | 0.7546417 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.0704   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.77      |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.367     |
|    value_loss           | 12.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -10.6    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 49       |
|    time_elapsed         | 66       |
|    total_timesteps      | 980      |
| train/                  |          |
|    approx_kl            | 7.440676 |
|    clip_fraction        | 0.79     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | -0.0479  |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0768   |
|    n_updates            | 960      |
|    policy_gradient_loss | -0.263   |
|    std                  | 0.367    |
|    value_loss           | 0.869    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -10.1      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 50         |
|    time_elapsed         | 67         |
|    total_timesteps      | 1000       |
| train/                  |            |
|    approx_kl            | 0.76628655 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.00478    |
|    learning_rate        | 0.0003     |
|    loss                 | 10         |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.237     |
|    std                  | 0.367      |
|    value_loss           | 24.6       |
----------------------------------------
---------------------------------------
| reward                  | -0.65     |
| reward_contact          | -0.884    |
| reward_motion           | 0.38      |
| reward_torque           | -0.0597   |
| reward_velocity         | -0.086    |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -10.2     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 51        |
|    time_elapsed         | 69        |
|    total_timesteps      | 1020      |
| train/                  |           |
|    approx_kl            | 0.7628484 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.000993 |
|    learning_rate        | 0.0003    |
|    loss                 | 9.21      |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.367     |
|    value_loss           | 21.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -9.94      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 52         |
|    time_elapsed         | 70         |
|    total_timesteps      | 1040       |
| train/                  |            |
|    approx_kl            | 0.62928766 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.00066    |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.367      |
|    value_loss           | 43.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -9.77     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 53        |
|    time_elapsed         | 71        |
|    total_timesteps      | 1060      |
| train/                  |           |
|    approx_kl            | 51.126637 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.0258    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.33      |
|    n_updates            | 1040      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.367     |
|    value_loss           | 1.53      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -9.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 54        |
|    time_elapsed         | 73        |
|    total_timesteps      | 1080      |
| train/                  |           |
|    approx_kl            | 1.3690621 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.00266  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.6       |
|    n_updates            | 1060      |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.367     |
|    value_loss           | 11        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -9.45     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 55        |
|    time_elapsed         | 74        |
|    total_timesteps      | 1100      |
| train/                  |           |
|    approx_kl            | 3.1188393 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.0457   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.033     |
|    n_updates            | 1080      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.367     |
|    value_loss           | 0.581     |
---------------------------------------
----------------------------------------
| reward                  | -0.614     |
| reward_contact          | -0.804     |
| reward_motion           | 0.327      |
| reward_torque           | -0.0595    |
| reward_velocity         | -0.0782    |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -9.27      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 56         |
|    time_elapsed         | 76         |
|    total_timesteps      | 1120       |
| train/                  |            |
|    approx_kl            | 0.83223957 |
|    clip_fraction        | 0.673      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.00129   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.14       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.367      |
|    value_loss           | 1.37       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -9.03     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 57        |
|    time_elapsed         | 77        |
|    total_timesteps      | 1140      |
| train/                  |           |
|    approx_kl            | 6.2610574 |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.0419    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.27      |
|    n_updates            | 1120      |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.367     |
|    value_loss           | 3.17      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -8.97     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 58        |
|    time_elapsed         | 78        |
|    total_timesteps      | 1160      |
| train/                  |           |
|    approx_kl            | 2.3472161 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.00523   |
|    learning_rate        | 0.0003    |
|    loss                 | 4.17      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.293    |
|    std                  | 0.367     |
|    value_loss           | 9.26      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -8.78     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 59        |
|    time_elapsed         | 80        |
|    total_timesteps      | 1180      |
| train/                  |           |
|    approx_kl            | 1.2200606 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.59      |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.367     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -8.52     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 60        |
|    time_elapsed         | 81        |
|    total_timesteps      | 1200      |
| train/                  |           |
|    approx_kl            | 1.6824077 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.115     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.523     |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.367     |
|    value_loss           | 1.97      |
---------------------------------------
----------------------------------------
| reward                  | -0.568     |
| reward_contact          | -0.737     |
| reward_motion           | 0.333      |
| reward_torque           | -0.0927    |
| reward_velocity         | -0.0717    |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -8.33      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 61         |
|    time_elapsed         | 82         |
|    total_timesteps      | 1220       |
| train/                  |            |
|    approx_kl            | 0.98155636 |
|    clip_fraction        | 0.74       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.545      |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.236     |
|    std                  | 0.367      |
|    value_loss           | 2.85       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -8.11    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 62       |
|    time_elapsed         | 84       |
|    total_timesteps      | 1240     |
| train/                  |          |
|    approx_kl            | 8.886837 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | -0.136   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.034    |
|    n_updates            | 1220     |
|    policy_gradient_loss | -0.29    |
|    std                  | 0.367    |
|    value_loss           | 0.77     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -7.75    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 63       |
|    time_elapsed         | 85       |
|    total_timesteps      | 1260     |
| train/                  |          |
|    approx_kl            | 9.595942 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.0273   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.997    |
|    n_updates            | 1240     |
|    policy_gradient_loss | -0.259   |
|    std                  | 0.367    |
|    value_loss           | 2.73     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -7.52      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 64         |
|    time_elapsed         | 86         |
|    total_timesteps      | 1280       |
| train/                  |            |
|    approx_kl            | 0.35467523 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.0142     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.22       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.367      |
|    value_loss           | 16.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -7.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 65        |
|    time_elapsed         | 88        |
|    total_timesteps      | 1300      |
| train/                  |           |
|    approx_kl            | 3.2980554 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.564     |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.367     |
|    value_loss           | 2.41      |
---------------------------------------
---------------------------------------
| reward                  | -0.493    |
| reward_contact          | -0.68     |
| reward_motion           | 0.338     |
| reward_torque           | -0.0856   |
| reward_velocity         | -0.0661   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -6.99     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 66        |
|    time_elapsed         | 89        |
|    total_timesteps      | 1320      |
| train/                  |           |
|    approx_kl            | 0.7314291 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.7       |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.225    |
|    std                  | 0.367     |
|    value_loss           | 6.19      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -6.67     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 67        |
|    time_elapsed         | 90        |
|    total_timesteps      | 1340      |
| train/                  |           |
|    approx_kl            | 4.0166464 |
|    clip_fraction        | 0.735     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.0187   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.01      |
|    n_updates            | 1320      |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.367     |
|    value_loss           | 7.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -6.54     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 68        |
|    time_elapsed         | 92        |
|    total_timesteps      | 1360      |
| train/                  |           |
|    approx_kl            | 1.0672095 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.153     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.51      |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.367     |
|    value_loss           | 12.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -6.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 69        |
|    time_elapsed         | 93        |
|    total_timesteps      | 1380      |
| train/                  |           |
|    approx_kl            | 1.0271624 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 0.711     |
|    n_updates            | 1360      |
|    policy_gradient_loss | -0.3      |
|    std                  | 0.367     |
|    value_loss           | 3.59      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -6.14     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 70        |
|    time_elapsed         | 95        |
|    total_timesteps      | 1400      |
| train/                  |           |
|    approx_kl            | 1.6942261 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.173     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -0.304    |
|    std                  | 0.367     |
|    value_loss           | 3.97      |
---------------------------------------
----------------------------------------
| reward                  | -0.445     |
| reward_contact          | -0.632     |
| reward_motion           | 0.329      |
| reward_torque           | -0.0801    |
| reward_velocity         | -0.0614    |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -5.93      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 71         |
|    time_elapsed         | 96         |
|    total_timesteps      | 1420       |
| train/                  |            |
|    approx_kl            | 0.61289483 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.031      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.93       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.367      |
|    value_loss           | 7.65       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -5.66    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 72       |
|    time_elapsed         | 97       |
|    total_timesteps      | 1440     |
| train/                  |          |
|    approx_kl            | 4.760761 |
|    clip_fraction        | 0.86     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | -0.0311  |
|    learning_rate        | 0.0003   |
|    loss                 | 2.59     |
|    n_updates            | 1420     |
|    policy_gradient_loss | -0.314   |
|    std                  | 0.367    |
|    value_loss           | 6        |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -5.31     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 73        |
|    time_elapsed         | 99        |
|    total_timesteps      | 1460      |
| train/                  |           |
|    approx_kl            | 1.7278557 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.277     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.97      |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.293    |
|    std                  | 0.367     |
|    value_loss           | 10.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -5.12      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 74         |
|    time_elapsed         | 100        |
|    total_timesteps      | 1480       |
| train/                  |            |
|    approx_kl            | 0.36230114 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.23       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.23      |
|    std                  | 0.367      |
|    value_loss           | 16.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -4.98     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 75        |
|    time_elapsed         | 101       |
|    total_timesteps      | 1500      |
| train/                  |           |
|    approx_kl            | 4.7136664 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0496    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.25      |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 3.39      |
---------------------------------------
--------------------------------------
| reward                  | -0.375   |
| reward_contact          | -0.59    |
| reward_motion           | 0.347    |
| reward_torque           | -0.0747  |
| reward_velocity         | -0.0573  |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -4.97    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 76       |
|    time_elapsed         | 103      |
|    total_timesteps      | 1520     |
| train/                  |          |
|    approx_kl            | 2.041217 |
|    clip_fraction        | 0.65     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | -0.312   |
|    learning_rate        | 0.0003   |
|    loss                 | 0.592    |
|    n_updates            | 1500     |
|    policy_gradient_loss | -0.213   |
|    std                  | 0.367    |
|    value_loss           | 2.43     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -4.79      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 77         |
|    time_elapsed         | 104        |
|    total_timesteps      | 1540       |
| train/                  |            |
|    approx_kl            | 0.29772422 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.68       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.367      |
|    value_loss           | 16.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -4.78     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 78        |
|    time_elapsed         | 105       |
|    total_timesteps      | 1560      |
| train/                  |           |
|    approx_kl            | 1.6960785 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | -0.0612   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.39      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.306    |
|    std                  | 0.367     |
|    value_loss           | 5.32      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -4.72      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 79         |
|    time_elapsed         | 107        |
|    total_timesteps      | 1580       |
| train/                  |            |
|    approx_kl            | 0.25298524 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.31       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.367      |
|    value_loss           | 16.1       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -4.42     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 80        |
|    time_elapsed         | 108       |
|    total_timesteps      | 1600      |
| train/                  |           |
|    approx_kl            | 2.0358167 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.271     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.25      |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.367     |
|    value_loss           | 4.44      |
---------------------------------------
---------------------------------------
| reward                  | -0.339    |
| reward_contact          | -0.553    |
| reward_motion           | 0.338     |
| reward_torque           | -0.0701   |
| reward_velocity         | -0.0537   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -4.27     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 81        |
|    time_elapsed         | 109       |
|    total_timesteps      | 1620      |
| train/                  |           |
|    approx_kl            | 0.6608197 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.43      |
|    n_updates            | 1600      |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.367     |
|    value_loss           | 20.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -4.12    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 82       |
|    time_elapsed         | 111      |
|    total_timesteps      | 1640     |
| train/                  |          |
|    approx_kl            | 6.046467 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | -0.466   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.77     |
|    n_updates            | 1620     |
|    policy_gradient_loss | -0.325   |
|    std                  | 0.367    |
|    value_loss           | 9.53     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -3.96      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 83         |
|    time_elapsed         | 112        |
|    total_timesteps      | 1660       |
| train/                  |            |
|    approx_kl            | 0.91752833 |
|    clip_fraction        | 0.69       |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | -0.105     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.235     |
|    std                  | 0.368      |
|    value_loss           | 4.27       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -3.92     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 84        |
|    time_elapsed         | 113       |
|    total_timesteps      | 1680      |
| train/                  |           |
|    approx_kl            | 2.7064743 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.0406    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.993     |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.368     |
|    value_loss           | 2.83      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -3.81     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 85        |
|    time_elapsed         | 115       |
|    total_timesteps      | 1700      |
| train/                  |           |
|    approx_kl            | 1.5058303 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.054     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.726     |
|    n_updates            | 1680      |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.368     |
|    value_loss           | 3.47      |
---------------------------------------
---------------------------------------
| reward                  | -0.284    |
| reward_contact          | -0.52     |
| reward_motion           | 0.353     |
| reward_torque           | -0.0659   |
| reward_velocity         | -0.0506   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -3.73     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 86        |
|    time_elapsed         | 116       |
|    total_timesteps      | 1720      |
| train/                  |           |
|    approx_kl            | 1.6174484 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.298     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 1700      |
|    policy_gradient_loss | -0.231    |
|    std                  | 0.368     |
|    value_loss           | 4.33      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -3.52    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 87       |
|    time_elapsed         | 117      |
|    total_timesteps      | 1740     |
| train/                  |          |
|    approx_kl            | 3.063159 |
|    clip_fraction        | 0.65     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | -0.0686  |
|    learning_rate        | 0.0003   |
|    loss                 | 1.36     |
|    n_updates            | 1720     |
|    policy_gradient_loss | -0.206   |
|    std                  | 0.368    |
|    value_loss           | 3.56     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -3.29     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 88        |
|    time_elapsed         | 119       |
|    total_timesteps      | 1760      |
| train/                  |           |
|    approx_kl            | 0.9784598 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.0843    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.92      |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.368     |
|    value_loss           | 13.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -3.15      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 89         |
|    time_elapsed         | 120        |
|    total_timesteps      | 1780       |
| train/                  |            |
|    approx_kl            | 0.72361726 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | -0.168     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.39       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.265     |
|    std                  | 0.368      |
|    value_loss           | 13.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.93     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 90        |
|    time_elapsed         | 122       |
|    total_timesteps      | 1800      |
| train/                  |           |
|    approx_kl            | 14.804603 |
|    clip_fraction        | 0.688     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.767     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0153    |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.368     |
|    value_loss           | 0.828     |
---------------------------------------
---------------------------------------
| reward                  | -0.257    |
| reward_contact          | -0.491    |
| reward_motion           | 0.344     |
| reward_torque           | -0.0623   |
| reward_velocity         | -0.0478   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.78     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 91        |
|    time_elapsed         | 123       |
|    total_timesteps      | 1820      |
| train/                  |           |
|    approx_kl            | 1.7053565 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.97      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.368     |
|    value_loss           | 18.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.61     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 92        |
|    time_elapsed         | 124       |
|    total_timesteps      | 1840      |
| train/                  |           |
|    approx_kl            | 2.5216413 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.222     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.368     |
|    value_loss           | 4.69      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -2.47    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 93       |
|    time_elapsed         | 126      |
|    total_timesteps      | 1860     |
| train/                  |          |
|    approx_kl            | 3.003579 |
|    clip_fraction        | 0.83     |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | -0.265   |
|    learning_rate        | 0.0003   |
|    loss                 | 2.46     |
|    n_updates            | 1840     |
|    policy_gradient_loss | -0.299   |
|    std                  | 0.368    |
|    value_loss           | 7.66     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.38     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 94        |
|    time_elapsed         | 127       |
|    total_timesteps      | 1880      |
| train/                  |           |
|    approx_kl            | 2.6797485 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | -0.144    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.79      |
|    n_updates            | 1860      |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.368     |
|    value_loss           | 8.77      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.26     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 95        |
|    time_elapsed         | 128       |
|    total_timesteps      | 1900      |
| train/                  |           |
|    approx_kl            | 0.7993112 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.61      |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.368     |
|    value_loss           | 7.73      |
---------------------------------------
---------------------------------------
| reward                  | -0.212    |
| reward_contact          | -0.465    |
| reward_motion           | 0.358     |
| reward_torque           | -0.059    |
| reward_velocity         | -0.0453   |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -2.17     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 96        |
|    time_elapsed         | 130       |
|    total_timesteps      | 1920      |
| train/                  |           |
|    approx_kl            | 2.3883867 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.00506   |
|    learning_rate        | 0.0003    |
|    loss                 | 4.6       |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.368     |
|    value_loss           | 10.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -2.01    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 97       |
|    time_elapsed         | 131      |
|    total_timesteps      | 1940     |
| train/                  |          |
|    approx_kl            | 3.248661 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | -0.0285  |
|    learning_rate        | 0.0003   |
|    loss                 | 2.79     |
|    n_updates            | 1920     |
|    policy_gradient_loss | -0.268   |
|    std                  | 0.368    |
|    value_loss           | 6.59     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -1.83     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 98        |
|    time_elapsed         | 132       |
|    total_timesteps      | 1960      |
| train/                  |           |
|    approx_kl            | 1.9759403 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | -0.0124   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.92      |
|    n_updates            | 1940      |
|    policy_gradient_loss | -0.293    |
|    std                  | 0.368     |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -1.88     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 99        |
|    time_elapsed         | 134       |
|    total_timesteps      | 1980      |
| train/                  |           |
|    approx_kl            | 2.1555698 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.71      |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.368     |
|    value_loss           | 13.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -1.82      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 100        |
|    time_elapsed         | 135        |
|    total_timesteps      | 2000       |
| train/                  |            |
|    approx_kl            | 0.37333643 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | -0.326     |
|    learning_rate        | 0.0003     |
|    loss                 | 17.9       |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.368      |
|    value_loss           | 40.4       |
----------------------------------------
--------------------------------------
| reward                  | -0.181   |
| reward_contact          | -0.442   |
| reward_motion           | 0.36     |
| reward_torque           | -0.0561  |
| reward_velocity         | -0.043   |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -1.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 101      |
|    time_elapsed         | 136      |
|    total_timesteps      | 2020     |
| train/                  |          |
|    approx_kl            | 0.827563 |
|    clip_fraction        | 0.538    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.173    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.613    |
|    n_updates            | 2000     |
|    policy_gradient_loss | -0.221   |
|    std                  | 0.368    |
|    value_loss           | 2.91     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -1.36    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 102      |
|    time_elapsed         | 138      |
|    total_timesteps      | 2040     |
| train/                  |          |
|    approx_kl            | 4.523207 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | -0.689   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.14     |
|    n_updates            | 2020     |
|    policy_gradient_loss | -0.282   |
|    std                  | 0.368    |
|    value_loss           | 8.7      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -1.11      |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 103        |
|    time_elapsed         | 139        |
|    total_timesteps      | 2060       |
| train/                  |            |
|    approx_kl            | 0.40797114 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | -0.0672    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.1        |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.368      |
|    value_loss           | 7.45       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | -0.945   |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 104      |
|    time_elapsed         | 141      |
|    total_timesteps      | 2080     |
| train/                  |          |
|    approx_kl            | 8.018424 |
|    clip_fraction        | 0.68     |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | 0.0294   |
|    learning_rate        | 0.0003   |
|    loss                 | 1.79     |
|    n_updates            | 2060     |
|    policy_gradient_loss | -0.281   |
|    std                  | 0.368    |
|    value_loss           | 4.48     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -0.515    |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 105       |
|    time_elapsed         | 142       |
|    total_timesteps      | 2100      |
| train/                  |           |
|    approx_kl            | 0.8612267 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.02     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.3      |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.368     |
|    value_loss           | 22.7      |
---------------------------------------
---------------------------------------
| reward                  | -0.173    |
| reward_contact          | -0.456    |
| reward_motion           | 0.35      |
| reward_torque           | -0.0546   |
| reward_velocity         | -0.012    |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | -0.185    |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 106       |
|    time_elapsed         | 143       |
|    total_timesteps      | 2120      |
| train/                  |           |
|    approx_kl            | 0.5350434 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.301     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.07      |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.368     |
|    value_loss           | 15.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 0.0583    |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 107       |
|    time_elapsed         | 145       |
|    total_timesteps      | 2140      |
| train/                  |           |
|    approx_kl            | 1.2079067 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.068    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.64      |
|    n_updates            | 2120      |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.368     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 0.358     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 108       |
|    time_elapsed         | 146       |
|    total_timesteps      | 2160      |
| train/                  |           |
|    approx_kl            | 1.3294865 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.0163   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.12      |
|    n_updates            | 2140      |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.368     |
|    value_loss           | 14.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 0.569     |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 109       |
|    time_elapsed         | 147       |
|    total_timesteps      | 2180      |
| train/                  |           |
|    approx_kl            | 0.5226887 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.131     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.27      |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.368     |
|    value_loss           | 9.85      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 0.817    |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 110      |
|    time_elapsed         | 149      |
|    total_timesteps      | 2200     |
| train/                  |          |
|    approx_kl            | 5.297024 |
|    clip_fraction        | 0.84     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.0598   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.3      |
|    n_updates            | 2180     |
|    policy_gradient_loss | -0.31    |
|    std                  | 0.368    |
|    value_loss           | 8.31     |
--------------------------------------
---------------------------------------
| reward                  | -0.124    |
| reward_contact          | -0.411    |
| reward_motion           | 0.34      |
| reward_torque           | -0.0529   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 1.46      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 111       |
|    time_elapsed         | 150       |
|    total_timesteps      | 2220      |
| train/                  |           |
|    approx_kl            | 1.3562043 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0575    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 2200      |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.368     |
|    value_loss           | 4.74      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 2.1       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 112       |
|    time_elapsed         | 151       |
|    total_timesteps      | 2240      |
| train/                  |           |
|    approx_kl            | 0.9197836 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.38      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.33      |
|    n_updates            | 2220      |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.368     |
|    value_loss           | 14.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 2.63      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 113       |
|    time_elapsed         | 153       |
|    total_timesteps      | 2260      |
| train/                  |           |
|    approx_kl            | 0.7231746 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.45      |
|    n_updates            | 2240      |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 10.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 2.67      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 114       |
|    time_elapsed         | 154       |
|    total_timesteps      | 2280      |
| train/                  |           |
|    approx_kl            | 2.0797975 |
|    clip_fraction        | 0.735     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.116    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.58      |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.368     |
|    value_loss           | 11.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 2.99      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 115       |
|    time_elapsed         | 155       |
|    total_timesteps      | 2300      |
| train/                  |           |
|    approx_kl            | 2.9682944 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.00474  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.24      |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.316    |
|    std                  | 0.368     |
|    value_loss           | 14.4      |
---------------------------------------
---------------------------------------
| reward                  | -0.0415   |
| reward_contact          | -0.329    |
| reward_motion           | 0.34      |
| reward_torque           | -0.0523   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 3.38      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 116       |
|    time_elapsed         | 157       |
|    total_timesteps      | 2320      |
| train/                  |           |
|    approx_kl            | 3.2363434 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.04      |
|    n_updates            | 2300      |
|    policy_gradient_loss | -0.291    |
|    std                  | 0.368     |
|    value_loss           | 9.78      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 3.55      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 117       |
|    time_elapsed         | 158       |
|    total_timesteps      | 2340      |
| train/                  |           |
|    approx_kl            | 0.8504929 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.305     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.03      |
|    n_updates            | 2320      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.368     |
|    value_loss           | 8.85      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 3.7        |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 118        |
|    time_elapsed         | 159        |
|    total_timesteps      | 2360       |
| train/                  |            |
|    approx_kl            | 0.67342424 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.0626     |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.276     |
|    std                  | 0.368      |
|    value_loss           | 28.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 3.83      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 119       |
|    time_elapsed         | 161       |
|    total_timesteps      | 2380      |
| train/                  |           |
|    approx_kl            | 1.4182115 |
|    clip_fraction        | 0.753     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.286     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.37      |
|    n_updates            | 2360      |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.368     |
|    value_loss           | 4.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 4.32      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 120       |
|    time_elapsed         | 162       |
|    total_timesteps      | 2400      |
| train/                  |           |
|    approx_kl            | 1.1811689 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.73      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.368     |
|    value_loss           | 18.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.0431    |
| reward_contact          | -0.255    |
| reward_motion           | 0.35      |
| reward_torque           | -0.0523   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 4.62      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 121       |
|    time_elapsed         | 163       |
|    total_timesteps      | 2420      |
| train/                  |           |
|    approx_kl            | 2.6742344 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.315    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.92      |
|    n_updates            | 2400      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.368     |
|    value_loss           | 13.4      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 4.86       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 122        |
|    time_elapsed         | 165        |
|    total_timesteps      | 2440       |
| train/                  |            |
|    approx_kl            | 0.77389824 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.0666    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.33       |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.233     |
|    std                  | 0.368      |
|    value_loss           | 9.83       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 5.03      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 123       |
|    time_elapsed         | 166       |
|    total_timesteps      | 2460      |
| train/                  |           |
|    approx_kl            | 4.7266784 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.91      |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.368     |
|    value_loss           | 5.59      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 5.08       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 124        |
|    time_elapsed         | 167        |
|    total_timesteps      | 2480       |
| train/                  |            |
|    approx_kl            | 0.67382526 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.238     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.42       |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.25      |
|    std                  | 0.369      |
|    value_loss           | 23.8       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 125        |
|    time_elapsed         | 169        |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.79338133 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.0532    |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.265     |
|    std                  | 0.369      |
|    value_loss           | 13         |
----------------------------------------
---------------------------------------
| reward                  | 0.0979    |
| reward_contact          | -0.212    |
| reward_motion           | 0.36      |
| reward_torque           | -0.0505   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 5.53      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 126       |
|    time_elapsed         | 170       |
|    total_timesteps      | 2520      |
| train/                  |           |
|    approx_kl            | 0.8300606 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.463     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.37      |
|    n_updates            | 2500      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.369     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 5.83      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 127       |
|    time_elapsed         | 172       |
|    total_timesteps      | 2540      |
| train/                  |           |
|    approx_kl            | 0.8108034 |
|    clip_fraction        | 0.4       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.643     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.324     |
|    n_updates            | 2520      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.368     |
|    value_loss           | 2.58      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 5.96       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 128        |
|    time_elapsed         | 173        |
|    total_timesteps      | 2560       |
| train/                  |            |
|    approx_kl            | 0.77740014 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.504      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.368      |
|    value_loss           | 3.31       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 6.14      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 129       |
|    time_elapsed         | 174       |
|    total_timesteps      | 2580      |
| train/                  |           |
|    approx_kl            | 0.6366047 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.178     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.99      |
|    n_updates            | 2560      |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.369     |
|    value_loss           | 11        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 6.45       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 130        |
|    time_elapsed         | 176        |
|    total_timesteps      | 2600       |
| train/                  |            |
|    approx_kl            | 0.99220604 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.25       |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.269     |
|    std                  | 0.368      |
|    value_loss           | 12.9       |
----------------------------------------
--------------------------------------
| reward                  | 0.199    |
| reward_contact          | -0.148   |
| reward_motion           | 0.39     |
| reward_torque           | -0.043   |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 6.58     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 131      |
|    time_elapsed         | 177      |
|    total_timesteps      | 2620     |
| train/                  |          |
|    approx_kl            | 4.655806 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.234    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.4      |
|    n_updates            | 2600     |
|    policy_gradient_loss | -0.285   |
|    std                  | 0.368    |
|    value_loss           | 9.9      |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 6.84     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 132      |
|    time_elapsed         | 178      |
|    total_timesteps      | 2640     |
| train/                  |          |
|    approx_kl            | 1.334123 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | -0.218   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.91     |
|    n_updates            | 2620     |
|    policy_gradient_loss | -0.275   |
|    std                  | 0.368    |
|    value_loss           | 11.1     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.08      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 133       |
|    time_elapsed         | 180       |
|    total_timesteps      | 2660      |
| train/                  |           |
|    approx_kl            | 2.1873524 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.22     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.07      |
|    n_updates            | 2640      |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.369     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.33      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 134       |
|    time_elapsed         | 181       |
|    total_timesteps      | 2680      |
| train/                  |           |
|    approx_kl            | 1.0812914 |
|    clip_fraction        | 0.628     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.183     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.91      |
|    n_updates            | 2660      |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.369     |
|    value_loss           | 5.75      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.58      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 135       |
|    time_elapsed         | 182       |
|    total_timesteps      | 2700      |
| train/                  |           |
|    approx_kl            | 1.5055653 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.213     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.18      |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.369     |
|    value_loss           | 7.97      |
---------------------------------------
---------------------------------------
| reward                  | 0.288     |
| reward_contact          | -0.0788   |
| reward_motion           | 0.41      |
| reward_torque           | -0.0429   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.85      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 136       |
|    time_elapsed         | 184       |
|    total_timesteps      | 2720      |
| train/                  |           |
|    approx_kl            | 1.5487279 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.659    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.69      |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.369     |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.07      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 137       |
|    time_elapsed         | 185       |
|    total_timesteps      | 2740      |
| train/                  |           |
|    approx_kl            | 4.7837806 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.0317    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.46      |
|    n_updates            | 2720      |
|    policy_gradient_loss | -0.294    |
|    std                  | 0.369     |
|    value_loss           | 19.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.12      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 138       |
|    time_elapsed         | 186       |
|    total_timesteps      | 2760      |
| train/                  |           |
|    approx_kl            | 3.5186653 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.00213   |
|    learning_rate        | 0.0003    |
|    loss                 | 7.48      |
|    n_updates            | 2740      |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.369     |
|    value_loss           | 18.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.36      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 139       |
|    time_elapsed         | 188       |
|    total_timesteps      | 2780      |
| train/                  |           |
|    approx_kl            | 1.6319008 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.369     |
|    value_loss           | 8.1       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.44      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 140       |
|    time_elapsed         | 189       |
|    total_timesteps      | 2800      |
| train/                  |           |
|    approx_kl            | 3.5240948 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.89     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.78      |
|    n_updates            | 2780      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.369     |
|    value_loss           | 26.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.343     |
| reward_contact          | -0.0425   |
| reward_motion           | 0.42      |
| reward_torque           | -0.0349   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.57      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 141       |
|    time_elapsed         | 190       |
|    total_timesteps      | 2820      |
| train/                  |           |
|    approx_kl            | 1.2220429 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.98      |
|    n_updates            | 2800      |
|    policy_gradient_loss | -0.293    |
|    std                  | 0.369     |
|    value_loss           | 18.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 8.82     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 142      |
|    time_elapsed         | 192      |
|    total_timesteps      | 2840     |
| train/                  |          |
|    approx_kl            | 1.772725 |
|    clip_fraction        | 0.64     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.421    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.66     |
|    n_updates            | 2820     |
|    policy_gradient_loss | -0.212   |
|    std                  | 0.369    |
|    value_loss           | 10.6     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 9.03     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 143      |
|    time_elapsed         | 193      |
|    total_timesteps      | 2860     |
| train/                  |          |
|    approx_kl            | 2.593671 |
|    clip_fraction        | 0.79     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.018    |
|    learning_rate        | 0.0003   |
|    loss                 | 9.99     |
|    n_updates            | 2840     |
|    policy_gradient_loss | -0.291   |
|    std                  | 0.369    |
|    value_loss           | 23       |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 9.27       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 144        |
|    time_elapsed         | 195        |
|    total_timesteps      | 2880       |
| train/                  |            |
|    approx_kl            | 0.84197825 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.474     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.46       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.369      |
|    value_loss           | 22.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 9.44      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 145       |
|    time_elapsed         | 196       |
|    total_timesteps      | 2900      |
| train/                  |           |
|    approx_kl            | 3.5123067 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.172     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.15      |
|    n_updates            | 2880      |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.369     |
|    value_loss           | 18.9      |
---------------------------------------
--------------------------------------
| reward                  | 0.399    |
| reward_contact          | -0.0237  |
| reward_motion           | 0.45     |
| reward_torque           | -0.027   |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 9.49     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 146      |
|    time_elapsed         | 197      |
|    total_timesteps      | 2920     |
| train/                  |          |
|    approx_kl            | 3.726751 |
|    clip_fraction        | 0.778    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.381    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.85     |
|    n_updates            | 2900     |
|    policy_gradient_loss | -0.275   |
|    std                  | 0.369    |
|    value_loss           | 12.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 9.56      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 147       |
|    time_elapsed         | 199       |
|    total_timesteps      | 2940      |
| train/                  |           |
|    approx_kl            | 1.0156096 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.73      |
|    n_updates            | 2920      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.369     |
|    value_loss           | 19.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 9.66      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 148       |
|    time_elapsed         | 200       |
|    total_timesteps      | 2960      |
| train/                  |           |
|    approx_kl            | 0.8744192 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.74      |
|    n_updates            | 2940      |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.369     |
|    value_loss           | 8.59      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 9.91      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 149       |
|    time_elapsed         | 201       |
|    total_timesteps      | 2980      |
| train/                  |           |
|    approx_kl            | 1.0589349 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.137    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.27      |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.369     |
|    value_loss           | 21.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 9.9       |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 150       |
|    time_elapsed         | 203       |
|    total_timesteps      | 3000      |
| train/                  |           |
|    approx_kl            | 3.3602116 |
|    clip_fraction        | 0.848     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -0.227    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.02      |
|    n_updates            | 2980      |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.369     |
|    value_loss           | 18.9      |
---------------------------------------
----------------------------------------
| reward                  | 0.412      |
| reward_contact          | -0.0218    |
| reward_motion           | 0.46       |
| reward_torque           | -0.0262    |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 151        |
|    time_elapsed         | 204        |
|    total_timesteps      | 3020       |
| train/                  |            |
|    approx_kl            | 0.85568917 |
|    clip_fraction        | 0.72       |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | -0.312     |
|    learning_rate        | 0.0003     |
|    loss                 | 12         |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.285     |
|    std                  | 0.369      |
|    value_loss           | 30         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 152       |
|    time_elapsed         | 205       |
|    total_timesteps      | 3040      |
| train/                  |           |
|    approx_kl            | 0.8552871 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.428     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.97      |
|    n_updates            | 3020      |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.369     |
|    value_loss           | 12.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 153        |
|    time_elapsed         | 207        |
|    total_timesteps      | 3060       |
| train/                  |            |
|    approx_kl            | 0.48822957 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.33       |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.238     |
|    std                  | 0.369      |
|    value_loss           | 10.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 10.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 154       |
|    time_elapsed         | 208       |
|    total_timesteps      | 3080      |
| train/                  |           |
|    approx_kl            | 1.2534387 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.77      |
|    n_updates            | 3060      |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.369     |
|    value_loss           | 14        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 10.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 155       |
|    time_elapsed         | 209       |
|    total_timesteps      | 3100      |
| train/                  |           |
|    approx_kl            | 0.8832537 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.06      |
|    n_updates            | 3080      |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.369     |
|    value_loss           | 13.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.425     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.47      |
| reward_torque           | -0.0233   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 10.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 156       |
|    time_elapsed         | 211       |
|    total_timesteps      | 3120      |
| train/                  |           |
|    approx_kl            | 1.7767743 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.424     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.21      |
|    n_updates            | 3100      |
|    policy_gradient_loss | -0.301    |
|    std                  | 0.369     |
|    value_loss           | 8.56      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 10.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 157      |
|    time_elapsed         | 212      |
|    total_timesteps      | 3140     |
| train/                  |          |
|    approx_kl            | 4.638888 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.5      |
|    learning_rate        | 0.0003   |
|    loss                 | 1.41     |
|    n_updates            | 3120     |
|    policy_gradient_loss | -0.284   |
|    std                  | 0.369    |
|    value_loss           | 5.98     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 158       |
|    time_elapsed         | 213       |
|    total_timesteps      | 3160      |
| train/                  |           |
|    approx_kl            | 1.1922854 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.89      |
|    n_updates            | 3140      |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.369     |
|    value_loss           | 8.4       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 159        |
|    time_elapsed         | 215        |
|    total_timesteps      | 3180       |
| train/                  |            |
|    approx_kl            | 0.75184065 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -0.082     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.19       |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.271     |
|    std                  | 0.369      |
|    value_loss           | 11         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 160       |
|    time_elapsed         | 216       |
|    total_timesteps      | 3200      |
| train/                  |           |
|    approx_kl            | 0.9381423 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.26      |
|    n_updates            | 3180      |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.369     |
|    value_loss           | 11.4      |
---------------------------------------
---------------------------------------
| reward                  | 0.448     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.47      |
| reward_torque           | -0.000442 |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 161       |
|    time_elapsed         | 217       |
|    total_timesteps      | 3220      |
| train/                  |           |
|    approx_kl            | 3.5043855 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.434     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.41      |
|    n_updates            | 3200      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.369     |
|    value_loss           | 6.48      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 162       |
|    time_elapsed         | 219       |
|    total_timesteps      | 3240      |
| train/                  |           |
|    approx_kl            | 3.1060753 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.289    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.38      |
|    n_updates            | 3220      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.369     |
|    value_loss           | 13.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 163       |
|    time_elapsed         | 220       |
|    total_timesteps      | 3260      |
| train/                  |           |
|    approx_kl            | 1.3947968 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.0126   |
|    learning_rate        | 0.0003    |
|    loss                 | 8.9       |
|    n_updates            | 3240      |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.369     |
|    value_loss           | 19.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 164        |
|    time_elapsed         | 221        |
|    total_timesteps      | 3280       |
| train/                  |            |
|    approx_kl            | 0.51744944 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 3260       |
|    policy_gradient_loss | -0.232     |
|    std                  | 0.369      |
|    value_loss           | 7.83       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 165       |
|    time_elapsed         | 223       |
|    total_timesteps      | 3300      |
| train/                  |           |
|    approx_kl            | 2.1057832 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.543     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.12      |
|    n_updates            | 3280      |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.369     |
|    value_loss           | 6.14      |
---------------------------------------
----------------------------------------
| reward                  | 0.448      |
| reward_contact          | -0.0218    |
| reward_motion           | 0.47       |
| reward_torque           | -0.000442  |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 11.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 166        |
|    time_elapsed         | 224        |
|    total_timesteps      | 3320       |
| train/                  |            |
|    approx_kl            | 0.26925874 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -0.361     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.1       |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.369      |
|    value_loss           | 30.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 167       |
|    time_elapsed         | 225       |
|    total_timesteps      | 3340      |
| train/                  |           |
|    approx_kl            | 0.9555646 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.423     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.04      |
|    n_updates            | 3320      |
|    policy_gradient_loss | -0.211    |
|    std                  | 0.369     |
|    value_loss           | 6         |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 168       |
|    time_elapsed         | 227       |
|    total_timesteps      | 3360      |
| train/                  |           |
|    approx_kl            | 0.6326392 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.373     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.2       |
|    n_updates            | 3340      |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.369     |
|    value_loss           | 8.51      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 169       |
|    time_elapsed         | 228       |
|    total_timesteps      | 3380      |
| train/                  |           |
|    approx_kl            | 0.5471379 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.568     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.48      |
|    n_updates            | 3360      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.369     |
|    value_loss           | 8.2       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 170       |
|    time_elapsed         | 229       |
|    total_timesteps      | 3400      |
| train/                  |           |
|    approx_kl            | 1.1802638 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.586     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.809     |
|    n_updates            | 3380      |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.369     |
|    value_loss           | 3.79      |
---------------------------------------
---------------------------------------
| reward                  | 0.448     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.47      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 171       |
|    time_elapsed         | 231       |
|    total_timesteps      | 3420      |
| train/                  |           |
|    approx_kl            | 2.0032277 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.293     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.91      |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.369     |
|    value_loss           | 9.2       |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 172      |
|    time_elapsed         | 232      |
|    total_timesteps      | 3440     |
| train/                  |          |
|    approx_kl            | 4.721346 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.379    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.74     |
|    n_updates            | 3420     |
|    policy_gradient_loss | -0.288   |
|    std                  | 0.369    |
|    value_loss           | 5.89     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 173       |
|    time_elapsed         | 233       |
|    total_timesteps      | 3460      |
| train/                  |           |
|    approx_kl            | 0.7567787 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.56      |
|    n_updates            | 3440      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 6.02      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 174      |
|    time_elapsed         | 235      |
|    total_timesteps      | 3480     |
| train/                  |          |
|    approx_kl            | 0.703776 |
|    clip_fraction        | 0.61     |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | 0.626    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.03     |
|    n_updates            | 3460     |
|    policy_gradient_loss | -0.238   |
|    std                  | 0.37     |
|    value_loss           | 6.52     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 175      |
|    time_elapsed         | 236      |
|    total_timesteps      | 3500     |
| train/                  |          |
|    approx_kl            | 1.722895 |
|    clip_fraction        | 0.64     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.169    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.04     |
|    n_updates            | 3480     |
|    policy_gradient_loss | -0.241   |
|    std                  | 0.37     |
|    value_loss           | 9.89     |
--------------------------------------
----------------------------------------
| reward                  | 0.458      |
| reward_contact          | -0.0218    |
| reward_motion           | 0.48       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 176        |
|    time_elapsed         | 237        |
|    total_timesteps      | 3520       |
| train/                  |            |
|    approx_kl            | 0.42518225 |
|    clip_fraction        | 0.433      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.37       |
|    value_loss           | 5.01       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 177       |
|    time_elapsed         | 239       |
|    total_timesteps      | 3540      |
| train/                  |           |
|    approx_kl            | 3.0648217 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 3520      |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.37      |
|    value_loss           | 7.14      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 178        |
|    time_elapsed         | 240        |
|    total_timesteps      | 3560       |
| train/                  |            |
|    approx_kl            | 0.55394024 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.37       |
|    value_loss           | 5.84       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 179       |
|    time_elapsed         | 241       |
|    total_timesteps      | 3580      |
| train/                  |           |
|    approx_kl            | 1.5932077 |
|    clip_fraction        | 0.718     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.577     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.333     |
|    n_updates            | 3560      |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.37      |
|    value_loss           | 2.63      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 180        |
|    time_elapsed         | 243        |
|    total_timesteps      | 3600       |
| train/                  |            |
|    approx_kl            | 0.39947468 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.662      |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.37       |
|    value_loss           | 2.76       |
----------------------------------------
---------------------------------------
| reward                  | 0.458     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.48      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 181       |
|    time_elapsed         | 244       |
|    total_timesteps      | 3620      |
| train/                  |           |
|    approx_kl            | 0.7745078 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -1.44     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.36      |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.37      |
|    value_loss           | 26.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 182       |
|    time_elapsed         | 245       |
|    total_timesteps      | 3640      |
| train/                  |           |
|    approx_kl            | 1.9314072 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.341     |
|    n_updates            | 3620      |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.37      |
|    value_loss           | 2.58      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 183        |
|    time_elapsed         | 247        |
|    total_timesteps      | 3660       |
| train/                  |            |
|    approx_kl            | 0.56180406 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | -0.563     |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.37       |
|    value_loss           | 47.6       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 184        |
|    time_elapsed         | 248        |
|    total_timesteps      | 3680       |
| train/                  |            |
|    approx_kl            | 0.29057464 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.88       |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.221     |
|    std                  | 0.37       |
|    value_loss           | 13         |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 185        |
|    time_elapsed         | 249        |
|    total_timesteps      | 3700       |
| train/                  |            |
|    approx_kl            | 0.62824273 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | -0.899     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.66       |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.215     |
|    std                  | 0.37       |
|    value_loss           | 15.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.451     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.48      |
| reward_torque           | -0.00726  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 186       |
|    time_elapsed         | 251       |
|    total_timesteps      | 3720      |
| train/                  |           |
|    approx_kl            | 2.8094113 |
|    clip_fraction        | 0.745     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.802     |
|    n_updates            | 3700      |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.37      |
|    value_loss           | 2.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 187       |
|    time_elapsed         | 252       |
|    total_timesteps      | 3740      |
| train/                  |           |
|    approx_kl            | 1.0179498 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.291     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.95      |
|    n_updates            | 3720      |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.37      |
|    value_loss           | 11        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 188       |
|    time_elapsed         | 254       |
|    total_timesteps      | 3760      |
| train/                  |           |
|    approx_kl            | 1.4018195 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.228     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.38      |
|    n_updates            | 3740      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.37      |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 189       |
|    time_elapsed         | 255       |
|    total_timesteps      | 3780      |
| train/                  |           |
|    approx_kl            | 0.7849232 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.57      |
|    n_updates            | 3760      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.37      |
|    value_loss           | 12.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 190       |
|    time_elapsed         | 256       |
|    total_timesteps      | 3800      |
| train/                  |           |
|    approx_kl            | 0.7878745 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.22      |
|    n_updates            | 3780      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.37      |
|    value_loss           | 7.25      |
---------------------------------------
--------------------------------------
| reward                  | 0.481    |
| reward_contact          | -0.0218  |
| reward_motion           | 0.51     |
| reward_torque           | -0.00726 |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 191      |
|    time_elapsed         | 258      |
|    total_timesteps      | 3820     |
| train/                  |          |
|    approx_kl            | 5.169173 |
|    clip_fraction        | 0.755    |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.505    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.22     |
|    n_updates            | 3800     |
|    policy_gradient_loss | -0.301   |
|    std                  | 0.37     |
|    value_loss           | 10.7     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 192       |
|    time_elapsed         | 259       |
|    total_timesteps      | 3840      |
| train/                  |           |
|    approx_kl            | 0.2685819 |
|    clip_fraction        | 0.38      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -0.74     |
|    learning_rate        | 0.0003    |
|    loss                 | 35.1      |
|    n_updates            | 3820      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.37      |
|    value_loss           | 86.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 193       |
|    time_elapsed         | 260       |
|    total_timesteps      | 3860      |
| train/                  |           |
|    approx_kl            | 0.9570967 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.24      |
|    n_updates            | 3840      |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.37      |
|    value_loss           | 9.63      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 194       |
|    time_elapsed         | 261       |
|    total_timesteps      | 3880      |
| train/                  |           |
|    approx_kl            | 0.5516043 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.74      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 3860      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.37      |
|    value_loss           | 16.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 195       |
|    time_elapsed         | 263       |
|    total_timesteps      | 3900      |
| train/                  |           |
|    approx_kl            | 1.9926668 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.363     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.92      |
|    n_updates            | 3880      |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.37      |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| reward                  | 0.461     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.49      |
| reward_torque           | -0.00726  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 196       |
|    time_elapsed         | 264       |
|    total_timesteps      | 3920      |
| train/                  |           |
|    approx_kl            | 1.8322089 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.562     |
|    n_updates            | 3900      |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.37      |
|    value_loss           | 3.56      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 197        |
|    time_elapsed         | 266        |
|    total_timesteps      | 3940       |
| train/                  |            |
|    approx_kl            | 0.57938606 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | -0.378     |
|    learning_rate        | 0.0003     |
|    loss                 | 8.22       |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.249     |
|    std                  | 0.37       |
|    value_loss           | 23.2       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 198        |
|    time_elapsed         | 267        |
|    total_timesteps      | 3960       |
| train/                  |            |
|    approx_kl            | 0.69371116 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.39       |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.37       |
|    value_loss           | 7.06       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 199       |
|    time_elapsed         | 268       |
|    total_timesteps      | 3980      |
| train/                  |           |
|    approx_kl            | 0.9832244 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.438     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.67      |
|    n_updates            | 3960      |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.37      |
|    value_loss           | 9.71      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 13         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 200        |
|    time_elapsed         | 270        |
|    total_timesteps      | 4000       |
| train/                  |            |
|    approx_kl            | 0.47502849 |
|    clip_fraction        | 0.683      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.37       |
|    value_loss           | 6.08       |
----------------------------------------
---------------------------------------
| reward                  | 0.481     |
| reward_contact          | -0.0218   |
| reward_motion           | 0.51      |
| reward_torque           | -0.00726  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 201       |
|    time_elapsed         | 271       |
|    total_timesteps      | 4020      |
| train/                  |           |
|    approx_kl            | 1.0783837 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.0256    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.66      |
|    n_updates            | 4000      |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.37      |
|    value_loss           | 20.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 202      |
|    time_elapsed         | 272      |
|    total_timesteps      | 4040     |
| train/                  |          |
|    approx_kl            | 7.791191 |
|    clip_fraction        | 0.805    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.309    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.78     |
|    n_updates            | 4020     |
|    policy_gradient_loss | -0.278   |
|    std                  | 0.37     |
|    value_loss           | 6.64     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 203       |
|    time_elapsed         | 274       |
|    total_timesteps      | 4060      |
| train/                  |           |
|    approx_kl            | 2.1184127 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.418     |
|    n_updates            | 4040      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.37      |
|    value_loss           | 1.57      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 204       |
|    time_elapsed         | 275       |
|    total_timesteps      | 4080      |
| train/                  |           |
|    approx_kl            | 4.8719687 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.763     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.513     |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.37      |
|    value_loss           | 4.64      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 13.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 205        |
|    time_elapsed         | 276        |
|    total_timesteps      | 4100       |
| train/                  |            |
|    approx_kl            | 0.24790111 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -1.68      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.01       |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.37       |
|    value_loss           | 29.9       |
----------------------------------------
----------------------------------------
| reward                  | 0.503      |
| reward_contact          | 0          |
| reward_motion           | 0.51       |
| reward_torque           | -0.00726   |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 13.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 206        |
|    time_elapsed         | 278        |
|    total_timesteps      | 4120       |
| train/                  |            |
|    approx_kl            | 0.45465347 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.463      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.4        |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.231     |
|    std                  | 0.37       |
|    value_loss           | 15.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 207       |
|    time_elapsed         | 279       |
|    total_timesteps      | 4140      |
| train/                  |           |
|    approx_kl            | 1.2211393 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.47      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 4120      |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.37      |
|    value_loss           | 10.6      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 208      |
|    time_elapsed         | 280      |
|    total_timesteps      | 4160     |
| train/                  |          |
|    approx_kl            | 0.891297 |
|    clip_fraction        | 0.53     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | -1.36    |
|    learning_rate        | 0.0003   |
|    loss                 | 16       |
|    n_updates            | 4140     |
|    policy_gradient_loss | -0.236   |
|    std                  | 0.37     |
|    value_loss           | 38.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 209       |
|    time_elapsed         | 282       |
|    total_timesteps      | 4180      |
| train/                  |           |
|    approx_kl            | 1.0408797 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.76      |
|    n_updates            | 4160      |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.37      |
|    value_loss           | 5.24      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 210      |
|    time_elapsed         | 283      |
|    total_timesteps      | 4200     |
| train/                  |          |
|    approx_kl            | 0.657515 |
|    clip_fraction        | 0.59     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | -0.957   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.53     |
|    n_updates            | 4180     |
|    policy_gradient_loss | -0.24    |
|    std                  | 0.37     |
|    value_loss           | 12.5     |
--------------------------------------
--------------------------------------
| reward                  | 0.513    |
| reward_contact          | 0        |
| reward_motion           | 0.52     |
| reward_torque           | -0.00726 |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 211      |
|    time_elapsed         | 284      |
|    total_timesteps      | 4220     |
| train/                  |          |
|    approx_kl            | 3.940767 |
|    clip_fraction        | 0.845    |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.535    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.99     |
|    n_updates            | 4200     |
|    policy_gradient_loss | -0.295   |
|    std                  | 0.37     |
|    value_loss           | 6.32     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 212       |
|    time_elapsed         | 286       |
|    total_timesteps      | 4240      |
| train/                  |           |
|    approx_kl            | 1.8739346 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0833    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.16      |
|    n_updates            | 4220      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 13.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 213       |
|    time_elapsed         | 287       |
|    total_timesteps      | 4260      |
| train/                  |           |
|    approx_kl            | 0.4656291 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.652    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.78      |
|    n_updates            | 4240      |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.37      |
|    value_loss           | 19        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 214       |
|    time_elapsed         | 288       |
|    total_timesteps      | 4280      |
| train/                  |           |
|    approx_kl            | 1.8318695 |
|    clip_fraction        | 0.675     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.552     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.369     |
|    value_loss           | 5.92      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 215       |
|    time_elapsed         | 290       |
|    total_timesteps      | 4300      |
| train/                  |           |
|    approx_kl            | 0.8172493 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.414    |
|    learning_rate        | 0.0003    |
|    loss                 | 13.6      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.369     |
|    value_loss           | 32.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.533     |
| reward_contact          | 0         |
| reward_motion           | 0.54      |
| reward_torque           | -0.00726  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 216       |
|    time_elapsed         | 291       |
|    total_timesteps      | 4320      |
| train/                  |           |
|    approx_kl            | 2.1568792 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.292    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.6       |
|    n_updates            | 4300      |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.369     |
|    value_loss           | 19.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 217       |
|    time_elapsed         | 292       |
|    total_timesteps      | 4340      |
| train/                  |           |
|    approx_kl            | 1.6590226 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0819    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 4320      |
|    policy_gradient_loss | -0.281    |
|    std                  | 0.369     |
|    value_loss           | 3.68      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 218      |
|    time_elapsed         | 294      |
|    total_timesteps      | 4360     |
| train/                  |          |
|    approx_kl            | 0.78845  |
|    clip_fraction        | 0.67     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.0365   |
|    learning_rate        | 0.0003   |
|    loss                 | 7.82     |
|    n_updates            | 4340     |
|    policy_gradient_loss | -0.263   |
|    std                  | 0.369    |
|    value_loss           | 22       |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 219       |
|    time_elapsed         | 295       |
|    total_timesteps      | 4380      |
| train/                  |           |
|    approx_kl            | 1.0484008 |
|    clip_fraction        | 0.725     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.291     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.91      |
|    n_updates            | 4360      |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.369     |
|    value_loss           | 13.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 220      |
|    time_elapsed         | 296      |
|    total_timesteps      | 4400     |
| train/                  |          |
|    approx_kl            | 1.508768 |
|    clip_fraction        | 0.72     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | -0.166   |
|    learning_rate        | 0.0003   |
|    loss                 | 7.34     |
|    n_updates            | 4380     |
|    policy_gradient_loss | -0.282   |
|    std                  | 0.369    |
|    value_loss           | 18.5     |
--------------------------------------
--------------------------------------
| reward                  | 0.523    |
| reward_contact          | 0        |
| reward_motion           | 0.53     |
| reward_torque           | -0.00726 |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 221      |
|    time_elapsed         | 298      |
|    total_timesteps      | 4420     |
| train/                  |          |
|    approx_kl            | 2.942251 |
|    clip_fraction        | 0.76     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | -0.676   |
|    learning_rate        | 0.0003   |
|    loss                 | 5.89     |
|    n_updates            | 4400     |
|    policy_gradient_loss | -0.287   |
|    std                  | 0.369    |
|    value_loss           | 17       |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 13         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 222        |
|    time_elapsed         | 299        |
|    total_timesteps      | 4440       |
| train/                  |            |
|    approx_kl            | 0.82630557 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.451      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.44       |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.244     |
|    std                  | 0.369      |
|    value_loss           | 12         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 223       |
|    time_elapsed         | 300       |
|    total_timesteps      | 4460      |
| train/                  |           |
|    approx_kl            | 1.6987473 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.1       |
|    n_updates            | 4440      |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.369     |
|    value_loss           | 11.9      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 224        |
|    time_elapsed         | 302        |
|    total_timesteps      | 4480       |
| train/                  |            |
|    approx_kl            | 0.76458585 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.37       |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.259     |
|    std                  | 0.369      |
|    value_loss           | 17.8       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 225        |
|    time_elapsed         | 303        |
|    total_timesteps      | 4500       |
| train/                  |            |
|    approx_kl            | 0.60840225 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.369      |
|    value_loss           | 6.73       |
----------------------------------------
---------------------------------------
| reward                  | 0.522     |
| reward_contact          | -0.0106   |
| reward_motion           | 0.55      |
| reward_torque           | -0.0175   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 226       |
|    time_elapsed         | 304       |
|    total_timesteps      | 4520      |
| train/                  |           |
|    approx_kl            | 0.6508215 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.447     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.37      |
|    n_updates            | 4500      |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.369     |
|    value_loss           | 11.9      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 227        |
|    time_elapsed         | 306        |
|    total_timesteps      | 4540       |
| train/                  |            |
|    approx_kl            | 0.88108855 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.27      |
|    std                  | 0.369      |
|    value_loss           | 5.89       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 228       |
|    time_elapsed         | 307       |
|    total_timesteps      | 4560      |
| train/                  |           |
|    approx_kl            | 0.5282096 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.0987    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.2       |
|    n_updates            | 4540      |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.369     |
|    value_loss           | 17.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 229       |
|    time_elapsed         | 308       |
|    total_timesteps      | 4580      |
| train/                  |           |
|    approx_kl            | 1.8066596 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.83      |
|    n_updates            | 4560      |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.37      |
|    value_loss           | 5.48      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 230       |
|    time_elapsed         | 310       |
|    total_timesteps      | 4600      |
| train/                  |           |
|    approx_kl            | 3.0168324 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.152     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.83      |
|    n_updates            | 4580      |
|    policy_gradient_loss | -0.31     |
|    std                  | 0.37      |
|    value_loss           | 14.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.491     |
| reward_contact          | -0.0106   |
| reward_motion           | 0.52      |
| reward_torque           | -0.0182   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 231       |
|    time_elapsed         | 311       |
|    total_timesteps      | 4620      |
| train/                  |           |
|    approx_kl            | 1.8134053 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.0526    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.72      |
|    n_updates            | 4600      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.369     |
|    value_loss           | 11.1      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 232        |
|    time_elapsed         | 312        |
|    total_timesteps      | 4640       |
| train/                  |            |
|    approx_kl            | 0.62769765 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.45       |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.369      |
|    value_loss           | 5.98       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 233       |
|    time_elapsed         | 313       |
|    total_timesteps      | 4660      |
| train/                  |           |
|    approx_kl            | 0.6945451 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.279     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.54      |
|    n_updates            | 4640      |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.369     |
|    value_loss           | 19.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 234      |
|    time_elapsed         | 315      |
|    total_timesteps      | 4680     |
| train/                  |          |
|    approx_kl            | 0.935112 |
|    clip_fraction        | 0.685    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.321    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.21     |
|    n_updates            | 4660     |
|    policy_gradient_loss | -0.221   |
|    std                  | 0.369    |
|    value_loss           | 12.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 235       |
|    time_elapsed         | 316       |
|    total_timesteps      | 4700      |
| train/                  |           |
|    approx_kl            | 1.2795604 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.465     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.69      |
|    n_updates            | 4680      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.369     |
|    value_loss           | 8.83      |
---------------------------------------
---------------------------------------
| reward                  | 0.481     |
| reward_contact          | -0.0106   |
| reward_motion           | 0.51      |
| reward_torque           | -0.0182   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 236       |
|    time_elapsed         | 317       |
|    total_timesteps      | 4720      |
| train/                  |           |
|    approx_kl            | 2.8698337 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.072    |
|    learning_rate        | 0.0003    |
|    loss                 | 12.7      |
|    n_updates            | 4700      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.369     |
|    value_loss           | 29.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 237       |
|    time_elapsed         | 319       |
|    total_timesteps      | 4740      |
| train/                  |           |
|    approx_kl            | 1.4133469 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.24      |
|    n_updates            | 4720      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.369     |
|    value_loss           | 8.26      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 238       |
|    time_elapsed         | 320       |
|    total_timesteps      | 4760      |
| train/                  |           |
|    approx_kl            | 0.8378382 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.243    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.25      |
|    n_updates            | 4740      |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.37      |
|    value_loss           | 19.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 239       |
|    time_elapsed         | 321       |
|    total_timesteps      | 4780      |
| train/                  |           |
|    approx_kl            | 2.7931628 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.234     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.31      |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.37      |
|    value_loss           | 13.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 240       |
|    time_elapsed         | 323       |
|    total_timesteps      | 4800      |
| train/                  |           |
|    approx_kl            | 1.4348143 |
|    clip_fraction        | 0.733     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.0789   |
|    learning_rate        | 0.0003    |
|    loss                 | 2.43      |
|    n_updates            | 4780      |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.369     |
|    value_loss           | 8.14      |
---------------------------------------
--------------------------------------
| reward                  | 0.481    |
| reward_contact          | -0.0106  |
| reward_motion           | 0.51     |
| reward_torque           | -0.0182  |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 241      |
|    time_elapsed         | 324      |
|    total_timesteps      | 4820     |
| train/                  |          |
|    approx_kl            | 3.685227 |
|    clip_fraction        | 0.835    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.292    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.16     |
|    n_updates            | 4800     |
|    policy_gradient_loss | -0.293   |
|    std                  | 0.369    |
|    value_loss           | 8.82     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 242       |
|    time_elapsed         | 325       |
|    total_timesteps      | 4840      |
| train/                  |           |
|    approx_kl            | 1.3510482 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.205     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 4820      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.369     |
|    value_loss           | 7.11      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 243      |
|    time_elapsed         | 327      |
|    total_timesteps      | 4860     |
| train/                  |          |
|    approx_kl            | 5.236856 |
|    clip_fraction        | 0.86     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.0854   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.73     |
|    n_updates            | 4840     |
|    policy_gradient_loss | -0.289   |
|    std                  | 0.369    |
|    value_loss           | 10.3     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 244        |
|    time_elapsed         | 328        |
|    total_timesteps      | 4880       |
| train/                  |            |
|    approx_kl            | 0.51765394 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.37      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.08       |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.369      |
|    value_loss           | 17.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 245       |
|    time_elapsed         | 329       |
|    total_timesteps      | 4900      |
| train/                  |           |
|    approx_kl            | 1.9822956 |
|    clip_fraction        | 0.705     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.441     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.15      |
|    n_updates            | 4880      |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.37      |
|    value_loss           | 6.79      |
---------------------------------------
---------------------------------------
| reward                  | 0.481     |
| reward_contact          | -0.0106   |
| reward_motion           | 0.51      |
| reward_torque           | -0.0182   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 246       |
|    time_elapsed         | 331       |
|    total_timesteps      | 4920      |
| train/                  |           |
|    approx_kl            | 1.5623286 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.391     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.38      |
|    n_updates            | 4900      |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.37      |
|    value_loss           | 8.35      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 247       |
|    time_elapsed         | 332       |
|    total_timesteps      | 4940      |
| train/                  |           |
|    approx_kl            | 2.0084283 |
|    clip_fraction        | 0.635     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.57     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.81      |
|    n_updates            | 4920      |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.37      |
|    value_loss           | 23.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 248       |
|    time_elapsed         | 333       |
|    total_timesteps      | 4960      |
| train/                  |           |
|    approx_kl            | 0.5520497 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.57      |
|    learning_rate        | 0.0003    |
|    loss                 | 10.6      |
|    n_updates            | 4940      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.37      |
|    value_loss           | 28.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 249       |
|    time_elapsed         | 335       |
|    total_timesteps      | 4980      |
| train/                  |           |
|    approx_kl            | 0.5685684 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.451     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.65      |
|    n_updates            | 4960      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 17.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 250        |
|    time_elapsed         | 336        |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.91630393 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -0.288     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.38       |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.37       |
|    value_loss           | 10.5       |
----------------------------------------
--------------------------------------
| reward                  | 0.491    |
| reward_contact          | -0.0106  |
| reward_motion           | 0.52     |
| reward_torque           | -0.0182  |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 251      |
|    time_elapsed         | 337      |
|    total_timesteps      | 5020     |
| train/                  |          |
|    approx_kl            | 2.410082 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.353    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.353    |
|    n_updates            | 5000     |
|    policy_gradient_loss | -0.29    |
|    std                  | 0.37     |
|    value_loss           | 3.2      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 252       |
|    time_elapsed         | 339       |
|    total_timesteps      | 5040      |
| train/                  |           |
|    approx_kl            | 1.4569461 |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.23      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.64      |
|    n_updates            | 5020      |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.37      |
|    value_loss           | 8.09      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 253       |
|    time_elapsed         | 340       |
|    total_timesteps      | 5060      |
| train/                  |           |
|    approx_kl            | 2.2421968 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.0202    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.79      |
|    n_updates            | 5040      |
|    policy_gradient_loss | -0.307    |
|    std                  | 0.37      |
|    value_loss           | 5.42      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 254       |
|    time_elapsed         | 342       |
|    total_timesteps      | 5080      |
| train/                  |           |
|    approx_kl            | 3.4474862 |
|    clip_fraction        | 0.583     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.86      |
|    n_updates            | 5060      |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.369     |
|    value_loss           | 4.87      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 255        |
|    time_elapsed         | 343        |
|    total_timesteps      | 5100       |
| train/                  |            |
|    approx_kl            | 0.52170455 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.807      |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.37       |
|    value_loss           | 5          |
----------------------------------------
---------------------------------------
| reward                  | 0.52      |
| reward_contact          | -0.0106   |
| reward_motion           | 0.55      |
| reward_torque           | -0.0197   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 256       |
|    time_elapsed         | 344       |
|    total_timesteps      | 5120      |
| train/                  |           |
|    approx_kl            | 3.0208645 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -0.838    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.73      |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.37      |
|    value_loss           | 8.69      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 257       |
|    time_elapsed         | 346       |
|    total_timesteps      | 5140      |
| train/                  |           |
|    approx_kl            | 1.0760164 |
|    clip_fraction        | 0.718     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.851     |
|    n_updates            | 5120      |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.37      |
|    value_loss           | 4.08      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 258       |
|    time_elapsed         | 347       |
|    total_timesteps      | 5160      |
| train/                  |           |
|    approx_kl            | 3.1003208 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.639     |
|    n_updates            | 5140      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 3.12      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 259      |
|    time_elapsed         | 348      |
|    total_timesteps      | 5180     |
| train/                  |          |
|    approx_kl            | 4.590366 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | -0.102   |
|    learning_rate        | 0.0003   |
|    loss                 | 1.83     |
|    n_updates            | 5160     |
|    policy_gradient_loss | -0.24    |
|    std                  | 0.37     |
|    value_loss           | 5.11     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 260       |
|    time_elapsed         | 350       |
|    total_timesteps      | 5200      |
| train/                  |           |
|    approx_kl            | 1.4484141 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.605    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.79      |
|    n_updates            | 5180      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.37      |
|    value_loss           | 14.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.527      |
| reward_contact          | -0.0133    |
| reward_motion           | 0.56       |
| reward_torque           | -0.0197    |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 11.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 261        |
|    time_elapsed         | 351        |
|    total_timesteps      | 5220       |
| train/                  |            |
|    approx_kl            | 0.75084394 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.64       |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.37       |
|    value_loss           | 12.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 262       |
|    time_elapsed         | 352       |
|    total_timesteps      | 5240      |
| train/                  |           |
|    approx_kl            | 0.8465751 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.485     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.95      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.37      |
|    value_loss           | 8.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 263       |
|    time_elapsed         | 353       |
|    total_timesteps      | 5260      |
| train/                  |           |
|    approx_kl            | 1.6509793 |
|    clip_fraction        | 0.763     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.79      |
|    n_updates            | 5240      |
|    policy_gradient_loss | -0.307    |
|    std                  | 0.37      |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 264       |
|    time_elapsed         | 355       |
|    total_timesteps      | 5280      |
| train/                  |           |
|    approx_kl            | 1.6249164 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.809     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0664    |
|    n_updates            | 5260      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.37      |
|    value_loss           | 1.43      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 265       |
|    time_elapsed         | 356       |
|    total_timesteps      | 5300      |
| train/                  |           |
|    approx_kl            | 2.7671487 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.221    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.71      |
|    n_updates            | 5280      |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.37      |
|    value_loss           | 8.09      |
---------------------------------------
---------------------------------------
| reward                  | 0.537     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.57      |
| reward_torque           | -0.0197   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 266       |
|    time_elapsed         | 358       |
|    total_timesteps      | 5320      |
| train/                  |           |
|    approx_kl            | 4.2214727 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.235    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.43      |
|    n_updates            | 5300      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 13.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 267       |
|    time_elapsed         | 359       |
|    total_timesteps      | 5340      |
| train/                  |           |
|    approx_kl            | 1.1908168 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.0903   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 5320      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 6.52      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 268       |
|    time_elapsed         | 360       |
|    total_timesteps      | 5360      |
| train/                  |           |
|    approx_kl            | 2.8139398 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.402     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.99      |
|    n_updates            | 5340      |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.37      |
|    value_loss           | 5.66      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 269       |
|    time_elapsed         | 361       |
|    total_timesteps      | 5380      |
| train/                  |           |
|    approx_kl            | 2.8360634 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.363     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.65      |
|    n_updates            | 5360      |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.37      |
|    value_loss           | 7.65      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 270       |
|    time_elapsed         | 363       |
|    total_timesteps      | 5400      |
| train/                  |           |
|    approx_kl            | 0.8769199 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.297     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.34      |
|    n_updates            | 5380      |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.37      |
|    value_loss           | 9.05      |
---------------------------------------
---------------------------------------
| reward                  | 0.557     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.59      |
| reward_torque           | -0.0197   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 271       |
|    time_elapsed         | 364       |
|    total_timesteps      | 5420      |
| train/                  |           |
|    approx_kl            | 2.3574874 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.47      |
|    n_updates            | 5400      |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.37      |
|    value_loss           | 6.98      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 272      |
|    time_elapsed         | 365      |
|    total_timesteps      | 5440     |
| train/                  |          |
|    approx_kl            | 0.934293 |
|    clip_fraction        | 0.53     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.154    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.54     |
|    n_updates            | 5420     |
|    policy_gradient_loss | -0.245   |
|    std                  | 0.37     |
|    value_loss           | 13.7     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 11.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 273        |
|    time_elapsed         | 367        |
|    total_timesteps      | 5460       |
| train/                  |            |
|    approx_kl            | 0.95157737 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.125      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.99       |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.24      |
|    std                  | 0.37       |
|    value_loss           | 14.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 274       |
|    time_elapsed         | 368       |
|    total_timesteps      | 5480      |
| train/                  |           |
|    approx_kl            | 1.2407006 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.332     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.74      |
|    n_updates            | 5460      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 12        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 11.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 275       |
|    time_elapsed         | 369       |
|    total_timesteps      | 5500      |
| train/                  |           |
|    approx_kl            | 0.7653509 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.53      |
|    n_updates            | 5480      |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.37      |
|    value_loss           | 13.5      |
---------------------------------------
--------------------------------------
| reward                  | 0.555    |
| reward_contact          | -0.0133  |
| reward_motion           | 0.59     |
| reward_torque           | -0.0221  |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 11.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 276      |
|    time_elapsed         | 371      |
|    total_timesteps      | 5520     |
| train/                  |          |
|    approx_kl            | 1.814838 |
|    clip_fraction        | 0.65     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.225    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.95     |
|    n_updates            | 5500     |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.37     |
|    value_loss           | 8.24     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 277       |
|    time_elapsed         | 372       |
|    total_timesteps      | 5540      |
| train/                  |           |
|    approx_kl            | 2.1798882 |
|    clip_fraction        | 0.778     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.11     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.6       |
|    n_updates            | 5520      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 13.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 278       |
|    time_elapsed         | 373       |
|    total_timesteps      | 5560      |
| train/                  |           |
|    approx_kl            | 1.6107107 |
|    clip_fraction        | 0.675     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.17      |
|    n_updates            | 5540      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 279       |
|    time_elapsed         | 375       |
|    total_timesteps      | 5580      |
| train/                  |           |
|    approx_kl            | 2.5696049 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.318     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.91      |
|    n_updates            | 5560      |
|    policy_gradient_loss | -0.303    |
|    std                  | 0.37      |
|    value_loss           | 15.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 280       |
|    time_elapsed         | 376       |
|    total_timesteps      | 5600      |
| train/                  |           |
|    approx_kl            | 1.9353669 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.152    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.5       |
|    n_updates            | 5580      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.37      |
|    value_loss           | 20.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.595     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.63      |
| reward_torque           | -0.0221   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 281       |
|    time_elapsed         | 377       |
|    total_timesteps      | 5620      |
| train/                  |           |
|    approx_kl            | 1.1725038 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.209     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.07      |
|    n_updates            | 5600      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 13.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 282       |
|    time_elapsed         | 379       |
|    total_timesteps      | 5640      |
| train/                  |           |
|    approx_kl            | 4.0169196 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.38      |
|    n_updates            | 5620      |
|    policy_gradient_loss | -0.298    |
|    std                  | 0.37      |
|    value_loss           | 13.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 283      |
|    time_elapsed         | 380      |
|    total_timesteps      | 5660     |
| train/                  |          |
|    approx_kl            | 1.670111 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.312    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.76     |
|    n_updates            | 5640     |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.37     |
|    value_loss           | 13.1     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 12.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 284      |
|    time_elapsed         | 381      |
|    total_timesteps      | 5680     |
| train/                  |          |
|    approx_kl            | 3.062095 |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.0996   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.15     |
|    n_updates            | 5660     |
|    policy_gradient_loss | -0.245   |
|    std                  | 0.37     |
|    value_loss           | 10.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 285       |
|    time_elapsed         | 383       |
|    total_timesteps      | 5700      |
| train/                  |           |
|    approx_kl            | 3.0150425 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.55      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.6       |
|    n_updates            | 5680      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 8.44      |
---------------------------------------
---------------------------------------
| reward                  | 0.612     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.64      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 286       |
|    time_elapsed         | 384       |
|    total_timesteps      | 5720      |
| train/                  |           |
|    approx_kl            | 2.0313718 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 5700      |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.37      |
|    value_loss           | 6.3       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 287       |
|    time_elapsed         | 385       |
|    total_timesteps      | 5740      |
| train/                  |           |
|    approx_kl            | 3.4099786 |
|    clip_fraction        | 0.663     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.567     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.01      |
|    n_updates            | 5720      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.37      |
|    value_loss           | 5.63      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 288       |
|    time_elapsed         | 387       |
|    total_timesteps      | 5760      |
| train/                  |           |
|    approx_kl            | 0.9918998 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.718     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.82      |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.37      |
|    value_loss           | 5.23      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 289       |
|    time_elapsed         | 388       |
|    total_timesteps      | 5780      |
| train/                  |           |
|    approx_kl            | 0.8612248 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.425     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 5760      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.37      |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 290       |
|    time_elapsed         | 389       |
|    total_timesteps      | 5800      |
| train/                  |           |
|    approx_kl            | 1.4670695 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.173    |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 5780      |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.37      |
|    value_loss           | 29.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.612     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.64      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 291       |
|    time_elapsed         | 391       |
|    total_timesteps      | 5820      |
| train/                  |           |
|    approx_kl            | 0.8944091 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.419     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.87      |
|    n_updates            | 5800      |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.37      |
|    value_loss           | 9.36      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 12.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 292       |
|    time_elapsed         | 392       |
|    total_timesteps      | 5840      |
| train/                  |           |
|    approx_kl            | 4.1111345 |
|    clip_fraction        | 0.778     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.397     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.05      |
|    n_updates            | 5820      |
|    policy_gradient_loss | -0.297    |
|    std                  | 0.37      |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 293       |
|    time_elapsed         | 393       |
|    total_timesteps      | 5860      |
| train/                  |           |
|    approx_kl            | 1.1858578 |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0206    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.85      |
|    n_updates            | 5840      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.37      |
|    value_loss           | 14.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 294       |
|    time_elapsed         | 395       |
|    total_timesteps      | 5880      |
| train/                  |           |
|    approx_kl            | 3.8028467 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.321     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 5860      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.37      |
|    value_loss           | 9.64      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 295      |
|    time_elapsed         | 396      |
|    total_timesteps      | 5900     |
| train/                  |          |
|    approx_kl            | 5.074647 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.647    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.57     |
|    n_updates            | 5880     |
|    policy_gradient_loss | -0.257   |
|    std                  | 0.37     |
|    value_loss           | 7.89     |
--------------------------------------
---------------------------------------
| reward                  | 0.652     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.68      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 296       |
|    time_elapsed         | 397       |
|    total_timesteps      | 5920      |
| train/                  |           |
|    approx_kl            | 2.2052631 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.323     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.39      |
|    n_updates            | 5900      |
|    policy_gradient_loss | -0.278    |
|    std                  | 0.37      |
|    value_loss           | 15        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 297       |
|    time_elapsed         | 399       |
|    total_timesteps      | 5940      |
| train/                  |           |
|    approx_kl            | 2.5870178 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.31      |
|    n_updates            | 5920      |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.37      |
|    value_loss           | 1.83      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 298       |
|    time_elapsed         | 400       |
|    total_timesteps      | 5960      |
| train/                  |           |
|    approx_kl            | 0.9364562 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.0593    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.11      |
|    n_updates            | 5940      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 299       |
|    time_elapsed         | 401       |
|    total_timesteps      | 5980      |
| train/                  |           |
|    approx_kl            | 1.7990044 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 5960      |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.37      |
|    value_loss           | 9.89      |
---------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 13.31
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 300       |
|    time_elapsed         | 403       |
|    total_timesteps      | 6000      |
| train/                  |           |
|    approx_kl            | 2.2221932 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.39      |
|    n_updates            | 5980      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.37      |
|    value_loss           | 8.69      |
---------------------------------------
--------------------------------------
| reward                  | 0.662    |
| reward_contact          | -0.0133  |
| reward_motion           | 0.69     |
| reward_torque           | -0.0148  |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 301      |
|    time_elapsed         | 404      |
|    total_timesteps      | 6020     |
| train/                  |          |
|    approx_kl            | 1.574832 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.628    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.86     |
|    n_updates            | 6000     |
|    policy_gradient_loss | -0.274   |
|    std                  | 0.37     |
|    value_loss           | 8.73     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 302       |
|    time_elapsed         | 405       |
|    total_timesteps      | 6040      |
| train/                  |           |
|    approx_kl            | 0.7873643 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.67     |
|    learning_rate        | 0.0003    |
|    loss                 | 12        |
|    n_updates            | 6020      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.37      |
|    value_loss           | 29.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 303       |
|    time_elapsed         | 407       |
|    total_timesteps      | 6060      |
| train/                  |           |
|    approx_kl            | 0.5396614 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.334     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.3      |
|    n_updates            | 6040      |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.37      |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 304       |
|    time_elapsed         | 408       |
|    total_timesteps      | 6080      |
| train/                  |           |
|    approx_kl            | 2.9550104 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0412    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.74      |
|    n_updates            | 6060      |
|    policy_gradient_loss | -0.302    |
|    std                  | 0.37      |
|    value_loss           | 18.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 305       |
|    time_elapsed         | 409       |
|    total_timesteps      | 6100      |
| train/                  |           |
|    approx_kl            | 1.7771559 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.357     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.76      |
|    n_updates            | 6080      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.692     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.72      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 306       |
|    time_elapsed         | 411       |
|    total_timesteps      | 6120      |
| train/                  |           |
|    approx_kl            | 1.4209485 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.208     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.12      |
|    n_updates            | 6100      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.37      |
|    value_loss           | 18.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 307      |
|    time_elapsed         | 412      |
|    total_timesteps      | 6140     |
| train/                  |          |
|    approx_kl            | 3.095333 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.452    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.82     |
|    n_updates            | 6120     |
|    policy_gradient_loss | -0.245   |
|    std                  | 0.37     |
|    value_loss           | 9.61     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 308       |
|    time_elapsed         | 413       |
|    total_timesteps      | 6160      |
| train/                  |           |
|    approx_kl            | 0.6503956 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.49      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.09      |
|    n_updates            | 6140      |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.37      |
|    value_loss           | 21.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 13.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 309        |
|    time_elapsed         | 415        |
|    total_timesteps      | 6180       |
| train/                  |            |
|    approx_kl            | 0.38364443 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.404      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.67       |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.37       |
|    value_loss           | 18.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 310       |
|    time_elapsed         | 416       |
|    total_timesteps      | 6200      |
| train/                  |           |
|    approx_kl            | 3.0210423 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.473     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.37      |
|    n_updates            | 6180      |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.37      |
|    value_loss           | 6.6       |
---------------------------------------
---------------------------------------
| reward                  | 0.692     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.72      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 311       |
|    time_elapsed         | 417       |
|    total_timesteps      | 6220      |
| train/                  |           |
|    approx_kl            | 2.0758352 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.467     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.17      |
|    n_updates            | 6200      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.37      |
|    value_loss           | 9.52      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 13.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 312       |
|    time_elapsed         | 419       |
|    total_timesteps      | 6240      |
| train/                  |           |
|    approx_kl            | 0.6835851 |
|    clip_fraction        | 0.51      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.38      |
|    n_updates            | 6220      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.37      |
|    value_loss           | 8.56      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 13.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 313      |
|    time_elapsed         | 420      |
|    total_timesteps      | 6260     |
| train/                  |          |
|    approx_kl            | 0.537919 |
|    clip_fraction        | 0.608    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.594    |
|    learning_rate        | 0.0003   |
|    loss                 | 6.29     |
|    n_updates            | 6240     |
|    policy_gradient_loss | -0.253   |
|    std                  | 0.37     |
|    value_loss           | 16.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 314       |
|    time_elapsed         | 421       |
|    total_timesteps      | 6280      |
| train/                  |           |
|    approx_kl            | 1.4162782 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.81      |
|    n_updates            | 6260      |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.37      |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 315       |
|    time_elapsed         | 423       |
|    total_timesteps      | 6300      |
| train/                  |           |
|    approx_kl            | 1.5888863 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.32      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.67      |
|    n_updates            | 6280      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.37      |
|    value_loss           | 8.71      |
---------------------------------------
---------------------------------------
| reward                  | 0.682     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.71      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 316       |
|    time_elapsed         | 424       |
|    total_timesteps      | 6320      |
| train/                  |           |
|    approx_kl            | 0.8808533 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.643     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 6300      |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.37      |
|    value_loss           | 5.78      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 317        |
|    time_elapsed         | 425        |
|    total_timesteps      | 6340       |
| train/                  |            |
|    approx_kl            | 0.79712474 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.188      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.54       |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.37       |
|    value_loss           | 10.1       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 318       |
|    time_elapsed         | 427       |
|    total_timesteps      | 6360      |
| train/                  |           |
|    approx_kl            | 1.7458647 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.09      |
|    n_updates            | 6340      |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.37      |
|    value_loss           | 6.29      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 319       |
|    time_elapsed         | 428       |
|    total_timesteps      | 6380      |
| train/                  |           |
|    approx_kl            | 1.4888645 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.209     |
|    learning_rate        | 0.0003    |
|    loss                 | 5         |
|    n_updates            | 6360      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 11.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 320        |
|    time_elapsed         | 429        |
|    total_timesteps      | 6400       |
| train/                  |            |
|    approx_kl            | 0.43478853 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.37       |
|    value_loss           | 9.61       |
----------------------------------------
---------------------------------------
| reward                  | 0.702     |
| reward_contact          | -0.0133   |
| reward_motion           | 0.73      |
| reward_torque           | -0.0148   |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 321       |
|    time_elapsed         | 431       |
|    total_timesteps      | 6420      |
| train/                  |           |
|    approx_kl            | 2.7179189 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.23      |
|    n_updates            | 6400      |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.37      |
|    value_loss           | 4.88      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 322      |
|    time_elapsed         | 432      |
|    total_timesteps      | 6440     |
| train/                  |          |
|    approx_kl            | 2.273297 |
|    clip_fraction        | 0.763    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.35     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.99     |
|    n_updates            | 6420     |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.37     |
|    value_loss           | 6.8      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 323       |
|    time_elapsed         | 433       |
|    total_timesteps      | 6460      |
| train/                  |           |
|    approx_kl            | 1.0391757 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.61      |
|    n_updates            | 6440      |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.37      |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 324       |
|    time_elapsed         | 435       |
|    total_timesteps      | 6480      |
| train/                  |           |
|    approx_kl            | 3.1810443 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.789     |
|    n_updates            | 6460      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.37      |
|    value_loss           | 3.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 325       |
|    time_elapsed         | 436       |
|    total_timesteps      | 6500      |
| train/                  |           |
|    approx_kl            | 1.9702488 |
|    clip_fraction        | 0.543     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.649     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 6480      |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.37      |
|    value_loss           | 5.17      |
---------------------------------------
---------------------------------------
| reward                  | 0.723     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.73      |
| reward_torque           | -0.00452  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 326       |
|    time_elapsed         | 437       |
|    total_timesteps      | 6520      |
| train/                  |           |
|    approx_kl            | 1.2070093 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.657     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.7       |
|    n_updates            | 6500      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.37      |
|    value_loss           | 7.48      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 327      |
|    time_elapsed         | 439      |
|    total_timesteps      | 6540     |
| train/                  |          |
|    approx_kl            | 2.378809 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.036    |
|    learning_rate        | 0.0003   |
|    loss                 | 7.64     |
|    n_updates            | 6520     |
|    policy_gradient_loss | -0.27    |
|    std                  | 0.37     |
|    value_loss           | 19.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 328       |
|    time_elapsed         | 440       |
|    total_timesteps      | 6560      |
| train/                  |           |
|    approx_kl            | 2.6140792 |
|    clip_fraction        | 0.688     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.69      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 6540      |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.37      |
|    value_loss           | 7.01      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 329        |
|    time_elapsed         | 441        |
|    total_timesteps      | 6580       |
| train/                  |            |
|    approx_kl            | 0.47035962 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.37       |
|    value_loss           | 4.11       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 330       |
|    time_elapsed         | 443       |
|    total_timesteps      | 6600      |
| train/                  |           |
|    approx_kl            | 2.2594216 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.143    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.08      |
|    n_updates            | 6580      |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.37      |
|    value_loss           | 15.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.75      |
| reward_torque           | -0.00388  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 331       |
|    time_elapsed         | 444       |
|    total_timesteps      | 6620      |
| train/                  |           |
|    approx_kl            | 1.7583605 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.654     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 6600      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.37      |
|    value_loss           | 7.3       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 332        |
|    time_elapsed         | 445        |
|    total_timesteps      | 6640       |
| train/                  |            |
|    approx_kl            | 0.51528335 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.178      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.249     |
|    std                  | 0.37       |
|    value_loss           | 27.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 333       |
|    time_elapsed         | 447       |
|    total_timesteps      | 6660      |
| train/                  |           |
|    approx_kl            | 1.3799736 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.406     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.07      |
|    n_updates            | 6640      |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.37      |
|    value_loss           | 8.33      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 334       |
|    time_elapsed         | 448       |
|    total_timesteps      | 6680      |
| train/                  |           |
|    approx_kl            | 2.4589279 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 3.87      |
|    n_updates            | 6660      |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.37      |
|    value_loss           | 10.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 335        |
|    time_elapsed         | 449        |
|    total_timesteps      | 6700       |
| train/                  |            |
|    approx_kl            | 0.45386454 |
|    clip_fraction        | 0.695      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.44       |
|    n_updates            | 6680       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.37       |
|    value_loss           | 1.56       |
----------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.75      |
| reward_torque           | -0.00388  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 336       |
|    time_elapsed         | 451       |
|    total_timesteps      | 6720      |
| train/                  |           |
|    approx_kl            | 2.1490269 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.413     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.07      |
|    n_updates            | 6700      |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.37      |
|    value_loss           | 7.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 337       |
|    time_elapsed         | 452       |
|    total_timesteps      | 6740      |
| train/                  |           |
|    approx_kl            | 1.6238695 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.575     |
|    n_updates            | 6720      |
|    policy_gradient_loss | -0.231    |
|    std                  | 0.37      |
|    value_loss           | 3.62      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 338        |
|    time_elapsed         | 453        |
|    total_timesteps      | 6760       |
| train/                  |            |
|    approx_kl            | 0.55363095 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.36       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.219     |
|    std                  | 0.37       |
|    value_loss           | 4.63       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 339       |
|    time_elapsed         | 455       |
|    total_timesteps      | 6780      |
| train/                  |           |
|    approx_kl            | 1.8595982 |
|    clip_fraction        | 0.718     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.649     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.915     |
|    n_updates            | 6760      |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.37      |
|    value_loss           | 3.25      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 340      |
|    time_elapsed         | 456      |
|    total_timesteps      | 6800     |
| train/                  |          |
|    approx_kl            | 5.15952  |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.684    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.451    |
|    n_updates            | 6780     |
|    policy_gradient_loss | -0.203   |
|    std                  | 0.37     |
|    value_loss           | 2.67     |
--------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.76      |
| reward_torque           | -0.00388  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 341       |
|    time_elapsed         | 457       |
|    total_timesteps      | 6820      |
| train/                  |           |
|    approx_kl            | 0.9590994 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.4      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.37      |
|    n_updates            | 6800      |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.37      |
|    value_loss           | 19.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 342       |
|    time_elapsed         | 459       |
|    total_timesteps      | 6840      |
| train/                  |           |
|    approx_kl            | 0.7435508 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.2       |
|    n_updates            | 6820      |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.37      |
|    value_loss           | 4.19      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 343       |
|    time_elapsed         | 460       |
|    total_timesteps      | 6860      |
| train/                  |           |
|    approx_kl            | 0.9294586 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.445     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.11      |
|    n_updates            | 6840      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.37      |
|    value_loss           | 10.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 344       |
|    time_elapsed         | 461       |
|    total_timesteps      | 6880      |
| train/                  |           |
|    approx_kl            | 3.9666176 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.443     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.18      |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.296    |
|    std                  | 0.37      |
|    value_loss           | 9.11      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 345      |
|    time_elapsed         | 463      |
|    total_timesteps      | 6900     |
| train/                  |          |
|    approx_kl            | 3.620467 |
|    clip_fraction        | 0.758    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.12     |
|    learning_rate        | 0.0003   |
|    loss                 | 2.87     |
|    n_updates            | 6880     |
|    policy_gradient_loss | -0.257   |
|    std                  | 0.37     |
|    value_loss           | 13.9     |
--------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.76      |
| reward_torque           | -0.00388  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 346       |
|    time_elapsed         | 464       |
|    total_timesteps      | 6920      |
| train/                  |           |
|    approx_kl            | 0.6686259 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.704     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.83      |
|    n_updates            | 6900      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 5.87      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 347      |
|    time_elapsed         | 465      |
|    total_timesteps      | 6940     |
| train/                  |          |
|    approx_kl            | 0.960742 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.543    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.7      |
|    n_updates            | 6920     |
|    policy_gradient_loss | -0.276   |
|    std                  | 0.37     |
|    value_loss           | 12.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 348       |
|    time_elapsed         | 467       |
|    total_timesteps      | 6960      |
| train/                  |           |
|    approx_kl            | 2.5449262 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.418     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.298    |
|    std                  | 0.37      |
|    value_loss           | 11.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 349        |
|    time_elapsed         | 468        |
|    total_timesteps      | 6980       |
| train/                  |            |
|    approx_kl            | 0.96867496 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.67       |
|    n_updates            | 6960       |
|    policy_gradient_loss | -0.207     |
|    std                  | 0.37       |
|    value_loss           | 11.9       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 350      |
|    time_elapsed         | 469      |
|    total_timesteps      | 7000     |
| train/                  |          |
|    approx_kl            | 1.333002 |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.695    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.25     |
|    n_updates            | 6980     |
|    policy_gradient_loss | -0.281   |
|    std                  | 0.37     |
|    value_loss           | 8.14     |
--------------------------------------
---------------------------------------
| reward                  | 0.773     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.78      |
| reward_torque           | -0.00388  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 351       |
|    time_elapsed         | 471       |
|    total_timesteps      | 7020      |
| train/                  |           |
|    approx_kl            | 0.5592409 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.17      |
|    n_updates            | 7000      |
|    policy_gradient_loss | -0.192    |
|    std                  | 0.37      |
|    value_loss           | 10.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 352       |
|    time_elapsed         | 472       |
|    total_timesteps      | 7040      |
| train/                  |           |
|    approx_kl            | 1.1286633 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 7020      |
|    policy_gradient_loss | -0.269    |
|    std                  | 0.37      |
|    value_loss           | 6.13      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 353        |
|    time_elapsed         | 473        |
|    total_timesteps      | 7060       |
| train/                  |            |
|    approx_kl            | 0.53381574 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.452      |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.37       |
|    value_loss           | 2.81       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 354       |
|    time_elapsed         | 475       |
|    total_timesteps      | 7080      |
| train/                  |           |
|    approx_kl            | 0.2526831 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.81      |
|    n_updates            | 7060      |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.37      |
|    value_loss           | 14.1      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 355        |
|    time_elapsed         | 476        |
|    total_timesteps      | 7100       |
| train/                  |            |
|    approx_kl            | 0.36128423 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.00138    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.224     |
|    std                  | 0.37       |
|    value_loss           | 12.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.775     |
| reward_contact          | -0.00274  |
| reward_motion           | 0.78      |
| reward_torque           | -0.00238  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 356       |
|    time_elapsed         | 477       |
|    total_timesteps      | 7120      |
| train/                  |           |
|    approx_kl            | 0.2902987 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.712     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.22      |
|    n_updates            | 7100      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.37      |
|    value_loss           | 7.44      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 357       |
|    time_elapsed         | 479       |
|    total_timesteps      | 7140      |
| train/                  |           |
|    approx_kl            | 0.5806804 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.12      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.52      |
|    n_updates            | 7120      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.37      |
|    value_loss           | 16.1      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 358        |
|    time_elapsed         | 480        |
|    total_timesteps      | 7160       |
| train/                  |            |
|    approx_kl            | 0.78352433 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.782      |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.37       |
|    value_loss           | 4.4        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 359       |
|    time_elapsed         | 481       |
|    total_timesteps      | 7180      |
| train/                  |           |
|    approx_kl            | 1.6997232 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.25      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 7160      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.37      |
|    value_loss           | 11.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 360       |
|    time_elapsed         | 483       |
|    total_timesteps      | 7200      |
| train/                  |           |
|    approx_kl            | 0.9533108 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.756     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.559     |
|    n_updates            | 7180      |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.37      |
|    value_loss           | 3.47      |
---------------------------------------
---------------------------------------
| reward                  | 0.798     |
| reward_contact          | 0         |
| reward_motion           | 0.8       |
| reward_torque           | -0.00238  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 361       |
|    time_elapsed         | 484       |
|    total_timesteps      | 7220      |
| train/                  |           |
|    approx_kl            | 1.5965703 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.621     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.38      |
|    n_updates            | 7200      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.37      |
|    value_loss           | 6.82      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 362       |
|    time_elapsed         | 485       |
|    total_timesteps      | 7240      |
| train/                  |           |
|    approx_kl            | 6.0904493 |
|    clip_fraction        | 0.745     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 7220      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 8.11      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 363       |
|    time_elapsed         | 487       |
|    total_timesteps      | 7260      |
| train/                  |           |
|    approx_kl            | 1.7231442 |
|    clip_fraction        | 0.763     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.44      |
|    n_updates            | 7240      |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.37      |
|    value_loss           | 9.51      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 364       |
|    time_elapsed         | 488       |
|    total_timesteps      | 7280      |
| train/                  |           |
|    approx_kl            | 1.3279308 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.354     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 7260      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.37      |
|    value_loss           | 10.9      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 365        |
|    time_elapsed         | 489        |
|    total_timesteps      | 7300       |
| train/                  |            |
|    approx_kl            | 0.32116127 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.12       |
|    n_updates            | 7280       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.37       |
|    value_loss           | 25         |
----------------------------------------
--------------------------------------
| reward                  | 0.808    |
| reward_contact          | 0        |
| reward_motion           | 0.81     |
| reward_torque           | -0.00238 |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 366      |
|    time_elapsed         | 491      |
|    total_timesteps      | 7320     |
| train/                  |          |
|    approx_kl            | 2.057619 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.35     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.481    |
|    n_updates            | 7300     |
|    policy_gradient_loss | -0.274   |
|    std                  | 0.37     |
|    value_loss           | 6.34     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 367       |
|    time_elapsed         | 492       |
|    total_timesteps      | 7340      |
| train/                  |           |
|    approx_kl            | 1.2701571 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.2       |
|    learning_rate        | 0.0003    |
|    loss                 | 4.25      |
|    n_updates            | 7320      |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.37      |
|    value_loss           | 12        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 368       |
|    time_elapsed         | 493       |
|    total_timesteps      | 7360      |
| train/                  |           |
|    approx_kl            | 1.0155538 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.367     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 7340      |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.37      |
|    value_loss           | 9.02      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 369       |
|    time_elapsed         | 495       |
|    total_timesteps      | 7380      |
| train/                  |           |
|    approx_kl            | 0.7782874 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.733     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.57      |
|    n_updates            | 7360      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.37      |
|    value_loss           | 4.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 370       |
|    time_elapsed         | 496       |
|    total_timesteps      | 7400      |
| train/                  |           |
|    approx_kl            | 3.8398697 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.52      |
|    n_updates            | 7380      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.37      |
|    value_loss           | 5.78      |
---------------------------------------
---------------------------------------
| reward                  | 0.818     |
| reward_contact          | 0         |
| reward_motion           | 0.82      |
| reward_torque           | -0.00238  |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 371       |
|    time_elapsed         | 497       |
|    total_timesteps      | 7420      |
| train/                  |           |
|    approx_kl            | 1.1646773 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.84      |
|    n_updates            | 7400      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.37      |
|    value_loss           | 7.93      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 372        |
|    time_elapsed         | 499        |
|    total_timesteps      | 7440       |
| train/                  |            |
|    approx_kl            | 0.15962775 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.83       |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.37       |
|    value_loss           | 16.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 373       |
|    time_elapsed         | 500       |
|    total_timesteps      | 7460      |
| train/                  |           |
|    approx_kl            | 1.2731456 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.28      |
|    n_updates            | 7440      |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.37      |
|    value_loss           | 7.36      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 374       |
|    time_elapsed         | 501       |
|    total_timesteps      | 7480      |
| train/                  |           |
|    approx_kl            | 1.3723955 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.01      |
|    n_updates            | 7460      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 5.36      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 375        |
|    time_elapsed         | 503        |
|    total_timesteps      | 7500       |
| train/                  |            |
|    approx_kl            | 0.29498512 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.02       |
|    n_updates            | 7480       |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.37       |
|    value_loss           | 14.5       |
----------------------------------------
---------------------------------------
| reward                  | 0.81      |
| reward_contact          | 0         |
| reward_motion           | 0.81      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 376       |
|    time_elapsed         | 504       |
|    total_timesteps      | 7520      |
| train/                  |           |
|    approx_kl            | 0.7411055 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.109     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.53      |
|    n_updates            | 7500      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 15.9      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 377      |
|    time_elapsed         | 505      |
|    total_timesteps      | 7540     |
| train/                  |          |
|    approx_kl            | 4.486467 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.553    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.86     |
|    n_updates            | 7520     |
|    policy_gradient_loss | -0.257   |
|    std                  | 0.37     |
|    value_loss           | 8.54     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 378       |
|    time_elapsed         | 507       |
|    total_timesteps      | 7560      |
| train/                  |           |
|    approx_kl            | 1.7187309 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.0152   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.98      |
|    n_updates            | 7540      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.37      |
|    value_loss           | 18        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 379       |
|    time_elapsed         | 508       |
|    total_timesteps      | 7580      |
| train/                  |           |
|    approx_kl            | 1.5899771 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.79      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.42      |
|    n_updates            | 7560      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.37      |
|    value_loss           | 4.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 380       |
|    time_elapsed         | 509       |
|    total_timesteps      | 7600      |
| train/                  |           |
|    approx_kl            | 2.1683044 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.23      |
|    n_updates            | 7580      |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.37      |
|    value_loss           | 11.1      |
---------------------------------------
--------------------------------------
| reward                  | 0.79     |
| reward_contact          | 0        |
| reward_motion           | 0.79     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 381      |
|    time_elapsed         | 511      |
|    total_timesteps      | 7620     |
| train/                  |          |
|    approx_kl            | 0.48454  |
|    clip_fraction        | 0.51     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.56     |
|    learning_rate        | 0.0003   |
|    loss                 | 4.75     |
|    n_updates            | 7600     |
|    policy_gradient_loss | -0.229   |
|    std                  | 0.37     |
|    value_loss           | 15.1     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 382        |
|    time_elapsed         | 512        |
|    total_timesteps      | 7640       |
| train/                  |            |
|    approx_kl            | 0.90925497 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.56       |
|    n_updates            | 7620       |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.37       |
|    value_loss           | 8.81       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 383       |
|    time_elapsed         | 513       |
|    total_timesteps      | 7660      |
| train/                  |           |
|    approx_kl            | 1.0228364 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.625     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.873     |
|    n_updates            | 7640      |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.37      |
|    value_loss           | 5.32      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 384       |
|    time_elapsed         | 514       |
|    total_timesteps      | 7680      |
| train/                  |           |
|    approx_kl            | 1.4985873 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.736     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.09      |
|    n_updates            | 7660      |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.371     |
|    value_loss           | 5.2       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 385       |
|    time_elapsed         | 516       |
|    total_timesteps      | 7700      |
| train/                  |           |
|    approx_kl            | 0.5604396 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.06      |
|    n_updates            | 7680      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.371     |
|    value_loss           | 7.59      |
---------------------------------------
---------------------------------------
| reward                  | 0.8       |
| reward_contact          | 0         |
| reward_motion           | 0.8       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 386       |
|    time_elapsed         | 517       |
|    total_timesteps      | 7720      |
| train/                  |           |
|    approx_kl            | 0.6122699 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.542     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 7700      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.371     |
|    value_loss           | 6.93      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 387       |
|    time_elapsed         | 518       |
|    total_timesteps      | 7740      |
| train/                  |           |
|    approx_kl            | 1.3155321 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.587     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.6       |
|    n_updates            | 7720      |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.371     |
|    value_loss           | 6.62      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 388      |
|    time_elapsed         | 520      |
|    total_timesteps      | 7760     |
| train/                  |          |
|    approx_kl            | 1.807093 |
|    clip_fraction        | 0.75     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.516    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.07     |
|    n_updates            | 7740     |
|    policy_gradient_loss | -0.245   |
|    std                  | 0.371    |
|    value_loss           | 9.7      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 389       |
|    time_elapsed         | 521       |
|    total_timesteps      | 7780      |
| train/                  |           |
|    approx_kl            | 0.3532567 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.847     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.63      |
|    n_updates            | 7760      |
|    policy_gradient_loss | -0.151    |
|    std                  | 0.371     |
|    value_loss           | 9.45      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 390       |
|    time_elapsed         | 522       |
|    total_timesteps      | 7800      |
| train/                  |           |
|    approx_kl            | 0.8899229 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.144    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.3       |
|    n_updates            | 7780      |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.371     |
|    value_loss           | 18.4      |
---------------------------------------
---------------------------------------
| reward                  | 0.79      |
| reward_contact          | 0         |
| reward_motion           | 0.79      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 391       |
|    time_elapsed         | 524       |
|    total_timesteps      | 7820      |
| train/                  |           |
|    approx_kl            | 1.7328672 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.0451    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.6       |
|    n_updates            | 7800      |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.371     |
|    value_loss           | 18.8      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 392      |
|    time_elapsed         | 525      |
|    total_timesteps      | 7840     |
| train/                  |          |
|    approx_kl            | 1.447362 |
|    clip_fraction        | 0.74     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.738    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.02     |
|    n_updates            | 7820     |
|    policy_gradient_loss | -0.252   |
|    std                  | 0.371    |
|    value_loss           | 5.59     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 393        |
|    time_elapsed         | 526        |
|    total_timesteps      | 7860       |
| train/                  |            |
|    approx_kl            | 0.66001177 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.24       |
|    n_updates            | 7840       |
|    policy_gradient_loss | -0.255     |
|    std                  | 0.371      |
|    value_loss           | 11.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 394       |
|    time_elapsed         | 528       |
|    total_timesteps      | 7880      |
| train/                  |           |
|    approx_kl            | 1.1776164 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.386     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.43      |
|    n_updates            | 7860      |
|    policy_gradient_loss | -0.213    |
|    std                  | 0.371     |
|    value_loss           | 13.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 395       |
|    time_elapsed         | 529       |
|    total_timesteps      | 7900      |
| train/                  |           |
|    approx_kl            | 2.3754237 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.465     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.62      |
|    n_updates            | 7880      |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.371     |
|    value_loss           | 10.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.79      |
| reward_contact          | 0         |
| reward_motion           | 0.79      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 396       |
|    time_elapsed         | 530       |
|    total_timesteps      | 7920      |
| train/                  |           |
|    approx_kl            | 3.2228715 |
|    clip_fraction        | 0.615     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0754    |
|    n_updates            | 7900      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.371     |
|    value_loss           | 0.728     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 397       |
|    time_elapsed         | 532       |
|    total_timesteps      | 7940      |
| train/                  |           |
|    approx_kl            | 1.2365621 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.196     |
|    n_updates            | 7920      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.371     |
|    value_loss           | 1.51      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 398       |
|    time_elapsed         | 533       |
|    total_timesteps      | 7960      |
| train/                  |           |
|    approx_kl            | 1.3137529 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.319     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.98      |
|    n_updates            | 7940      |
|    policy_gradient_loss | -0.303    |
|    std                  | 0.371     |
|    value_loss           | 18.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 399        |
|    time_elapsed         | 534        |
|    total_timesteps      | 7980       |
| train/                  |            |
|    approx_kl            | 0.99434537 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | -0.0381    |
|    learning_rate        | 0.0003     |
|    loss                 | 5.11       |
|    n_updates            | 7960       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.371      |
|    value_loss           | 17.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 400       |
|    time_elapsed         | 536       |
|    total_timesteps      | 8000      |
| train/                  |           |
|    approx_kl            | 1.6446403 |
|    clip_fraction        | 0.768     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.758     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.989     |
|    n_updates            | 7980      |
|    policy_gradient_loss | -0.198    |
|    std                  | 0.371     |
|    value_loss           | 4         |
---------------------------------------
---------------------------------------
| reward                  | 0.79      |
| reward_contact          | 0         |
| reward_motion           | 0.79      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 401       |
|    time_elapsed         | 537       |
|    total_timesteps      | 8020      |
| train/                  |           |
|    approx_kl            | 1.9667292 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.219     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.24      |
|    n_updates            | 8000      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.371     |
|    value_loss           | 14.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 402        |
|    time_elapsed         | 538        |
|    total_timesteps      | 8040       |
| train/                  |            |
|    approx_kl            | 0.32521915 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | -0.429     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.1       |
|    n_updates            | 8020       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.371      |
|    value_loss           | 30.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 403       |
|    time_elapsed         | 540       |
|    total_timesteps      | 8060      |
| train/                  |           |
|    approx_kl            | 2.1658413 |
|    clip_fraction        | 0.705     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.542     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.02      |
|    n_updates            | 8040      |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.371     |
|    value_loss           | 5.25      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 404      |
|    time_elapsed         | 541      |
|    total_timesteps      | 8080     |
| train/                  |          |
|    approx_kl            | 2.600026 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.77     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.24     |
|    n_updates            | 8060     |
|    policy_gradient_loss | -0.238   |
|    std                  | 0.371    |
|    value_loss           | 4.77     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 405       |
|    time_elapsed         | 542       |
|    total_timesteps      | 8100      |
| train/                  |           |
|    approx_kl            | 2.6290958 |
|    clip_fraction        | 0.703     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.212     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.42      |
|    n_updates            | 8080      |
|    policy_gradient_loss | -0.213    |
|    std                  | 0.371     |
|    value_loss           | 14.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.8       |
| reward_contact          | 0         |
| reward_motion           | 0.8       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 406       |
|    time_elapsed         | 544       |
|    total_timesteps      | 8120      |
| train/                  |           |
|    approx_kl            | 1.3631881 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -0.295    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.82      |
|    n_updates            | 8100      |
|    policy_gradient_loss | -0.231    |
|    std                  | 0.371     |
|    value_loss           | 15.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 407       |
|    time_elapsed         | 545       |
|    total_timesteps      | 8140      |
| train/                  |           |
|    approx_kl            | 1.2602915 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.437    |
|    learning_rate        | 0.0003    |
|    loss                 | 10.2      |
|    n_updates            | 8120      |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.371     |
|    value_loss           | 27.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 408       |
|    time_elapsed         | 546       |
|    total_timesteps      | 8160      |
| train/                  |           |
|    approx_kl            | 0.9813065 |
|    clip_fraction        | 0.675     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.396     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.74      |
|    n_updates            | 8140      |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.371     |
|    value_loss           | 14.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 409       |
|    time_elapsed         | 548       |
|    total_timesteps      | 8180      |
| train/                  |           |
|    approx_kl            | 1.0973082 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.42      |
|    n_updates            | 8160      |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.371     |
|    value_loss           | 11.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 410       |
|    time_elapsed         | 549       |
|    total_timesteps      | 8200      |
| train/                  |           |
|    approx_kl            | 1.1014456 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.98      |
|    n_updates            | 8180      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.371     |
|    value_loss           | 6.55      |
---------------------------------------
---------------------------------------
| reward                  | 0.83      |
| reward_contact          | 0         |
| reward_motion           | 0.83      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 411       |
|    time_elapsed         | 550       |
|    total_timesteps      | 8220      |
| train/                  |           |
|    approx_kl            | 2.5532482 |
|    clip_fraction        | 0.663     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.21      |
|    n_updates            | 8200      |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.371     |
|    value_loss           | 9.07      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 412      |
|    time_elapsed         | 552      |
|    total_timesteps      | 8240     |
| train/                  |          |
|    approx_kl            | 0.714482 |
|    clip_fraction        | 0.61     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.313    |
|    learning_rate        | 0.0003   |
|    loss                 | 10.5     |
|    n_updates            | 8220     |
|    policy_gradient_loss | -0.241   |
|    std                  | 0.371    |
|    value_loss           | 25.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 413       |
|    time_elapsed         | 553       |
|    total_timesteps      | 8260      |
| train/                  |           |
|    approx_kl            | 1.0805902 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.33      |
|    n_updates            | 8240      |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.371     |
|    value_loss           | 9.28      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 414      |
|    time_elapsed         | 554      |
|    total_timesteps      | 8280     |
| train/                  |          |
|    approx_kl            | 4.100561 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.635    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.35     |
|    n_updates            | 8260     |
|    policy_gradient_loss | -0.264   |
|    std                  | 0.371    |
|    value_loss           | 7.96     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 415       |
|    time_elapsed         | 556       |
|    total_timesteps      | 8300      |
| train/                  |           |
|    approx_kl            | 2.0413573 |
|    clip_fraction        | 0.815     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.97      |
|    n_updates            | 8280      |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.371     |
|    value_loss           | 9.13      |
---------------------------------------
---------------------------------------
| reward                  | 0.84      |
| reward_contact          | 0         |
| reward_motion           | 0.84      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 416       |
|    time_elapsed         | 557       |
|    total_timesteps      | 8320      |
| train/                  |           |
|    approx_kl            | 1.9773562 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.652     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.88      |
|    n_updates            | 8300      |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.371     |
|    value_loss           | 6.46      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 417        |
|    time_elapsed         | 558        |
|    total_timesteps      | 8340       |
| train/                  |            |
|    approx_kl            | 0.31879658 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -0.194     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.14       |
|    n_updates            | 8320       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.371      |
|    value_loss           | 15.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 418       |
|    time_elapsed         | 560       |
|    total_timesteps      | 8360      |
| train/                  |           |
|    approx_kl            | 1.0086888 |
|    clip_fraction        | 0.513     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.832     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.46      |
|    n_updates            | 8340      |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.371     |
|    value_loss           | 4.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 419       |
|    time_elapsed         | 561       |
|    total_timesteps      | 8380      |
| train/                  |           |
|    approx_kl            | 1.5625786 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.15      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.03      |
|    n_updates            | 8360      |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.371     |
|    value_loss           | 15        |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 420      |
|    time_elapsed         | 562      |
|    total_timesteps      | 8400     |
| train/                  |          |
|    approx_kl            | 3.368166 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.676    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.656    |
|    n_updates            | 8380     |
|    policy_gradient_loss | -0.302   |
|    std                  | 0.371    |
|    value_loss           | 3.7      |
--------------------------------------
---------------------------------------
| reward                  | 0.85      |
| reward_contact          | 0         |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 421       |
|    time_elapsed         | 564       |
|    total_timesteps      | 8420      |
| train/                  |           |
|    approx_kl            | 1.2351627 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.586     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.74      |
|    n_updates            | 8400      |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.371     |
|    value_loss           | 8.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 422       |
|    time_elapsed         | 565       |
|    total_timesteps      | 8440      |
| train/                  |           |
|    approx_kl            | 0.4336384 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.669     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.81      |
|    n_updates            | 8420      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.371     |
|    value_loss           | 7.55      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 423        |
|    time_elapsed         | 566        |
|    total_timesteps      | 8460       |
| train/                  |            |
|    approx_kl            | 0.82782745 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.92       |
|    n_updates            | 8440       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.371      |
|    value_loss           | 12.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 424       |
|    time_elapsed         | 568       |
|    total_timesteps      | 8480      |
| train/                  |           |
|    approx_kl            | 1.3172487 |
|    clip_fraction        | 0.635     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.618     |
|    n_updates            | 8460      |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.371     |
|    value_loss           | 3.01      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 425       |
|    time_elapsed         | 569       |
|    total_timesteps      | 8500      |
| train/                  |           |
|    approx_kl            | 1.6673669 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.463     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.7       |
|    n_updates            | 8480      |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.371     |
|    value_loss           | 6.5       |
---------------------------------------
---------------------------------------
| reward                  | 0.85      |
| reward_contact          | 0         |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 426       |
|    time_elapsed         | 570       |
|    total_timesteps      | 8520      |
| train/                  |           |
|    approx_kl            | 2.5562742 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.271     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.99      |
|    n_updates            | 8500      |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.371     |
|    value_loss           | 14.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 427       |
|    time_elapsed         | 572       |
|    total_timesteps      | 8540      |
| train/                  |           |
|    approx_kl            | 2.2163951 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.91     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.6      |
|    n_updates            | 8520      |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.371     |
|    value_loss           | 29.5      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 428        |
|    time_elapsed         | 573        |
|    total_timesteps      | 8560       |
| train/                  |            |
|    approx_kl            | 0.35776645 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.119      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.47       |
|    n_updates            | 8540       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.371      |
|    value_loss           | 19         |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 429      |
|    time_elapsed         | 575      |
|    total_timesteps      | 8580     |
| train/                  |          |
|    approx_kl            | 1.235665 |
|    clip_fraction        | 0.61     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.759    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.35     |
|    n_updates            | 8560     |
|    policy_gradient_loss | -0.233   |
|    std                  | 0.371    |
|    value_loss           | 4.05     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 430       |
|    time_elapsed         | 576       |
|    total_timesteps      | 8600      |
| train/                  |           |
|    approx_kl            | 0.9083628 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 8580      |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.371     |
|    value_loss           | 7.48      |
---------------------------------------
----------------------------------------
| reward                  | 0.86       |
| reward_contact          | 0          |
| reward_motion           | 0.86       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 431        |
|    time_elapsed         | 577        |
|    total_timesteps      | 8620       |
| train/                  |            |
|    approx_kl            | 0.90821284 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.35       |
|    n_updates            | 8600       |
|    policy_gradient_loss | -0.262     |
|    std                  | 0.371      |
|    value_loss           | 12.7       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 432        |
|    time_elapsed         | 579        |
|    total_timesteps      | 8640       |
| train/                  |            |
|    approx_kl            | 0.34611338 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.21       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.84       |
|    n_updates            | 8620       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.371      |
|    value_loss           | 22.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 433       |
|    time_elapsed         | 580       |
|    total_timesteps      | 8660      |
| train/                  |           |
|    approx_kl            | 1.0691055 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.00888  |
|    learning_rate        | 0.0003    |
|    loss                 | 7.35      |
|    n_updates            | 8640      |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.371     |
|    value_loss           | 18.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 434      |
|    time_elapsed         | 581      |
|    total_timesteps      | 8680     |
| train/                  |          |
|    approx_kl            | 2.535721 |
|    clip_fraction        | 0.748    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.656    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.875    |
|    n_updates            | 8660     |
|    policy_gradient_loss | -0.257   |
|    std                  | 0.371    |
|    value_loss           | 3.52     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 435       |
|    time_elapsed         | 583       |
|    total_timesteps      | 8700      |
| train/                  |           |
|    approx_kl            | 2.4314473 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.598    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.57      |
|    n_updates            | 8680      |
|    policy_gradient_loss | -0.298    |
|    std                  | 0.371     |
|    value_loss           | 10.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.87      |
| reward_contact          | 0         |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 436       |
|    time_elapsed         | 584       |
|    total_timesteps      | 8720      |
| train/                  |           |
|    approx_kl            | 1.7646767 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.735     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.61      |
|    n_updates            | 8700      |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.371     |
|    value_loss           | 4.94      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 437        |
|    time_elapsed         | 585        |
|    total_timesteps      | 8740       |
| train/                  |            |
|    approx_kl            | 0.72336113 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.7        |
|    n_updates            | 8720       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.371      |
|    value_loss           | 7.9        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 438        |
|    time_elapsed         | 587        |
|    total_timesteps      | 8760       |
| train/                  |            |
|    approx_kl            | 0.60871077 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.81       |
|    n_updates            | 8740       |
|    policy_gradient_loss | -0.24      |
|    std                  | 0.371      |
|    value_loss           | 9.41       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 439       |
|    time_elapsed         | 588       |
|    total_timesteps      | 8780      |
| train/                  |           |
|    approx_kl            | 1.1750704 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.8      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.63      |
|    n_updates            | 8760      |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.371     |
|    value_loss           | 19.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 440       |
|    time_elapsed         | 589       |
|    total_timesteps      | 8800      |
| train/                  |           |
|    approx_kl            | 1.7021548 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.947    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.1       |
|    n_updates            | 8780      |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.371     |
|    value_loss           | 11.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.87      |
| reward_contact          | 0         |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 441       |
|    time_elapsed         | 591       |
|    total_timesteps      | 8820      |
| train/                  |           |
|    approx_kl            | 0.9378854 |
|    clip_fraction        | 0.518     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.14      |
|    n_updates            | 8800      |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.371     |
|    value_loss           | 8.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 442       |
|    time_elapsed         | 592       |
|    total_timesteps      | 8840      |
| train/                  |           |
|    approx_kl            | 2.7058415 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.205     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.48      |
|    n_updates            | 8820      |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.371     |
|    value_loss           | 11.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 443        |
|    time_elapsed         | 593        |
|    total_timesteps      | 8860       |
| train/                  |            |
|    approx_kl            | 0.38039455 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.98       |
|    n_updates            | 8840       |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.371      |
|    value_loss           | 17         |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 444        |
|    time_elapsed         | 595        |
|    total_timesteps      | 8880       |
| train/                  |            |
|    approx_kl            | 0.47862735 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.235     |
|    std                  | 0.371      |
|    value_loss           | 5.66       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 445       |
|    time_elapsed         | 596       |
|    total_timesteps      | 8900      |
| train/                  |           |
|    approx_kl            | 1.0618569 |
|    clip_fraction        | 0.695     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.256     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 8880      |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.371     |
|    value_loss           | 8.44      |
---------------------------------------
---------------------------------------
| reward                  | 0.87      |
| reward_contact          | 0         |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 446       |
|    time_elapsed         | 597       |
|    total_timesteps      | 8920      |
| train/                  |           |
|    approx_kl            | 0.1756296 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.02     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.54      |
|    n_updates            | 8900      |
|    policy_gradient_loss | -0.14     |
|    std                  | 0.371     |
|    value_loss           | 23.5      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 447        |
|    time_elapsed         | 599        |
|    total_timesteps      | 8940       |
| train/                  |            |
|    approx_kl            | 0.60022956 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 8920       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.371      |
|    value_loss           | 4.48       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 448        |
|    time_elapsed         | 600        |
|    total_timesteps      | 8960       |
| train/                  |            |
|    approx_kl            | 0.65230304 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.422      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.45       |
|    n_updates            | 8940       |
|    policy_gradient_loss | -0.224     |
|    std                  | 0.371      |
|    value_loss           | 9.51       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 449        |
|    time_elapsed         | 601        |
|    total_timesteps      | 8980       |
| train/                  |            |
|    approx_kl            | 0.25538525 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.17       |
|    n_updates            | 8960       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.371      |
|    value_loss           | 15.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 450       |
|    time_elapsed         | 603       |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 1.1397421 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 8980      |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.371     |
|    value_loss           | 3.64      |
---------------------------------------
----------------------------------------
| reward                  | 0.85       |
| reward_contact          | 0          |
| reward_motion           | 0.85       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 451        |
|    time_elapsed         | 604        |
|    total_timesteps      | 9020       |
| train/                  |            |
|    approx_kl            | 0.99954027 |
|    clip_fraction        | 0.695      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.21       |
|    n_updates            | 9000       |
|    policy_gradient_loss | -0.255     |
|    std                  | 0.371      |
|    value_loss           | 4.2        |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 452      |
|    time_elapsed         | 605      |
|    total_timesteps      | 9040     |
| train/                  |          |
|    approx_kl            | 0.954103 |
|    clip_fraction        | 0.66     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.752    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.77     |
|    n_updates            | 9020     |
|    policy_gradient_loss | -0.223   |
|    std                  | 0.371    |
|    value_loss           | 5.95     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 453       |
|    time_elapsed         | 607       |
|    total_timesteps      | 9060      |
| train/                  |           |
|    approx_kl            | 0.7645916 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.506     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.63      |
|    n_updates            | 9040      |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.371     |
|    value_loss           | 10.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 454       |
|    time_elapsed         | 608       |
|    total_timesteps      | 9080      |
| train/                  |           |
|    approx_kl            | 0.9821482 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.512     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 9060      |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.371     |
|    value_loss           | 9.64      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 455      |
|    time_elapsed         | 609      |
|    total_timesteps      | 9100     |
| train/                  |          |
|    approx_kl            | 2.530671 |
|    clip_fraction        | 0.72     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.659    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.2      |
|    n_updates            | 9080     |
|    policy_gradient_loss | -0.245   |
|    std                  | 0.371    |
|    value_loss           | 6.79     |
--------------------------------------
----------------------------------------
| reward                  | 0.83       |
| reward_contact          | -0.0397    |
| reward_motion           | 0.87       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 456        |
|    time_elapsed         | 611        |
|    total_timesteps      | 9120       |
| train/                  |            |
|    approx_kl            | 0.46652323 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.0884     |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 9100       |
|    policy_gradient_loss | -0.235     |
|    std                  | 0.371      |
|    value_loss           | 32.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 457       |
|    time_elapsed         | 612       |
|    total_timesteps      | 9140      |
| train/                  |           |
|    approx_kl            | 3.1031272 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.85      |
|    n_updates            | 9120      |
|    policy_gradient_loss | -0.301    |
|    std                  | 0.371     |
|    value_loss           | 14.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 458       |
|    time_elapsed         | 613       |
|    total_timesteps      | 9160      |
| train/                  |           |
|    approx_kl            | 1.3774514 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.45      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.52      |
|    n_updates            | 9140      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.371     |
|    value_loss           | 7.44      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 459        |
|    time_elapsed         | 615        |
|    total_timesteps      | 9180       |
| train/                  |            |
|    approx_kl            | 0.18271552 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.474     |
|    learning_rate        | 0.0003     |
|    loss                 | 10.9       |
|    n_updates            | 9160       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.371      |
|    value_loss           | 30.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 460       |
|    time_elapsed         | 616       |
|    total_timesteps      | 9200      |
| train/                  |           |
|    approx_kl            | 1.1276278 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.261     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.954     |
|    n_updates            | 9180      |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.371     |
|    value_loss           | 4.32      |
---------------------------------------
---------------------------------------
| reward                  | 0.802     |
| reward_contact          | -0.0684   |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 461       |
|    time_elapsed         | 617       |
|    total_timesteps      | 9220      |
| train/                  |           |
|    approx_kl            | 1.6022427 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.25      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.43      |
|    n_updates            | 9200      |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.371     |
|    value_loss           | 12.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 462       |
|    time_elapsed         | 619       |
|    total_timesteps      | 9240      |
| train/                  |           |
|    approx_kl            | 2.5093544 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.09      |
|    n_updates            | 9220      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.371     |
|    value_loss           | 5.68      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 463        |
|    time_elapsed         | 620        |
|    total_timesteps      | 9260       |
| train/                  |            |
|    approx_kl            | 0.31802273 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.43       |
|    n_updates            | 9240       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.37       |
|    value_loss           | 16.1       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 464      |
|    time_elapsed         | 621      |
|    total_timesteps      | 9280     |
| train/                  |          |
|    approx_kl            | 1.989334 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.503    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.43     |
|    n_updates            | 9260     |
|    policy_gradient_loss | -0.227   |
|    std                  | 0.371    |
|    value_loss           | 4.6      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 465        |
|    time_elapsed         | 623        |
|    total_timesteps      | 9300       |
| train/                  |            |
|    approx_kl            | 0.48157263 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.207     |
|    learning_rate        | 0.0003     |
|    loss                 | 10.7       |
|    n_updates            | 9280       |
|    policy_gradient_loss | -0.226     |
|    std                  | 0.37       |
|    value_loss           | 27.2       |
----------------------------------------
--------------------------------------
| reward                  | 0.786    |
| reward_contact          | -0.084   |
| reward_motion           | 0.87     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 466      |
|    time_elapsed         | 624      |
|    total_timesteps      | 9320     |
| train/                  |          |
|    approx_kl            | 0.617496 |
|    clip_fraction        | 0.61     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | -1.3     |
|    learning_rate        | 0.0003   |
|    loss                 | 7.13     |
|    n_updates            | 9300     |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.371    |
|    value_loss           | 18.9     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 467       |
|    time_elapsed         | 625       |
|    total_timesteps      | 9340      |
| train/                  |           |
|    approx_kl            | 0.4002367 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.23      |
|    n_updates            | 9320      |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.37      |
|    value_loss           | 10.8      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 468        |
|    time_elapsed         | 627        |
|    total_timesteps      | 9360       |
| train/                  |            |
|    approx_kl            | 0.46617466 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.752      |
|    n_updates            | 9340       |
|    policy_gradient_loss | -0.249     |
|    std                  | 0.37       |
|    value_loss           | 5.09       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 469       |
|    time_elapsed         | 628       |
|    total_timesteps      | 9380      |
| train/                  |           |
|    approx_kl            | 0.6660431 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.54      |
|    n_updates            | 9360      |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.37      |
|    value_loss           | 5.54      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 470       |
|    time_elapsed         | 629       |
|    total_timesteps      | 9400      |
| train/                  |           |
|    approx_kl            | 2.4381163 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.157     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.01      |
|    n_updates            | 9380      |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.37      |
|    value_loss           | 17.8      |
---------------------------------------
---------------------------------------
| reward                  | 0.796     |
| reward_contact          | -0.084    |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 471       |
|    time_elapsed         | 631       |
|    total_timesteps      | 9420      |
| train/                  |           |
|    approx_kl            | 1.8493847 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.28      |
|    n_updates            | 9400      |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.37      |
|    value_loss           | 6.41      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 472       |
|    time_elapsed         | 632       |
|    total_timesteps      | 9440      |
| train/                  |           |
|    approx_kl            | 0.5590439 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.228     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.45      |
|    n_updates            | 9420      |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.37      |
|    value_loss           | 13.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 473       |
|    time_elapsed         | 633       |
|    total_timesteps      | 9460      |
| train/                  |           |
|    approx_kl            | 2.1504686 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.74      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.13      |
|    n_updates            | 9440      |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.37      |
|    value_loss           | 5.69      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 474        |
|    time_elapsed         | 635        |
|    total_timesteps      | 9480       |
| train/                  |            |
|    approx_kl            | 0.97490996 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 9460       |
|    policy_gradient_loss | -0.229     |
|    std                  | 0.37       |
|    value_loss           | 9.03       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 475       |
|    time_elapsed         | 636       |
|    total_timesteps      | 9500      |
| train/                  |           |
|    approx_kl            | 1.6935867 |
|    clip_fraction        | 0.695     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.44      |
|    n_updates            | 9480      |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.37      |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.816     |
| reward_contact          | -0.084    |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 476       |
|    time_elapsed         | 637       |
|    total_timesteps      | 9520      |
| train/                  |           |
|    approx_kl            | 1.8375777 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.28      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.99      |
|    n_updates            | 9500      |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.37      |
|    value_loss           | 20.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 477        |
|    time_elapsed         | 639        |
|    total_timesteps      | 9540       |
| train/                  |            |
|    approx_kl            | 0.44093314 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.6       |
|    n_updates            | 9520       |
|    policy_gradient_loss | -0.252     |
|    std                  | 0.37       |
|    value_loss           | 40.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 478       |
|    time_elapsed         | 640       |
|    total_timesteps      | 9560      |
| train/                  |           |
|    approx_kl            | 0.5281214 |
|    clip_fraction        | 0.555     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.469     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.73      |
|    n_updates            | 9540      |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.37      |
|    value_loss           | 9.8       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 479       |
|    time_elapsed         | 641       |
|    total_timesteps      | 9580      |
| train/                  |           |
|    approx_kl            | 0.7132823 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.62      |
|    n_updates            | 9560      |
|    policy_gradient_loss | -0.291    |
|    std                  | 0.37      |
|    value_loss           | 15.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 480       |
|    time_elapsed         | 643       |
|    total_timesteps      | 9600      |
| train/                  |           |
|    approx_kl            | 0.6929316 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.47      |
|    n_updates            | 9580      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.37      |
|    value_loss           | 5.86      |
---------------------------------------
----------------------------------------
| reward                  | 0.753      |
| reward_contact          | -0.157     |
| reward_motion           | 0.91       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 481        |
|    time_elapsed         | 644        |
|    total_timesteps      | 9620       |
| train/                  |            |
|    approx_kl            | 0.24323606 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.429      |
|    learning_rate        | 0.0003     |
|    loss                 | 24.6       |
|    n_updates            | 9600       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.37       |
|    value_loss           | 67.5       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 482        |
|    time_elapsed         | 645        |
|    total_timesteps      | 9640       |
| train/                  |            |
|    approx_kl            | 0.27727944 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.85       |
|    n_updates            | 9620       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.37       |
|    value_loss           | 17.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 483       |
|    time_elapsed         | 647       |
|    total_timesteps      | 9660      |
| train/                  |           |
|    approx_kl            | 1.6207231 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.372    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.55      |
|    n_updates            | 9640      |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 5.64      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 484       |
|    time_elapsed         | 648       |
|    total_timesteps      | 9680      |
| train/                  |           |
|    approx_kl            | 0.5955674 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.45     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.66      |
|    n_updates            | 9660      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 24.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 485      |
|    time_elapsed         | 649      |
|    total_timesteps      | 9700     |
| train/                  |          |
|    approx_kl            | 0.544334 |
|    clip_fraction        | 0.5      |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | -0.899   |
|    learning_rate        | 0.0003   |
|    loss                 | 6.35     |
|    n_updates            | 9680     |
|    policy_gradient_loss | -0.222   |
|    std                  | 0.37     |
|    value_loss           | 18.9     |
--------------------------------------
---------------------------------------
| reward                  | 0.753     |
| reward_contact          | -0.157    |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 486       |
|    time_elapsed         | 651       |
|    total_timesteps      | 9720      |
| train/                  |           |
|    approx_kl            | 0.8980163 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.331     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.3       |
|    n_updates            | 9700      |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.37      |
|    value_loss           | 9.9       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 487       |
|    time_elapsed         | 652       |
|    total_timesteps      | 9740      |
| train/                  |           |
|    approx_kl            | 1.7320112 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.77      |
|    n_updates            | 9720      |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.37      |
|    value_loss           | 7.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 488       |
|    time_elapsed         | 653       |
|    total_timesteps      | 9760      |
| train/                  |           |
|    approx_kl            | 2.2601812 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.758     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.822     |
|    n_updates            | 9740      |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.37      |
|    value_loss           | 3.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 489       |
|    time_elapsed         | 655       |
|    total_timesteps      | 9780      |
| train/                  |           |
|    approx_kl            | 1.0876926 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.268     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.17      |
|    n_updates            | 9760      |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.37      |
|    value_loss           | 18.8      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 490      |
|    time_elapsed         | 656      |
|    total_timesteps      | 9800     |
| train/                  |          |
|    approx_kl            | 4.008241 |
|    clip_fraction        | 0.828    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.443    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.1      |
|    n_updates            | 9780     |
|    policy_gradient_loss | -0.291   |
|    std                  | 0.37     |
|    value_loss           | 12       |
--------------------------------------
--------------------------------------
| reward                  | 0.773    |
| reward_contact          | -0.157   |
| reward_motion           | 0.93     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 491      |
|    time_elapsed         | 658      |
|    total_timesteps      | 9820     |
| train/                  |          |
|    approx_kl            | 3.100872 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.0718   |
|    learning_rate        | 0.0003   |
|    loss                 | 5.69     |
|    n_updates            | 9800     |
|    policy_gradient_loss | -0.237   |
|    std                  | 0.37     |
|    value_loss           | 14       |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 492       |
|    time_elapsed         | 659       |
|    total_timesteps      | 9840      |
| train/                  |           |
|    approx_kl            | 1.4689773 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.71      |
|    n_updates            | 9820      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.37      |
|    value_loss           | 3.14      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 493       |
|    time_elapsed         | 660       |
|    total_timesteps      | 9860      |
| train/                  |           |
|    approx_kl            | 1.2519296 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 9840      |
|    policy_gradient_loss | -0.176    |
|    std                  | 0.37      |
|    value_loss           | 6.3       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 494        |
|    time_elapsed         | 662        |
|    total_timesteps      | 9880       |
| train/                  |            |
|    approx_kl            | 0.78375465 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -0.0348    |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.246     |
|    std                  | 0.37       |
|    value_loss           | 36.2       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 495      |
|    time_elapsed         | 663      |
|    total_timesteps      | 9900     |
| train/                  |          |
|    approx_kl            | 2.707623 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.279    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.61     |
|    n_updates            | 9880     |
|    policy_gradient_loss | -0.264   |
|    std                  | 0.37     |
|    value_loss           | 13.8     |
--------------------------------------
----------------------------------------
| reward                  | 0.763      |
| reward_contact          | -0.157     |
| reward_motion           | 0.92       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 496        |
|    time_elapsed         | 664        |
|    total_timesteps      | 9920       |
| train/                  |            |
|    approx_kl            | 0.30025092 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.37       |
|    value_loss           | 19.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 497       |
|    time_elapsed         | 666       |
|    total_timesteps      | 9940      |
| train/                  |           |
|    approx_kl            | 0.6875914 |
|    clip_fraction        | 0.613     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 9920      |
|    policy_gradient_loss | -0.165    |
|    std                  | 0.37      |
|    value_loss           | 4.11      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 498       |
|    time_elapsed         | 667       |
|    total_timesteps      | 9960      |
| train/                  |           |
|    approx_kl            | 2.1920729 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.0603    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.36      |
|    n_updates            | 9940      |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.37      |
|    value_loss           | 20.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 499        |
|    time_elapsed         | 668        |
|    total_timesteps      | 9980       |
| train/                  |            |
|    approx_kl            | 0.55864686 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.396      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.5        |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.248     |
|    std                  | 0.37       |
|    value_loss           | 22.8       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 500      |
|    time_elapsed         | 670      |
|    total_timesteps      | 10000    |
| train/                  |          |
|    approx_kl            | 3.938397 |
|    clip_fraction        | 0.828    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.626    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.17     |
|    n_updates            | 9980     |
|    policy_gradient_loss | -0.296   |
|    std                  | 0.37     |
|    value_loss           | 5.45     |
--------------------------------------
--------------------------------------
| reward                  | 0.754    |
| reward_contact          | -0.166   |
| reward_motion           | 0.92     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 501      |
|    time_elapsed         | 671      |
|    total_timesteps      | 10020    |
| train/                  |          |
|    approx_kl            | 1.277475 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.683    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.34     |
|    n_updates            | 10000    |
|    policy_gradient_loss | -0.263   |
|    std                  | 0.37     |
|    value_loss           | 5.48     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 502        |
|    time_elapsed         | 672        |
|    total_timesteps      | 10040      |
| train/                  |            |
|    approx_kl            | 0.38039833 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.613     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.37       |
|    value_loss           | 35.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 503       |
|    time_elapsed         | 674       |
|    total_timesteps      | 10060     |
| train/                  |           |
|    approx_kl            | 2.2415695 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.27      |
|    n_updates            | 10040     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.37      |
|    value_loss           | 8.64      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 504       |
|    time_elapsed         | 675       |
|    total_timesteps      | 10080     |
| train/                  |           |
|    approx_kl            | 0.1666094 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.457    |
|    learning_rate        | 0.0003    |
|    loss                 | 15.9      |
|    n_updates            | 10060     |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.37      |
|    value_loss           | 42.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 505       |
|    time_elapsed         | 676       |
|    total_timesteps      | 10100     |
| train/                  |           |
|    approx_kl            | 1.7698473 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.57      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.54      |
|    n_updates            | 10080     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.37      |
|    value_loss           | 7.79      |
---------------------------------------
----------------------------------------
| reward                  | 0.754      |
| reward_contact          | -0.166     |
| reward_motion           | 0.92       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 506        |
|    time_elapsed         | 678        |
|    total_timesteps      | 10120      |
| train/                  |            |
|    approx_kl            | 0.22908483 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.24       |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.37       |
|    value_loss           | 19.4       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 507        |
|    time_elapsed         | 679        |
|    total_timesteps      | 10140      |
| train/                  |            |
|    approx_kl            | 0.93026507 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.44       |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.258     |
|    std                  | 0.37       |
|    value_loss           | 9.88       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 508       |
|    time_elapsed         | 680       |
|    total_timesteps      | 10160     |
| train/                  |           |
|    approx_kl            | 0.9087004 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.38      |
|    n_updates            | 10140     |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.37      |
|    value_loss           | 6.31      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 509        |
|    time_elapsed         | 682        |
|    total_timesteps      | 10180      |
| train/                  |            |
|    approx_kl            | 0.74120516 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.23       |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.244     |
|    std                  | 0.37       |
|    value_loss           | 10         |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 510        |
|    time_elapsed         | 683        |
|    total_timesteps      | 10200      |
| train/                  |            |
|    approx_kl            | 0.19585541 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.437      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.1       |
|    n_updates            | 10180      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.37       |
|    value_loss           | 44.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.754     |
| reward_contact          | -0.166    |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 511       |
|    time_elapsed         | 684       |
|    total_timesteps      | 10220     |
| train/                  |           |
|    approx_kl            | 0.4689079 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.65      |
|    n_updates            | 10200     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.37      |
|    value_loss           | 13        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 512        |
|    time_elapsed         | 686        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.74168897 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.51       |
|    learning_rate        | 0.0003     |
|    loss                 | 14.4       |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.274     |
|    std                  | 0.37       |
|    value_loss           | 33.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 513       |
|    time_elapsed         | 687       |
|    total_timesteps      | 10260     |
| train/                  |           |
|    approx_kl            | 1.4476148 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.593     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.2       |
|    n_updates            | 10240     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.37      |
|    value_loss           | 7.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 514       |
|    time_elapsed         | 688       |
|    total_timesteps      | 10280     |
| train/                  |           |
|    approx_kl            | 2.4793804 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.32      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 10260     |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.37      |
|    value_loss           | 5.2       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 515       |
|    time_elapsed         | 690       |
|    total_timesteps      | 10300     |
| train/                  |           |
|    approx_kl            | 1.2852726 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.253     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.21      |
|    n_updates            | 10280     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.37      |
|    value_loss           | 14.6      |
---------------------------------------
----------------------------------------
| reward                  | 0.707      |
| reward_contact          | -0.213     |
| reward_motion           | 0.92       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 516        |
|    time_elapsed         | 691        |
|    total_timesteps      | 10320      |
| train/                  |            |
|    approx_kl            | 0.70053464 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 1          |
|    n_updates            | 10300      |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.37       |
|    value_loss           | 3.88       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 517        |
|    time_elapsed         | 692        |
|    total_timesteps      | 10340      |
| train/                  |            |
|    approx_kl            | 0.69723845 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -1.47      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.75       |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.26      |
|    std                  | 0.37       |
|    value_loss           | 17.4       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 518      |
|    time_elapsed         | 694      |
|    total_timesteps      | 10360    |
| train/                  |          |
|    approx_kl            | 2.189453 |
|    clip_fraction        | 0.713    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | -0.284   |
|    learning_rate        | 0.0003   |
|    loss                 | 7.52     |
|    n_updates            | 10340    |
|    policy_gradient_loss | -0.271   |
|    std                  | 0.37     |
|    value_loss           | 18.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 519       |
|    time_elapsed         | 695       |
|    total_timesteps      | 10380     |
| train/                  |           |
|    approx_kl            | 1.8356037 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.515     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.09      |
|    n_updates            | 10360     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.37      |
|    value_loss           | 8.22      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 520        |
|    time_elapsed         | 696        |
|    total_timesteps      | 10400      |
| train/                  |            |
|    approx_kl            | 0.85143006 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.1        |
|    n_updates            | 10380      |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.37       |
|    value_loss           | 8.51       |
----------------------------------------
---------------------------------------
| reward                  | 0.707     |
| reward_contact          | -0.213    |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 521       |
|    time_elapsed         | 698       |
|    total_timesteps      | 10420     |
| train/                  |           |
|    approx_kl            | 2.5694244 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.471     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.87      |
|    n_updates            | 10400     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.37      |
|    value_loss           | 12.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 522       |
|    time_elapsed         | 699       |
|    total_timesteps      | 10440     |
| train/                  |           |
|    approx_kl            | 0.9870617 |
|    clip_fraction        | 0.668     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.668     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.97      |
|    n_updates            | 10420     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.37      |
|    value_loss           | 5.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 523       |
|    time_elapsed         | 700       |
|    total_timesteps      | 10460     |
| train/                  |           |
|    approx_kl            | 1.2012309 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.539     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.94      |
|    n_updates            | 10440     |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.37      |
|    value_loss           | 9.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 524       |
|    time_elapsed         | 702       |
|    total_timesteps      | 10480     |
| train/                  |           |
|    approx_kl            | 2.3705404 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.93      |
|    n_updates            | 10460     |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.37      |
|    value_loss           | 9.03      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 525       |
|    time_elapsed         | 703       |
|    total_timesteps      | 10500     |
| train/                  |           |
|    approx_kl            | 0.6408253 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.35      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.4       |
|    n_updates            | 10480     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.369     |
|    value_loss           | 19.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.717     |
| reward_contact          | -0.213    |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 526       |
|    time_elapsed         | 704       |
|    total_timesteps      | 10520     |
| train/                  |           |
|    approx_kl            | 1.3373648 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.295     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.41      |
|    n_updates            | 10500     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 17        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 527       |
|    time_elapsed         | 706       |
|    total_timesteps      | 10540     |
| train/                  |           |
|    approx_kl            | 1.5309842 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.734     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.95      |
|    n_updates            | 10520     |
|    policy_gradient_loss | -0.278    |
|    std                  | 0.369     |
|    value_loss           | 6.87      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 528        |
|    time_elapsed         | 707        |
|    total_timesteps      | 10560      |
| train/                  |            |
|    approx_kl            | 0.97021294 |
|    clip_fraction        | 0.84       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.237      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.08       |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.244     |
|    std                  | 0.369      |
|    value_loss           | 8.2        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 529       |
|    time_elapsed         | 708       |
|    total_timesteps      | 10580     |
| train/                  |           |
|    approx_kl            | 2.0248399 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.262     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.04      |
|    n_updates            | 10560     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.369     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 530       |
|    time_elapsed         | 710       |
|    total_timesteps      | 10600     |
| train/                  |           |
|    approx_kl            | 1.0435425 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.486     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.25      |
|    n_updates            | 10580     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.369     |
|    value_loss           | 9.14      |
---------------------------------------
---------------------------------------
| reward                  | 0.695     |
| reward_contact          | -0.235    |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 531       |
|    time_elapsed         | 711       |
|    total_timesteps      | 10620     |
| train/                  |           |
|    approx_kl            | 1.0998981 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.141     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.42      |
|    n_updates            | 10600     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.369     |
|    value_loss           | 13.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 532       |
|    time_elapsed         | 712       |
|    total_timesteps      | 10640     |
| train/                  |           |
|    approx_kl            | 0.8039075 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.55      |
|    n_updates            | 10620     |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.369     |
|    value_loss           | 4.83      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 533       |
|    time_elapsed         | 714       |
|    total_timesteps      | 10660     |
| train/                  |           |
|    approx_kl            | 1.2866613 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -1.55     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.67      |
|    n_updates            | 10640     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.369     |
|    value_loss           | 22.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 534       |
|    time_elapsed         | 715       |
|    total_timesteps      | 10680     |
| train/                  |           |
|    approx_kl            | 0.5566357 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.295     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.43      |
|    n_updates            | 10660     |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.369     |
|    value_loss           | 13        |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 535      |
|    time_elapsed         | 716      |
|    total_timesteps      | 10700    |
| train/                  |          |
|    approx_kl            | 4.710138 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.49     |
|    learning_rate        | 0.0003   |
|    loss                 | 3.64     |
|    n_updates            | 10680    |
|    policy_gradient_loss | -0.295   |
|    std                  | 0.369    |
|    value_loss           | 9.63     |
--------------------------------------
---------------------------------------
| reward                  | 0.705     |
| reward_contact          | -0.235    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 536       |
|    time_elapsed         | 717       |
|    total_timesteps      | 10720     |
| train/                  |           |
|    approx_kl            | 0.5784705 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.642     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.3       |
|    n_updates            | 10700     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 6.87      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 537        |
|    time_elapsed         | 719        |
|    total_timesteps      | 10740      |
| train/                  |            |
|    approx_kl            | 0.58071035 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.46       |
|    n_updates            | 10720      |
|    policy_gradient_loss | -0.255     |
|    std                  | 0.369      |
|    value_loss           | 15.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 538       |
|    time_elapsed         | 720       |
|    total_timesteps      | 10760     |
| train/                  |           |
|    approx_kl            | 0.6371856 |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -1.75     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.17      |
|    n_updates            | 10740     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.369     |
|    value_loss           | 21.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 539        |
|    time_elapsed         | 721        |
|    total_timesteps      | 10780      |
| train/                  |            |
|    approx_kl            | 0.24024926 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.369      |
|    value_loss           | 30.2       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 540      |
|    time_elapsed         | 723      |
|    total_timesteps      | 10800    |
| train/                  |          |
|    approx_kl            | 2.063504 |
|    clip_fraction        | 0.715    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.643    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.873    |
|    n_updates            | 10780    |
|    policy_gradient_loss | -0.243   |
|    std                  | 0.369    |
|    value_loss           | 3.94     |
--------------------------------------
---------------------------------------
| reward                  | 0.696     |
| reward_contact          | -0.244    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 541       |
|    time_elapsed         | 724       |
|    total_timesteps      | 10820     |
| train/                  |           |
|    approx_kl            | 0.4974186 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.401     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.82      |
|    n_updates            | 10800     |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.369     |
|    value_loss           | 14.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 542        |
|    time_elapsed         | 725        |
|    total_timesteps      | 10840      |
| train/                  |            |
|    approx_kl            | 0.49391553 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.275      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.07       |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.369      |
|    value_loss           | 11.1       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 543       |
|    time_elapsed         | 727       |
|    total_timesteps      | 10860     |
| train/                  |           |
|    approx_kl            | 0.9113657 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.586     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.91      |
|    n_updates            | 10840     |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.369     |
|    value_loss           | 5.44      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 544        |
|    time_elapsed         | 728        |
|    total_timesteps      | 10880      |
| train/                  |            |
|    approx_kl            | 0.99891245 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.0609    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.92       |
|    n_updates            | 10860      |
|    policy_gradient_loss | -0.252     |
|    std                  | 0.369      |
|    value_loss           | 18.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 545       |
|    time_elapsed         | 729       |
|    total_timesteps      | 10900     |
| train/                  |           |
|    approx_kl            | 4.3965917 |
|    clip_fraction        | 0.798     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.574     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 10880     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.369     |
|    value_loss           | 8.37      |
---------------------------------------
---------------------------------------
| reward                  | 0.696     |
| reward_contact          | -0.244    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 546       |
|    time_elapsed         | 731       |
|    total_timesteps      | 10920     |
| train/                  |           |
|    approx_kl            | 3.0411632 |
|    clip_fraction        | 0.823     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.39      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.16      |
|    n_updates            | 10900     |
|    policy_gradient_loss | -0.297    |
|    std                  | 0.369     |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 547       |
|    time_elapsed         | 732       |
|    total_timesteps      | 10940     |
| train/                  |           |
|    approx_kl            | 2.4604416 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.292     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.93      |
|    n_updates            | 10920     |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.369     |
|    value_loss           | 14.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 548       |
|    time_elapsed         | 733       |
|    total_timesteps      | 10960     |
| train/                  |           |
|    approx_kl            | 3.1641095 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.92      |
|    n_updates            | 10940     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.369     |
|    value_loss           | 11        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 549       |
|    time_elapsed         | 735       |
|    total_timesteps      | 10980     |
| train/                  |           |
|    approx_kl            | 1.0735407 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.352     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.78      |
|    n_updates            | 10960     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.369     |
|    value_loss           | 9.08      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 550      |
|    time_elapsed         | 736      |
|    total_timesteps      | 11000    |
| train/                  |          |
|    approx_kl            | 1.619027 |
|    clip_fraction        | 0.665    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.451    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.86     |
|    n_updates            | 10980    |
|    policy_gradient_loss | -0.272   |
|    std                  | 0.369    |
|    value_loss           | 7.33     |
--------------------------------------
---------------------------------------
| reward                  | 0.716     |
| reward_contact          | -0.244    |
| reward_motion           | 0.96      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 551       |
|    time_elapsed         | 737       |
|    total_timesteps      | 11020     |
| train/                  |           |
|    approx_kl            | 0.6531028 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.585     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.72      |
|    n_updates            | 11000     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.369     |
|    value_loss           | 11.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 552       |
|    time_elapsed         | 739       |
|    total_timesteps      | 11040     |
| train/                  |           |
|    approx_kl            | 2.9844568 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.502     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.21      |
|    n_updates            | 11020     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.369     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 553       |
|    time_elapsed         | 740       |
|    total_timesteps      | 11060     |
| train/                  |           |
|    approx_kl            | 1.8320178 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.252     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.22      |
|    n_updates            | 11040     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.369     |
|    value_loss           | 16.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 554      |
|    time_elapsed         | 741      |
|    total_timesteps      | 11080    |
| train/                  |          |
|    approx_kl            | 3.410827 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.151    |
|    learning_rate        | 0.0003   |
|    loss                 | 8.53     |
|    n_updates            | 11060    |
|    policy_gradient_loss | -0.277   |
|    std                  | 0.369    |
|    value_loss           | 19.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 555       |
|    time_elapsed         | 743       |
|    total_timesteps      | 11100     |
| train/                  |           |
|    approx_kl            | 3.3850124 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.317     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.78      |
|    n_updates            | 11080     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.369     |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| reward                  | 0.756     |
| reward_contact          | -0.204    |
| reward_motion           | 0.96      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 556       |
|    time_elapsed         | 744       |
|    total_timesteps      | 11120     |
| train/                  |           |
|    approx_kl            | 1.5078033 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 11100     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.369     |
|    value_loss           | 11.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 557       |
|    time_elapsed         | 745       |
|    total_timesteps      | 11140     |
| train/                  |           |
|    approx_kl            | 0.9822323 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.548     |
|    n_updates            | 11120     |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.369     |
|    value_loss           | 2.23      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 558       |
|    time_elapsed         | 747       |
|    total_timesteps      | 11160     |
| train/                  |           |
|    approx_kl            | 1.2830712 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.433     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.41      |
|    n_updates            | 11140     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.369     |
|    value_loss           | 13.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 559      |
|    time_elapsed         | 748      |
|    total_timesteps      | 11180    |
| train/                  |          |
|    approx_kl            | 2.515709 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.322    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.75     |
|    n_updates            | 11160    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.369    |
|    value_loss           | 13.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 560       |
|    time_elapsed         | 749       |
|    total_timesteps      | 11200     |
| train/                  |           |
|    approx_kl            | 0.8944109 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 11180     |
|    policy_gradient_loss | -0.225    |
|    std                  | 0.369     |
|    value_loss           | 7.19      |
---------------------------------------
---------------------------------------
| reward                  | 0.774     |
| reward_contact          | -0.176    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 561       |
|    time_elapsed         | 751       |
|    total_timesteps      | 11220     |
| train/                  |           |
|    approx_kl            | 0.5359573 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.562     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.87      |
|    n_updates            | 11200     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 562       |
|    time_elapsed         | 752       |
|    total_timesteps      | 11240     |
| train/                  |           |
|    approx_kl            | 1.0566055 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.07     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.13      |
|    n_updates            | 11220     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.369     |
|    value_loss           | 15        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 563       |
|    time_elapsed         | 753       |
|    total_timesteps      | 11260     |
| train/                  |           |
|    approx_kl            | 3.4909112 |
|    clip_fraction        | 0.805     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.708     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 11240     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.369     |
|    value_loss           | 3.92      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 564       |
|    time_elapsed         | 755       |
|    total_timesteps      | 11280     |
| train/                  |           |
|    approx_kl            | 3.6869056 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.334     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.34      |
|    n_updates            | 11260     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.369     |
|    value_loss           | 9.95      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 565       |
|    time_elapsed         | 756       |
|    total_timesteps      | 11300     |
| train/                  |           |
|    approx_kl            | 1.5896502 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.374     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 11280     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.369     |
|    value_loss           | 10.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.8        |
| reward_contact          | -0.16      |
| reward_motion           | 0.96       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 566        |
|    time_elapsed         | 757        |
|    total_timesteps      | 11320      |
| train/                  |            |
|    approx_kl            | 0.46887073 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.52       |
|    n_updates            | 11300      |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.369      |
|    value_loss           | 4.85       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 567       |
|    time_elapsed         | 759       |
|    total_timesteps      | 11340     |
| train/                  |           |
|    approx_kl            | 1.5833832 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | -0.0113   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 11320     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.369     |
|    value_loss           | 11.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 568       |
|    time_elapsed         | 760       |
|    total_timesteps      | 11360     |
| train/                  |           |
|    approx_kl            | 0.9264738 |
|    clip_fraction        | 0.657     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.532     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.38      |
|    n_updates            | 11340     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.369     |
|    value_loss           | 10        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 569       |
|    time_elapsed         | 761       |
|    total_timesteps      | 11380     |
| train/                  |           |
|    approx_kl            | 3.0781064 |
|    clip_fraction        | 0.653     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.564    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.39      |
|    n_updates            | 11360     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.369     |
|    value_loss           | 15        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 570       |
|    time_elapsed         | 763       |
|    total_timesteps      | 11400     |
| train/                  |           |
|    approx_kl            | 1.0712785 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | -4.2      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.23      |
|    n_updates            | 11380     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.369     |
|    value_loss           | 20.4      |
---------------------------------------
----------------------------------------
| reward                  | 0.79       |
| reward_contact          | -0.16      |
| reward_motion           | 0.95       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 571        |
|    time_elapsed         | 764        |
|    total_timesteps      | 11420      |
| train/                  |            |
|    approx_kl            | 0.34785685 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.00217    |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 11400      |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.369      |
|    value_loss           | 13.7       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 572        |
|    time_elapsed         | 765        |
|    total_timesteps      | 11440      |
| train/                  |            |
|    approx_kl            | 0.73405904 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.78       |
|    n_updates            | 11420      |
|    policy_gradient_loss | -0.232     |
|    std                  | 0.369      |
|    value_loss           | 14.6       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 573        |
|    time_elapsed         | 767        |
|    total_timesteps      | 11460      |
| train/                  |            |
|    approx_kl            | 0.77939945 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.74       |
|    n_updates            | 11440      |
|    policy_gradient_loss | -0.269     |
|    std                  | 0.369      |
|    value_loss           | 23.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 574       |
|    time_elapsed         | 768       |
|    total_timesteps      | 11480     |
| train/                  |           |
|    approx_kl            | 0.7317966 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.000922 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.27      |
|    n_updates            | 11460     |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.369     |
|    value_loss           | 6.33      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 575       |
|    time_elapsed         | 769       |
|    total_timesteps      | 11500     |
| train/                  |           |
|    approx_kl            | 0.6954694 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.432     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 11480     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.369     |
|    value_loss           | 10.2      |
---------------------------------------
----------------------------------------
| reward                  | 0.761      |
| reward_contact          | -0.179     |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 576        |
|    time_elapsed         | 771        |
|    total_timesteps      | 11520      |
| train/                  |            |
|    approx_kl            | 0.89230937 |
|    clip_fraction        | 0.72       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.421      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.56       |
|    n_updates            | 11500      |
|    policy_gradient_loss | -0.285     |
|    std                  | 0.369      |
|    value_loss           | 9.35       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 577        |
|    time_elapsed         | 772        |
|    total_timesteps      | 11540      |
| train/                  |            |
|    approx_kl            | 0.66103405 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 11520      |
|    policy_gradient_loss | -0.246     |
|    std                  | 0.369      |
|    value_loss           | 12.7       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 578        |
|    time_elapsed         | 773        |
|    total_timesteps      | 11560      |
| train/                  |            |
|    approx_kl            | 0.49081287 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.44       |
|    n_updates            | 11540      |
|    policy_gradient_loss | -0.244     |
|    std                  | 0.369      |
|    value_loss           | 9.79       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 579        |
|    time_elapsed         | 775        |
|    total_timesteps      | 11580      |
| train/                  |            |
|    approx_kl            | 0.42019653 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.47       |
|    n_updates            | 11560      |
|    policy_gradient_loss | -0.238     |
|    std                  | 0.369      |
|    value_loss           | 11.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 580       |
|    time_elapsed         | 776       |
|    total_timesteps      | 11600     |
| train/                  |           |
|    approx_kl            | 1.9630612 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.274     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.18      |
|    n_updates            | 11580     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.369     |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.809     |
| reward_contact          | -0.131    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 581       |
|    time_elapsed         | 777       |
|    total_timesteps      | 11620     |
| train/                  |           |
|    approx_kl            | 0.7926239 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.417     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.14      |
|    n_updates            | 11600     |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.369     |
|    value_loss           | 8.79      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 582       |
|    time_elapsed         | 779       |
|    total_timesteps      | 11640     |
| train/                  |           |
|    approx_kl            | 1.6577396 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.121    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.67      |
|    n_updates            | 11620     |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.369     |
|    value_loss           | 5.32      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 583        |
|    time_elapsed         | 780        |
|    total_timesteps      | 11660      |
| train/                  |            |
|    approx_kl            | 0.22245894 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | -1.45      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.5       |
|    n_updates            | 11640      |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.369      |
|    value_loss           | 43.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 584       |
|    time_elapsed         | 781       |
|    total_timesteps      | 11680     |
| train/                  |           |
|    approx_kl            | 1.5503939 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | -0.985    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.11      |
|    n_updates            | 11660     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.369     |
|    value_loss           | 9.38      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 585       |
|    time_elapsed         | 782       |
|    total_timesteps      | 11700     |
| train/                  |           |
|    approx_kl            | 1.8251591 |
|    clip_fraction        | 0.725     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.279    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 11680     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.369     |
|    value_loss           | 7.23      |
---------------------------------------
----------------------------------------
| reward                  | 0.78       |
| reward_contact          | -0.16      |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 586        |
|    time_elapsed         | 784        |
|    total_timesteps      | 11720      |
| train/                  |            |
|    approx_kl            | 0.56971693 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 11700      |
|    policy_gradient_loss | -0.246     |
|    std                  | 0.369      |
|    value_loss           | 16.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 587       |
|    time_elapsed         | 785       |
|    total_timesteps      | 11740     |
| train/                  |           |
|    approx_kl            | 0.7048419 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.254     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.27      |
|    n_updates            | 11720     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.369     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 588       |
|    time_elapsed         | 786       |
|    total_timesteps      | 11760     |
| train/                  |           |
|    approx_kl            | 0.6410925 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.451     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.36      |
|    n_updates            | 11740     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.369     |
|    value_loss           | 9.83      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 589       |
|    time_elapsed         | 788       |
|    total_timesteps      | 11780     |
| train/                  |           |
|    approx_kl            | 3.2680137 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.479     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 11760     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.369     |
|    value_loss           | 4.45      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 590       |
|    time_elapsed         | 789       |
|    total_timesteps      | 11800     |
| train/                  |           |
|    approx_kl            | 1.8069651 |
|    clip_fraction        | 0.778     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.371     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.25      |
|    n_updates            | 11780     |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.369     |
|    value_loss           | 10.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.78      |
| reward_contact          | -0.16     |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 591       |
|    time_elapsed         | 791       |
|    total_timesteps      | 11820     |
| train/                  |           |
|    approx_kl            | 1.2538211 |
|    clip_fraction        | 0.715     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.204     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.84      |
|    n_updates            | 11800     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.369     |
|    value_loss           | 14.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 592       |
|    time_elapsed         | 792       |
|    total_timesteps      | 11840     |
| train/                  |           |
|    approx_kl            | 1.2012227 |
|    clip_fraction        | 0.675     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.724     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.887     |
|    n_updates            | 11820     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.369     |
|    value_loss           | 4.14      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 593      |
|    time_elapsed         | 793      |
|    total_timesteps      | 11860    |
| train/                  |          |
|    approx_kl            | 2.077329 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.466    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.81     |
|    n_updates            | 11840    |
|    policy_gradient_loss | -0.259   |
|    std                  | 0.369    |
|    value_loss           | 6.13     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 594        |
|    time_elapsed         | 794        |
|    total_timesteps      | 11880      |
| train/                  |            |
|    approx_kl            | 0.27855802 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 11860      |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.369      |
|    value_loss           | 33.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 595       |
|    time_elapsed         | 796       |
|    total_timesteps      | 11900     |
| train/                  |           |
|    approx_kl            | 1.1353664 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.564     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.26      |
|    n_updates            | 11880     |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.369     |
|    value_loss           | 6.22      |
---------------------------------------
---------------------------------------
| reward                  | 0.767     |
| reward_contact          | -0.183    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 596       |
|    time_elapsed         | 797       |
|    total_timesteps      | 11920     |
| train/                  |           |
|    approx_kl            | 0.1053772 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.281     |
|    learning_rate        | 0.0003    |
|    loss                 | 44        |
|    n_updates            | 11900     |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.369     |
|    value_loss           | 98.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 597       |
|    time_elapsed         | 798       |
|    total_timesteps      | 11940     |
| train/                  |           |
|    approx_kl            | 1.6504765 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.312     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.32      |
|    n_updates            | 11920     |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.369     |
|    value_loss           | 12        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 598       |
|    time_elapsed         | 800       |
|    total_timesteps      | 11960     |
| train/                  |           |
|    approx_kl            | 0.3335653 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.0003    |
|    loss                 | 18.6      |
|    n_updates            | 11940     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 45.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 599        |
|    time_elapsed         | 801        |
|    total_timesteps      | 11980      |
| train/                  |            |
|    approx_kl            | 0.55196446 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.97       |
|    n_updates            | 11960      |
|    policy_gradient_loss | -0.259     |
|    std                  | 0.369      |
|    value_loss           | 18.2       |
----------------------------------------
Num timesteps: 12000
Best mean reward: 13.31 - Last mean reward per episode: 14.55
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 600       |
|    time_elapsed         | 803       |
|    total_timesteps      | 12000     |
| train/                  |           |
|    approx_kl            | 0.9212883 |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.45      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.18      |
|    n_updates            | 11980     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.715     |
| reward_contact          | -0.225    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 601       |
|    time_elapsed         | 804       |
|    total_timesteps      | 12020     |
| train/                  |           |
|    approx_kl            | 1.0028355 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.248     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.63      |
|    n_updates            | 12000     |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.369     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 602       |
|    time_elapsed         | 805       |
|    total_timesteps      | 12040     |
| train/                  |           |
|    approx_kl            | 2.0890872 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.21      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.39      |
|    n_updates            | 12020     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.369     |
|    value_loss           | 4.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 603       |
|    time_elapsed         | 807       |
|    total_timesteps      | 12060     |
| train/                  |           |
|    approx_kl            | 1.7562538 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.191     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.19      |
|    n_updates            | 12040     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.369     |
|    value_loss           | 8.56      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 604        |
|    time_elapsed         | 808        |
|    total_timesteps      | 12080      |
| train/                  |            |
|    approx_kl            | 0.45895883 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.0315    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.48       |
|    n_updates            | 12060      |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.369      |
|    value_loss           | 8.6        |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 605      |
|    time_elapsed         | 809      |
|    total_timesteps      | 12100    |
| train/                  |          |
|    approx_kl            | 2.755972 |
|    clip_fraction        | 0.788    |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | 0.47     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.95     |
|    n_updates            | 12080    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.369    |
|    value_loss           | 6.51     |
--------------------------------------
---------------------------------------
| reward                  | 0.715     |
| reward_contact          | -0.225    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 606       |
|    time_elapsed         | 811       |
|    total_timesteps      | 12120     |
| train/                  |           |
|    approx_kl            | 3.8221586 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.167     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.8       |
|    n_updates            | 12100     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.369     |
|    value_loss           | 15.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 607        |
|    time_elapsed         | 812        |
|    total_timesteps      | 12140      |
| train/                  |            |
|    approx_kl            | 0.98209137 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.17       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.17       |
|    n_updates            | 12120      |
|    policy_gradient_loss | -0.236     |
|    std                  | 0.369      |
|    value_loss           | 13.6       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 608        |
|    time_elapsed         | 813        |
|    total_timesteps      | 12160      |
| train/                  |            |
|    approx_kl            | 0.79628164 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.03       |
|    n_updates            | 12140      |
|    policy_gradient_loss | -0.235     |
|    std                  | 0.369      |
|    value_loss           | 9.59       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 609       |
|    time_elapsed         | 814       |
|    total_timesteps      | 12180     |
| train/                  |           |
|    approx_kl            | 0.3710513 |
|    clip_fraction        | 0.28      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.315     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.9      |
|    n_updates            | 12160     |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.369     |
|    value_loss           | 34.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 610       |
|    time_elapsed         | 816       |
|    total_timesteps      | 12200     |
| train/                  |           |
|    approx_kl            | 1.6641299 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.7       |
|    n_updates            | 12180     |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.369     |
|    value_loss           | 13.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.658     |
| reward_contact          | -0.282    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 611       |
|    time_elapsed         | 817       |
|    total_timesteps      | 12220     |
| train/                  |           |
|    approx_kl            | 0.5677512 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.153     |
|    learning_rate        | 0.0003    |
|    loss                 | 20.8      |
|    n_updates            | 12200     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.369     |
|    value_loss           | 50.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 612       |
|    time_elapsed         | 818       |
|    total_timesteps      | 12240     |
| train/                  |           |
|    approx_kl            | 1.3767731 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.29      |
|    n_updates            | 12220     |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.369     |
|    value_loss           | 4.56      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 613      |
|    time_elapsed         | 820      |
|    total_timesteps      | 12260    |
| train/                  |          |
|    approx_kl            | 2.019954 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.045    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.06     |
|    n_updates            | 12240    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.369    |
|    value_loss           | 11.9     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 614       |
|    time_elapsed         | 821       |
|    total_timesteps      | 12280     |
| train/                  |           |
|    approx_kl            | 0.1969353 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.0003    |
|    loss                 | 22.8      |
|    n_updates            | 12260     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.369     |
|    value_loss           | 54.8      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 615        |
|    time_elapsed         | 822        |
|    total_timesteps      | 12300      |
| train/                  |            |
|    approx_kl            | 0.48337242 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -0.634     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.01       |
|    n_updates            | 12280      |
|    policy_gradient_loss | -0.255     |
|    std                  | 0.369      |
|    value_loss           | 18.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.684     |
| reward_contact          | -0.266    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 616       |
|    time_elapsed         | 824       |
|    total_timesteps      | 12320     |
| train/                  |           |
|    approx_kl            | 1.1747413 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.847     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.671     |
|    n_updates            | 12300     |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.369     |
|    value_loss           | 2.67      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 617      |
|    time_elapsed         | 825      |
|    total_timesteps      | 12340    |
| train/                  |          |
|    approx_kl            | 2.041107 |
|    clip_fraction        | 0.67     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.631    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.83     |
|    n_updates            | 12320    |
|    policy_gradient_loss | -0.246   |
|    std                  | 0.369    |
|    value_loss           | 6.06     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 618       |
|    time_elapsed         | 826       |
|    total_timesteps      | 12360     |
| train/                  |           |
|    approx_kl            | 1.9566326 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -0.426    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 12340     |
|    policy_gradient_loss | -0.278    |
|    std                  | 0.369     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 619       |
|    time_elapsed         | 828       |
|    total_timesteps      | 12380     |
| train/                  |           |
|    approx_kl            | 3.2534833 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.607     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 12360     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.369     |
|    value_loss           | 3.97      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 620        |
|    time_elapsed         | 829        |
|    total_timesteps      | 12400      |
| train/                  |            |
|    approx_kl            | 0.34963593 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | -2.71      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.25       |
|    n_updates            | 12380      |
|    policy_gradient_loss | -0.224     |
|    std                  | 0.369      |
|    value_loss           | 22.1       |
----------------------------------------
----------------------------------------
| reward                  | 0.684      |
| reward_contact          | -0.266     |
| reward_motion           | 0.95       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 621        |
|    time_elapsed         | 830        |
|    total_timesteps      | 12420      |
| train/                  |            |
|    approx_kl            | 0.49758402 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 12400      |
|    policy_gradient_loss | -0.234     |
|    std                  | 0.369      |
|    value_loss           | 9.84       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 622       |
|    time_elapsed         | 832       |
|    total_timesteps      | 12440     |
| train/                  |           |
|    approx_kl            | 1.4267272 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.865     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.255     |
|    n_updates            | 12420     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.369     |
|    value_loss           | 1.56      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 623       |
|    time_elapsed         | 833       |
|    total_timesteps      | 12460     |
| train/                  |           |
|    approx_kl            | 0.8965189 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.522     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.33      |
|    n_updates            | 12440     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.369     |
|    value_loss           | 6.13      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 624        |
|    time_elapsed         | 834        |
|    total_timesteps      | 12480      |
| train/                  |            |
|    approx_kl            | 0.43497238 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.13       |
|    n_updates            | 12460      |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.369      |
|    value_loss           | 7.07       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 625        |
|    time_elapsed         | 836        |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.44135022 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.99       |
|    n_updates            | 12480      |
|    policy_gradient_loss | -0.247     |
|    std                  | 0.369      |
|    value_loss           | 24.5       |
----------------------------------------
----------------------------------------
| reward                  | 0.651      |
| reward_contact          | -0.289     |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 626        |
|    time_elapsed         | 837        |
|    total_timesteps      | 12520      |
| train/                  |            |
|    approx_kl            | 0.95823956 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 12500      |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.369      |
|    value_loss           | 6.09       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 627       |
|    time_elapsed         | 838       |
|    total_timesteps      | 12540     |
| train/                  |           |
|    approx_kl            | 1.8900017 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.623     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.15      |
|    n_updates            | 12520     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.368     |
|    value_loss           | 6.73      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 628       |
|    time_elapsed         | 840       |
|    total_timesteps      | 12560     |
| train/                  |           |
|    approx_kl            | 1.1497267 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.249    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.43      |
|    n_updates            | 12540     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.368     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 629       |
|    time_elapsed         | 841       |
|    total_timesteps      | 12580     |
| train/                  |           |
|    approx_kl            | 0.5710892 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.43      |
|    n_updates            | 12560     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.368     |
|    value_loss           | 12.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 630       |
|    time_elapsed         | 842       |
|    total_timesteps      | 12600     |
| train/                  |           |
|    approx_kl            | 1.1500554 |
|    clip_fraction        | 0.808     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.93      |
|    n_updates            | 12580     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.368     |
|    value_loss           | 7.4       |
---------------------------------------
---------------------------------------
| reward                  | 0.663     |
| reward_contact          | -0.267    |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 631       |
|    time_elapsed         | 844       |
|    total_timesteps      | 12620     |
| train/                  |           |
|    approx_kl            | 6.2390056 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.512     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 12600     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.368     |
|    value_loss           | 7.12      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 632       |
|    time_elapsed         | 845       |
|    total_timesteps      | 12640     |
| train/                  |           |
|    approx_kl            | 1.0904596 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.65      |
|    n_updates            | 12620     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.369     |
|    value_loss           | 6.21      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 633       |
|    time_elapsed         | 846       |
|    total_timesteps      | 12660     |
| train/                  |           |
|    approx_kl            | 3.2009685 |
|    clip_fraction        | 0.773     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.377     |
|    n_updates            | 12640     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.368     |
|    value_loss           | 2.33      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 634        |
|    time_elapsed         | 848        |
|    total_timesteps      | 12680      |
| train/                  |            |
|    approx_kl            | 0.85540974 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.214      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 12660      |
|    policy_gradient_loss | -0.271     |
|    std                  | 0.368      |
|    value_loss           | 11.9       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 635        |
|    time_elapsed         | 849        |
|    total_timesteps      | 12700      |
| train/                  |            |
|    approx_kl            | 0.97158664 |
|    clip_fraction        | 0.658      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.0671     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.79       |
|    n_updates            | 12680      |
|    policy_gradient_loss | -0.256     |
|    std                  | 0.368      |
|    value_loss           | 6.92       |
----------------------------------------
---------------------------------------
| reward                  | 0.663     |
| reward_contact          | -0.267    |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 636       |
|    time_elapsed         | 850       |
|    total_timesteps      | 12720     |
| train/                  |           |
|    approx_kl            | 1.7922767 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.0325   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.19      |
|    n_updates            | 12700     |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.368     |
|    value_loss           | 16        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 637       |
|    time_elapsed         | 852       |
|    total_timesteps      | 12740     |
| train/                  |           |
|    approx_kl            | 1.9488245 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.296     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.27      |
|    n_updates            | 12720     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.368     |
|    value_loss           | 8.69      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 638       |
|    time_elapsed         | 853       |
|    total_timesteps      | 12760     |
| train/                  |           |
|    approx_kl            | 0.6473023 |
|    clip_fraction        | 0.628     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.495     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.88      |
|    n_updates            | 12740     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.368     |
|    value_loss           | 13.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 639        |
|    time_elapsed         | 854        |
|    total_timesteps      | 12780      |
| train/                  |            |
|    approx_kl            | 0.49670336 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.268      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.42       |
|    n_updates            | 12760      |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.368      |
|    value_loss           | 10.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 640       |
|    time_elapsed         | 856       |
|    total_timesteps      | 12800     |
| train/                  |           |
|    approx_kl            | 1.1745776 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.382     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.18      |
|    n_updates            | 12780     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 10.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.662     |
| reward_contact          | -0.258    |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 641       |
|    time_elapsed         | 857       |
|    total_timesteps      | 12820     |
| train/                  |           |
|    approx_kl            | 2.9150689 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.85      |
|    n_updates            | 12800     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.368     |
|    value_loss           | 9.42      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 642       |
|    time_elapsed         | 858       |
|    total_timesteps      | 12840     |
| train/                  |           |
|    approx_kl            | 2.1273625 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 12820     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.368     |
|    value_loss           | 8.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 643       |
|    time_elapsed         | 860       |
|    total_timesteps      | 12860     |
| train/                  |           |
|    approx_kl            | 0.9048707 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.1       |
|    n_updates            | 12840     |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.368     |
|    value_loss           | 7.59      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 644       |
|    time_elapsed         | 861       |
|    total_timesteps      | 12880     |
| train/                  |           |
|    approx_kl            | 0.5275006 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.3       |
|    n_updates            | 12860     |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.368     |
|    value_loss           | 23.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 645       |
|    time_elapsed         | 862       |
|    total_timesteps      | 12900     |
| train/                  |           |
|    approx_kl            | 1.7192236 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.543     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.38      |
|    n_updates            | 12880     |
|    policy_gradient_loss | -0.278    |
|    std                  | 0.368     |
|    value_loss           | 7.48      |
---------------------------------------
--------------------------------------
| reward                  | 0.639    |
| reward_contact          | -0.271   |
| reward_motion           | 0.91     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 646      |
|    time_elapsed         | 864      |
|    total_timesteps      | 12920    |
| train/                  |          |
|    approx_kl            | 0.534973 |
|    clip_fraction        | 0.653    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.456    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.1      |
|    n_updates            | 12900    |
|    policy_gradient_loss | -0.226   |
|    std                  | 0.368    |
|    value_loss           | 8.84     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 647       |
|    time_elapsed         | 865       |
|    total_timesteps      | 12940     |
| train/                  |           |
|    approx_kl            | 0.7526329 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.106    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.58      |
|    n_updates            | 12920     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.368     |
|    value_loss           | 10.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 648       |
|    time_elapsed         | 866       |
|    total_timesteps      | 12960     |
| train/                  |           |
|    approx_kl            | 1.0623697 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.482     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.58      |
|    n_updates            | 12940     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.368     |
|    value_loss           | 9.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 649       |
|    time_elapsed         | 868       |
|    total_timesteps      | 12980     |
| train/                  |           |
|    approx_kl            | 2.3732154 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.76      |
|    n_updates            | 12960     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.368     |
|    value_loss           | 5.54      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 650       |
|    time_elapsed         | 869       |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 1.9922122 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.29      |
|    n_updates            | 12980     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.368     |
|    value_loss           | 8.63      |
---------------------------------------
--------------------------------------
| reward                  | 0.629    |
| reward_contact          | -0.271   |
| reward_motion           | 0.9      |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 651      |
|    time_elapsed         | 870      |
|    total_timesteps      | 13020    |
| train/                  |          |
|    approx_kl            | 1.456349 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.475    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.86     |
|    n_updates            | 13000    |
|    policy_gradient_loss | -0.259   |
|    std                  | 0.368    |
|    value_loss           | 12.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 652       |
|    time_elapsed         | 872       |
|    total_timesteps      | 13040     |
| train/                  |           |
|    approx_kl            | 2.2894776 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.789     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.916     |
|    n_updates            | 13020     |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.368     |
|    value_loss           | 3.28      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 653        |
|    time_elapsed         | 873        |
|    total_timesteps      | 13060      |
| train/                  |            |
|    approx_kl            | 0.47573718 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.21       |
|    n_updates            | 13040      |
|    policy_gradient_loss | -0.234     |
|    std                  | 0.368      |
|    value_loss           | 7.33       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 654       |
|    time_elapsed         | 874       |
|    total_timesteps      | 13080     |
| train/                  |           |
|    approx_kl            | 1.1537746 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.415     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.07      |
|    n_updates            | 13060     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.368     |
|    value_loss           | 10.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 655       |
|    time_elapsed         | 876       |
|    total_timesteps      | 13100     |
| train/                  |           |
|    approx_kl            | 0.7946258 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -0.148    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 13080     |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.368     |
|    value_loss           | 10.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.629     |
| reward_contact          | -0.271    |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 656       |
|    time_elapsed         | 877       |
|    total_timesteps      | 13120     |
| train/                  |           |
|    approx_kl            | 6.1873617 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.55      |
|    n_updates            | 13100     |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.368     |
|    value_loss           | 6.57      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 657       |
|    time_elapsed         | 878       |
|    total_timesteps      | 13140     |
| train/                  |           |
|    approx_kl            | 1.2638472 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.67      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.88      |
|    n_updates            | 13120     |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.368     |
|    value_loss           | 5.58      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 658       |
|    time_elapsed         | 880       |
|    total_timesteps      | 13160     |
| train/                  |           |
|    approx_kl            | 1.8975416 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.274     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.25      |
|    n_updates            | 13140     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.368     |
|    value_loss           | 11.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 659        |
|    time_elapsed         | 881        |
|    total_timesteps      | 13180      |
| train/                  |            |
|    approx_kl            | 0.35954228 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.553     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.18       |
|    n_updates            | 13160      |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.368      |
|    value_loss           | 22.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 660       |
|    time_elapsed         | 882       |
|    total_timesteps      | 13200     |
| train/                  |           |
|    approx_kl            | 1.0207897 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.609     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.81      |
|    n_updates            | 13180     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.368     |
|    value_loss           | 7.06      |
---------------------------------------
--------------------------------------
| reward                  | 0.639    |
| reward_contact          | -0.271   |
| reward_motion           | 0.91     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 661      |
|    time_elapsed         | 884      |
|    total_timesteps      | 13220    |
| train/                  |          |
|    approx_kl            | 3.08028  |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.655    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.93     |
|    n_updates            | 13200    |
|    policy_gradient_loss | -0.252   |
|    std                  | 0.368    |
|    value_loss           | 6.01     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 662      |
|    time_elapsed         | 885      |
|    total_timesteps      | 13240    |
| train/                  |          |
|    approx_kl            | 0.562298 |
|    clip_fraction        | 0.628    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.372    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.82     |
|    n_updates            | 13220    |
|    policy_gradient_loss | -0.222   |
|    std                  | 0.368    |
|    value_loss           | 9.79     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 663       |
|    time_elapsed         | 886       |
|    total_timesteps      | 13260     |
| train/                  |           |
|    approx_kl            | 1.2756237 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.414     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.93      |
|    n_updates            | 13240     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.368     |
|    value_loss           | 8.91      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 664       |
|    time_elapsed         | 888       |
|    total_timesteps      | 13280     |
| train/                  |           |
|    approx_kl            | 1.3275007 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.395     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.26      |
|    n_updates            | 13260     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.368     |
|    value_loss           | 8.28      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 14.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 665        |
|    time_elapsed         | 889        |
|    total_timesteps      | 13300      |
| train/                  |            |
|    approx_kl            | 0.97877675 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.236     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.84       |
|    n_updates            | 13280      |
|    policy_gradient_loss | -0.24      |
|    std                  | 0.368      |
|    value_loss           | 22.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.639     |
| reward_contact          | -0.271    |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 666       |
|    time_elapsed         | 890       |
|    total_timesteps      | 13320     |
| train/                  |           |
|    approx_kl            | 1.4537374 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.724     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.863     |
|    n_updates            | 13300     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 4.39      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 667       |
|    time_elapsed         | 892       |
|    total_timesteps      | 13340     |
| train/                  |           |
|    approx_kl            | 1.1461315 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.91      |
|    n_updates            | 13320     |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.368     |
|    value_loss           | 8.5       |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 668      |
|    time_elapsed         | 893      |
|    total_timesteps      | 13360    |
| train/                  |          |
|    approx_kl            | 1.51067  |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.594    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.84     |
|    n_updates            | 13340    |
|    policy_gradient_loss | -0.276   |
|    std                  | 0.367    |
|    value_loss           | 7.64     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 669       |
|    time_elapsed         | 894       |
|    total_timesteps      | 13380     |
| train/                  |           |
|    approx_kl            | 1.2700405 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.0708    |
|    learning_rate        | 0.0003    |
|    loss                 | 9.78      |
|    n_updates            | 13360     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.367     |
|    value_loss           | 23.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 670       |
|    time_elapsed         | 896       |
|    total_timesteps      | 13400     |
| train/                  |           |
|    approx_kl            | 1.7258393 |
|    clip_fraction        | 0.745     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.458     |
|    n_updates            | 13380     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.367     |
|    value_loss           | 2.24      |
---------------------------------------
--------------------------------------
| reward                  | 0.649    |
| reward_contact          | -0.271   |
| reward_motion           | 0.92     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 14.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 671      |
|    time_elapsed         | 897      |
|    total_timesteps      | 13420    |
| train/                  |          |
|    approx_kl            | 0.856224 |
|    clip_fraction        | 0.75     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.313    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.03     |
|    n_updates            | 13400    |
|    policy_gradient_loss | -0.262   |
|    std                  | 0.367    |
|    value_loss           | 10.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 14.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 672       |
|    time_elapsed         | 898       |
|    total_timesteps      | 13440     |
| train/                  |           |
|    approx_kl            | 1.2570956 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.361     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.29      |
|    n_updates            | 13420     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.368     |
|    value_loss           | 17.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 673       |
|    time_elapsed         | 900       |
|    total_timesteps      | 13460     |
| train/                  |           |
|    approx_kl            | 1.6358064 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.538     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.44      |
|    n_updates            | 13440     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.368     |
|    value_loss           | 9.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 674       |
|    time_elapsed         | 901       |
|    total_timesteps      | 13480     |
| train/                  |           |
|    approx_kl            | 0.7915986 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.709     |
|    learning_rate        | 0.0003    |
|    loss                 | 2         |
|    n_updates            | 13460     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.368     |
|    value_loss           | 6.46      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 675      |
|    time_elapsed         | 902      |
|    total_timesteps      | 13500    |
| train/                  |          |
|    approx_kl            | 1.513127 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.664    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.63     |
|    n_updates            | 13480    |
|    policy_gradient_loss | -0.281   |
|    std                  | 0.368    |
|    value_loss           | 5.18     |
--------------------------------------
----------------------------------------
| reward                  | 0.678      |
| reward_contact          | -0.252     |
| reward_motion           | 0.93       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 676        |
|    time_elapsed         | 904        |
|    total_timesteps      | 13520      |
| train/                  |            |
|    approx_kl            | 0.22610621 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.74       |
|    n_updates            | 13500      |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.368      |
|    value_loss           | 20.5       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 677      |
|    time_elapsed         | 905      |
|    total_timesteps      | 13540    |
| train/                  |          |
|    approx_kl            | 1.430487 |
|    clip_fraction        | 0.658    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.323    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.18     |
|    n_updates            | 13520    |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.368    |
|    value_loss           | 12.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 678       |
|    time_elapsed         | 906       |
|    total_timesteps      | 13560     |
| train/                  |           |
|    approx_kl            | 1.3649868 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.57      |
|    n_updates            | 13540     |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.368     |
|    value_loss           | 9.61      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 679       |
|    time_elapsed         | 908       |
|    total_timesteps      | 13580     |
| train/                  |           |
|    approx_kl            | 2.2554958 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.518     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.12      |
|    n_updates            | 13560     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.367     |
|    value_loss           | 9.62      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 680       |
|    time_elapsed         | 909       |
|    total_timesteps      | 13600     |
| train/                  |           |
|    approx_kl            | 2.1112018 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.433     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.55      |
|    n_updates            | 13580     |
|    policy_gradient_loss | -0.295    |
|    std                  | 0.367     |
|    value_loss           | 7.22      |
---------------------------------------
---------------------------------------
| reward                  | 0.712     |
| reward_contact          | -0.228    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 681       |
|    time_elapsed         | 910       |
|    total_timesteps      | 13620     |
| train/                  |           |
|    approx_kl            | 1.5409355 |
|    clip_fraction        | 0.695     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.571     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.53      |
|    n_updates            | 13600     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.367     |
|    value_loss           | 11.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 682       |
|    time_elapsed         | 912       |
|    total_timesteps      | 13640     |
| train/                  |           |
|    approx_kl            | 5.5417037 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.509     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.94      |
|    n_updates            | 13620     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.368     |
|    value_loss           | 9.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 683       |
|    time_elapsed         | 913       |
|    total_timesteps      | 13660     |
| train/                  |           |
|    approx_kl            | 0.8061016 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.68      |
|    n_updates            | 13640     |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.368     |
|    value_loss           | 16.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 684       |
|    time_elapsed         | 914       |
|    total_timesteps      | 13680     |
| train/                  |           |
|    approx_kl            | 1.6380637 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.108     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.98      |
|    n_updates            | 13660     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.368     |
|    value_loss           | 19.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 685       |
|    time_elapsed         | 916       |
|    total_timesteps      | 13700     |
| train/                  |           |
|    approx_kl            | 1.1884413 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.522     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.19      |
|    n_updates            | 13680     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.368     |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.741     |
| reward_contact          | -0.199    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 686       |
|    time_elapsed         | 917       |
|    total_timesteps      | 13720     |
| train/                  |           |
|    approx_kl            | 1.4276567 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.607     |
|    n_updates            | 13700     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.368     |
|    value_loss           | 3.98      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 687       |
|    time_elapsed         | 918       |
|    total_timesteps      | 13740     |
| train/                  |           |
|    approx_kl            | 1.5174245 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.742     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.72      |
|    n_updates            | 13720     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.368     |
|    value_loss           | 6.39      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 688       |
|    time_elapsed         | 920       |
|    total_timesteps      | 13760     |
| train/                  |           |
|    approx_kl            | 3.5040474 |
|    clip_fraction        | 0.805     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.423     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.65      |
|    n_updates            | 13740     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.368     |
|    value_loss           | 10.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 689       |
|    time_elapsed         | 921       |
|    total_timesteps      | 13780     |
| train/                  |           |
|    approx_kl            | 1.1251925 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.13      |
|    n_updates            | 13760     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.368     |
|    value_loss           | 17        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 690        |
|    time_elapsed         | 922        |
|    total_timesteps      | 13800      |
| train/                  |            |
|    approx_kl            | 0.90045184 |
|    clip_fraction        | 0.69       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.32       |
|    n_updates            | 13780      |
|    policy_gradient_loss | -0.27      |
|    std                  | 0.368      |
|    value_loss           | 23.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.72      |
| reward_contact          | -0.22     |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 691       |
|    time_elapsed         | 924       |
|    total_timesteps      | 13820     |
| train/                  |           |
|    approx_kl            | 2.6498187 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.275     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.46      |
|    n_updates            | 13800     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.368     |
|    value_loss           | 7.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 692       |
|    time_elapsed         | 925       |
|    total_timesteps      | 13840     |
| train/                  |           |
|    approx_kl            | 1.2966323 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.336     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.74      |
|    n_updates            | 13820     |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.368     |
|    value_loss           | 12.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 693       |
|    time_elapsed         | 926       |
|    total_timesteps      | 13860     |
| train/                  |           |
|    approx_kl            | 0.5586621 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.13      |
|    n_updates            | 13840     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.368     |
|    value_loss           | 13.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 694       |
|    time_elapsed         | 928       |
|    total_timesteps      | 13880     |
| train/                  |           |
|    approx_kl            | 2.9029717 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.621     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.57      |
|    n_updates            | 13860     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.368     |
|    value_loss           | 6.53      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 695       |
|    time_elapsed         | 929       |
|    total_timesteps      | 13900     |
| train/                  |           |
|    approx_kl            | 2.4357014 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.408     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.25      |
|    n_updates            | 13880     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.368     |
|    value_loss           | 12.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.743     |
| reward_contact          | -0.197    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 696       |
|    time_elapsed         | 930       |
|    total_timesteps      | 13920     |
| train/                  |           |
|    approx_kl            | 4.4331584 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -0.225    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.2       |
|    n_updates            | 13900     |
|    policy_gradient_loss | -0.291    |
|    std                  | 0.368     |
|    value_loss           | 13.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 697       |
|    time_elapsed         | 932       |
|    total_timesteps      | 13940     |
| train/                  |           |
|    approx_kl            | 1.2918022 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.254     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.78      |
|    n_updates            | 13920     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.368     |
|    value_loss           | 15.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 698       |
|    time_elapsed         | 933       |
|    total_timesteps      | 13960     |
| train/                  |           |
|    approx_kl            | 1.3121213 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.71      |
|    n_updates            | 13940     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 8.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 699       |
|    time_elapsed         | 934       |
|    total_timesteps      | 13980     |
| train/                  |           |
|    approx_kl            | 1.8030115 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.195     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.2       |
|    n_updates            | 13960     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.368     |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 700       |
|    time_elapsed         | 936       |
|    total_timesteps      | 14000     |
| train/                  |           |
|    approx_kl            | 1.6088867 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.891     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.37      |
|    n_updates            | 13980     |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.368     |
|    value_loss           | 1.88      |
---------------------------------------
---------------------------------------
| reward                  | 0.805     |
| reward_contact          | -0.145    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 701       |
|    time_elapsed         | 937       |
|    total_timesteps      | 14020     |
| train/                  |           |
|    approx_kl            | 1.9933331 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.532     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.36      |
|    n_updates            | 14000     |
|    policy_gradient_loss | -0.213    |
|    std                  | 0.368     |
|    value_loss           | 8.87      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 702       |
|    time_elapsed         | 938       |
|    total_timesteps      | 14040     |
| train/                  |           |
|    approx_kl            | 2.4054956 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.397     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.09      |
|    n_updates            | 14020     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 703       |
|    time_elapsed         | 940       |
|    total_timesteps      | 14060     |
| train/                  |           |
|    approx_kl            | 1.4323224 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.531     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.71      |
|    n_updates            | 14040     |
|    policy_gradient_loss | -0.291    |
|    std                  | 0.368     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 704       |
|    time_elapsed         | 941       |
|    total_timesteps      | 14080     |
| train/                  |           |
|    approx_kl            | 1.2565507 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.412     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.72      |
|    n_updates            | 14060     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.368     |
|    value_loss           | 14.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 705        |
|    time_elapsed         | 942        |
|    total_timesteps      | 14100      |
| train/                  |            |
|    approx_kl            | 0.31148264 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.992     |
|    learning_rate        | 0.0003     |
|    loss                 | 14.1       |
|    n_updates            | 14080      |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.368      |
|    value_loss           | 34.1       |
----------------------------------------
---------------------------------------
| reward                  | 0.795     |
| reward_contact          | -0.145    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 706       |
|    time_elapsed         | 944       |
|    total_timesteps      | 14120     |
| train/                  |           |
|    approx_kl            | 2.4729073 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.4       |
|    n_updates            | 14100     |
|    policy_gradient_loss | -0.231    |
|    std                  | 0.368     |
|    value_loss           | 6.78      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 707       |
|    time_elapsed         | 945       |
|    total_timesteps      | 14140     |
| train/                  |           |
|    approx_kl            | 2.3324938 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.347     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 14120     |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.368     |
|    value_loss           | 7.72      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 708      |
|    time_elapsed         | 946      |
|    total_timesteps      | 14160    |
| train/                  |          |
|    approx_kl            | 1.795125 |
|    clip_fraction        | 0.72     |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.118    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.25     |
|    n_updates            | 14140    |
|    policy_gradient_loss | -0.244   |
|    std                  | 0.368    |
|    value_loss           | 14.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 709       |
|    time_elapsed         | 948       |
|    total_timesteps      | 14180     |
| train/                  |           |
|    approx_kl            | 1.7747214 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.31      |
|    n_updates            | 14160     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.368     |
|    value_loss           | 7.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 710       |
|    time_elapsed         | 949       |
|    total_timesteps      | 14200     |
| train/                  |           |
|    approx_kl            | 1.8316525 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.182     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.89      |
|    n_updates            | 14180     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.368     |
|    value_loss           | 10.4      |
---------------------------------------
--------------------------------------
| reward                  | 0.851    |
| reward_contact          | -0.0887  |
| reward_motion           | 0.94     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 711      |
|    time_elapsed         | 950      |
|    total_timesteps      | 14220    |
| train/                  |          |
|    approx_kl            | 1.325776 |
|    clip_fraction        | 0.5      |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.628    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.26     |
|    n_updates            | 14200    |
|    policy_gradient_loss | -0.195   |
|    std                  | 0.368    |
|    value_loss           | 6.5      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 712       |
|    time_elapsed         | 951       |
|    total_timesteps      | 14240     |
| train/                  |           |
|    approx_kl            | 0.2578436 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 14220     |
|    policy_gradient_loss | -0.146    |
|    std                  | 0.368     |
|    value_loss           | 10.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 713       |
|    time_elapsed         | 953       |
|    total_timesteps      | 14260     |
| train/                  |           |
|    approx_kl            | 0.6838021 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.98      |
|    n_updates            | 14240     |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.368     |
|    value_loss           | 9.59      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 714      |
|    time_elapsed         | 954      |
|    total_timesteps      | 14280    |
| train/                  |          |
|    approx_kl            | 5.180992 |
|    clip_fraction        | 0.785    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | -0.403   |
|    learning_rate        | 0.0003   |
|    loss                 | 5.13     |
|    n_updates            | 14260    |
|    policy_gradient_loss | -0.285   |
|    std                  | 0.368    |
|    value_loss           | 13.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 715       |
|    time_elapsed         | 955       |
|    total_timesteps      | 14300     |
| train/                  |           |
|    approx_kl            | 0.8987698 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.59      |
|    n_updates            | 14280     |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.368     |
|    value_loss           | 8.59      |
---------------------------------------
---------------------------------------
| reward                  | 0.883     |
| reward_contact          | -0.0575   |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 716       |
|    time_elapsed         | 957       |
|    total_timesteps      | 14320     |
| train/                  |           |
|    approx_kl            | 0.5297139 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.465     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.27      |
|    n_updates            | 14300     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.368     |
|    value_loss           | 11.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 717       |
|    time_elapsed         | 958       |
|    total_timesteps      | 14340     |
| train/                  |           |
|    approx_kl            | 4.4742126 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.512     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.8       |
|    n_updates            | 14320     |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.368     |
|    value_loss           | 10.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 718      |
|    time_elapsed         | 959      |
|    total_timesteps      | 14360    |
| train/                  |          |
|    approx_kl            | 1.166587 |
|    clip_fraction        | 0.743    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.622    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.68     |
|    n_updates            | 14340    |
|    policy_gradient_loss | -0.212   |
|    std                  | 0.368    |
|    value_loss           | 6.33     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 719       |
|    time_elapsed         | 961       |
|    total_timesteps      | 14380     |
| train/                  |           |
|    approx_kl            | 0.7531762 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.539     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.41      |
|    n_updates            | 14360     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 15.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 720       |
|    time_elapsed         | 962       |
|    total_timesteps      | 14400     |
| train/                  |           |
|    approx_kl            | 2.3524704 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.359     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.68      |
|    n_updates            | 14380     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.368     |
|    value_loss           | 13.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.883      |
| reward_contact          | -0.0575    |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 721        |
|    time_elapsed         | 963        |
|    total_timesteps      | 14420      |
| train/                  |            |
|    approx_kl            | 0.73020655 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.0363     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.12       |
|    n_updates            | 14400      |
|    policy_gradient_loss | -0.242     |
|    std                  | 0.368      |
|    value_loss           | 12.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 722       |
|    time_elapsed         | 965       |
|    total_timesteps      | 14440     |
| train/                  |           |
|    approx_kl            | 1.3701423 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.54      |
|    n_updates            | 14420     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.368     |
|    value_loss           | 10        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 723       |
|    time_elapsed         | 966       |
|    total_timesteps      | 14460     |
| train/                  |           |
|    approx_kl            | 1.8133868 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.519     |
|    n_updates            | 14440     |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.368     |
|    value_loss           | 2.12      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 724       |
|    time_elapsed         | 967       |
|    total_timesteps      | 14480     |
| train/                  |           |
|    approx_kl            | 1.5543833 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.547     |
|    n_updates            | 14460     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.368     |
|    value_loss           | 2.57      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 725        |
|    time_elapsed         | 969        |
|    total_timesteps      | 14500      |
| train/                  |            |
|    approx_kl            | 0.92852384 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | -0.508     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.73       |
|    n_updates            | 14480      |
|    policy_gradient_loss | -0.275     |
|    std                  | 0.368      |
|    value_loss           | 11.6       |
----------------------------------------
----------------------------------------
| reward                  | 0.906      |
| reward_contact          | -0.0345    |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 726        |
|    time_elapsed         | 970        |
|    total_timesteps      | 14520      |
| train/                  |            |
|    approx_kl            | 0.96044457 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.58       |
|    n_updates            | 14500      |
|    policy_gradient_loss | -0.23      |
|    std                  | 0.368      |
|    value_loss           | 8.65       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 727       |
|    time_elapsed         | 971       |
|    total_timesteps      | 14540     |
| train/                  |           |
|    approx_kl            | 3.1052089 |
|    clip_fraction        | 0.748     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.627     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78      |
|    n_updates            | 14520     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.368     |
|    value_loss           | 4.8       |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 728      |
|    time_elapsed         | 973      |
|    total_timesteps      | 14560    |
| train/                  |          |
|    approx_kl            | 0.415545 |
|    clip_fraction        | 0.698    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0.646    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.22     |
|    n_updates            | 14540    |
|    policy_gradient_loss | -0.169   |
|    std                  | 0.368    |
|    value_loss           | 4.04     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 729       |
|    time_elapsed         | 974       |
|    total_timesteps      | 14580     |
| train/                  |           |
|    approx_kl            | 3.1053607 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.481     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 14560     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.368     |
|    value_loss           | 6.79      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 730       |
|    time_elapsed         | 975       |
|    total_timesteps      | 14600     |
| train/                  |           |
|    approx_kl            | 0.5235724 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.792     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.564     |
|    n_updates            | 14580     |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.368     |
|    value_loss           | 2.95      |
---------------------------------------
---------------------------------------
| reward                  | 0.916     |
| reward_contact          | -0.0345   |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 731       |
|    time_elapsed         | 977       |
|    total_timesteps      | 14620     |
| train/                  |           |
|    approx_kl            | 1.3582476 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.271     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.94      |
|    n_updates            | 14600     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.368     |
|    value_loss           | 10.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 732      |
|    time_elapsed         | 978      |
|    total_timesteps      | 14640    |
| train/                  |          |
|    approx_kl            | 1.186704 |
|    clip_fraction        | 0.75     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.222    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.3      |
|    n_updates            | 14620    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.368    |
|    value_loss           | 8.88     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 733        |
|    time_elapsed         | 979        |
|    total_timesteps      | 14660      |
| train/                  |            |
|    approx_kl            | 0.95752126 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.24       |
|    n_updates            | 14640      |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.368      |
|    value_loss           | 9.04       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 734       |
|    time_elapsed         | 981       |
|    total_timesteps      | 14680     |
| train/                  |           |
|    approx_kl            | 0.8837778 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.286     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.98      |
|    n_updates            | 14660     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.368     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 735       |
|    time_elapsed         | 982       |
|    total_timesteps      | 14700     |
| train/                  |           |
|    approx_kl            | 2.4304821 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.23      |
|    n_updates            | 14680     |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.368     |
|    value_loss           | 6.77      |
---------------------------------------
---------------------------------------
| reward                  | 0.916     |
| reward_contact          | -0.0345   |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 736       |
|    time_elapsed         | 983       |
|    total_timesteps      | 14720     |
| train/                  |           |
|    approx_kl            | 2.8067925 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.65      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.43      |
|    n_updates            | 14700     |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.368     |
|    value_loss           | 9.39      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 737       |
|    time_elapsed         | 985       |
|    total_timesteps      | 14740     |
| train/                  |           |
|    approx_kl            | 1.6468518 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.762     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.51      |
|    n_updates            | 14720     |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.368     |
|    value_loss           | 4.34      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 738       |
|    time_elapsed         | 986       |
|    total_timesteps      | 14760     |
| train/                  |           |
|    approx_kl            | 0.5393984 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.314     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.91      |
|    n_updates            | 14740     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.368     |
|    value_loss           | 16.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 739       |
|    time_elapsed         | 987       |
|    total_timesteps      | 14780     |
| train/                  |           |
|    approx_kl            | 0.7408016 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.5       |
|    n_updates            | 14760     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 8.1       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 740       |
|    time_elapsed         | 989       |
|    total_timesteps      | 14800     |
| train/                  |           |
|    approx_kl            | 2.2233832 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.589     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.55      |
|    n_updates            | 14780     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.368     |
|    value_loss           | 7.31      |
---------------------------------------
---------------------------------------
| reward                  | 0.926     |
| reward_contact          | -0.0345   |
| reward_motion           | 0.96      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 741       |
|    time_elapsed         | 990       |
|    total_timesteps      | 14820     |
| train/                  |           |
|    approx_kl            | 1.1498393 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.292     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.5       |
|    n_updates            | 14800     |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.368     |
|    value_loss           | 19.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 742       |
|    time_elapsed         | 991       |
|    total_timesteps      | 14840     |
| train/                  |           |
|    approx_kl            | 2.5037262 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.749     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.8       |
|    n_updates            | 14820     |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.368     |
|    value_loss           | 5.08      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 743        |
|    time_elapsed         | 993        |
|    total_timesteps      | 14860      |
| train/                  |            |
|    approx_kl            | 0.74162054 |
|    clip_fraction        | 0.73       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 1          |
|    n_updates            | 14840      |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.368      |
|    value_loss           | 3.02       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 744       |
|    time_elapsed         | 994       |
|    total_timesteps      | 14880     |
| train/                  |           |
|    approx_kl            | 1.0713196 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.839     |
|    n_updates            | 14860     |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.369     |
|    value_loss           | 3.24      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 745       |
|    time_elapsed         | 995       |
|    total_timesteps      | 14900     |
| train/                  |           |
|    approx_kl            | 1.1344063 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.52      |
|    n_updates            | 14880     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.368     |
|    value_loss           | 4.71      |
---------------------------------------
---------------------------------------
| reward                  | 0.949     |
| reward_contact          | -0.021    |
| reward_motion           | 0.97      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 746       |
|    time_elapsed         | 997       |
|    total_timesteps      | 14920     |
| train/                  |           |
|    approx_kl            | 1.9574902 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.833     |
|    n_updates            | 14900     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.369     |
|    value_loss           | 5.41      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 747       |
|    time_elapsed         | 998       |
|    total_timesteps      | 14940     |
| train/                  |           |
|    approx_kl            | 1.0214074 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.798     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.7       |
|    n_updates            | 14920     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.369     |
|    value_loss           | 9.22      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 748       |
|    time_elapsed         | 999       |
|    total_timesteps      | 14960     |
| train/                  |           |
|    approx_kl            | 0.6935095 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 14940     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.369     |
|    value_loss           | 7.77      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 749       |
|    time_elapsed         | 1001      |
|    total_timesteps      | 14980     |
| train/                  |           |
|    approx_kl            | 1.0634612 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.28      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.46      |
|    n_updates            | 14960     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.369     |
|    value_loss           | 13.5      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 750        |
|    time_elapsed         | 1002       |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.90946215 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 14980      |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.369      |
|    value_loss           | 7.76       |
----------------------------------------
--------------------------------------
| reward                  | 0.936    |
| reward_contact          | -0.0436  |
| reward_motion           | 0.98     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 751      |
|    time_elapsed         | 1003     |
|    total_timesteps      | 15020    |
| train/                  |          |
|    approx_kl            | 0.419452 |
|    clip_fraction        | 0.575    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.639    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.78     |
|    n_updates            | 15000    |
|    policy_gradient_loss | -0.17    |
|    std                  | 0.368    |
|    value_loss           | 5.85     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 752       |
|    time_elapsed         | 1005      |
|    total_timesteps      | 15040     |
| train/                  |           |
|    approx_kl            | 1.3079875 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.461     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 15020     |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.368     |
|    value_loss           | 12.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 753       |
|    time_elapsed         | 1006      |
|    total_timesteps      | 15060     |
| train/                  |           |
|    approx_kl            | 0.8148308 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.472     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.5       |
|    n_updates            | 15040     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.369     |
|    value_loss           | 8.5       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 754       |
|    time_elapsed         | 1007      |
|    total_timesteps      | 15080     |
| train/                  |           |
|    approx_kl            | 0.8387607 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.56      |
|    n_updates            | 15060     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.368     |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 755       |
|    time_elapsed         | 1009      |
|    total_timesteps      | 15100     |
| train/                  |           |
|    approx_kl            | 1.7670107 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.02      |
|    n_updates            | 15080     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.368     |
|    value_loss           | 6.62      |
---------------------------------------
--------------------------------------
| reward                  | 0.936    |
| reward_contact          | -0.0436  |
| reward_motion           | 0.98     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 756      |
|    time_elapsed         | 1010     |
|    total_timesteps      | 15120    |
| train/                  |          |
|    approx_kl            | 1.448874 |
|    clip_fraction        | 0.663    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.412    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.72     |
|    n_updates            | 15100    |
|    policy_gradient_loss | -0.228   |
|    std                  | 0.369    |
|    value_loss           | 5.83     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 757        |
|    time_elapsed         | 1011       |
|    total_timesteps      | 15140      |
| train/                  |            |
|    approx_kl            | 0.44333783 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | -0.164     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.4        |
|    n_updates            | 15120      |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.369      |
|    value_loss           | 12.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 758       |
|    time_elapsed         | 1013      |
|    total_timesteps      | 15160     |
| train/                  |           |
|    approx_kl            | 1.0503825 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.33      |
|    n_updates            | 15140     |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.369     |
|    value_loss           | 4.2       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 759       |
|    time_elapsed         | 1014      |
|    total_timesteps      | 15180     |
| train/                  |           |
|    approx_kl            | 1.4834017 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.686     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.8       |
|    n_updates            | 15160     |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.369     |
|    value_loss           | 5.15      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 760        |
|    time_elapsed         | 1015       |
|    total_timesteps      | 15200      |
| train/                  |            |
|    approx_kl            | 0.27086505 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.3       |
|    n_updates            | 15180      |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.369      |
|    value_loss           | 42.7       |
----------------------------------------
---------------------------------------
| reward                  | 0.899     |
| reward_contact          | -0.0713   |
| reward_motion           | 0.97      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 761       |
|    time_elapsed         | 1017      |
|    total_timesteps      | 15220     |
| train/                  |           |
|    approx_kl            | 1.5722138 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.21      |
|    n_updates            | 15200     |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.369     |
|    value_loss           | 8.29      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 762       |
|    time_elapsed         | 1018      |
|    total_timesteps      | 15240     |
| train/                  |           |
|    approx_kl            | 0.7518723 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.289     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.35      |
|    n_updates            | 15220     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.369     |
|    value_loss           | 12.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 763       |
|    time_elapsed         | 1019      |
|    total_timesteps      | 15260     |
| train/                  |           |
|    approx_kl            | 1.0397257 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.45     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.91      |
|    n_updates            | 15240     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.369     |
|    value_loss           | 20.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 764       |
|    time_elapsed         | 1021      |
|    total_timesteps      | 15280     |
| train/                  |           |
|    approx_kl            | 0.6733367 |
|    clip_fraction        | 0.533     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.526     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.67      |
|    n_updates            | 15260     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.369     |
|    value_loss           | 5.22      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 765       |
|    time_elapsed         | 1022      |
|    total_timesteps      | 15300     |
| train/                  |           |
|    approx_kl            | 1.9497954 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.575     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.25      |
|    n_updates            | 15280     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.369     |
|    value_loss           | 4.54      |
---------------------------------------
--------------------------------------
| reward                  | 0.879    |
| reward_contact          | -0.0713  |
| reward_motion           | 0.95     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 766      |
|    time_elapsed         | 1023     |
|    total_timesteps      | 15320    |
| train/                  |          |
|    approx_kl            | 1.182512 |
|    clip_fraction        | 0.64     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.692    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.48     |
|    n_updates            | 15300    |
|    policy_gradient_loss | -0.248   |
|    std                  | 0.368    |
|    value_loss           | 4.49     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 767       |
|    time_elapsed         | 1025      |
|    total_timesteps      | 15340     |
| train/                  |           |
|    approx_kl            | 0.5254921 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.43      |
|    n_updates            | 15320     |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.368     |
|    value_loss           | 14        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 768       |
|    time_elapsed         | 1026      |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.4212575 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.0172   |
|    learning_rate        | 0.0003    |
|    loss                 | 14.3      |
|    n_updates            | 15340     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.368     |
|    value_loss           | 34.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 769       |
|    time_elapsed         | 1027      |
|    total_timesteps      | 15380     |
| train/                  |           |
|    approx_kl            | 1.3057425 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.411     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.03      |
|    n_updates            | 15360     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.368     |
|    value_loss           | 11.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 770        |
|    time_elapsed         | 1029       |
|    total_timesteps      | 15400      |
| train/                  |            |
|    approx_kl            | 0.17361356 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | -0.107     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.8       |
|    n_updates            | 15380      |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.368      |
|    value_loss           | 37         |
----------------------------------------
----------------------------------------
| reward                  | 0.851      |
| reward_contact          | -0.0987    |
| reward_motion           | 0.95       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 771        |
|    time_elapsed         | 1030       |
|    total_timesteps      | 15420      |
| train/                  |            |
|    approx_kl            | 0.24621749 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.92       |
|    n_updates            | 15400      |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.368      |
|    value_loss           | 5.78       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 772        |
|    time_elapsed         | 1031       |
|    total_timesteps      | 15440      |
| train/                  |            |
|    approx_kl            | 0.37118736 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.96       |
|    n_updates            | 15420      |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.368      |
|    value_loss           | 10.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 773       |
|    time_elapsed         | 1033      |
|    total_timesteps      | 15460     |
| train/                  |           |
|    approx_kl            | 0.8077057 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.455     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.3       |
|    n_updates            | 15440     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 5.74      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 774        |
|    time_elapsed         | 1034       |
|    total_timesteps      | 15480      |
| train/                  |            |
|    approx_kl            | 0.46934968 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.83       |
|    n_updates            | 15460      |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.368      |
|    value_loss           | 14.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 775       |
|    time_elapsed         | 1035      |
|    total_timesteps      | 15500     |
| train/                  |           |
|    approx_kl            | 1.0824893 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.0759   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.56      |
|    n_updates            | 15480     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.368     |
|    value_loss           | 9.83      |
---------------------------------------
---------------------------------------
| reward                  | 0.841     |
| reward_contact          | -0.0987   |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 776       |
|    time_elapsed         | 1037      |
|    total_timesteps      | 15520     |
| train/                  |           |
|    approx_kl            | 1.9073712 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.317     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.34      |
|    n_updates            | 15500     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.368     |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 777       |
|    time_elapsed         | 1038      |
|    total_timesteps      | 15540     |
| train/                  |           |
|    approx_kl            | 1.0506021 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.424     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 15520     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.368     |
|    value_loss           | 8.39      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 778      |
|    time_elapsed         | 1039     |
|    total_timesteps      | 15560    |
| train/                  |          |
|    approx_kl            | 2.523791 |
|    clip_fraction        | 0.775    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.314    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.84     |
|    n_updates            | 15540    |
|    policy_gradient_loss | -0.236   |
|    std                  | 0.368    |
|    value_loss           | 8.75     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 779       |
|    time_elapsed         | 1041      |
|    total_timesteps      | 15580     |
| train/                  |           |
|    approx_kl            | 0.8296959 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.06      |
|    n_updates            | 15560     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.368     |
|    value_loss           | 8.08      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 780       |
|    time_elapsed         | 1042      |
|    total_timesteps      | 15600     |
| train/                  |           |
|    approx_kl            | 3.0971267 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.84      |
|    n_updates            | 15580     |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.368     |
|    value_loss           | 11.3      |
---------------------------------------
----------------------------------------
| reward                  | 0.841      |
| reward_contact          | -0.0987    |
| reward_motion           | 0.94       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 781        |
|    time_elapsed         | 1043       |
|    total_timesteps      | 15620      |
| train/                  |            |
|    approx_kl            | 0.70359385 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.218      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.96       |
|    n_updates            | 15600      |
|    policy_gradient_loss | -0.254     |
|    std                  | 0.368      |
|    value_loss           | 17         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 782       |
|    time_elapsed         | 1045      |
|    total_timesteps      | 15640     |
| train/                  |           |
|    approx_kl            | 3.0942173 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.47      |
|    n_updates            | 15620     |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.368     |
|    value_loss           | 8.52      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 783        |
|    time_elapsed         | 1046       |
|    total_timesteps      | 15660      |
| train/                  |            |
|    approx_kl            | 0.98037636 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 15640      |
|    policy_gradient_loss | -0.24      |
|    std                  | 0.368      |
|    value_loss           | 6.69       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 784       |
|    time_elapsed         | 1047      |
|    total_timesteps      | 15680     |
| train/                  |           |
|    approx_kl            | 0.8469807 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.224    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.7       |
|    n_updates            | 15660     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.368     |
|    value_loss           | 16.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 785       |
|    time_elapsed         | 1049      |
|    total_timesteps      | 15700     |
| train/                  |           |
|    approx_kl            | 3.9263103 |
|    clip_fraction        | 0.753     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.19     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.92      |
|    n_updates            | 15680     |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.368     |
|    value_loss           | 21.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.818     |
| reward_contact          | -0.122    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 786       |
|    time_elapsed         | 1050      |
|    total_timesteps      | 15720     |
| train/                  |           |
|    approx_kl            | 0.6937558 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.4      |
|    n_updates            | 15700     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.368     |
|    value_loss           | 25.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 787       |
|    time_elapsed         | 1051      |
|    total_timesteps      | 15740     |
| train/                  |           |
|    approx_kl            | 0.9541742 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.285     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.2       |
|    n_updates            | 15720     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.368     |
|    value_loss           | 10.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 788      |
|    time_elapsed         | 1053     |
|    total_timesteps      | 15760    |
| train/                  |          |
|    approx_kl            | 1.34445  |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.334    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.32     |
|    n_updates            | 15740    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.368    |
|    value_loss           | 13.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 789       |
|    time_elapsed         | 1054      |
|    total_timesteps      | 15780     |
| train/                  |           |
|    approx_kl            | 1.3725659 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.3       |
|    n_updates            | 15760     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.368     |
|    value_loss           | 5.92      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 790       |
|    time_elapsed         | 1055      |
|    total_timesteps      | 15800     |
| train/                  |           |
|    approx_kl            | 2.1669123 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.5       |
|    n_updates            | 15780     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.368     |
|    value_loss           | 6.9       |
---------------------------------------
---------------------------------------
| reward                  | 0.839     |
| reward_contact          | -0.101    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 791       |
|    time_elapsed         | 1057      |
|    total_timesteps      | 15820     |
| train/                  |           |
|    approx_kl            | 1.2968076 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.45      |
|    n_updates            | 15800     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.368     |
|    value_loss           | 15.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 792       |
|    time_elapsed         | 1058      |
|    total_timesteps      | 15840     |
| train/                  |           |
|    approx_kl            | 0.9904664 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.366     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 15820     |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.368     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 793       |
|    time_elapsed         | 1059      |
|    total_timesteps      | 15860     |
| train/                  |           |
|    approx_kl            | 1.0315427 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.54      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.84      |
|    n_updates            | 15840     |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.368     |
|    value_loss           | 9.52      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 794      |
|    time_elapsed         | 1061     |
|    total_timesteps      | 15880    |
| train/                  |          |
|    approx_kl            | 3.527329 |
|    clip_fraction        | 0.68     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.837    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.48     |
|    n_updates            | 15860    |
|    policy_gradient_loss | -0.214   |
|    std                  | 0.368    |
|    value_loss           | 2.1      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 795       |
|    time_elapsed         | 1062      |
|    total_timesteps      | 15900     |
| train/                  |           |
|    approx_kl            | 1.0376886 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.35      |
|    n_updates            | 15880     |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.368     |
|    value_loss           | 4.99      |
---------------------------------------
--------------------------------------
| reward                  | 0.839    |
| reward_contact          | -0.101   |
| reward_motion           | 0.94     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 796      |
|    time_elapsed         | 1063     |
|    total_timesteps      | 15920    |
| train/                  |          |
|    approx_kl            | 5.093858 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.332    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.95     |
|    n_updates            | 15900    |
|    policy_gradient_loss | -0.294   |
|    std                  | 0.368    |
|    value_loss           | 13.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 797       |
|    time_elapsed         | 1065      |
|    total_timesteps      | 15940     |
| train/                  |           |
|    approx_kl            | 3.3615823 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.397     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.84      |
|    n_updates            | 15920     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.368     |
|    value_loss           | 10.4      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 798        |
|    time_elapsed         | 1066       |
|    total_timesteps      | 15960      |
| train/                  |            |
|    approx_kl            | 0.86774963 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.854      |
|    n_updates            | 15940      |
|    policy_gradient_loss | -0.252     |
|    std                  | 0.369      |
|    value_loss           | 3.77       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 799        |
|    time_elapsed         | 1067       |
|    total_timesteps      | 15980      |
| train/                  |            |
|    approx_kl            | 0.88970774 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 15960      |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.369      |
|    value_loss           | 8.08       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 800        |
|    time_elapsed         | 1069       |
|    total_timesteps      | 16000      |
| train/                  |            |
|    approx_kl            | 0.83285445 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 15980      |
|    policy_gradient_loss | -0.239     |
|    std                  | 0.368      |
|    value_loss           | 12.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.839     |
| reward_contact          | -0.101    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 801       |
|    time_elapsed         | 1070      |
|    total_timesteps      | 16020     |
| train/                  |           |
|    approx_kl            | 1.7447678 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0825    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.12      |
|    n_updates            | 16000     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.368     |
|    value_loss           | 15.6      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 802      |
|    time_elapsed         | 1071     |
|    total_timesteps      | 16040    |
| train/                  |          |
|    approx_kl            | 1.672458 |
|    clip_fraction        | 0.728    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.293    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.68     |
|    n_updates            | 16020    |
|    policy_gradient_loss | -0.26    |
|    std                  | 0.368    |
|    value_loss           | 13.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 803       |
|    time_elapsed         | 1073      |
|    total_timesteps      | 16060     |
| train/                  |           |
|    approx_kl            | 1.4114813 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.518     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.06      |
|    n_updates            | 16040     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.368     |
|    value_loss           | 8.03      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 804      |
|    time_elapsed         | 1074     |
|    total_timesteps      | 16080    |
| train/                  |          |
|    approx_kl            | 4.684655 |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.336    |
|    learning_rate        | 0.0003   |
|    loss                 | 6.22     |
|    n_updates            | 16060    |
|    policy_gradient_loss | -0.266   |
|    std                  | 0.368    |
|    value_loss           | 14.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 805       |
|    time_elapsed         | 1075      |
|    total_timesteps      | 16100     |
| train/                  |           |
|    approx_kl            | 2.1435704 |
|    clip_fraction        | 0.778     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.317     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.48      |
|    n_updates            | 16080     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.368     |
|    value_loss           | 11.8      |
---------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.101    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 806       |
|    time_elapsed         | 1077      |
|    total_timesteps      | 16120     |
| train/                  |           |
|    approx_kl            | 2.5381966 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.531     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.53      |
|    n_updates            | 16100     |
|    policy_gradient_loss | -0.269    |
|    std                  | 0.368     |
|    value_loss           | 9.64      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 807      |
|    time_elapsed         | 1078     |
|    total_timesteps      | 16140    |
| train/                  |          |
|    approx_kl            | 2.194102 |
|    clip_fraction        | 0.738    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.36     |
|    learning_rate        | 0.0003   |
|    loss                 | 4.52     |
|    n_updates            | 16120    |
|    policy_gradient_loss | -0.244   |
|    std                  | 0.368    |
|    value_loss           | 12.7     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 808       |
|    time_elapsed         | 1079      |
|    total_timesteps      | 16160     |
| train/                  |           |
|    approx_kl            | 3.0109584 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.385     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.69      |
|    n_updates            | 16140     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.368     |
|    value_loss           | 8.14      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 809       |
|    time_elapsed         | 1081      |
|    total_timesteps      | 16180     |
| train/                  |           |
|    approx_kl            | 2.2359035 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.342     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.44      |
|    n_updates            | 16160     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.368     |
|    value_loss           | 12.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 810      |
|    time_elapsed         | 1082     |
|    total_timesteps      | 16200    |
| train/                  |          |
|    approx_kl            | 1.822332 |
|    clip_fraction        | 0.76     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.577    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.08     |
|    n_updates            | 16180    |
|    policy_gradient_loss | -0.253   |
|    std                  | 0.368    |
|    value_loss           | 9.96     |
--------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.101    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 811       |
|    time_elapsed         | 1083      |
|    total_timesteps      | 16220     |
| train/                  |           |
|    approx_kl            | 1.5603311 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.35      |
|    n_updates            | 16200     |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.368     |
|    value_loss           | 11.2      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 812        |
|    time_elapsed         | 1085       |
|    total_timesteps      | 16240      |
| train/                  |            |
|    approx_kl            | 0.20628773 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 16220      |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.368      |
|    value_loss           | 29.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 813       |
|    time_elapsed         | 1086      |
|    total_timesteps      | 16260     |
| train/                  |           |
|    approx_kl            | 1.5721074 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.627     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.21      |
|    n_updates            | 16240     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.368     |
|    value_loss           | 6.5       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 814       |
|    time_elapsed         | 1087      |
|    total_timesteps      | 16280     |
| train/                  |           |
|    approx_kl            | 1.3839254 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.267     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.21      |
|    n_updates            | 16260     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.368     |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 815       |
|    time_elapsed         | 1089      |
|    total_timesteps      | 16300     |
| train/                  |           |
|    approx_kl            | 1.8114847 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.462     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.11      |
|    n_updates            | 16280     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.368     |
|    value_loss           | 12.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.839     |
| reward_contact          | -0.101    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 816       |
|    time_elapsed         | 1090      |
|    total_timesteps      | 16320     |
| train/                  |           |
|    approx_kl            | 1.2124071 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.7       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 16300     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.368     |
|    value_loss           | 6.02      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 817      |
|    time_elapsed         | 1091     |
|    total_timesteps      | 16340    |
| train/                  |          |
|    approx_kl            | 4.622121 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.58     |
|    learning_rate        | 0.0003   |
|    loss                 | 2.23     |
|    n_updates            | 16320    |
|    policy_gradient_loss | -0.255   |
|    std                  | 0.368    |
|    value_loss           | 6.2      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 818        |
|    time_elapsed         | 1093       |
|    total_timesteps      | 16360      |
| train/                  |            |
|    approx_kl            | 0.60418695 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.767      |
|    n_updates            | 16340      |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.368      |
|    value_loss           | 2.91       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 819       |
|    time_elapsed         | 1094      |
|    total_timesteps      | 16380     |
| train/                  |           |
|    approx_kl            | 1.7368611 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.541     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.43      |
|    n_updates            | 16360     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.368     |
|    value_loss           | 8.72      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 820       |
|    time_elapsed         | 1095      |
|    total_timesteps      | 16400     |
| train/                  |           |
|    approx_kl            | 1.2548093 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 16380     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.368     |
|    value_loss           | 4.54      |
---------------------------------------
---------------------------------------
| reward                  | 0.839     |
| reward_contact          | -0.101    |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 821       |
|    time_elapsed         | 1097      |
|    total_timesteps      | 16420     |
| train/                  |           |
|    approx_kl            | 0.6223611 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.383     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.6       |
|    n_updates            | 16400     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.368     |
|    value_loss           | 21.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 822       |
|    time_elapsed         | 1098      |
|    total_timesteps      | 16440     |
| train/                  |           |
|    approx_kl            | 1.5147022 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.888     |
|    n_updates            | 16420     |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.368     |
|    value_loss           | 3.8       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 823       |
|    time_elapsed         | 1099      |
|    total_timesteps      | 16460     |
| train/                  |           |
|    approx_kl            | 1.9746866 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.505     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 16440     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.368     |
|    value_loss           | 7.11      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 824       |
|    time_elapsed         | 1101      |
|    total_timesteps      | 16480     |
| train/                  |           |
|    approx_kl            | 1.1387571 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.385     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.17      |
|    n_updates            | 16460     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.368     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 825       |
|    time_elapsed         | 1102      |
|    total_timesteps      | 16500     |
| train/                  |           |
|    approx_kl            | 0.3755599 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.392     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.77      |
|    n_updates            | 16480     |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.368     |
|    value_loss           | 8.21      |
---------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.101    |
| reward_motion           | 0.95      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 826       |
|    time_elapsed         | 1103      |
|    total_timesteps      | 16520     |
| train/                  |           |
|    approx_kl            | 1.3565035 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.29      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.46      |
|    n_updates            | 16500     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.368     |
|    value_loss           | 14.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 827       |
|    time_elapsed         | 1105      |
|    total_timesteps      | 16540     |
| train/                  |           |
|    approx_kl            | 1.4764894 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 16520     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 5.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 828       |
|    time_elapsed         | 1106      |
|    total_timesteps      | 16560     |
| train/                  |           |
|    approx_kl            | 1.5873724 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.822     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 16540     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 3.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 829       |
|    time_elapsed         | 1107      |
|    total_timesteps      | 16580     |
| train/                  |           |
|    approx_kl            | 0.7017298 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.763     |
|    n_updates            | 16560     |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.368     |
|    value_loss           | 2.3       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 830       |
|    time_elapsed         | 1109      |
|    total_timesteps      | 16600     |
| train/                  |           |
|    approx_kl            | 2.1529195 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.365     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.62      |
|    n_updates            | 16580     |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.368     |
|    value_loss           | 14.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.849      |
| reward_contact          | -0.101     |
| reward_motion           | 0.95       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 831        |
|    time_elapsed         | 1110       |
|    total_timesteps      | 16620      |
| train/                  |            |
|    approx_kl            | 0.70451254 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 16600      |
|    policy_gradient_loss | -0.253     |
|    std                  | 0.368      |
|    value_loss           | 5.22       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 832        |
|    time_elapsed         | 1111       |
|    total_timesteps      | 16640      |
| train/                  |            |
|    approx_kl            | 0.47102496 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.597     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.36       |
|    n_updates            | 16620      |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.368      |
|    value_loss           | 14.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 833       |
|    time_elapsed         | 1113      |
|    total_timesteps      | 16660     |
| train/                  |           |
|    approx_kl            | 2.1119347 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0549    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.69      |
|    n_updates            | 16640     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.368     |
|    value_loss           | 18.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 834       |
|    time_elapsed         | 1114      |
|    total_timesteps      | 16680     |
| train/                  |           |
|    approx_kl            | 1.3971847 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.835     |
|    n_updates            | 16660     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.368     |
|    value_loss           | 4.44      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 835      |
|    time_elapsed         | 1115     |
|    total_timesteps      | 16700    |
| train/                  |          |
|    approx_kl            | 2.103593 |
|    clip_fraction        | 0.74     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.298    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.14     |
|    n_updates            | 16680    |
|    policy_gradient_loss | -0.237   |
|    std                  | 0.368    |
|    value_loss           | 10.3     |
--------------------------------------
----------------------------------------
| reward                  | 0.849      |
| reward_contact          | -0.101     |
| reward_motion           | 0.95       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 836        |
|    time_elapsed         | 1117       |
|    total_timesteps      | 16720      |
| train/                  |            |
|    approx_kl            | 0.33765808 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -1.28      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.54       |
|    n_updates            | 16700      |
|    policy_gradient_loss | -0.212     |
|    std                  | 0.367      |
|    value_loss           | 18.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 837       |
|    time_elapsed         | 1118      |
|    total_timesteps      | 16740     |
| train/                  |           |
|    approx_kl            | 1.4275049 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.33      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.87      |
|    n_updates            | 16720     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.368     |
|    value_loss           | 13.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 838       |
|    time_elapsed         | 1119      |
|    total_timesteps      | 16760     |
| train/                  |           |
|    approx_kl            | 1.4384356 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.58      |
|    n_updates            | 16740     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.367     |
|    value_loss           | 11        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 839       |
|    time_elapsed         | 1121      |
|    total_timesteps      | 16780     |
| train/                  |           |
|    approx_kl            | 1.7641388 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.143     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.09      |
|    n_updates            | 16760     |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.367     |
|    value_loss           | 16.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 840       |
|    time_elapsed         | 1122      |
|    total_timesteps      | 16800     |
| train/                  |           |
|    approx_kl            | 1.8399218 |
|    clip_fraction        | 0.653     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.24      |
|    n_updates            | 16780     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 4.97      |
---------------------------------------
---------------------------------------
| reward                  | 0.809     |
| reward_contact          | -0.101    |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 841       |
|    time_elapsed         | 1123      |
|    total_timesteps      | 16820     |
| train/                  |           |
|    approx_kl            | 1.4839512 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -2.74     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.88      |
|    n_updates            | 16800     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.368     |
|    value_loss           | 16.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 842       |
|    time_elapsed         | 1125      |
|    total_timesteps      | 16840     |
| train/                  |           |
|    approx_kl            | 1.2989886 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 16820     |
|    policy_gradient_loss | -0.283    |
|    std                  | 0.368     |
|    value_loss           | 6.34      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 843       |
|    time_elapsed         | 1126      |
|    total_timesteps      | 16860     |
| train/                  |           |
|    approx_kl            | 1.2021722 |
|    clip_fraction        | 0.695     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.303     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.15      |
|    n_updates            | 16840     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.368     |
|    value_loss           | 10.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 844      |
|    time_elapsed         | 1127     |
|    total_timesteps      | 16880    |
| train/                  |          |
|    approx_kl            | 2.389579 |
|    clip_fraction        | 0.703    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.258    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.48     |
|    n_updates            | 16860    |
|    policy_gradient_loss | -0.244   |
|    std                  | 0.368    |
|    value_loss           | 13.7     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 845       |
|    time_elapsed         | 1129      |
|    total_timesteps      | 16900     |
| train/                  |           |
|    approx_kl            | 1.8380854 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.657     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.43      |
|    n_updates            | 16880     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.367     |
|    value_loss           | 5.49      |
---------------------------------------
----------------------------------------
| reward                  | 0.789      |
| reward_contact          | -0.101     |
| reward_motion           | 0.89       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 846        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 16920      |
| train/                  |            |
|    approx_kl            | 0.25343525 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.76      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 16900      |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.367      |
|    value_loss           | 28.1       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 847       |
|    time_elapsed         | 1131      |
|    total_timesteps      | 16940     |
| train/                  |           |
|    approx_kl            | 0.4087084 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.494     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.43      |
|    n_updates            | 16920     |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.367     |
|    value_loss           | 7.59      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 848        |
|    time_elapsed         | 1132       |
|    total_timesteps      | 16960      |
| train/                  |            |
|    approx_kl            | 0.97907585 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 16940      |
|    policy_gradient_loss | -0.231     |
|    std                  | 0.367      |
|    value_loss           | 7.73       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 849       |
|    time_elapsed         | 1134      |
|    total_timesteps      | 16980     |
| train/                  |           |
|    approx_kl            | 1.6338261 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.0003    |
|    loss                 | 1         |
|    n_updates            | 16960     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.367     |
|    value_loss           | 4.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 850       |
|    time_elapsed         | 1135      |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 0.5514429 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 16980     |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.367     |
|    value_loss           | 7.12      |
---------------------------------------
--------------------------------------
| reward                  | 0.811    |
| reward_contact          | -0.0786  |
| reward_motion           | 0.89     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 851      |
|    time_elapsed         | 1136     |
|    total_timesteps      | 17020    |
| train/                  |          |
|    approx_kl            | 2.07537  |
|    clip_fraction        | 0.618    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.506    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.04     |
|    n_updates            | 17000    |
|    policy_gradient_loss | -0.24    |
|    std                  | 0.367    |
|    value_loss           | 6.46     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 852       |
|    time_elapsed         | 1138      |
|    total_timesteps      | 17040     |
| train/                  |           |
|    approx_kl            | 2.1361465 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.712     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.14      |
|    n_updates            | 17020     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.367     |
|    value_loss           | 4.59      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 853       |
|    time_elapsed         | 1139      |
|    total_timesteps      | 17060     |
| train/                  |           |
|    approx_kl            | 1.3752952 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.91      |
|    n_updates            | 17040     |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.367     |
|    value_loss           | 3.71      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 854        |
|    time_elapsed         | 1140       |
|    total_timesteps      | 17080      |
| train/                  |            |
|    approx_kl            | 0.93879205 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.432      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.97       |
|    n_updates            | 17060      |
|    policy_gradient_loss | -0.238     |
|    std                  | 0.367      |
|    value_loss           | 12.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 855       |
|    time_elapsed         | 1142      |
|    total_timesteps      | 17100     |
| train/                  |           |
|    approx_kl            | 1.5448511 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 17080     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.367     |
|    value_loss           | 4.92      |
---------------------------------------
----------------------------------------
| reward                  | 0.771      |
| reward_contact          | -0.0786    |
| reward_motion           | 0.85       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 856        |
|    time_elapsed         | 1143       |
|    total_timesteps      | 17120      |
| train/                  |            |
|    approx_kl            | 0.29243022 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.495     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.36       |
|    n_updates            | 17100      |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.367      |
|    value_loss           | 19.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 857       |
|    time_elapsed         | 1144      |
|    total_timesteps      | 17140     |
| train/                  |           |
|    approx_kl            | 0.9236639 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.424     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.73      |
|    n_updates            | 17120     |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.367     |
|    value_loss           | 6.97      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 858       |
|    time_elapsed         | 1146      |
|    total_timesteps      | 17160     |
| train/                  |           |
|    approx_kl            | 1.0361003 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.000454  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.97      |
|    n_updates            | 17140     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.367     |
|    value_loss           | 14.8      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 859        |
|    time_elapsed         | 1147       |
|    total_timesteps      | 17180      |
| train/                  |            |
|    approx_kl            | 0.39408445 |
|    clip_fraction        | 0.678      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 17160      |
|    policy_gradient_loss | -0.219     |
|    std                  | 0.367      |
|    value_loss           | 6.1        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 860       |
|    time_elapsed         | 1148      |
|    total_timesteps      | 17200     |
| train/                  |           |
|    approx_kl            | 0.8586907 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.642     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.483     |
|    n_updates            | 17180     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.367     |
|    value_loss           | 2.67      |
---------------------------------------
---------------------------------------
| reward                  | 0.799     |
| reward_contact          | -0.0509   |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 861       |
|    time_elapsed         | 1150      |
|    total_timesteps      | 17220     |
| train/                  |           |
|    approx_kl            | 0.8363719 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.817     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0803    |
|    n_updates            | 17200     |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.367     |
|    value_loss           | 1.66      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 862       |
|    time_elapsed         | 1151      |
|    total_timesteps      | 17240     |
| train/                  |           |
|    approx_kl            | 1.9282821 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.7       |
|    n_updates            | 17220     |
|    policy_gradient_loss | -0.302    |
|    std                  | 0.367     |
|    value_loss           | 8.63      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 863       |
|    time_elapsed         | 1152      |
|    total_timesteps      | 17260     |
| train/                  |           |
|    approx_kl            | 1.7815994 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.743     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.183     |
|    n_updates            | 17240     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.367     |
|    value_loss           | 3.48      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 864       |
|    time_elapsed         | 1154      |
|    total_timesteps      | 17280     |
| train/                  |           |
|    approx_kl            | 3.0689552 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.819     |
|    n_updates            | 17260     |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.367     |
|    value_loss           | 4.03      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 865       |
|    time_elapsed         | 1155      |
|    total_timesteps      | 17300     |
| train/                  |           |
|    approx_kl            | 0.4789129 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.636    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.12      |
|    n_updates            | 17280     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.367     |
|    value_loss           | 20.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.809     |
| reward_contact          | -0.0509   |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 866       |
|    time_elapsed         | 1156      |
|    total_timesteps      | 17320     |
| train/                  |           |
|    approx_kl            | 0.6879139 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 17300     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 7.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 867       |
|    time_elapsed         | 1158      |
|    total_timesteps      | 17340     |
| train/                  |           |
|    approx_kl            | 0.4886039 |
|    clip_fraction        | 0.403     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.857     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.528     |
|    n_updates            | 17320     |
|    policy_gradient_loss | -0.0924   |
|    std                  | 0.367     |
|    value_loss           | 2.02      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 868       |
|    time_elapsed         | 1159      |
|    total_timesteps      | 17360     |
| train/                  |           |
|    approx_kl            | 2.5836625 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.607     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.47      |
|    n_updates            | 17340     |
|    policy_gradient_loss | -0.297    |
|    std                  | 0.367     |
|    value_loss           | 6.41      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 869       |
|    time_elapsed         | 1160      |
|    total_timesteps      | 17380     |
| train/                  |           |
|    approx_kl            | 3.1892295 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.884     |
|    n_updates            | 17360     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.367     |
|    value_loss           | 3.45      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 870       |
|    time_elapsed         | 1162      |
|    total_timesteps      | 17400     |
| train/                  |           |
|    approx_kl            | 0.3738061 |
|    clip_fraction        | 0.37      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.369     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 17380     |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.367     |
|    value_loss           | 8.3       |
---------------------------------------
---------------------------------------
| reward                  | 0.837     |
| reward_contact          | -0.0235   |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 871       |
|    time_elapsed         | 1163      |
|    total_timesteps      | 17420     |
| train/                  |           |
|    approx_kl            | 3.4315953 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 17400     |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.367     |
|    value_loss           | 10        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 872       |
|    time_elapsed         | 1164      |
|    total_timesteps      | 17440     |
| train/                  |           |
|    approx_kl            | 1.0499514 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.53      |
|    n_updates            | 17420     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.367     |
|    value_loss           | 7.36      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 873        |
|    time_elapsed         | 1166       |
|    total_timesteps      | 17460      |
| train/                  |            |
|    approx_kl            | 0.78846455 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.0314    |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 17440      |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.367      |
|    value_loss           | 13.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 874       |
|    time_elapsed         | 1167      |
|    total_timesteps      | 17480     |
| train/                  |           |
|    approx_kl            | 1.4725709 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.523     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.05      |
|    n_updates            | 17460     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.367     |
|    value_loss           | 9         |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 875       |
|    time_elapsed         | 1168      |
|    total_timesteps      | 17500     |
| train/                  |           |
|    approx_kl            | 1.2017399 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.321     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.15      |
|    n_updates            | 17480     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 12.3      |
---------------------------------------
--------------------------------------
| reward                  | 0.847    |
| reward_contact          | -0.0235  |
| reward_motion           | 0.87     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 876      |
|    time_elapsed         | 1170     |
|    total_timesteps      | 17520    |
| train/                  |          |
|    approx_kl            | 1.095198 |
|    clip_fraction        | 0.805    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.738    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.646    |
|    n_updates            | 17500    |
|    policy_gradient_loss | -0.239   |
|    std                  | 0.367    |
|    value_loss           | 2.95     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 877      |
|    time_elapsed         | 1171     |
|    total_timesteps      | 17540    |
| train/                  |          |
|    approx_kl            | 2.708303 |
|    clip_fraction        | 0.83     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.174    |
|    learning_rate        | 0.0003   |
|    loss                 | 7.24     |
|    n_updates            | 17520    |
|    policy_gradient_loss | -0.286   |
|    std                  | 0.367    |
|    value_loss           | 17.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 878       |
|    time_elapsed         | 1172      |
|    total_timesteps      | 17560     |
| train/                  |           |
|    approx_kl            | 2.9069788 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.689     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 17540     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.367     |
|    value_loss           | 6.56      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 879       |
|    time_elapsed         | 1174      |
|    total_timesteps      | 17580     |
| train/                  |           |
|    approx_kl            | 1.2850863 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.00931  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.76      |
|    n_updates            | 17560     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.367     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 880       |
|    time_elapsed         | 1175      |
|    total_timesteps      | 17600     |
| train/                  |           |
|    approx_kl            | 1.7444848 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.0537   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.56      |
|    n_updates            | 17580     |
|    policy_gradient_loss | -0.266    |
|    std                  | 0.367     |
|    value_loss           | 17.7      |
---------------------------------------
--------------------------------------
| reward                  | 0.847    |
| reward_contact          | -0.0235  |
| reward_motion           | 0.87     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 881      |
|    time_elapsed         | 1176     |
|    total_timesteps      | 17620    |
| train/                  |          |
|    approx_kl            | 1.727477 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.688    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.27     |
|    n_updates            | 17600    |
|    policy_gradient_loss | -0.291   |
|    std                  | 0.367    |
|    value_loss           | 6.13     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 882        |
|    time_elapsed         | 1178       |
|    total_timesteps      | 17640      |
| train/                  |            |
|    approx_kl            | 0.78188276 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.79       |
|    n_updates            | 17620      |
|    policy_gradient_loss | -0.229     |
|    std                  | 0.367      |
|    value_loss           | 5.48       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 883       |
|    time_elapsed         | 1179      |
|    total_timesteps      | 17660     |
| train/                  |           |
|    approx_kl            | 2.6341522 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.223     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.79      |
|    n_updates            | 17640     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.367     |
|    value_loss           | 14.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 884       |
|    time_elapsed         | 1180      |
|    total_timesteps      | 17680     |
| train/                  |           |
|    approx_kl            | 0.7275156 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.345     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.6       |
|    n_updates            | 17660     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.367     |
|    value_loss           | 7.68      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 885       |
|    time_elapsed         | 1182      |
|    total_timesteps      | 17700     |
| train/                  |           |
|    approx_kl            | 1.1788062 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.17      |
|    n_updates            | 17680     |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.367     |
|    value_loss           | 5.75      |
---------------------------------------
---------------------------------------
| reward                  | 0.87      |
| reward_contact          | 0         |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 886       |
|    time_elapsed         | 1183      |
|    total_timesteps      | 17720     |
| train/                  |           |
|    approx_kl            | 2.1436212 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0379    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.94      |
|    n_updates            | 17700     |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.367     |
|    value_loss           | 20.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 887       |
|    time_elapsed         | 1184      |
|    total_timesteps      | 17740     |
| train/                  |           |
|    approx_kl            | 3.3659503 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.705     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 17720     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.367     |
|    value_loss           | 5.24      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 888       |
|    time_elapsed         | 1185      |
|    total_timesteps      | 17760     |
| train/                  |           |
|    approx_kl            | 1.5547061 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.998     |
|    n_updates            | 17740     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.367     |
|    value_loss           | 4.26      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 889       |
|    time_elapsed         | 1187      |
|    total_timesteps      | 17780     |
| train/                  |           |
|    approx_kl            | 1.1646384 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.01      |
|    n_updates            | 17760     |
|    policy_gradient_loss | -0.225    |
|    std                  | 0.367     |
|    value_loss           | 16.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 890       |
|    time_elapsed         | 1188      |
|    total_timesteps      | 17800     |
| train/                  |           |
|    approx_kl            | 0.9515613 |
|    clip_fraction        | 0.658     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.442     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.58      |
|    n_updates            | 17780     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.367     |
|    value_loss           | 12.4      |
---------------------------------------
--------------------------------------
| reward                  | 0.87     |
| reward_contact          | 0        |
| reward_motion           | 0.87     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 891      |
|    time_elapsed         | 1189     |
|    total_timesteps      | 17820    |
| train/                  |          |
|    approx_kl            | 2.546288 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.44     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.99     |
|    n_updates            | 17800    |
|    policy_gradient_loss | -0.258   |
|    std                  | 0.367    |
|    value_loss           | 7.51     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 892      |
|    time_elapsed         | 1191     |
|    total_timesteps      | 17840    |
| train/                  |          |
|    approx_kl            | 2.810531 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.0109   |
|    learning_rate        | 0.0003   |
|    loss                 | 7.93     |
|    n_updates            | 17820    |
|    policy_gradient_loss | -0.295   |
|    std                  | 0.367    |
|    value_loss           | 19.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 893       |
|    time_elapsed         | 1192      |
|    total_timesteps      | 17860     |
| train/                  |           |
|    approx_kl            | 2.3293085 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 17840     |
|    policy_gradient_loss | -0.29     |
|    std                  | 0.367     |
|    value_loss           | 6.71      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 894       |
|    time_elapsed         | 1193      |
|    total_timesteps      | 17880     |
| train/                  |           |
|    approx_kl            | 1.5361824 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.406     |
|    n_updates            | 17860     |
|    policy_gradient_loss | -0.221    |
|    std                  | 0.367     |
|    value_loss           | 2.38      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 895      |
|    time_elapsed         | 1195     |
|    total_timesteps      | 17900    |
| train/                  |          |
|    approx_kl            | 2.526233 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.921    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.324    |
|    n_updates            | 17880    |
|    policy_gradient_loss | -0.204   |
|    std                  | 0.367    |
|    value_loss           | 1.67     |
--------------------------------------
---------------------------------------
| reward                  | 0.86      |
| reward_contact          | 0         |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 896       |
|    time_elapsed         | 1196      |
|    total_timesteps      | 17920     |
| train/                  |           |
|    approx_kl            | 2.2565944 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.524     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 17900     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.368     |
|    value_loss           | 8.76      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 897       |
|    time_elapsed         | 1197      |
|    total_timesteps      | 17940     |
| train/                  |           |
|    approx_kl            | 1.4119667 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.289     |
|    n_updates            | 17920     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.368     |
|    value_loss           | 2.38      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 898        |
|    time_elapsed         | 1199       |
|    total_timesteps      | 17960      |
| train/                  |            |
|    approx_kl            | 0.98192424 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.57       |
|    n_updates            | 17940      |
|    policy_gradient_loss | -0.254     |
|    std                  | 0.368      |
|    value_loss           | 13.4       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 899      |
|    time_elapsed         | 1200     |
|    total_timesteps      | 17980    |
| train/                  |          |
|    approx_kl            | 2.785714 |
|    clip_fraction        | 0.783    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.296    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.88     |
|    n_updates            | 17960    |
|    policy_gradient_loss | -0.261   |
|    std                  | 0.368    |
|    value_loss           | 7.75     |
--------------------------------------
Num timesteps: 18000
Best mean reward: 14.55 - Last mean reward per episode: 17.29
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 900        |
|    time_elapsed         | 1201       |
|    total_timesteps      | 18000      |
| train/                  |            |
|    approx_kl            | 0.46450225 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 17980      |
|    policy_gradient_loss | -0.249     |
|    std                  | 0.367      |
|    value_loss           | 8.97       |
----------------------------------------
---------------------------------------
| reward                  | 0.86      |
| reward_contact          | 0         |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 901       |
|    time_elapsed         | 1203      |
|    total_timesteps      | 18020     |
| train/                  |           |
|    approx_kl            | 2.2152812 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.79      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.44      |
|    n_updates            | 18000     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.367     |
|    value_loss           | 4.54      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 902        |
|    time_elapsed         | 1204       |
|    total_timesteps      | 18040      |
| train/                  |            |
|    approx_kl            | 0.16961339 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.466      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 18020      |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.367      |
|    value_loss           | 30.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 903       |
|    time_elapsed         | 1205      |
|    total_timesteps      | 18060     |
| train/                  |           |
|    approx_kl            | 0.6740106 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -2.51     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.92      |
|    n_updates            | 18040     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.367     |
|    value_loss           | 19.6      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 904      |
|    time_elapsed         | 1207     |
|    total_timesteps      | 18080    |
| train/                  |          |
|    approx_kl            | 6.05104  |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.271    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.47     |
|    n_updates            | 18060    |
|    policy_gradient_loss | -0.261   |
|    std                  | 0.367    |
|    value_loss           | 8.61     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 905       |
|    time_elapsed         | 1208      |
|    total_timesteps      | 18100     |
| train/                  |           |
|    approx_kl            | 1.0567803 |
|    clip_fraction        | 0.748     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.354     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.19      |
|    n_updates            | 18080     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.367     |
|    value_loss           | 15.3      |
---------------------------------------
--------------------------------------
| reward                  | 0.851    |
| reward_contact          | -0.00924 |
| reward_motion           | 0.86     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 906      |
|    time_elapsed         | 1209     |
|    total_timesteps      | 18120    |
| train/                  |          |
|    approx_kl            | 2.04273  |
|    clip_fraction        | 0.75     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.488    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.88     |
|    n_updates            | 18100    |
|    policy_gradient_loss | -0.267   |
|    std                  | 0.367    |
|    value_loss           | 7.88     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 907       |
|    time_elapsed         | 1211      |
|    total_timesteps      | 18140     |
| train/                  |           |
|    approx_kl            | 1.7056233 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.202     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.64      |
|    n_updates            | 18120     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 9.83      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 908        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 18160      |
| train/                  |            |
|    approx_kl            | 0.36287916 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | -1.36      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.92       |
|    n_updates            | 18140      |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.367      |
|    value_loss           | 18.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 909       |
|    time_elapsed         | 1213      |
|    total_timesteps      | 18180     |
| train/                  |           |
|    approx_kl            | 0.3940521 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.758     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.19      |
|    n_updates            | 18160     |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.367     |
|    value_loss           | 9.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 910       |
|    time_elapsed         | 1215      |
|    total_timesteps      | 18200     |
| train/                  |           |
|    approx_kl            | 0.5399476 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.364    |
|    learning_rate        | 0.0003    |
|    loss                 | 11.3      |
|    n_updates            | 18180     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.367     |
|    value_loss           | 28.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.841      |
| reward_contact          | -0.00924   |
| reward_motion           | 0.85       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 911        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 18220      |
| train/                  |            |
|    approx_kl            | 0.26552904 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.18       |
|    n_updates            | 18200      |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.367      |
|    value_loss           | 16.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 912       |
|    time_elapsed         | 1217      |
|    total_timesteps      | 18240     |
| train/                  |           |
|    approx_kl            | 2.0231519 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.71      |
|    n_updates            | 18220     |
|    policy_gradient_loss | -0.222    |
|    std                  | 0.367     |
|    value_loss           | 4.48      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 913       |
|    time_elapsed         | 1219      |
|    total_timesteps      | 18260     |
| train/                  |           |
|    approx_kl            | 0.5282732 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.72      |
|    n_updates            | 18240     |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.367     |
|    value_loss           | 5.94      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 914       |
|    time_elapsed         | 1220      |
|    total_timesteps      | 18280     |
| train/                  |           |
|    approx_kl            | 0.3138418 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.71      |
|    n_updates            | 18260     |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.367     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 915       |
|    time_elapsed         | 1221      |
|    total_timesteps      | 18300     |
| train/                  |           |
|    approx_kl            | 0.4087265 |
|    clip_fraction        | 0.433     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.515     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.55      |
|    n_updates            | 18280     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.367     |
|    value_loss           | 8.06      |
---------------------------------------
---------------------------------------
| reward                  | 0.841     |
| reward_contact          | -0.00924  |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 916       |
|    time_elapsed         | 1223      |
|    total_timesteps      | 18320     |
| train/                  |           |
|    approx_kl            | 1.3577582 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.773     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.16      |
|    n_updates            | 18300     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.367     |
|    value_loss           | 4.37      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 917        |
|    time_elapsed         | 1224       |
|    total_timesteps      | 18340      |
| train/                  |            |
|    approx_kl            | 0.24675782 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.73       |
|    n_updates            | 18320      |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.367      |
|    value_loss           | 23.3       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 918        |
|    time_elapsed         | 1225       |
|    total_timesteps      | 18360      |
| train/                  |            |
|    approx_kl            | 0.60366964 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.0309     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.77       |
|    n_updates            | 18340      |
|    policy_gradient_loss | -0.251     |
|    std                  | 0.367      |
|    value_loss           | 14.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 919       |
|    time_elapsed         | 1227      |
|    total_timesteps      | 18380     |
| train/                  |           |
|    approx_kl            | 1.3958158 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.426     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.36      |
|    n_updates            | 18360     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.367     |
|    value_loss           | 5.98      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 920      |
|    time_elapsed         | 1228     |
|    total_timesteps      | 18400    |
| train/                  |          |
|    approx_kl            | 0.83965  |
|    clip_fraction        | 0.593    |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.569    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.76     |
|    n_updates            | 18380    |
|    policy_gradient_loss | -0.251   |
|    std                  | 0.367    |
|    value_loss           | 7.67     |
--------------------------------------
---------------------------------------
| reward                  | 0.81      |
| reward_contact          | -0.0404   |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 921       |
|    time_elapsed         | 1229      |
|    total_timesteps      | 18420     |
| train/                  |           |
|    approx_kl            | 1.8139626 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.15      |
|    n_updates            | 18400     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.367     |
|    value_loss           | 5.56      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 922        |
|    time_elapsed         | 1231       |
|    total_timesteps      | 18440      |
| train/                  |            |
|    approx_kl            | 0.20442896 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | -0.812     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.71       |
|    n_updates            | 18420      |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.367      |
|    value_loss           | 15.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 923       |
|    time_elapsed         | 1232      |
|    total_timesteps      | 18460     |
| train/                  |           |
|    approx_kl            | 2.2042313 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.24      |
|    n_updates            | 18440     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.367     |
|    value_loss           | 12        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 924        |
|    time_elapsed         | 1233       |
|    total_timesteps      | 18480      |
| train/                  |            |
|    approx_kl            | 0.76030844 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.52       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.06       |
|    n_updates            | 18460      |
|    policy_gradient_loss | -0.248     |
|    std                  | 0.367      |
|    value_loss           | 8.61       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 925        |
|    time_elapsed         | 1235       |
|    total_timesteps      | 18500      |
| train/                  |            |
|    approx_kl            | 0.47158384 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.236      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.59       |
|    n_updates            | 18480      |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.367      |
|    value_loss           | 15.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.8       |
| reward_contact          | -0.0404   |
| reward_motion           | 0.84      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 926       |
|    time_elapsed         | 1236      |
|    total_timesteps      | 18520     |
| train/                  |           |
|    approx_kl            | 0.7100424 |
|    clip_fraction        | 0.583     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -0.442    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.47      |
|    n_updates            | 18500     |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.367     |
|    value_loss           | 12        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 927       |
|    time_elapsed         | 1237      |
|    total_timesteps      | 18540     |
| train/                  |           |
|    approx_kl            | 1.3427018 |
|    clip_fraction        | 0.51      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.521     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.691     |
|    n_updates            | 18520     |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.367     |
|    value_loss           | 2.83      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 928      |
|    time_elapsed         | 1239     |
|    total_timesteps      | 18560    |
| train/                  |          |
|    approx_kl            | 3.16948  |
|    clip_fraction        | 0.84     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.436    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.12     |
|    n_updates            | 18540    |
|    policy_gradient_loss | -0.299   |
|    std                  | 0.367    |
|    value_loss           | 8.96     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 929        |
|    time_elapsed         | 1240       |
|    total_timesteps      | 18580      |
| train/                  |            |
|    approx_kl            | 0.72807235 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.89       |
|    n_updates            | 18560      |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.367      |
|    value_loss           | 3.64       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 930        |
|    time_elapsed         | 1241       |
|    total_timesteps      | 18600      |
| train/                  |            |
|    approx_kl            | 0.51415986 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | -0.0294    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.31       |
|    n_updates            | 18580      |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.367      |
|    value_loss           | 12.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.787     |
| reward_contact          | -0.053    |
| reward_motion           | 0.84      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 931       |
|    time_elapsed         | 1243      |
|    total_timesteps      | 18620     |
| train/                  |           |
|    approx_kl            | 1.6009821 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.06      |
|    n_updates            | 18600     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.367     |
|    value_loss           | 3.61      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 932        |
|    time_elapsed         | 1244       |
|    total_timesteps      | 18640      |
| train/                  |            |
|    approx_kl            | 0.83520603 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | -0.0736    |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 18620      |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.367      |
|    value_loss           | 11.9       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 933        |
|    time_elapsed         | 1245       |
|    total_timesteps      | 18660      |
| train/                  |            |
|    approx_kl            | 0.48147485 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 18640      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.367      |
|    value_loss           | 4.49       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 934       |
|    time_elapsed         | 1247      |
|    total_timesteps      | 18680     |
| train/                  |           |
|    approx_kl            | 0.7224851 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.627     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.85      |
|    n_updates            | 18660     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.367     |
|    value_loss           | 5.53      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 935        |
|    time_elapsed         | 1248       |
|    total_timesteps      | 18700      |
| train/                  |            |
|    approx_kl            | 0.76814747 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 18680      |
|    policy_gradient_loss | -0.27      |
|    std                  | 0.367      |
|    value_loss           | 10.6       |
----------------------------------------
---------------------------------------
| reward                  | 0.767     |
| reward_contact          | -0.053    |
| reward_motion           | 0.82      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 936       |
|    time_elapsed         | 1249      |
|    total_timesteps      | 18720     |
| train/                  |           |
|    approx_kl            | 0.8497117 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.319     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.65      |
|    n_updates            | 18700     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.367     |
|    value_loss           | 11.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 937       |
|    time_elapsed         | 1251      |
|    total_timesteps      | 18740     |
| train/                  |           |
|    approx_kl            | 1.4159373 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.502     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.66      |
|    n_updates            | 18720     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.367     |
|    value_loss           | 6.82      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 938       |
|    time_elapsed         | 1252      |
|    total_timesteps      | 18760     |
| train/                  |           |
|    approx_kl            | 0.6298537 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.337     |
|    n_updates            | 18740     |
|    policy_gradient_loss | -0.145    |
|    std                  | 0.367     |
|    value_loss           | 1.91      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 939       |
|    time_elapsed         | 1253      |
|    total_timesteps      | 18780     |
| train/                  |           |
|    approx_kl            | 4.0999236 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.755     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.608     |
|    n_updates            | 18760     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.367     |
|    value_loss           | 2.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 940       |
|    time_elapsed         | 1255      |
|    total_timesteps      | 18800     |
| train/                  |           |
|    approx_kl            | 1.2406868 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -0.294    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.53      |
|    n_updates            | 18780     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.367     |
|    value_loss           | 4.86      |
---------------------------------------
---------------------------------------
| reward                  | 0.807     |
| reward_contact          | -0.053    |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 941       |
|    time_elapsed         | 1256      |
|    total_timesteps      | 18820     |
| train/                  |           |
|    approx_kl            | 0.7367089 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.633     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.78      |
|    n_updates            | 18800     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.367     |
|    value_loss           | 5.03      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 942       |
|    time_elapsed         | 1257      |
|    total_timesteps      | 18840     |
| train/                  |           |
|    approx_kl            | 0.6660925 |
|    clip_fraction        | 0.595     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.93     |
|    learning_rate        | 0.0003    |
|    loss                 | 9.26      |
|    n_updates            | 18820     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.367     |
|    value_loss           | 22.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 943       |
|    time_elapsed         | 1259      |
|    total_timesteps      | 18860     |
| train/                  |           |
|    approx_kl            | 0.6686132 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.99      |
|    n_updates            | 18840     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.367     |
|    value_loss           | 6.11      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 944      |
|    time_elapsed         | 1260     |
|    total_timesteps      | 18880    |
| train/                  |          |
|    approx_kl            | 4.026633 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | -1.05    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.02     |
|    n_updates            | 18860    |
|    policy_gradient_loss | -0.277   |
|    std                  | 0.367    |
|    value_loss           | 12.5     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 945        |
|    time_elapsed         | 1261       |
|    total_timesteps      | 18900      |
| train/                  |            |
|    approx_kl            | 0.39672923 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 18880      |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.367      |
|    value_loss           | 7.35       |
----------------------------------------
----------------------------------------
| reward                  | 0.809      |
| reward_contact          | -0.0613    |
| reward_motion           | 0.87       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 946        |
|    time_elapsed         | 1263       |
|    total_timesteps      | 18920      |
| train/                  |            |
|    approx_kl            | 0.63525933 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.03       |
|    n_updates            | 18900      |
|    policy_gradient_loss | -0.249     |
|    std                  | 0.367      |
|    value_loss           | 10.8       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 947       |
|    time_elapsed         | 1264      |
|    total_timesteps      | 18940     |
| train/                  |           |
|    approx_kl            | 1.2440295 |
|    clip_fraction        | 0.663     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.557     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 18920     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.367     |
|    value_loss           | 6.01      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 948       |
|    time_elapsed         | 1265      |
|    total_timesteps      | 18960     |
| train/                  |           |
|    approx_kl            | 1.0799984 |
|    clip_fraction        | 0.623     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.716     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.33      |
|    n_updates            | 18940     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.367     |
|    value_loss           | 5.22      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 949       |
|    time_elapsed         | 1267      |
|    total_timesteps      | 18980     |
| train/                  |           |
|    approx_kl            | 2.7648332 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.379     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.8       |
|    n_updates            | 18960     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.367     |
|    value_loss           | 9.33      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 950       |
|    time_elapsed         | 1268      |
|    total_timesteps      | 19000     |
| train/                  |           |
|    approx_kl            | 0.8937277 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.486     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.5       |
|    n_updates            | 18980     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.367     |
|    value_loss           | 14.6      |
---------------------------------------
--------------------------------------
| reward                  | 0.809    |
| reward_contact          | -0.0613  |
| reward_motion           | 0.87     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 951      |
|    time_elapsed         | 1269     |
|    total_timesteps      | 19020    |
| train/                  |          |
|    approx_kl            | 0.997481 |
|    clip_fraction        | 0.728    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.733    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.01     |
|    n_updates            | 19000    |
|    policy_gradient_loss | -0.202   |
|    std                  | 0.367    |
|    value_loss           | 3.45     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 952       |
|    time_elapsed         | 1271      |
|    total_timesteps      | 19040     |
| train/                  |           |
|    approx_kl            | 1.9205297 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.441     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.37      |
|    n_updates            | 19020     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.367     |
|    value_loss           | 6.38      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 953       |
|    time_elapsed         | 1272      |
|    total_timesteps      | 19060     |
| train/                  |           |
|    approx_kl            | 0.7758526 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.01      |
|    n_updates            | 19040     |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.367     |
|    value_loss           | 3.35      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 954       |
|    time_elapsed         | 1273      |
|    total_timesteps      | 19080     |
| train/                  |           |
|    approx_kl            | 0.5928213 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.232     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.4      |
|    n_updates            | 19060     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.367     |
|    value_loss           | 26.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 955       |
|    time_elapsed         | 1275      |
|    total_timesteps      | 19100     |
| train/                  |           |
|    approx_kl            | 0.3977681 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.772     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 19080     |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.367     |
|    value_loss           | 5.35      |
---------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.0613   |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 956       |
|    time_elapsed         | 1276      |
|    total_timesteps      | 19120     |
| train/                  |           |
|    approx_kl            | 2.2745664 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.157    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.52      |
|    n_updates            | 19100     |
|    policy_gradient_loss | -0.301    |
|    std                  | 0.367     |
|    value_loss           | 14.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 957      |
|    time_elapsed         | 1277     |
|    total_timesteps      | 19140    |
| train/                  |          |
|    approx_kl            | 2.587028 |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.784    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.931    |
|    n_updates            | 19120    |
|    policy_gradient_loss | -0.28    |
|    std                  | 0.367    |
|    value_loss           | 4.32     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 958       |
|    time_elapsed         | 1279      |
|    total_timesteps      | 19160     |
| train/                  |           |
|    approx_kl            | 3.4435997 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.31      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.29      |
|    n_updates            | 19140     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.367     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 959       |
|    time_elapsed         | 1280      |
|    total_timesteps      | 19180     |
| train/                  |           |
|    approx_kl            | 1.1090307 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.853     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.434     |
|    n_updates            | 19160     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 2.45      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 960       |
|    time_elapsed         | 1281      |
|    total_timesteps      | 19200     |
| train/                  |           |
|    approx_kl            | 1.5324739 |
|    clip_fraction        | 0.653     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.498    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.37      |
|    n_updates            | 19180     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 15.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.0613   |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 961       |
|    time_elapsed         | 1283      |
|    total_timesteps      | 19220     |
| train/                  |           |
|    approx_kl            | 1.7408295 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.131     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.63      |
|    n_updates            | 19200     |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.367     |
|    value_loss           | 9.13      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 962       |
|    time_elapsed         | 1284      |
|    total_timesteps      | 19240     |
| train/                  |           |
|    approx_kl            | 2.8336809 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.0498   |
|    learning_rate        | 0.0003    |
|    loss                 | 7.44      |
|    n_updates            | 19220     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.367     |
|    value_loss           | 17.9      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 963        |
|    time_elapsed         | 1285       |
|    total_timesteps      | 19260      |
| train/                  |            |
|    approx_kl            | 0.34739265 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 19240      |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.367      |
|    value_loss           | 27.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 964       |
|    time_elapsed         | 1287      |
|    total_timesteps      | 19280     |
| train/                  |           |
|    approx_kl            | 0.5796513 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.276     |
|    n_updates            | 19260     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.367     |
|    value_loss           | 2.34      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 965       |
|    time_elapsed         | 1288      |
|    total_timesteps      | 19300     |
| train/                  |           |
|    approx_kl            | 3.3172157 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.49      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 19280     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.367     |
|    value_loss           | 8.59      |
---------------------------------------
---------------------------------------
| reward                  | 0.828     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 966       |
|    time_elapsed         | 1289      |
|    total_timesteps      | 19320     |
| train/                  |           |
|    approx_kl            | 1.7051243 |
|    clip_fraction        | 0.813     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.625     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.06      |
|    n_updates            | 19300     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.367     |
|    value_loss           | 4.98      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 967       |
|    time_elapsed         | 1291      |
|    total_timesteps      | 19340     |
| train/                  |           |
|    approx_kl            | 0.9927637 |
|    clip_fraction        | 0.583     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.696     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.66      |
|    n_updates            | 19320     |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.367     |
|    value_loss           | 5.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 968       |
|    time_elapsed         | 1292      |
|    total_timesteps      | 19360     |
| train/                  |           |
|    approx_kl            | 2.7937963 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.24      |
|    n_updates            | 19340     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.367     |
|    value_loss           | 5.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 969       |
|    time_elapsed         | 1293      |
|    total_timesteps      | 19380     |
| train/                  |           |
|    approx_kl            | 2.6113098 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.03      |
|    n_updates            | 19360     |
|    policy_gradient_loss | -0.306    |
|    std                  | 0.367     |
|    value_loss           | 15.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 970       |
|    time_elapsed         | 1295      |
|    total_timesteps      | 19400     |
| train/                  |           |
|    approx_kl            | 3.4680822 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.93      |
|    n_updates            | 19380     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.367     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| reward                  | 0.818     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 971       |
|    time_elapsed         | 1296      |
|    total_timesteps      | 19420     |
| train/                  |           |
|    approx_kl            | 1.0404643 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.712     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.33      |
|    n_updates            | 19400     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.367     |
|    value_loss           | 7.57      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 972      |
|    time_elapsed         | 1297     |
|    total_timesteps      | 19440    |
| train/                  |          |
|    approx_kl            | 5.529129 |
|    clip_fraction        | 0.79     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.264    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.02     |
|    n_updates            | 19420    |
|    policy_gradient_loss | -0.274   |
|    std                  | 0.367    |
|    value_loss           | 9.82     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 973        |
|    time_elapsed         | 1299       |
|    total_timesteps      | 19460      |
| train/                  |            |
|    approx_kl            | 0.68393135 |
|    clip_fraction        | 0.77       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.609      |
|    n_updates            | 19440      |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.367      |
|    value_loss           | 3.3        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 974       |
|    time_elapsed         | 1300      |
|    total_timesteps      | 19480     |
| train/                  |           |
|    approx_kl            | 1.9533424 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.33     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.66      |
|    n_updates            | 19460     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.367     |
|    value_loss           | 24.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 975       |
|    time_elapsed         | 1301      |
|    total_timesteps      | 19500     |
| train/                  |           |
|    approx_kl            | 2.0411274 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.255     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 19480     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.367     |
|    value_loss           | 6.39      |
---------------------------------------
---------------------------------------
| reward                  | 0.818     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 976       |
|    time_elapsed         | 1303      |
|    total_timesteps      | 19520     |
| train/                  |           |
|    approx_kl            | 3.0352933 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.382     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.32      |
|    n_updates            | 19500     |
|    policy_gradient_loss | -0.309    |
|    std                  | 0.367     |
|    value_loss           | 13.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 977       |
|    time_elapsed         | 1304      |
|    total_timesteps      | 19540     |
| train/                  |           |
|    approx_kl            | 1.4658226 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.196     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.15      |
|    n_updates            | 19520     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.367     |
|    value_loss           | 17.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 978       |
|    time_elapsed         | 1305      |
|    total_timesteps      | 19560     |
| train/                  |           |
|    approx_kl            | 1.4423373 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.911     |
|    n_updates            | 19540     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.367     |
|    value_loss           | 3.91      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 979       |
|    time_elapsed         | 1307      |
|    total_timesteps      | 19580     |
| train/                  |           |
|    approx_kl            | 0.3521358 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.416     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.14      |
|    n_updates            | 19560     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.367     |
|    value_loss           | 10.8      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 980      |
|    time_elapsed         | 1308     |
|    total_timesteps      | 19600    |
| train/                  |          |
|    approx_kl            | 2.575397 |
|    clip_fraction        | 0.725    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.338    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.4      |
|    n_updates            | 19580    |
|    policy_gradient_loss | -0.266   |
|    std                  | 0.367    |
|    value_loss           | 13.9     |
--------------------------------------
--------------------------------------
| reward                  | 0.808    |
| reward_contact          | -0.0925  |
| reward_motion           | 0.9      |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 981      |
|    time_elapsed         | 1309     |
|    total_timesteps      | 19620    |
| train/                  |          |
|    approx_kl            | 3.38389  |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.747    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.666    |
|    n_updates            | 19600    |
|    policy_gradient_loss | -0.291   |
|    std                  | 0.367    |
|    value_loss           | 3.74     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 982       |
|    time_elapsed         | 1311      |
|    total_timesteps      | 19640     |
| train/                  |           |
|    approx_kl            | 1.0699872 |
|    clip_fraction        | 0.528     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.12      |
|    n_updates            | 19620     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.367     |
|    value_loss           | 6.6       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 983       |
|    time_elapsed         | 1312      |
|    total_timesteps      | 19660     |
| train/                  |           |
|    approx_kl            | 1.2760963 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 19640     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.367     |
|    value_loss           | 6.26      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 15.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 984      |
|    time_elapsed         | 1313     |
|    total_timesteps      | 19680    |
| train/                  |          |
|    approx_kl            | 1.342853 |
|    clip_fraction        | 0.5      |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.218    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.4      |
|    n_updates            | 19660    |
|    policy_gradient_loss | -0.228   |
|    std                  | 0.367    |
|    value_loss           | 12.6     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 985        |
|    time_elapsed         | 1315       |
|    total_timesteps      | 19700      |
| train/                  |            |
|    approx_kl            | 0.69855404 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 19680      |
|    policy_gradient_loss | -0.259     |
|    std                  | 0.367      |
|    value_loss           | 4.76       |
----------------------------------------
---------------------------------------
| reward                  | 0.808     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 986       |
|    time_elapsed         | 1316      |
|    total_timesteps      | 19720     |
| train/                  |           |
|    approx_kl            | 2.3005445 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.23      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.62      |
|    n_updates            | 19700     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.367     |
|    value_loss           | 15.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 987       |
|    time_elapsed         | 1317      |
|    total_timesteps      | 19740     |
| train/                  |           |
|    approx_kl            | 2.2195385 |
|    clip_fraction        | 0.805     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.74      |
|    n_updates            | 19720     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.367     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 988       |
|    time_elapsed         | 1319      |
|    total_timesteps      | 19760     |
| train/                  |           |
|    approx_kl            | 2.1973011 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.503     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.41      |
|    n_updates            | 19740     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.367     |
|    value_loss           | 6.87      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 989       |
|    time_elapsed         | 1320      |
|    total_timesteps      | 19780     |
| train/                  |           |
|    approx_kl            | 4.4490128 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.349     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.01      |
|    n_updates            | 19760     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.367     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 990       |
|    time_elapsed         | 1321      |
|    total_timesteps      | 19800     |
| train/                  |           |
|    approx_kl            | 2.4265974 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.95      |
|    n_updates            | 19780     |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.367     |
|    value_loss           | 7.04      |
---------------------------------------
---------------------------------------
| reward                  | 0.808     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 991       |
|    time_elapsed         | 1323      |
|    total_timesteps      | 19820     |
| train/                  |           |
|    approx_kl            | 2.6406255 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.592     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 19800     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.367     |
|    value_loss           | 6.33      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 992       |
|    time_elapsed         | 1324      |
|    total_timesteps      | 19840     |
| train/                  |           |
|    approx_kl            | 1.5426486 |
|    clip_fraction        | 0.745     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.702     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.53      |
|    n_updates            | 19820     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.367     |
|    value_loss           | 5.14      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 993        |
|    time_elapsed         | 1325       |
|    total_timesteps      | 19860      |
| train/                  |            |
|    approx_kl            | 0.60189694 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.676      |
|    n_updates            | 19840      |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.367      |
|    value_loss           | 2.6        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 994       |
|    time_elapsed         | 1327      |
|    total_timesteps      | 19880     |
| train/                  |           |
|    approx_kl            | 0.8673892 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.32      |
|    n_updates            | 19860     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.367     |
|    value_loss           | 18.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 995       |
|    time_elapsed         | 1328      |
|    total_timesteps      | 19900     |
| train/                  |           |
|    approx_kl            | 1.0640467 |
|    clip_fraction        | 0.628     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.43      |
|    n_updates            | 19880     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.367     |
|    value_loss           | 4.96      |
---------------------------------------
---------------------------------------
| reward                  | 0.808     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 996       |
|    time_elapsed         | 1329      |
|    total_timesteps      | 19920     |
| train/                  |           |
|    approx_kl            | 1.0008914 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.139     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.39      |
|    n_updates            | 19900     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.367     |
|    value_loss           | 15.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 997       |
|    time_elapsed         | 1331      |
|    total_timesteps      | 19940     |
| train/                  |           |
|    approx_kl            | 1.0450741 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.54      |
|    n_updates            | 19920     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.367     |
|    value_loss           | 8.11      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 998        |
|    time_elapsed         | 1332       |
|    total_timesteps      | 19960      |
| train/                  |            |
|    approx_kl            | 0.85653543 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.374      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.14       |
|    n_updates            | 19940      |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.367      |
|    value_loss           | 9.34       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 15.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 999       |
|    time_elapsed         | 1333      |
|    total_timesteps      | 19980     |
| train/                  |           |
|    approx_kl            | 1.8844445 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.717     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.03      |
|    n_updates            | 19960     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 4.7       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1000       |
|    time_elapsed         | 1335       |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.84241086 |
|    clip_fraction        | 0.668      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.2        |
|    n_updates            | 19980      |
|    policy_gradient_loss | -0.26      |
|    std                  | 0.367      |
|    value_loss           | 17.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.808     |
| reward_contact          | -0.0925   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1001      |
|    time_elapsed         | 1336      |
|    total_timesteps      | 20020     |
| train/                  |           |
|    approx_kl            | 1.3693545 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.422     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.07      |
|    n_updates            | 20000     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.367     |
|    value_loss           | 7.86      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1002      |
|    time_elapsed         | 1337      |
|    total_timesteps      | 20040     |
| train/                  |           |
|    approx_kl            | 0.9576443 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0907    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.65      |
|    n_updates            | 20020     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.367     |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1003      |
|    time_elapsed         | 1339      |
|    total_timesteps      | 20060     |
| train/                  |           |
|    approx_kl            | 2.8677442 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.391     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.97      |
|    n_updates            | 20040     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.367     |
|    value_loss           | 9.09      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1004      |
|    time_elapsed         | 1340      |
|    total_timesteps      | 20080     |
| train/                  |           |
|    approx_kl            | 1.0001026 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.651     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.75      |
|    n_updates            | 20060     |
|    policy_gradient_loss | -0.295    |
|    std                  | 0.367     |
|    value_loss           | 9.05      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1005      |
|    time_elapsed         | 1341      |
|    total_timesteps      | 20100     |
| train/                  |           |
|    approx_kl            | 1.7891047 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0633    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.21      |
|    n_updates            | 20080     |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.367     |
|    value_loss           | 12.1      |
---------------------------------------
--------------------------------------
| reward                  | 0.807    |
| reward_contact          | -0.0832  |
| reward_motion           | 0.89     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1006     |
|    time_elapsed         | 1343     |
|    total_timesteps      | 20120    |
| train/                  |          |
|    approx_kl            | 1.239942 |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.471    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.28     |
|    n_updates            | 20100    |
|    policy_gradient_loss | -0.262   |
|    std                  | 0.367    |
|    value_loss           | 9.78     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1007      |
|    time_elapsed         | 1344      |
|    total_timesteps      | 20140     |
| train/                  |           |
|    approx_kl            | 3.5806892 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.722     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.523     |
|    n_updates            | 20120     |
|    policy_gradient_loss | -0.294    |
|    std                  | 0.367     |
|    value_loss           | 4.39      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1008      |
|    time_elapsed         | 1346      |
|    total_timesteps      | 20160     |
| train/                  |           |
|    approx_kl            | 1.6259613 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.94      |
|    n_updates            | 20140     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 3.7       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1009      |
|    time_elapsed         | 1347      |
|    total_timesteps      | 20180     |
| train/                  |           |
|    approx_kl            | 1.8366749 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 20160     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.367     |
|    value_loss           | 10.7      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1010       |
|    time_elapsed         | 1348       |
|    total_timesteps      | 20200      |
| train/                  |            |
|    approx_kl            | 0.74322134 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.17       |
|    n_updates            | 20180      |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.367      |
|    value_loss           | 6.97       |
----------------------------------------
---------------------------------------
| reward                  | 0.817     |
| reward_contact          | -0.0832   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1011      |
|    time_elapsed         | 1350      |
|    total_timesteps      | 20220     |
| train/                  |           |
|    approx_kl            | 1.0211831 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.69      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.47      |
|    n_updates            | 20200     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.367     |
|    value_loss           | 6.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1012      |
|    time_elapsed         | 1351      |
|    total_timesteps      | 20240     |
| train/                  |           |
|    approx_kl            | 0.8927933 |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.796     |
|    n_updates            | 20220     |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.367     |
|    value_loss           | 4.32      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1013     |
|    time_elapsed         | 1352     |
|    total_timesteps      | 20260    |
| train/                  |          |
|    approx_kl            | 1.439659 |
|    clip_fraction        | 0.79     |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | -0.0385  |
|    learning_rate        | 0.0003   |
|    loss                 | 3.21     |
|    n_updates            | 20240    |
|    policy_gradient_loss | -0.288   |
|    std                  | 0.367    |
|    value_loss           | 11.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1014      |
|    time_elapsed         | 1354      |
|    total_timesteps      | 20280     |
| train/                  |           |
|    approx_kl            | 2.7245858 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.789     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.67      |
|    n_updates            | 20260     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.367     |
|    value_loss           | 5.67      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1015      |
|    time_elapsed         | 1355      |
|    total_timesteps      | 20300     |
| train/                  |           |
|    approx_kl            | 3.1811018 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.475     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.22      |
|    n_updates            | 20280     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.367     |
|    value_loss           | 9.05      |
---------------------------------------
----------------------------------------
| reward                  | 0.827      |
| reward_contact          | -0.0832    |
| reward_motion           | 0.91       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1016       |
|    time_elapsed         | 1356       |
|    total_timesteps      | 20320      |
| train/                  |            |
|    approx_kl            | 0.53516835 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 20300      |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.367      |
|    value_loss           | 11.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1017      |
|    time_elapsed         | 1358      |
|    total_timesteps      | 20340     |
| train/                  |           |
|    approx_kl            | 1.8438545 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.664     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.02      |
|    n_updates            | 20320     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.367     |
|    value_loss           | 6.81      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1018      |
|    time_elapsed         | 1359      |
|    total_timesteps      | 20360     |
| train/                  |           |
|    approx_kl            | 1.0981072 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.761     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.49      |
|    n_updates            | 20340     |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.367     |
|    value_loss           | 5.02      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 16.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1019     |
|    time_elapsed         | 1360     |
|    total_timesteps      | 20380    |
| train/                  |          |
|    approx_kl            | 1.549394 |
|    clip_fraction        | 0.575    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | -0.0178  |
|    learning_rate        | 0.0003   |
|    loss                 | 6.51     |
|    n_updates            | 20360    |
|    policy_gradient_loss | -0.231   |
|    std                  | 0.367    |
|    value_loss           | 18.2     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1020       |
|    time_elapsed         | 1362       |
|    total_timesteps      | 20400      |
| train/                  |            |
|    approx_kl            | 0.71227187 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.63       |
|    n_updates            | 20380      |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.367      |
|    value_loss           | 6.71       |
----------------------------------------
---------------------------------------
| reward                  | 0.848     |
| reward_contact          | -0.052    |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1021      |
|    time_elapsed         | 1363      |
|    total_timesteps      | 20420     |
| train/                  |           |
|    approx_kl            | 2.2691922 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.26      |
|    n_updates            | 20400     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.367     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 16.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1022      |
|    time_elapsed         | 1364      |
|    total_timesteps      | 20440     |
| train/                  |           |
|    approx_kl            | 1.7663773 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.7       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.03      |
|    n_updates            | 20420     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.367     |
|    value_loss           | 6.21      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1023      |
|    time_elapsed         | 1366      |
|    total_timesteps      | 20460     |
| train/                  |           |
|    approx_kl            | 1.8133471 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.279     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.93      |
|    n_updates            | 20440     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.367     |
|    value_loss           | 17.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1024      |
|    time_elapsed         | 1367      |
|    total_timesteps      | 20480     |
| train/                  |           |
|    approx_kl            | 2.8294606 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.51      |
|    n_updates            | 20460     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.367     |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1025      |
|    time_elapsed         | 1368      |
|    total_timesteps      | 20500     |
| train/                  |           |
|    approx_kl            | 1.1633073 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.565     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.81      |
|    n_updates            | 20480     |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.367     |
|    value_loss           | 7.85      |
---------------------------------------
---------------------------------------
| reward                  | 0.858     |
| reward_contact          | -0.052    |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1026      |
|    time_elapsed         | 1370      |
|    total_timesteps      | 20520     |
| train/                  |           |
|    approx_kl            | 1.6817791 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.168     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.94      |
|    n_updates            | 20500     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.367     |
|    value_loss           | 13.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1027      |
|    time_elapsed         | 1371      |
|    total_timesteps      | 20540     |
| train/                  |           |
|    approx_kl            | 1.4492959 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.23      |
|    n_updates            | 20520     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.367     |
|    value_loss           | 13.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1028      |
|    time_elapsed         | 1372      |
|    total_timesteps      | 20560     |
| train/                  |           |
|    approx_kl            | 3.8658204 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 20540     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.367     |
|    value_loss           | 8.11      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1029      |
|    time_elapsed         | 1374      |
|    total_timesteps      | 20580     |
| train/                  |           |
|    approx_kl            | 2.9831471 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 20560     |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.367     |
|    value_loss           | 3.72      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1030       |
|    time_elapsed         | 1375       |
|    total_timesteps      | 20600      |
| train/                  |            |
|    approx_kl            | 0.97209895 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.67       |
|    n_updates            | 20580      |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.367      |
|    value_loss           | 7.86       |
----------------------------------------
---------------------------------------
| reward                  | 0.851     |
| reward_contact          | -0.0394   |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1031      |
|    time_elapsed         | 1376      |
|    total_timesteps      | 20620     |
| train/                  |           |
|    approx_kl            | 0.6275121 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.322     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.23      |
|    n_updates            | 20600     |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.367     |
|    value_loss           | 15.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1032      |
|    time_elapsed         | 1378      |
|    total_timesteps      | 20640     |
| train/                  |           |
|    approx_kl            | 0.7290599 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.698     |
|    n_updates            | 20620     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.367     |
|    value_loss           | 3.06      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1033      |
|    time_elapsed         | 1379      |
|    total_timesteps      | 20660     |
| train/                  |           |
|    approx_kl            | 1.0238789 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.491     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.31      |
|    n_updates            | 20640     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.367     |
|    value_loss           | 14.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1034      |
|    time_elapsed         | 1380      |
|    total_timesteps      | 20680     |
| train/                  |           |
|    approx_kl            | 0.8864115 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.373     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.67      |
|    n_updates            | 20660     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.367     |
|    value_loss           | 11.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1035      |
|    time_elapsed         | 1382      |
|    total_timesteps      | 20700     |
| train/                  |           |
|    approx_kl            | 1.5904546 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.692     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.03      |
|    n_updates            | 20680     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.367     |
|    value_loss           | 4.85      |
---------------------------------------
---------------------------------------
| reward                  | 0.861     |
| reward_contact          | -0.0394   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1036      |
|    time_elapsed         | 1383      |
|    total_timesteps      | 20720     |
| train/                  |           |
|    approx_kl            | 1.9458731 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -0.468    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.35      |
|    n_updates            | 20700     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.367     |
|    value_loss           | 15        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1037      |
|    time_elapsed         | 1384      |
|    total_timesteps      | 20740     |
| train/                  |           |
|    approx_kl            | 3.5767326 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.84      |
|    n_updates            | 20720     |
|    policy_gradient_loss | -0.277    |
|    std                  | 0.367     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1038      |
|    time_elapsed         | 1386      |
|    total_timesteps      | 20760     |
| train/                  |           |
|    approx_kl            | 2.1323774 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.374     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.73      |
|    n_updates            | 20740     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.367     |
|    value_loss           | 7.91      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1039      |
|    time_elapsed         | 1387      |
|    total_timesteps      | 20780     |
| train/                  |           |
|    approx_kl            | 0.6377555 |
|    clip_fraction        | 0.615     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.9      |
|    n_updates            | 20760     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.367     |
|    value_loss           | 30.4      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1040       |
|    time_elapsed         | 1388       |
|    total_timesteps      | 20800      |
| train/                  |            |
|    approx_kl            | 0.77430046 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.259      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.24       |
|    n_updates            | 20780      |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.367      |
|    value_loss           | 12.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.841     |
| reward_contact          | -0.0394   |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1041      |
|    time_elapsed         | 1390      |
|    total_timesteps      | 20820     |
| train/                  |           |
|    approx_kl            | 2.5632658 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.176    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.13      |
|    n_updates            | 20800     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.367     |
|    value_loss           | 20.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1042      |
|    time_elapsed         | 1391      |
|    total_timesteps      | 20840     |
| train/                  |           |
|    approx_kl            | 2.5505948 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.74      |
|    n_updates            | 20820     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.367     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1043      |
|    time_elapsed         | 1392      |
|    total_timesteps      | 20860     |
| train/                  |           |
|    approx_kl            | 0.8408886 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 20840     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.367     |
|    value_loss           | 4         |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1044      |
|    time_elapsed         | 1394      |
|    total_timesteps      | 20880     |
| train/                  |           |
|    approx_kl            | 2.4688733 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.0508   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.2       |
|    n_updates            | 20860     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.367     |
|    value_loss           | 19.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1045      |
|    time_elapsed         | 1395      |
|    total_timesteps      | 20900     |
| train/                  |           |
|    approx_kl            | 2.4802935 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.219     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.91      |
|    n_updates            | 20880     |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.366     |
|    value_loss           | 15        |
---------------------------------------
---------------------------------------
| reward                  | 0.859     |
| reward_contact          | -0.0312   |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1046      |
|    time_elapsed         | 1396      |
|    total_timesteps      | 20920     |
| train/                  |           |
|    approx_kl            | 0.6504687 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.124    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.19      |
|    n_updates            | 20900     |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.366     |
|    value_loss           | 16.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1047      |
|    time_elapsed         | 1398      |
|    total_timesteps      | 20940     |
| train/                  |           |
|    approx_kl            | 2.0005064 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.675     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.25      |
|    n_updates            | 20920     |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.366     |
|    value_loss           | 5.44      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1048      |
|    time_elapsed         | 1399      |
|    total_timesteps      | 20960     |
| train/                  |           |
|    approx_kl            | 2.6590135 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.66      |
|    n_updates            | 20940     |
|    policy_gradient_loss | -0.299    |
|    std                  | 0.366     |
|    value_loss           | 5.74      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1049      |
|    time_elapsed         | 1400      |
|    total_timesteps      | 20980     |
| train/                  |           |
|    approx_kl            | 2.9305236 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.33      |
|    n_updates            | 20960     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.366     |
|    value_loss           | 6.76      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1050     |
|    time_elapsed         | 1402     |
|    total_timesteps      | 21000    |
| train/                  |          |
|    approx_kl            | 2.859227 |
|    clip_fraction        | 0.62     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.355    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.69     |
|    n_updates            | 20980    |
|    policy_gradient_loss | -0.232   |
|    std                  | 0.366    |
|    value_loss           | 13.9     |
--------------------------------------
--------------------------------------
| reward                  | 0.859    |
| reward_contact          | -0.0312  |
| reward_motion           | 0.89     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1051     |
|    time_elapsed         | 1403     |
|    total_timesteps      | 21020    |
| train/                  |          |
|    approx_kl            | 1.857357 |
|    clip_fraction        | 0.74     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.48     |
|    learning_rate        | 0.0003   |
|    loss                 | 4.33     |
|    n_updates            | 21000    |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.367    |
|    value_loss           | 11.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1052      |
|    time_elapsed         | 1404      |
|    total_timesteps      | 21040     |
| train/                  |           |
|    approx_kl            | 1.2808653 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.952     |
|    n_updates            | 21020     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.367     |
|    value_loss           | 4.5       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1053      |
|    time_elapsed         | 1406      |
|    total_timesteps      | 21060     |
| train/                  |           |
|    approx_kl            | 1.8450947 |
|    clip_fraction        | 0.66      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.439    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.08      |
|    n_updates            | 21040     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 7.73      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1054      |
|    time_elapsed         | 1407      |
|    total_timesteps      | 21080     |
| train/                  |           |
|    approx_kl            | 1.0492002 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.413     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 21060     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.367     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1055      |
|    time_elapsed         | 1408      |
|    total_timesteps      | 21100     |
| train/                  |           |
|    approx_kl            | 1.3649979 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.822     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.985     |
|    n_updates            | 21080     |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.367     |
|    value_loss           | 3.14      |
---------------------------------------
---------------------------------------
| reward                  | 0.859     |
| reward_contact          | -0.0312   |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1056      |
|    time_elapsed         | 1410      |
|    total_timesteps      | 21120     |
| train/                  |           |
|    approx_kl            | 1.1752369 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.397     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.07      |
|    n_updates            | 21100     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.367     |
|    value_loss           | 14.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1057      |
|    time_elapsed         | 1411      |
|    total_timesteps      | 21140     |
| train/                  |           |
|    approx_kl            | 2.0597024 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.496     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.46      |
|    n_updates            | 21120     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 8.31      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1058      |
|    time_elapsed         | 1412      |
|    total_timesteps      | 21160     |
| train/                  |           |
|    approx_kl            | 1.2324294 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.32      |
|    n_updates            | 21140     |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.367     |
|    value_loss           | 6.46      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1059      |
|    time_elapsed         | 1414      |
|    total_timesteps      | 21180     |
| train/                  |           |
|    approx_kl            | 2.4322991 |
|    clip_fraction        | 0.663     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.29      |
|    learning_rate        | 0.0003    |
|    loss                 | 5         |
|    n_updates            | 21160     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.367     |
|    value_loss           | 13.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1060       |
|    time_elapsed         | 1415       |
|    total_timesteps      | 21200      |
| train/                  |            |
|    approx_kl            | 0.79894483 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.32       |
|    n_updates            | 21180      |
|    policy_gradient_loss | -0.251     |
|    std                  | 0.366      |
|    value_loss           | 7.02       |
----------------------------------------
---------------------------------------
| reward                  | 0.869     |
| reward_contact          | -0.0312   |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1061      |
|    time_elapsed         | 1416      |
|    total_timesteps      | 21220     |
| train/                  |           |
|    approx_kl            | 1.9406861 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 21200     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.366     |
|    value_loss           | 5.54      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1062       |
|    time_elapsed         | 1418       |
|    total_timesteps      | 21240      |
| train/                  |            |
|    approx_kl            | 0.81207305 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.438      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.85       |
|    n_updates            | 21220      |
|    policy_gradient_loss | -0.26      |
|    std                  | 0.366      |
|    value_loss           | 8.8        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1063      |
|    time_elapsed         | 1419      |
|    total_timesteps      | 21260     |
| train/                  |           |
|    approx_kl            | 2.2835577 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.07      |
|    n_updates            | 21240     |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.366     |
|    value_loss           | 5.55      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1064      |
|    time_elapsed         | 1420      |
|    total_timesteps      | 21280     |
| train/                  |           |
|    approx_kl            | 1.2333392 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.425     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.64      |
|    n_updates            | 21260     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.366     |
|    value_loss           | 15.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1065      |
|    time_elapsed         | 1422      |
|    total_timesteps      | 21300     |
| train/                  |           |
|    approx_kl            | 2.9983175 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.708     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.81      |
|    n_updates            | 21280     |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.366     |
|    value_loss           | 5.25      |
---------------------------------------
---------------------------------------
| reward                  | 0.9       |
| reward_contact          | 0         |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1066      |
|    time_elapsed         | 1423      |
|    total_timesteps      | 21320     |
| train/                  |           |
|    approx_kl            | 1.6302366 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.411     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.04      |
|    n_updates            | 21300     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.366     |
|    value_loss           | 15.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1067      |
|    time_elapsed         | 1424      |
|    total_timesteps      | 21340     |
| train/                  |           |
|    approx_kl            | 3.0985763 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.49      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.57      |
|    n_updates            | 21320     |
|    policy_gradient_loss | -0.303    |
|    std                  | 0.366     |
|    value_loss           | 11.6      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1068       |
|    time_elapsed         | 1426       |
|    total_timesteps      | 21360      |
| train/                  |            |
|    approx_kl            | 0.42424846 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.74       |
|    n_updates            | 21340      |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.366      |
|    value_loss           | 15.9       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1069       |
|    time_elapsed         | 1427       |
|    total_timesteps      | 21380      |
| train/                  |            |
|    approx_kl            | 0.73494273 |
|    clip_fraction        | 0.653      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 21360      |
|    policy_gradient_loss | -0.237     |
|    std                  | 0.366      |
|    value_loss           | 4.66       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1070     |
|    time_elapsed         | 1428     |
|    total_timesteps      | 21400    |
| train/                  |          |
|    approx_kl            | 1.611995 |
|    clip_fraction        | 0.69     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.347    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.6      |
|    n_updates            | 21380    |
|    policy_gradient_loss | -0.226   |
|    std                  | 0.366    |
|    value_loss           | 9.78     |
--------------------------------------
---------------------------------------
| reward                  | 0.91      |
| reward_contact          | 0         |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1071      |
|    time_elapsed         | 1430      |
|    total_timesteps      | 21420     |
| train/                  |           |
|    approx_kl            | 1.6013888 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.543     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.28      |
|    n_updates            | 21400     |
|    policy_gradient_loss | -0.293    |
|    std                  | 0.366     |
|    value_loss           | 4.96      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1072      |
|    time_elapsed         | 1431      |
|    total_timesteps      | 21440     |
| train/                  |           |
|    approx_kl            | 1.1400784 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.814     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.343     |
|    n_updates            | 21420     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.366     |
|    value_loss           | 2.69      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1073      |
|    time_elapsed         | 1432      |
|    total_timesteps      | 21460     |
| train/                  |           |
|    approx_kl            | 1.3678263 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.665     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 21440     |
|    policy_gradient_loss | -0.305    |
|    std                  | 0.366     |
|    value_loss           | 7.21      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1074      |
|    time_elapsed         | 1434      |
|    total_timesteps      | 21480     |
| train/                  |           |
|    approx_kl            | 2.2713556 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.388     |
|    n_updates            | 21460     |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.366     |
|    value_loss           | 2.13      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1075     |
|    time_elapsed         | 1435     |
|    total_timesteps      | 21500    |
| train/                  |          |
|    approx_kl            | 1.854343 |
|    clip_fraction        | 0.76     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.514    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.5      |
|    n_updates            | 21480    |
|    policy_gradient_loss | -0.231   |
|    std                  | 0.366    |
|    value_loss           | 7.89     |
--------------------------------------
---------------------------------------
| reward                  | 0.91      |
| reward_contact          | 0         |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1076      |
|    time_elapsed         | 1436      |
|    total_timesteps      | 21520     |
| train/                  |           |
|    approx_kl            | 1.1974633 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.0371   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.89      |
|    n_updates            | 21500     |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.366     |
|    value_loss           | 17.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1077      |
|    time_elapsed         | 1438      |
|    total_timesteps      | 21540     |
| train/                  |           |
|    approx_kl            | 1.8617337 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.25      |
|    n_updates            | 21520     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.366     |
|    value_loss           | 6.58      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1078      |
|    time_elapsed         | 1439      |
|    total_timesteps      | 21560     |
| train/                  |           |
|    approx_kl            | 1.7608111 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.502     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.27      |
|    n_updates            | 21540     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.366     |
|    value_loss           | 8.68      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1079      |
|    time_elapsed         | 1440      |
|    total_timesteps      | 21580     |
| train/                  |           |
|    approx_kl            | 1.1293268 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.973     |
|    n_updates            | 21560     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.366     |
|    value_loss           | 3.62      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1080      |
|    time_elapsed         | 1442      |
|    total_timesteps      | 21600     |
| train/                  |           |
|    approx_kl            | 2.0655224 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.594     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 21580     |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.366     |
|    value_loss           | 8.98      |
---------------------------------------
---------------------------------------
| reward                  | 0.92      |
| reward_contact          | 0         |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1081      |
|    time_elapsed         | 1443      |
|    total_timesteps      | 21620     |
| train/                  |           |
|    approx_kl            | 0.9287505 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.585     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.98      |
|    n_updates            | 21600     |
|    policy_gradient_loss | -0.285    |
|    std                  | 0.366     |
|    value_loss           | 8.43      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1082     |
|    time_elapsed         | 1444     |
|    total_timesteps      | 21640    |
| train/                  |          |
|    approx_kl            | 0.821104 |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.799    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.918    |
|    n_updates            | 21620    |
|    policy_gradient_loss | -0.209   |
|    std                  | 0.366    |
|    value_loss           | 3.73     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1083       |
|    time_elapsed         | 1446       |
|    total_timesteps      | 21660      |
| train/                  |            |
|    approx_kl            | 0.86966383 |
|    clip_fraction        | 0.688      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.0108    |
|    learning_rate        | 0.0003     |
|    loss                 | 5.33       |
|    n_updates            | 21640      |
|    policy_gradient_loss | -0.231     |
|    std                  | 0.366      |
|    value_loss           | 15.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1084      |
|    time_elapsed         | 1447      |
|    total_timesteps      | 21680     |
| train/                  |           |
|    approx_kl            | 1.3321804 |
|    clip_fraction        | 0.753     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.304     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.24      |
|    n_updates            | 21660     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.366     |
|    value_loss           | 14        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1085       |
|    time_elapsed         | 1448       |
|    total_timesteps      | 21700      |
| train/                  |            |
|    approx_kl            | 0.78500193 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.474      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.48       |
|    n_updates            | 21680      |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.366      |
|    value_loss           | 12         |
----------------------------------------
---------------------------------------
| reward                  | 0.92      |
| reward_contact          | 0         |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1086      |
|    time_elapsed         | 1450      |
|    total_timesteps      | 21720     |
| train/                  |           |
|    approx_kl            | 3.3572862 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.18      |
|    n_updates            | 21700     |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.366     |
|    value_loss           | 7.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1087      |
|    time_elapsed         | 1451      |
|    total_timesteps      | 21740     |
| train/                  |           |
|    approx_kl            | 1.9371372 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.737     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.53      |
|    n_updates            | 21720     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.366     |
|    value_loss           | 4.51      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1088      |
|    time_elapsed         | 1452      |
|    total_timesteps      | 21760     |
| train/                  |           |
|    approx_kl            | 1.3055851 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.73      |
|    n_updates            | 21740     |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.366     |
|    value_loss           | 5.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1089      |
|    time_elapsed         | 1454      |
|    total_timesteps      | 21780     |
| train/                  |           |
|    approx_kl            | 3.4531784 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.526     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.79      |
|    n_updates            | 21760     |
|    policy_gradient_loss | -0.283    |
|    std                  | 0.366     |
|    value_loss           | 8.72      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1090      |
|    time_elapsed         | 1455      |
|    total_timesteps      | 21800     |
| train/                  |           |
|    approx_kl            | 2.3099644 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.158     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.29      |
|    n_updates            | 21780     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.366     |
|    value_loss           | 8.71      |
---------------------------------------
----------------------------------------
| reward                  | 0.92       |
| reward_contact          | 0          |
| reward_motion           | 0.92       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1091       |
|    time_elapsed         | 1456       |
|    total_timesteps      | 21820      |
| train/                  |            |
|    approx_kl            | 0.90642494 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.97       |
|    n_updates            | 21800      |
|    policy_gradient_loss | -0.252     |
|    std                  | 0.366      |
|    value_loss           | 8.4        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1092      |
|    time_elapsed         | 1458      |
|    total_timesteps      | 21840     |
| train/                  |           |
|    approx_kl            | 0.8462168 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.135    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.14      |
|    n_updates            | 21820     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.366     |
|    value_loss           | 8.82      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1093     |
|    time_elapsed         | 1459     |
|    total_timesteps      | 21860    |
| train/                  |          |
|    approx_kl            | 3.933134 |
|    clip_fraction        | 0.68     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.503    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.27     |
|    n_updates            | 21840    |
|    policy_gradient_loss | -0.217   |
|    std                  | 0.366    |
|    value_loss           | 8.5      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1094       |
|    time_elapsed         | 1460       |
|    total_timesteps      | 21880      |
| train/                  |            |
|    approx_kl            | 0.46975002 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 21860      |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.366      |
|    value_loss           | 8.02       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1095      |
|    time_elapsed         | 1462      |
|    total_timesteps      | 21900     |
| train/                  |           |
|    approx_kl            | 2.9656868 |
|    clip_fraction        | 0.758     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.68      |
|    n_updates            | 21880     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.366     |
|    value_loss           | 14.4      |
---------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1096      |
|    time_elapsed         | 1463      |
|    total_timesteps      | 21920     |
| train/                  |           |
|    approx_kl            | 3.5688646 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.27      |
|    n_updates            | 21900     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.366     |
|    value_loss           | 9.45      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1097      |
|    time_elapsed         | 1464      |
|    total_timesteps      | 21940     |
| train/                  |           |
|    approx_kl            | 3.7789955 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.477     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 21920     |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.366     |
|    value_loss           | 7.38      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1098       |
|    time_elapsed         | 1466       |
|    total_timesteps      | 21960      |
| train/                  |            |
|    approx_kl            | 0.65321803 |
|    clip_fraction        | 0.625      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00432    |
|    n_updates            | 21940      |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.366      |
|    value_loss           | 0.93       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1099      |
|    time_elapsed         | 1467      |
|    total_timesteps      | 21980     |
| train/                  |           |
|    approx_kl            | 1.2499613 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.514     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 21960     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.366     |
|    value_loss           | 6.98      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1100      |
|    time_elapsed         | 1468      |
|    total_timesteps      | 22000     |
| train/                  |           |
|    approx_kl            | 1.7378792 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.97      |
|    n_updates            | 21980     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.366     |
|    value_loss           | 9.74      |
---------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1101      |
|    time_elapsed         | 1470      |
|    total_timesteps      | 22020     |
| train/                  |           |
|    approx_kl            | 1.7269404 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.256     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 22000     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.366     |
|    value_loss           | 9.85      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1102      |
|    time_elapsed         | 1471      |
|    total_timesteps      | 22040     |
| train/                  |           |
|    approx_kl            | 1.3437948 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.66      |
|    n_updates            | 22020     |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.366     |
|    value_loss           | 8.43      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1103       |
|    time_elapsed         | 1472       |
|    total_timesteps      | 22060      |
| train/                  |            |
|    approx_kl            | 0.51576644 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.96       |
|    n_updates            | 22040      |
|    policy_gradient_loss | -0.238     |
|    std                  | 0.366      |
|    value_loss           | 16.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1104      |
|    time_elapsed         | 1474      |
|    total_timesteps      | 22080     |
| train/                  |           |
|    approx_kl            | 3.9146726 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.934     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.124     |
|    n_updates            | 22060     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.366     |
|    value_loss           | 1.23      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1105     |
|    time_elapsed         | 1475     |
|    total_timesteps      | 22100    |
| train/                  |          |
|    approx_kl            | 1.134874 |
|    clip_fraction        | 0.573    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.238    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.98     |
|    n_updates            | 22080    |
|    policy_gradient_loss | -0.227   |
|    std                  | 0.366    |
|    value_loss           | 9.94     |
--------------------------------------
--------------------------------------
| reward                  | 0.94     |
| reward_contact          | 0        |
| reward_motion           | 0.94     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1106     |
|    time_elapsed         | 1476     |
|    total_timesteps      | 22120    |
| train/                  |          |
|    approx_kl            | 2.330417 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.684    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.49     |
|    n_updates            | 22100    |
|    policy_gradient_loss | -0.268   |
|    std                  | 0.366    |
|    value_loss           | 7.09     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1107       |
|    time_elapsed         | 1478       |
|    total_timesteps      | 22140      |
| train/                  |            |
|    approx_kl            | 0.78893554 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.14       |
|    n_updates            | 22120      |
|    policy_gradient_loss | -0.238     |
|    std                  | 0.366      |
|    value_loss           | 11         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1108      |
|    time_elapsed         | 1479      |
|    total_timesteps      | 22160     |
| train/                  |           |
|    approx_kl            | 1.2071291 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.37      |
|    n_updates            | 22140     |
|    policy_gradient_loss | -0.223    |
|    std                  | 0.366     |
|    value_loss           | 6.57      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1109     |
|    time_elapsed         | 1480     |
|    total_timesteps      | 22180    |
| train/                  |          |
|    approx_kl            | 5.067526 |
|    clip_fraction        | 0.645    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.773    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.399    |
|    n_updates            | 22160    |
|    policy_gradient_loss | -0.239   |
|    std                  | 0.366    |
|    value_loss           | 2.63     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1110      |
|    time_elapsed         | 1482      |
|    total_timesteps      | 22200     |
| train/                  |           |
|    approx_kl            | 2.8129861 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.61      |
|    n_updates            | 22180     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.366     |
|    value_loss           | 7.01      |
---------------------------------------
----------------------------------------
| reward                  | 0.93       |
| reward_contact          | 0          |
| reward_motion           | 0.93       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1111       |
|    time_elapsed         | 1483       |
|    total_timesteps      | 22220      |
| train/                  |            |
|    approx_kl            | 0.95790064 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.09       |
|    n_updates            | 22200      |
|    policy_gradient_loss | -0.23      |
|    std                  | 0.366      |
|    value_loss           | 9.06       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1112      |
|    time_elapsed         | 1484      |
|    total_timesteps      | 22240     |
| train/                  |           |
|    approx_kl            | 1.4249538 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.167    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.48      |
|    n_updates            | 22220     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.366     |
|    value_loss           | 19.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1113      |
|    time_elapsed         | 1486      |
|    total_timesteps      | 22260     |
| train/                  |           |
|    approx_kl            | 2.4663541 |
|    clip_fraction        | 0.715     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.709     |
|    n_updates            | 22240     |
|    policy_gradient_loss | -0.248    |
|    std                  | 0.366     |
|    value_loss           | 2.57      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1114      |
|    time_elapsed         | 1487      |
|    total_timesteps      | 22280     |
| train/                  |           |
|    approx_kl            | 1.3991657 |
|    clip_fraction        | 0.703     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.57      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.68      |
|    n_updates            | 22260     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.366     |
|    value_loss           | 6.49      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1115      |
|    time_elapsed         | 1488      |
|    total_timesteps      | 22300     |
| train/                  |           |
|    approx_kl            | 1.4985496 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.185     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.68      |
|    n_updates            | 22280     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.366     |
|    value_loss           | 17.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.93       |
| reward_contact          | 0          |
| reward_motion           | 0.93       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1116       |
|    time_elapsed         | 1490       |
|    total_timesteps      | 22320      |
| train/                  |            |
|    approx_kl            | 0.86254627 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | -0.689     |
|    learning_rate        | 0.0003     |
|    loss                 | 8.62       |
|    n_updates            | 22300      |
|    policy_gradient_loss | -0.241     |
|    std                  | 0.366      |
|    value_loss           | 23.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1117      |
|    time_elapsed         | 1491      |
|    total_timesteps      | 22340     |
| train/                  |           |
|    approx_kl            | 1.4387369 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 22320     |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.366     |
|    value_loss           | 6.98      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1118      |
|    time_elapsed         | 1492      |
|    total_timesteps      | 22360     |
| train/                  |           |
|    approx_kl            | 1.0869899 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.314     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.62      |
|    n_updates            | 22340     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.366     |
|    value_loss           | 12.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1119      |
|    time_elapsed         | 1494      |
|    total_timesteps      | 22380     |
| train/                  |           |
|    approx_kl            | 1.0035943 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.39      |
|    n_updates            | 22360     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.365     |
|    value_loss           | 6.64      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1120      |
|    time_elapsed         | 1495      |
|    total_timesteps      | 22400     |
| train/                  |           |
|    approx_kl            | 1.2969471 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.27      |
|    n_updates            | 22380     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.365     |
|    value_loss           | 3.94      |
---------------------------------------
--------------------------------------
| reward                  | 0.94     |
| reward_contact          | 0        |
| reward_motion           | 0.94     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1121     |
|    time_elapsed         | 1496     |
|    total_timesteps      | 22420    |
| train/                  |          |
|    approx_kl            | 1.604583 |
|    clip_fraction        | 0.628    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.58     |
|    learning_rate        | 0.0003   |
|    loss                 | 1.09     |
|    n_updates            | 22400    |
|    policy_gradient_loss | -0.258   |
|    std                  | 0.365    |
|    value_loss           | 4.05     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1122     |
|    time_elapsed         | 1498     |
|    total_timesteps      | 22440    |
| train/                  |          |
|    approx_kl            | 1.802747 |
|    clip_fraction        | 0.783    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | -0.144   |
|    learning_rate        | 0.0003   |
|    loss                 | 3.52     |
|    n_updates            | 22420    |
|    policy_gradient_loss | -0.285   |
|    std                  | 0.365    |
|    value_loss           | 10       |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1123      |
|    time_elapsed         | 1499      |
|    total_timesteps      | 22460     |
| train/                  |           |
|    approx_kl            | 2.1021726 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.36      |
|    n_updates            | 22440     |
|    policy_gradient_loss | -0.278    |
|    std                  | 0.365     |
|    value_loss           | 6.98      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1124      |
|    time_elapsed         | 1500      |
|    total_timesteps      | 22480     |
| train/                  |           |
|    approx_kl            | 0.6726929 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.568     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.79      |
|    n_updates            | 22460     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.365     |
|    value_loss           | 7.94      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1125      |
|    time_elapsed         | 1502      |
|    total_timesteps      | 22500     |
| train/                  |           |
|    approx_kl            | 2.7126572 |
|    clip_fraction        | 0.808     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.767     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.24      |
|    n_updates            | 22480     |
|    policy_gradient_loss | -0.297    |
|    std                  | 0.365     |
|    value_loss           | 4.36      |
---------------------------------------
---------------------------------------
| reward                  | 0.94      |
| reward_contact          | 0         |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1126      |
|    time_elapsed         | 1503      |
|    total_timesteps      | 22520     |
| train/                  |           |
|    approx_kl            | 1.1854159 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.23      |
|    n_updates            | 22500     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.365     |
|    value_loss           | 6.29      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1127       |
|    time_elapsed         | 1504       |
|    total_timesteps      | 22540      |
| train/                  |            |
|    approx_kl            | 0.47409412 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.499      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.83       |
|    n_updates            | 22520      |
|    policy_gradient_loss | -0.228     |
|    std                  | 0.365      |
|    value_loss           | 17.4       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1128      |
|    time_elapsed         | 1506      |
|    total_timesteps      | 22560     |
| train/                  |           |
|    approx_kl            | 1.3031702 |
|    clip_fraction        | 0.705     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.509     |
|    n_updates            | 22540     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.365     |
|    value_loss           | 2.25      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1129     |
|    time_elapsed         | 1507     |
|    total_timesteps      | 22580    |
| train/                  |          |
|    approx_kl            | 3.255706 |
|    clip_fraction        | 0.823    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.724    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.01     |
|    n_updates            | 22560    |
|    policy_gradient_loss | -0.284   |
|    std                  | 0.365    |
|    value_loss           | 4.95     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1130      |
|    time_elapsed         | 1508      |
|    total_timesteps      | 22600     |
| train/                  |           |
|    approx_kl            | 1.2341013 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.43      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 22580     |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.365     |
|    value_loss           | 11.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.94      |
| reward_contact          | 0         |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1131      |
|    time_elapsed         | 1510      |
|    total_timesteps      | 22620     |
| train/                  |           |
|    approx_kl            | 2.0092633 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.419     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 22600     |
|    policy_gradient_loss | -0.24     |
|    std                  | 0.365     |
|    value_loss           | 11.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1132      |
|    time_elapsed         | 1511      |
|    total_timesteps      | 22640     |
| train/                  |           |
|    approx_kl            | 1.5602033 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.274     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.34      |
|    n_updates            | 22620     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.365     |
|    value_loss           | 11.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1133      |
|    time_elapsed         | 1512      |
|    total_timesteps      | 22660     |
| train/                  |           |
|    approx_kl            | 1.6080754 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.61      |
|    n_updates            | 22640     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.365     |
|    value_loss           | 7.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1134      |
|    time_elapsed         | 1514      |
|    total_timesteps      | 22680     |
| train/                  |           |
|    approx_kl            | 1.1155075 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.709     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.43      |
|    n_updates            | 22660     |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.365     |
|    value_loss           | 11        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1135      |
|    time_elapsed         | 1515      |
|    total_timesteps      | 22700     |
| train/                  |           |
|    approx_kl            | 1.0943031 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.26      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.44      |
|    n_updates            | 22680     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.365     |
|    value_loss           | 10.2      |
---------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1136      |
|    time_elapsed         | 1516      |
|    total_timesteps      | 22720     |
| train/                  |           |
|    approx_kl            | 2.8757427 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.27      |
|    n_updates            | 22700     |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.365     |
|    value_loss           | 3.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1137      |
|    time_elapsed         | 1518      |
|    total_timesteps      | 22740     |
| train/                  |           |
|    approx_kl            | 1.1666181 |
|    clip_fraction        | 0.608     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.65      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.52      |
|    n_updates            | 22720     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.365     |
|    value_loss           | 6.77      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1138      |
|    time_elapsed         | 1519      |
|    total_timesteps      | 22760     |
| train/                  |           |
|    approx_kl            | 1.2761068 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 22740     |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.365     |
|    value_loss           | 7.8       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1139      |
|    time_elapsed         | 1520      |
|    total_timesteps      | 22780     |
| train/                  |           |
|    approx_kl            | 0.3956841 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.318     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.39      |
|    n_updates            | 22760     |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.365     |
|    value_loss           | 9.43      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1140       |
|    time_elapsed         | 1522       |
|    total_timesteps      | 22800      |
| train/                  |            |
|    approx_kl            | 0.30879346 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.47       |
|    n_updates            | 22780      |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.365      |
|    value_loss           | 16.2       |
----------------------------------------
---------------------------------------
| reward                  | 0.94      |
| reward_contact          | 0         |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1141      |
|    time_elapsed         | 1523      |
|    total_timesteps      | 22820     |
| train/                  |           |
|    approx_kl            | 2.5396435 |
|    clip_fraction        | 0.805     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 22800     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.365     |
|    value_loss           | 10.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1142      |
|    time_elapsed         | 1524      |
|    total_timesteps      | 22840     |
| train/                  |           |
|    approx_kl            | 3.1500785 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.775     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.02      |
|    n_updates            | 22820     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.365     |
|    value_loss           | 3.23      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.2       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1143       |
|    time_elapsed         | 1526       |
|    total_timesteps      | 22860      |
| train/                  |            |
|    approx_kl            | 0.57128465 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 22840      |
|    policy_gradient_loss | -0.23      |
|    std                  | 0.365      |
|    value_loss           | 7.75       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1144      |
|    time_elapsed         | 1527      |
|    total_timesteps      | 22880     |
| train/                  |           |
|    approx_kl            | 1.4112977 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.662     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.79      |
|    n_updates            | 22860     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.365     |
|    value_loss           | 5.43      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1145       |
|    time_elapsed         | 1528       |
|    total_timesteps      | 22900      |
| train/                  |            |
|    approx_kl            | 0.81365204 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -0.0978    |
|    learning_rate        | 0.0003     |
|    loss                 | 6.9        |
|    n_updates            | 22880      |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.365      |
|    value_loss           | 16.8       |
----------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1146      |
|    time_elapsed         | 1530      |
|    total_timesteps      | 22920     |
| train/                  |           |
|    approx_kl            | 2.0114288 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.8       |
|    n_updates            | 22900     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.365     |
|    value_loss           | 12        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1147       |
|    time_elapsed         | 1531       |
|    total_timesteps      | 22940      |
| train/                  |            |
|    approx_kl            | 0.27035007 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.67       |
|    n_updates            | 22920      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.365      |
|    value_loss           | 26.7       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1148      |
|    time_elapsed         | 1532      |
|    total_timesteps      | 22960     |
| train/                  |           |
|    approx_kl            | 0.6199684 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.433     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.5       |
|    n_updates            | 22940     |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.365     |
|    value_loss           | 9.99      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1149     |
|    time_elapsed         | 1534     |
|    total_timesteps      | 22980    |
| train/                  |          |
|    approx_kl            | 1.42568  |
|    clip_fraction        | 0.7      |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.119    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.46     |
|    n_updates            | 22960    |
|    policy_gradient_loss | -0.231   |
|    std                  | 0.365    |
|    value_loss           | 13.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1150      |
|    time_elapsed         | 1535      |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 3.4469593 |
|    clip_fraction        | 0.555     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.737     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.734     |
|    n_updates            | 22980     |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.365     |
|    value_loss           | 3.22      |
---------------------------------------
---------------------------------------
| reward                  | 0.92      |
| reward_contact          | 0         |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1151      |
|    time_elapsed         | 1536      |
|    total_timesteps      | 23020     |
| train/                  |           |
|    approx_kl            | 1.4725455 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.317     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.02      |
|    n_updates            | 23000     |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.365     |
|    value_loss           | 14.1      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1152       |
|    time_elapsed         | 1538       |
|    total_timesteps      | 23040      |
| train/                  |            |
|    approx_kl            | 0.68599665 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 23020      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.365      |
|    value_loss           | 8.23       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1153      |
|    time_elapsed         | 1539      |
|    total_timesteps      | 23060     |
| train/                  |           |
|    approx_kl            | 1.4196614 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.804     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.738     |
|    n_updates            | 23040     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.365     |
|    value_loss           | 3.25      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1154      |
|    time_elapsed         | 1540      |
|    total_timesteps      | 23080     |
| train/                  |           |
|    approx_kl            | 1.5354137 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.518     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 23060     |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.365     |
|    value_loss           | 7.87      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1155     |
|    time_elapsed         | 1542     |
|    total_timesteps      | 23100    |
| train/                  |          |
|    approx_kl            | 4.483211 |
|    clip_fraction        | 0.625    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.409    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.42     |
|    n_updates            | 23080    |
|    policy_gradient_loss | -0.25    |
|    std                  | 0.365    |
|    value_loss           | 10.8     |
--------------------------------------
---------------------------------------
| reward                  | 0.919     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1156      |
|    time_elapsed         | 1543      |
|    total_timesteps      | 23120     |
| train/                  |           |
|    approx_kl            | 2.555268  |
|    clip_fraction        | 0.758     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.97      |
|    n_updates            | 23100     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.365     |
|    value_loss           | 10.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1157      |
|    time_elapsed         | 1544      |
|    total_timesteps      | 23140     |
| train/                  |           |
|    approx_kl            | 2.0492861 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.43      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.69      |
|    n_updates            | 23120     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.365     |
|    value_loss           | 14.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1158      |
|    time_elapsed         | 1546      |
|    total_timesteps      | 23160     |
| train/                  |           |
|    approx_kl            | 2.2645032 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.36      |
|    n_updates            | 23140     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.365     |
|    value_loss           | 7.94      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1159     |
|    time_elapsed         | 1547     |
|    total_timesteps      | 23180    |
| train/                  |          |
|    approx_kl            | 2.570678 |
|    clip_fraction        | 0.788    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.578    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.44     |
|    n_updates            | 23160    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.365    |
|    value_loss           | 10.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1160      |
|    time_elapsed         | 1548      |
|    total_timesteps      | 23200     |
| train/                  |           |
|    approx_kl            | 1.8772011 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.76      |
|    n_updates            | 23180     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.365     |
|    value_loss           | 11        |
---------------------------------------
----------------------------------------
| reward                  | 0.909      |
| reward_contact          | -0.000859  |
| reward_motion           | 0.91       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1161       |
|    time_elapsed         | 1550       |
|    total_timesteps      | 23220      |
| train/                  |            |
|    approx_kl            | 0.49281436 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.131     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.02       |
|    n_updates            | 23200      |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.365      |
|    value_loss           | 10.2       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1162      |
|    time_elapsed         | 1551      |
|    total_timesteps      | 23240     |
| train/                  |           |
|    approx_kl            | 1.6747408 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -0.634    |
|    learning_rate        | 0.0003    |
|    loss                 | 8.38      |
|    n_updates            | 23220     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.365     |
|    value_loss           | 24.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1163      |
|    time_elapsed         | 1552      |
|    total_timesteps      | 23260     |
| train/                  |           |
|    approx_kl            | 0.8490553 |
|    clip_fraction        | 0.658     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.255     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.16      |
|    n_updates            | 23240     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.365     |
|    value_loss           | 13.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1164     |
|    time_elapsed         | 1554     |
|    total_timesteps      | 23280    |
| train/                  |          |
|    approx_kl            | 3.666613 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.511    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.45     |
|    n_updates            | 23260    |
|    policy_gradient_loss | -0.256   |
|    std                  | 0.365    |
|    value_loss           | 9.28     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1165      |
|    time_elapsed         | 1555      |
|    total_timesteps      | 23300     |
| train/                  |           |
|    approx_kl            | 1.9932003 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.428     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.1       |
|    n_updates            | 23280     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.365     |
|    value_loss           | 10.1      |
---------------------------------------
----------------------------------------
| reward                  | 0.909      |
| reward_contact          | -0.000859  |
| reward_motion           | 0.91       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.9       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1166       |
|    time_elapsed         | 1556       |
|    total_timesteps      | 23320      |
| train/                  |            |
|    approx_kl            | 0.24416561 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.78       |
|    n_updates            | 23300      |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.365      |
|    value_loss           | 16.9       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1167      |
|    time_elapsed         | 1558      |
|    total_timesteps      | 23340     |
| train/                  |           |
|    approx_kl            | 1.6498766 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.29      |
|    n_updates            | 23320     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.365     |
|    value_loss           | 10        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1168      |
|    time_elapsed         | 1559      |
|    total_timesteps      | 23360     |
| train/                  |           |
|    approx_kl            | 0.8650425 |
|    clip_fraction        | 0.535     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.527     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.28      |
|    n_updates            | 23340     |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.365     |
|    value_loss           | 8.7       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1169      |
|    time_elapsed         | 1560      |
|    total_timesteps      | 23380     |
| train/                  |           |
|    approx_kl            | 0.3066004 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.795     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.22      |
|    n_updates            | 23360     |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.365     |
|    value_loss           | 8.34      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1170      |
|    time_elapsed         | 1561      |
|    total_timesteps      | 23400     |
| train/                  |           |
|    approx_kl            | 1.3941516 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.403     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.59      |
|    n_updates            | 23380     |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.365     |
|    value_loss           | 12.8      |
---------------------------------------
---------------------------------------
| reward                  | 0.889     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1171      |
|    time_elapsed         | 1563      |
|    total_timesteps      | 23420     |
| train/                  |           |
|    approx_kl            | 1.5350059 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.285     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.39      |
|    n_updates            | 23400     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.365     |
|    value_loss           | 11.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1172      |
|    time_elapsed         | 1564      |
|    total_timesteps      | 23440     |
| train/                  |           |
|    approx_kl            | 1.0186547 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.356    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.33      |
|    n_updates            | 23420     |
|    policy_gradient_loss | -0.259    |
|    std                  | 0.365     |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1173      |
|    time_elapsed         | 1565      |
|    total_timesteps      | 23460     |
| train/                  |           |
|    approx_kl            | 3.3185349 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.334     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.07      |
|    n_updates            | 23440     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.365     |
|    value_loss           | 11.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1174      |
|    time_elapsed         | 1567      |
|    total_timesteps      | 23480     |
| train/                  |           |
|    approx_kl            | 5.7473884 |
|    clip_fraction        | 0.785     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.404     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.28      |
|    n_updates            | 23460     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.365     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1175      |
|    time_elapsed         | 1568      |
|    total_timesteps      | 23500     |
| train/                  |           |
|    approx_kl            | 1.4789809 |
|    clip_fraction        | 0.778     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.95      |
|    n_updates            | 23480     |
|    policy_gradient_loss | -0.283    |
|    std                  | 0.365     |
|    value_loss           | 7.21      |
---------------------------------------
---------------------------------------
| reward                  | 0.889     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1176      |
|    time_elapsed         | 1569      |
|    total_timesteps      | 23520     |
| train/                  |           |
|    approx_kl            | 1.613748  |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.107     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.87      |
|    n_updates            | 23500     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.365     |
|    value_loss           | 16.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1177      |
|    time_elapsed         | 1571      |
|    total_timesteps      | 23540     |
| train/                  |           |
|    approx_kl            | 1.3577958 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.556     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.44      |
|    n_updates            | 23520     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.365     |
|    value_loss           | 8.94      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1178      |
|    time_elapsed         | 1572      |
|    total_timesteps      | 23560     |
| train/                  |           |
|    approx_kl            | 3.4363708 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.386     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.2       |
|    n_updates            | 23540     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.365     |
|    value_loss           | 10.8      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1179       |
|    time_elapsed         | 1573       |
|    total_timesteps      | 23580      |
| train/                  |            |
|    approx_kl            | 0.68894696 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 23560      |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.365      |
|    value_loss           | 4.06       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1180      |
|    time_elapsed         | 1575      |
|    total_timesteps      | 23600     |
| train/                  |           |
|    approx_kl            | 0.9097341 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.0747    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.61      |
|    n_updates            | 23580     |
|    policy_gradient_loss | -0.229    |
|    std                  | 0.365     |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------------
| reward                  | 0.889     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1181      |
|    time_elapsed         | 1576      |
|    total_timesteps      | 23620     |
| train/                  |           |
|    approx_kl            | 0.5090607 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.05      |
|    n_updates            | 23600     |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.365     |
|    value_loss           | 11.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1182     |
|    time_elapsed         | 1577     |
|    total_timesteps      | 23640    |
| train/                  |          |
|    approx_kl            | 3.136486 |
|    clip_fraction        | 0.82     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.601    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.61     |
|    n_updates            | 23620    |
|    policy_gradient_loss | -0.283   |
|    std                  | 0.365    |
|    value_loss           | 9.74     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1183       |
|    time_elapsed         | 1579       |
|    total_timesteps      | 23660      |
| train/                  |            |
|    approx_kl            | 0.73785305 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.67       |
|    n_updates            | 23640      |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.365      |
|    value_loss           | 5.27       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1184      |
|    time_elapsed         | 1580      |
|    total_timesteps      | 23680     |
| train/                  |           |
|    approx_kl            | 1.1214815 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.27      |
|    n_updates            | 23660     |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.365     |
|    value_loss           | 6.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1185      |
|    time_elapsed         | 1581      |
|    total_timesteps      | 23700     |
| train/                  |           |
|    approx_kl            | 1.1637287 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.579     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.22      |
|    n_updates            | 23680     |
|    policy_gradient_loss | -0.211    |
|    std                  | 0.365     |
|    value_loss           | 6.78      |
---------------------------------------
---------------------------------------
| reward                  | 0.889     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1186      |
|    time_elapsed         | 1583      |
|    total_timesteps      | 23720     |
| train/                  |           |
|    approx_kl            | 1.0649623 |
|    clip_fraction        | 0.705     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.95      |
|    n_updates            | 23700     |
|    policy_gradient_loss | -0.28     |
|    std                  | 0.365     |
|    value_loss           | 9.91      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1187       |
|    time_elapsed         | 1584       |
|    total_timesteps      | 23740      |
| train/                  |            |
|    approx_kl            | 0.89698166 |
|    clip_fraction        | 0.69       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | -0.163     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.71       |
|    n_updates            | 23720      |
|    policy_gradient_loss | -0.226     |
|    std                  | 0.365      |
|    value_loss           | 16.5       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1188       |
|    time_elapsed         | 1585       |
|    total_timesteps      | 23760      |
| train/                  |            |
|    approx_kl            | 0.66408354 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.116      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.5        |
|    n_updates            | 23740      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.365      |
|    value_loss           | 15.6       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1189      |
|    time_elapsed         | 1587      |
|    total_timesteps      | 23780     |
| train/                  |           |
|    approx_kl            | 3.1474328 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.415     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.96      |
|    n_updates            | 23760     |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.365     |
|    value_loss           | 5.97      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1190      |
|    time_elapsed         | 1588      |
|    total_timesteps      | 23800     |
| train/                  |           |
|    approx_kl            | 3.1025553 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.383     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.57      |
|    n_updates            | 23780     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.365     |
|    value_loss           | 8.64      |
---------------------------------------
---------------------------------------
| reward                  | 0.889     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1191      |
|    time_elapsed         | 1589      |
|    total_timesteps      | 23820     |
| train/                  |           |
|    approx_kl            | 1.5102198 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.144     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.78      |
|    n_updates            | 23800     |
|    policy_gradient_loss | -0.233    |
|    std                  | 0.365     |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1192      |
|    time_elapsed         | 1591      |
|    total_timesteps      | 23840     |
| train/                  |           |
|    approx_kl            | 0.5576649 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.56      |
|    n_updates            | 23820     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.365     |
|    value_loss           | 11.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1193      |
|    time_elapsed         | 1592      |
|    total_timesteps      | 23860     |
| train/                  |           |
|    approx_kl            | 1.4630028 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.412     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.17      |
|    n_updates            | 23840     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.365     |
|    value_loss           | 11.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1194      |
|    time_elapsed         | 1593      |
|    total_timesteps      | 23880     |
| train/                  |           |
|    approx_kl            | 0.3191824 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.524     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.2      |
|    n_updates            | 23860     |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.365     |
|    value_loss           | 34.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1195      |
|    time_elapsed         | 1595      |
|    total_timesteps      | 23900     |
| train/                  |           |
|    approx_kl            | 1.2965139 |
|    clip_fraction        | 0.723     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.191     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.93      |
|    n_updates            | 23880     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.365     |
|    value_loss           | 14.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.869     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1196      |
|    time_elapsed         | 1596      |
|    total_timesteps      | 23920     |
| train/                  |           |
|    approx_kl            | 1.6713078 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.502     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.51      |
|    n_updates            | 23900     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.365     |
|    value_loss           | 8.9       |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1197     |
|    time_elapsed         | 1597     |
|    total_timesteps      | 23940    |
| train/                  |          |
|    approx_kl            | 2.491441 |
|    clip_fraction        | 0.773    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.244    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.67     |
|    n_updates            | 23920    |
|    policy_gradient_loss | -0.251   |
|    std                  | 0.365    |
|    value_loss           | 13.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1198      |
|    time_elapsed         | 1599      |
|    total_timesteps      | 23960     |
| train/                  |           |
|    approx_kl            | 2.4605331 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.64      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.4       |
|    n_updates            | 23940     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.365     |
|    value_loss           | 5.22      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1199      |
|    time_elapsed         | 1600      |
|    total_timesteps      | 23980     |
| train/                  |           |
|    approx_kl            | 3.5276704 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.58      |
|    n_updates            | 23960     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.364     |
|    value_loss           | 9.18      |
---------------------------------------
Num timesteps: 24000
Best mean reward: 17.29 - Last mean reward per episode: 17.67
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1200      |
|    time_elapsed         | 1601      |
|    total_timesteps      | 24000     |
| train/                  |           |
|    approx_kl            | 1.3366514 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.567     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.4       |
|    n_updates            | 23980     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.364     |
|    value_loss           | 9.31      |
---------------------------------------
---------------------------------------
| reward                  | 0.859     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1201      |
|    time_elapsed         | 1603      |
|    total_timesteps      | 24020     |
| train/                  |           |
|    approx_kl            | 4.6052423 |
|    clip_fraction        | 0.733     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.699     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.07      |
|    n_updates            | 24000     |
|    policy_gradient_loss | -0.241    |
|    std                  | 0.365     |
|    value_loss           | 5.46      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1202      |
|    time_elapsed         | 1604      |
|    total_timesteps      | 24040     |
| train/                  |           |
|    approx_kl            | 1.1076653 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.413     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 24020     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.364     |
|    value_loss           | 5.98      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1203       |
|    time_elapsed         | 1605       |
|    total_timesteps      | 24060      |
| train/                  |            |
|    approx_kl            | 0.77350634 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.01       |
|    n_updates            | 24040      |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.364      |
|    value_loss           | 7.44       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1204      |
|    time_elapsed         | 1607      |
|    total_timesteps      | 24080     |
| train/                  |           |
|    approx_kl            | 1.2758065 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.55      |
|    n_updates            | 24060     |
|    policy_gradient_loss | -0.168    |
|    std                  | 0.365     |
|    value_loss           | 5.27      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1205      |
|    time_elapsed         | 1608      |
|    total_timesteps      | 24100     |
| train/                  |           |
|    approx_kl            | 2.4452126 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.358     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.98      |
|    n_updates            | 24080     |
|    policy_gradient_loss | -0.279    |
|    std                  | 0.365     |
|    value_loss           | 11.5      |
---------------------------------------
---------------------------------------
| reward                  | 0.859     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1206      |
|    time_elapsed         | 1609      |
|    total_timesteps      | 24120     |
| train/                  |           |
|    approx_kl            | 1.5695635 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.426     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.95      |
|    n_updates            | 24100     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.365     |
|    value_loss           | 11.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1207      |
|    time_elapsed         | 1611      |
|    total_timesteps      | 24140     |
| train/                  |           |
|    approx_kl            | 1.4764564 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.43      |
|    n_updates            | 24120     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.365     |
|    value_loss           | 12.3      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1208       |
|    time_elapsed         | 1612       |
|    total_timesteps      | 24160      |
| train/                  |            |
|    approx_kl            | 0.63081497 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.555      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.53       |
|    n_updates            | 24140      |
|    policy_gradient_loss | -0.235     |
|    std                  | 0.365      |
|    value_loss           | 11.3       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1209       |
|    time_elapsed         | 1613       |
|    total_timesteps      | 24180      |
| train/                  |            |
|    approx_kl            | 0.10966712 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.26       |
|    n_updates            | 24160      |
|    policy_gradient_loss | -0.0967    |
|    std                  | 0.365      |
|    value_loss           | 21         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1210      |
|    time_elapsed         | 1615      |
|    total_timesteps      | 24200     |
| train/                  |           |
|    approx_kl            | 1.4222282 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.41      |
|    n_updates            | 24180     |
|    policy_gradient_loss | -0.216    |
|    std                  | 0.365     |
|    value_loss           | 8.6       |
---------------------------------------
---------------------------------------
| reward                  | 0.849     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.85      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1211      |
|    time_elapsed         | 1616      |
|    total_timesteps      | 24220     |
| train/                  |           |
|    approx_kl            | 0.2092719 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.693     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.61      |
|    n_updates            | 24200     |
|    policy_gradient_loss | -0.163    |
|    std                  | 0.365     |
|    value_loss           | 5.12      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1212      |
|    time_elapsed         | 1617      |
|    total_timesteps      | 24240     |
| train/                  |           |
|    approx_kl            | 1.8042217 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.394     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.05      |
|    n_updates            | 24220     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.365     |
|    value_loss           | 11.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1213      |
|    time_elapsed         | 1619      |
|    total_timesteps      | 24260     |
| train/                  |           |
|    approx_kl            | 1.3126993 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.42      |
|    n_updates            | 24240     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.365     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1214      |
|    time_elapsed         | 1620      |
|    total_timesteps      | 24280     |
| train/                  |           |
|    approx_kl            | 1.3147229 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.72      |
|    n_updates            | 24260     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.365     |
|    value_loss           | 7.06      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1215      |
|    time_elapsed         | 1621      |
|    total_timesteps      | 24300     |
| train/                  |           |
|    approx_kl            | 2.3637278 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.376     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.18      |
|    n_updates            | 24280     |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.365     |
|    value_loss           | 14.4      |
---------------------------------------
---------------------------------------
| reward                  | 0.829     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.83      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1216      |
|    time_elapsed         | 1623      |
|    total_timesteps      | 24320     |
| train/                  |           |
|    approx_kl            | 2.5700452 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.355     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.13      |
|    n_updates            | 24300     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.365     |
|    value_loss           | 11.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1217     |
|    time_elapsed         | 1624     |
|    total_timesteps      | 24340    |
| train/                  |          |
|    approx_kl            | 3.827353 |
|    clip_fraction        | 0.825    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.434    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.85     |
|    n_updates            | 24320    |
|    policy_gradient_loss | -0.27    |
|    std                  | 0.365    |
|    value_loss           | 10.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1218      |
|    time_elapsed         | 1625      |
|    total_timesteps      | 24360     |
| train/                  |           |
|    approx_kl            | 2.1638412 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 24340     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.365     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1219      |
|    time_elapsed         | 1627      |
|    total_timesteps      | 24380     |
| train/                  |           |
|    approx_kl            | 1.5787618 |
|    clip_fraction        | 0.74      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.16      |
|    n_updates            | 24360     |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.365     |
|    value_loss           | 7.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1220      |
|    time_elapsed         | 1628      |
|    total_timesteps      | 24400     |
| train/                  |           |
|    approx_kl            | 0.2567009 |
|    clip_fraction        | 0.288     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.07     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.8      |
|    n_updates            | 24380     |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.365     |
|    value_loss           | 32        |
---------------------------------------
---------------------------------------
| reward                  | 0.829     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.83      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1221      |
|    time_elapsed         | 1629      |
|    total_timesteps      | 24420     |
| train/                  |           |
|    approx_kl            | 1.3617547 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.77      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.74      |
|    n_updates            | 24400     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.364     |
|    value_loss           | 5.29      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1222      |
|    time_elapsed         | 1631      |
|    total_timesteps      | 24440     |
| train/                  |           |
|    approx_kl            | 2.4483678 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.87      |
|    n_updates            | 24420     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.364     |
|    value_loss           | 5.7       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1223      |
|    time_elapsed         | 1632      |
|    total_timesteps      | 24460     |
| train/                  |           |
|    approx_kl            | 2.7746556 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.67      |
|    n_updates            | 24440     |
|    policy_gradient_loss | -0.294    |
|    std                  | 0.364     |
|    value_loss           | 15.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1224      |
|    time_elapsed         | 1633      |
|    total_timesteps      | 24480     |
| train/                  |           |
|    approx_kl            | 1.0511161 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.629     |
|    n_updates            | 24460     |
|    policy_gradient_loss | -0.212    |
|    std                  | 0.364     |
|    value_loss           | 2.57      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.5     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1225     |
|    time_elapsed         | 1635     |
|    total_timesteps      | 24500    |
| train/                  |          |
|    approx_kl            | 5.377561 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.644    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.33     |
|    n_updates            | 24480    |
|    policy_gradient_loss | -0.265   |
|    std                  | 0.364    |
|    value_loss           | 4.86     |
--------------------------------------
----------------------------------------
| reward                  | 0.829      |
| reward_contact          | -0.000859  |
| reward_motion           | 0.83       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1226       |
|    time_elapsed         | 1636       |
|    total_timesteps      | 24520      |
| train/                  |            |
|    approx_kl            | 0.93217176 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.413      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 24500      |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.364      |
|    value_loss           | 5.53       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1227      |
|    time_elapsed         | 1637      |
|    total_timesteps      | 24540     |
| train/                  |           |
|    approx_kl            | 0.5098488 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.55      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.07      |
|    n_updates            | 24520     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.364     |
|    value_loss           | 8.47      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1228      |
|    time_elapsed         | 1639      |
|    total_timesteps      | 24560     |
| train/                  |           |
|    approx_kl            | 1.1689695 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0596    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.93      |
|    n_updates            | 24540     |
|    policy_gradient_loss | -0.251    |
|    std                  | 0.364     |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1229      |
|    time_elapsed         | 1640      |
|    total_timesteps      | 24580     |
| train/                  |           |
|    approx_kl            | 1.4668556 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.498     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.3       |
|    n_updates            | 24560     |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.364     |
|    value_loss           | 10.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1230      |
|    time_elapsed         | 1641      |
|    total_timesteps      | 24600     |
| train/                  |           |
|    approx_kl            | 2.6484578 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.744     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.804     |
|    n_updates            | 24580     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.365     |
|    value_loss           | 3.62      |
---------------------------------------
---------------------------------------
| reward                  | 0.839     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.84      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1231      |
|    time_elapsed         | 1643      |
|    total_timesteps      | 24620     |
| train/                  |           |
|    approx_kl            | 3.157597  |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.23      |
|    n_updates            | 24600     |
|    policy_gradient_loss | -0.265    |
|    std                  | 0.365     |
|    value_loss           | 11.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1232      |
|    time_elapsed         | 1644      |
|    total_timesteps      | 24640     |
| train/                  |           |
|    approx_kl            | 1.3643802 |
|    clip_fraction        | 0.743     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.434     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.18      |
|    n_updates            | 24620     |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.365     |
|    value_loss           | 12        |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1233       |
|    time_elapsed         | 1645       |
|    total_timesteps      | 24660      |
| train/                  |            |
|    approx_kl            | 0.78915656 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.785      |
|    n_updates            | 24640      |
|    policy_gradient_loss | -0.222     |
|    std                  | 0.365      |
|    value_loss           | 3.62       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1234     |
|    time_elapsed         | 1647     |
|    total_timesteps      | 24680    |
| train/                  |          |
|    approx_kl            | 2.218742 |
|    clip_fraction        | 0.75     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.661    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.9      |
|    n_updates            | 24660    |
|    policy_gradient_loss | -0.251   |
|    std                  | 0.365    |
|    value_loss           | 5.52     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1235      |
|    time_elapsed         | 1648      |
|    total_timesteps      | 24700     |
| train/                  |           |
|    approx_kl            | 0.7348618 |
|    clip_fraction        | 0.613     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.569     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.56      |
|    n_updates            | 24680     |
|    policy_gradient_loss | -0.224    |
|    std                  | 0.365     |
|    value_loss           | 6.08      |
---------------------------------------
---------------------------------------
| reward                  | 0.859     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.86      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1236      |
|    time_elapsed         | 1649      |
|    total_timesteps      | 24720     |
| train/                  |           |
|    approx_kl            | 1.8285192 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.0003    |
|    loss                 | 0.748     |
|    n_updates            | 24700     |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.365     |
|    value_loss           | 4.34      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1237     |
|    time_elapsed         | 1651     |
|    total_timesteps      | 24740    |
| train/                  |          |
|    approx_kl            | 1.75601  |
|    clip_fraction        | 0.79     |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.196    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.56     |
|    n_updates            | 24720    |
|    policy_gradient_loss | -0.284   |
|    std                  | 0.365    |
|    value_loss           | 12.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1238      |
|    time_elapsed         | 1652      |
|    total_timesteps      | 24760     |
| train/                  |           |
|    approx_kl            | 0.9187668 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.267     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.44      |
|    n_updates            | 24740     |
|    policy_gradient_loss | -0.271    |
|    std                  | 0.365     |
|    value_loss           | 15.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1239      |
|    time_elapsed         | 1653      |
|    total_timesteps      | 24780     |
| train/                  |           |
|    approx_kl            | 1.5791551 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.41      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.53      |
|    n_updates            | 24760     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.364     |
|    value_loss           | 11.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1240      |
|    time_elapsed         | 1655      |
|    total_timesteps      | 24800     |
| train/                  |           |
|    approx_kl            | 0.9756655 |
|    clip_fraction        | 0.735     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.0644    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.3       |
|    n_updates            | 24780     |
|    policy_gradient_loss | -0.252    |
|    std                  | 0.364     |
|    value_loss           | 9.28      |
---------------------------------------
---------------------------------------
| reward                  | 0.869     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.87      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1241      |
|    time_elapsed         | 1656      |
|    total_timesteps      | 24820     |
| train/                  |           |
|    approx_kl            | 1.4345218 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.98      |
|    n_updates            | 24800     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.364     |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1242      |
|    time_elapsed         | 1657      |
|    total_timesteps      | 24840     |
| train/                  |           |
|    approx_kl            | 1.2797056 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.0134   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.68      |
|    n_updates            | 24820     |
|    policy_gradient_loss | -0.269    |
|    std                  | 0.364     |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1243      |
|    time_elapsed         | 1659      |
|    total_timesteps      | 24860     |
| train/                  |           |
|    approx_kl            | 1.8440064 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.346     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.82      |
|    n_updates            | 24840     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.365     |
|    value_loss           | 9.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1244      |
|    time_elapsed         | 1660      |
|    total_timesteps      | 24880     |
| train/                  |           |
|    approx_kl            | 1.2848762 |
|    clip_fraction        | 0.758     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.767     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.54      |
|    n_updates            | 24860     |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.365     |
|    value_loss           | 5.11      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1245      |
|    time_elapsed         | 1661      |
|    total_timesteps      | 24900     |
| train/                  |           |
|    approx_kl            | 2.0721178 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.316     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.55      |
|    n_updates            | 24880     |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.365     |
|    value_loss           | 18.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.879     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1246      |
|    time_elapsed         | 1663      |
|    total_timesteps      | 24920     |
| train/                  |           |
|    approx_kl            | 1.3763144 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.61      |
|    n_updates            | 24900     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.364     |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1247      |
|    time_elapsed         | 1664      |
|    total_timesteps      | 24940     |
| train/                  |           |
|    approx_kl            | 3.7328079 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.35      |
|    n_updates            | 24920     |
|    policy_gradient_loss | -0.269    |
|    std                  | 0.364     |
|    value_loss           | 5.76      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1248     |
|    time_elapsed         | 1665     |
|    total_timesteps      | 24960    |
| train/                  |          |
|    approx_kl            | 2.290962 |
|    clip_fraction        | 0.808    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.577    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.39     |
|    n_updates            | 24940    |
|    policy_gradient_loss | -0.272   |
|    std                  | 0.364    |
|    value_loss           | 8.43     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1249      |
|    time_elapsed         | 1667      |
|    total_timesteps      | 24980     |
| train/                  |           |
|    approx_kl            | 0.5027941 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.396     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.2      |
|    n_updates            | 24960     |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.364     |
|    value_loss           | 24.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1250      |
|    time_elapsed         | 1668      |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 2.2129998 |
|    clip_fraction        | 0.725     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.493     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 24980     |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.364     |
|    value_loss           | 6.15      |
---------------------------------------
---------------------------------------
| reward                  | 0.879     |
| reward_contact          | -0.000859 |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1251      |
|    time_elapsed         | 1670      |
|    total_timesteps      | 25020     |
| train/                  |           |
|    approx_kl            | 1.9941615 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.1       |
|    n_updates            | 25000     |
|    policy_gradient_loss | -0.292    |
|    std                  | 0.364     |
|    value_loss           | 18.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1252      |
|    time_elapsed         | 1671      |
|    total_timesteps      | 25040     |
| train/                  |           |
|    approx_kl            | 0.8198678 |
|    clip_fraction        | 0.565     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.754     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.45      |
|    n_updates            | 25020     |
|    policy_gradient_loss | -0.218    |
|    std                  | 0.364     |
|    value_loss           | 4.48      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1253     |
|    time_elapsed         | 1672     |
|    total_timesteps      | 25060    |
| train/                  |          |
|    approx_kl            | 4.38943  |
|    clip_fraction        | 0.823    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.29     |
|    learning_rate        | 0.0003   |
|    loss                 | 6.42     |
|    n_updates            | 25040    |
|    policy_gradient_loss | -0.296   |
|    std                  | 0.364    |
|    value_loss           | 14.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1254      |
|    time_elapsed         | 1674      |
|    total_timesteps      | 25080     |
| train/                  |           |
|    approx_kl            | 3.0225675 |
|    clip_fraction        | 0.813     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.81      |
|    n_updates            | 25060     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.364     |
|    value_loss           | 8.07      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1255      |
|    time_elapsed         | 1675      |
|    total_timesteps      | 25100     |
| train/                  |           |
|    approx_kl            | 1.9880333 |
|    clip_fraction        | 0.768     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.03      |
|    n_updates            | 25080     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.364     |
|    value_loss           | 6.79      |
---------------------------------------
---------------------------------------
| reward                  | 0.88      |
| reward_contact          | 0         |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1256      |
|    time_elapsed         | 1676      |
|    total_timesteps      | 25120     |
| train/                  |           |
|    approx_kl            | 1.0294012 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.93      |
|    n_updates            | 25100     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.364     |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1257      |
|    time_elapsed         | 1678      |
|    total_timesteps      | 25140     |
| train/                  |           |
|    approx_kl            | 0.3139173 |
|    clip_fraction        | 0.32      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.631     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.8      |
|    n_updates            | 25120     |
|    policy_gradient_loss | -0.131    |
|    std                  | 0.364     |
|    value_loss           | 33.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1258     |
|    time_elapsed         | 1679     |
|    total_timesteps      | 25160    |
| train/                  |          |
|    approx_kl            | 2.051954 |
|    clip_fraction        | 0.71     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.641    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.27     |
|    n_updates            | 25140    |
|    policy_gradient_loss | -0.254   |
|    std                  | 0.364    |
|    value_loss           | 7.12     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1259      |
|    time_elapsed         | 1680      |
|    total_timesteps      | 25180     |
| train/                  |           |
|    approx_kl            | 1.0351003 |
|    clip_fraction        | 0.623     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.411     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.63      |
|    n_updates            | 25160     |
|    policy_gradient_loss | -0.228    |
|    std                  | 0.364     |
|    value_loss           | 10.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1260      |
|    time_elapsed         | 1682      |
|    total_timesteps      | 25200     |
| train/                  |           |
|    approx_kl            | 3.5535133 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 25180     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.364     |
|    value_loss           | 4.81      |
---------------------------------------
---------------------------------------
| reward                  | 0.88      |
| reward_contact          | 0         |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1261      |
|    time_elapsed         | 1683      |
|    total_timesteps      | 25220     |
| train/                  |           |
|    approx_kl            | 0.7260853 |
|    clip_fraction        | 0.535     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.88      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.432     |
|    n_updates            | 25200     |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.364     |
|    value_loss           | 2.44      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1262      |
|    time_elapsed         | 1684      |
|    total_timesteps      | 25240     |
| train/                  |           |
|    approx_kl            | 1.6308504 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.359     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.64      |
|    n_updates            | 25220     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.364     |
|    value_loss           | 7.57      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1263      |
|    time_elapsed         | 1686      |
|    total_timesteps      | 25260     |
| train/                  |           |
|    approx_kl            | 2.3760612 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.59      |
|    n_updates            | 25240     |
|    policy_gradient_loss | -0.249    |
|    std                  | 0.364     |
|    value_loss           | 17.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 17.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1264     |
|    time_elapsed         | 1687     |
|    total_timesteps      | 25280    |
| train/                  |          |
|    approx_kl            | 2.265073 |
|    clip_fraction        | 0.77     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | -0.212   |
|    learning_rate        | 0.0003   |
|    loss                 | 8.42     |
|    n_updates            | 25260    |
|    policy_gradient_loss | -0.276   |
|    std                  | 0.364    |
|    value_loss           | 22.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1265      |
|    time_elapsed         | 1688      |
|    total_timesteps      | 25300     |
| train/                  |           |
|    approx_kl            | 3.1522238 |
|    clip_fraction        | 0.805     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.605     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 25280     |
|    policy_gradient_loss | -0.261    |
|    std                  | 0.364     |
|    value_loss           | 8.29      |
---------------------------------------
---------------------------------------
| reward                  | 0.88      |
| reward_contact          | 0         |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1266      |
|    time_elapsed         | 1690      |
|    total_timesteps      | 25320     |
| train/                  |           |
|    approx_kl            | 1.3106415 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.297     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.25      |
|    n_updates            | 25300     |
|    policy_gradient_loss | -0.239    |
|    std                  | 0.364     |
|    value_loss           | 16.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1267      |
|    time_elapsed         | 1691      |
|    total_timesteps      | 25340     |
| train/                  |           |
|    approx_kl            | 1.3187907 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.45      |
|    learning_rate        | 0.0003    |
|    loss                 | 4.82      |
|    n_updates            | 25320     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.364     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1268      |
|    time_elapsed         | 1692      |
|    total_timesteps      | 25360     |
| train/                  |           |
|    approx_kl            | 2.6412182 |
|    clip_fraction        | 0.735     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 25340     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.364     |
|    value_loss           | 6.24      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1269      |
|    time_elapsed         | 1694      |
|    total_timesteps      | 25380     |
| train/                  |           |
|    approx_kl            | 1.1497904 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.223     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.16      |
|    n_updates            | 25360     |
|    policy_gradient_loss | -0.254    |
|    std                  | 0.364     |
|    value_loss           | 15.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1270      |
|    time_elapsed         | 1695      |
|    total_timesteps      | 25400     |
| train/                  |           |
|    approx_kl            | 1.9894856 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.62      |
|    n_updates            | 25380     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.364     |
|    value_loss           | 3.39      |
---------------------------------------
---------------------------------------
| reward                  | 0.89      |
| reward_contact          | 0         |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1271      |
|    time_elapsed         | 1696      |
|    total_timesteps      | 25420     |
| train/                  |           |
|    approx_kl            | 1.5010535 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.37      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.59      |
|    n_updates            | 25400     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.364     |
|    value_loss           | 8.65      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1272      |
|    time_elapsed         | 1698      |
|    total_timesteps      | 25440     |
| train/                  |           |
|    approx_kl            | 1.6503075 |
|    clip_fraction        | 0.738     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.0941    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.12      |
|    n_updates            | 25420     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.364     |
|    value_loss           | 16.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1273      |
|    time_elapsed         | 1699      |
|    total_timesteps      | 25460     |
| train/                  |           |
|    approx_kl            | 3.8661458 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.86      |
|    n_updates            | 25440     |
|    policy_gradient_loss | -0.246    |
|    std                  | 0.364     |
|    value_loss           | 11.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1274      |
|    time_elapsed         | 1700      |
|    total_timesteps      | 25480     |
| train/                  |           |
|    approx_kl            | 0.8687505 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.0512   |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 25460     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.364     |
|    value_loss           | 11.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1275      |
|    time_elapsed         | 1702      |
|    total_timesteps      | 25500     |
| train/                  |           |
|    approx_kl            | 3.2313924 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.456     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.24      |
|    n_updates            | 25480     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.364     |
|    value_loss           | 10.6      |
---------------------------------------
---------------------------------------
| reward                  | 0.89      |
| reward_contact          | 0         |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 17.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1276      |
|    time_elapsed         | 1703      |
|    total_timesteps      | 25520     |
| train/                  |           |
|    approx_kl            | 2.5201824 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.023     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.32      |
|    n_updates            | 25500     |
|    policy_gradient_loss | -0.264    |
|    std                  | 0.364     |
|    value_loss           | 16.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1277      |
|    time_elapsed         | 1704      |
|    total_timesteps      | 25540     |
| train/                  |           |
|    approx_kl            | 4.4520416 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.33      |
|    n_updates            | 25520     |
|    policy_gradient_loss | -0.243    |
|    std                  | 0.364     |
|    value_loss           | 6.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1278      |
|    time_elapsed         | 1706      |
|    total_timesteps      | 25560     |
| train/                  |           |
|    approx_kl            | 1.6436085 |
|    clip_fraction        | 0.668     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.41      |
|    n_updates            | 25540     |
|    policy_gradient_loss | -0.234    |
|    std                  | 0.364     |
|    value_loss           | 10.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1279      |
|    time_elapsed         | 1707      |
|    total_timesteps      | 25580     |
| train/                  |           |
|    approx_kl            | 2.0315502 |
|    clip_fraction        | 0.508     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.23      |
|    n_updates            | 25560     |
|    policy_gradient_loss | -0.217    |
|    std                  | 0.364     |
|    value_loss           | 9.5       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1280      |
|    time_elapsed         | 1708      |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 1.9756235 |
|    clip_fraction        | 0.755     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.179    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.67      |
|    n_updates            | 25580     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.364     |
|    value_loss           | 17.6      |
---------------------------------------
---------------------------------------
| reward                  | 0.89      |
| reward_contact          | 0         |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1281      |
|    time_elapsed         | 1710      |
|    total_timesteps      | 25620     |
| train/                  |           |
|    approx_kl            | 1.0910629 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.565     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.01      |
|    n_updates            | 25600     |
|    policy_gradient_loss | -0.235    |
|    std                  | 0.364     |
|    value_loss           | 7.94      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1282      |
|    time_elapsed         | 1711      |
|    total_timesteps      | 25640     |
| train/                  |           |
|    approx_kl            | 1.4490274 |
|    clip_fraction        | 0.533     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.797     |
|    n_updates            | 25620     |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.364     |
|    value_loss           | 2.88      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18       |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1283     |
|    time_elapsed         | 1712     |
|    total_timesteps      | 25660    |
| train/                  |          |
|    approx_kl            | 2.992275 |
|    clip_fraction        | 0.705    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.544    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.54     |
|    n_updates            | 25640    |
|    policy_gradient_loss | -0.257   |
|    std                  | 0.364    |
|    value_loss           | 6.91     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1284      |
|    time_elapsed         | 1714      |
|    total_timesteps      | 25680     |
| train/                  |           |
|    approx_kl            | 1.4850025 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.79      |
|    n_updates            | 25660     |
|    policy_gradient_loss | -0.286    |
|    std                  | 0.364     |
|    value_loss           | 8.52      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1285      |
|    time_elapsed         | 1715      |
|    total_timesteps      | 25700     |
| train/                  |           |
|    approx_kl            | 2.9819283 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.292     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.39      |
|    n_updates            | 25680     |
|    policy_gradient_loss | -0.262    |
|    std                  | 0.364     |
|    value_loss           | 15.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.89      |
| reward_contact          | 0         |
| reward_motion           | 0.89      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1286      |
|    time_elapsed         | 1716      |
|    total_timesteps      | 25720     |
| train/                  |           |
|    approx_kl            | 3.9041874 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.216     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.68      |
|    n_updates            | 25700     |
|    policy_gradient_loss | -0.288    |
|    std                  | 0.364     |
|    value_loss           | 19.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1287      |
|    time_elapsed         | 1718      |
|    total_timesteps      | 25740     |
| train/                  |           |
|    approx_kl            | 1.7450272 |
|    clip_fraction        | 0.822     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.346     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.17      |
|    n_updates            | 25720     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.364     |
|    value_loss           | 11.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1288      |
|    time_elapsed         | 1719      |
|    total_timesteps      | 25760     |
| train/                  |           |
|    approx_kl            | 3.3443508 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 5         |
|    n_updates            | 25740     |
|    policy_gradient_loss | -0.242    |
|    std                  | 0.364     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1289      |
|    time_elapsed         | 1720      |
|    total_timesteps      | 25780     |
| train/                  |           |
|    approx_kl            | 1.4281662 |
|    clip_fraction        | 0.735     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.317    |
|    learning_rate        | 0.0003    |
|    loss                 | 9.3       |
|    n_updates            | 25760     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.364     |
|    value_loss           | 26.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1290      |
|    time_elapsed         | 1722      |
|    total_timesteps      | 25800     |
| train/                  |           |
|    approx_kl            | 0.6878421 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 25780     |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.364     |
|    value_loss           | 5.34      |
---------------------------------------
---------------------------------------
| reward                  | 0.88      |
| reward_contact          | 0         |
| reward_motion           | 0.88      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1291      |
|    time_elapsed         | 1723      |
|    total_timesteps      | 25820     |
| train/                  |           |
|    approx_kl            | 1.1928962 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.18      |
|    n_updates            | 25800     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.364     |
|    value_loss           | 8.08      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.1     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1292     |
|    time_elapsed         | 1724     |
|    total_timesteps      | 25840    |
| train/                  |          |
|    approx_kl            | 2.995114 |
|    clip_fraction        | 0.65     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.524    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.73     |
|    n_updates            | 25820    |
|    policy_gradient_loss | -0.239   |
|    std                  | 0.364    |
|    value_loss           | 8.54     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1293     |
|    time_elapsed         | 1726     |
|    total_timesteps      | 25860    |
| train/                  |          |
|    approx_kl            | 2.104603 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.315    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.45     |
|    n_updates            | 25840    |
|    policy_gradient_loss | -0.237   |
|    std                  | 0.364    |
|    value_loss           | 13.1     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1294      |
|    time_elapsed         | 1727      |
|    total_timesteps      | 25880     |
| train/                  |           |
|    approx_kl            | 0.8165354 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.0504   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.93      |
|    n_updates            | 25860     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.364     |
|    value_loss           | 18.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1295      |
|    time_elapsed         | 1729      |
|    total_timesteps      | 25900     |
| train/                  |           |
|    approx_kl            | 7.2182236 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.475     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 25880     |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.364     |
|    value_loss           | 10.1      |
---------------------------------------
---------------------------------------
| reward                  | 0.9       |
| reward_contact          | 0         |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1296      |
|    time_elapsed         | 1730      |
|    total_timesteps      | 25920     |
| train/                  |           |
|    approx_kl            | 3.8872056 |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.406     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.89      |
|    n_updates            | 25900     |
|    policy_gradient_loss | -0.281    |
|    std                  | 0.364     |
|    value_loss           | 13        |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1297     |
|    time_elapsed         | 1731     |
|    total_timesteps      | 25940    |
| train/                  |          |
|    approx_kl            | 1.772786 |
|    clip_fraction        | 0.743    |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.168    |
|    learning_rate        | 0.0003   |
|    loss                 | 6.68     |
|    n_updates            | 25920    |
|    policy_gradient_loss | -0.243   |
|    std                  | 0.364    |
|    value_loss           | 15.9     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1298     |
|    time_elapsed         | 1733     |
|    total_timesteps      | 25960    |
| train/                  |          |
|    approx_kl            | 2.767926 |
|    clip_fraction        | 0.74     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.607    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.65     |
|    n_updates            | 25940    |
|    policy_gradient_loss | -0.271   |
|    std                  | 0.364    |
|    value_loss           | 8.16     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1299      |
|    time_elapsed         | 1734      |
|    total_timesteps      | 25980     |
| train/                  |           |
|    approx_kl            | 2.2949855 |
|    clip_fraction        | 0.753     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.441     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.01      |
|    n_updates            | 25960     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.364     |
|    value_loss           | 13        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1300      |
|    time_elapsed         | 1735      |
|    total_timesteps      | 26000     |
| train/                  |           |
|    approx_kl            | 0.7170109 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.738     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.965     |
|    n_updates            | 25980     |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.364     |
|    value_loss           | 4.09      |
---------------------------------------
---------------------------------------
| reward                  | 0.91      |
| reward_contact          | 0         |
| reward_motion           | 0.91      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1301      |
|    time_elapsed         | 1737      |
|    total_timesteps      | 26020     |
| train/                  |           |
|    approx_kl            | 3.2599907 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.436     |
|    n_updates            | 26000     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.364     |
|    value_loss           | 4.15      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1302      |
|    time_elapsed         | 1738      |
|    total_timesteps      | 26040     |
| train/                  |           |
|    approx_kl            | 3.6230354 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.4       |
|    learning_rate        | 0.0003    |
|    loss                 | 5.38      |
|    n_updates            | 26020     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.364     |
|    value_loss           | 12.5      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1303     |
|    time_elapsed         | 1739     |
|    total_timesteps      | 26060    |
| train/                  |          |
|    approx_kl            | 1.107766 |
|    clip_fraction        | 0.615    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.5      |
|    learning_rate        | 0.0003   |
|    loss                 | 4.02     |
|    n_updates            | 26040    |
|    policy_gradient_loss | -0.249   |
|    std                  | 0.364    |
|    value_loss           | 14.2     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1304      |
|    time_elapsed         | 1741      |
|    total_timesteps      | 26080     |
| train/                  |           |
|    approx_kl            | 3.1326172 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.66      |
|    n_updates            | 26060     |
|    policy_gradient_loss | -0.281    |
|    std                  | 0.364     |
|    value_loss           | 11.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1305     |
|    time_elapsed         | 1742     |
|    total_timesteps      | 26100    |
| train/                  |          |
|    approx_kl            | 2.223744 |
|    clip_fraction        | 0.57     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.865    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.364    |
|    n_updates            | 26080    |
|    policy_gradient_loss | -0.205   |
|    std                  | 0.364    |
|    value_loss           | 2.35     |
--------------------------------------
---------------------------------------
| reward                  | 0.9       |
| reward_contact          | 0         |
| reward_motion           | 0.9       |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.4      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1306      |
|    time_elapsed         | 1743      |
|    total_timesteps      | 26120     |
| train/                  |           |
|    approx_kl            | 0.5570587 |
|    clip_fraction        | 0.548     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.042    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.31      |
|    n_updates            | 26100     |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.364     |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1307      |
|    time_elapsed         | 1745      |
|    total_timesteps      | 26140     |
| train/                  |           |
|    approx_kl            | 1.8557132 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.189     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.88      |
|    n_updates            | 26120     |
|    policy_gradient_loss | -0.245    |
|    std                  | 0.365     |
|    value_loss           | 18.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1308      |
|    time_elapsed         | 1746      |
|    total_timesteps      | 26160     |
| train/                  |           |
|    approx_kl            | 1.4942986 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.394     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.38      |
|    n_updates            | 26140     |
|    policy_gradient_loss | -0.247    |
|    std                  | 0.365     |
|    value_loss           | 12.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1309      |
|    time_elapsed         | 1747      |
|    total_timesteps      | 26180     |
| train/                  |           |
|    approx_kl            | 1.3177277 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -0.276    |
|    learning_rate        | 0.0003    |
|    loss                 | 5.73      |
|    n_updates            | 26160     |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.365     |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1310      |
|    time_elapsed         | 1749      |
|    total_timesteps      | 26200     |
| train/                  |           |
|    approx_kl            | 2.2942984 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.717     |
|    n_updates            | 26180     |
|    policy_gradient_loss | -0.312    |
|    std                  | 0.364     |
|    value_loss           | 4.27      |
---------------------------------------
---------------------------------------
| reward                  | 0.92      |
| reward_contact          | 0         |
| reward_motion           | 0.92      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1311      |
|    time_elapsed         | 1750      |
|    total_timesteps      | 26220     |
| train/                  |           |
|    approx_kl            | 1.2994217 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.494     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.53      |
|    n_updates            | 26200     |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.364     |
|    value_loss           | 10        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1312      |
|    time_elapsed         | 1751      |
|    total_timesteps      | 26240     |
| train/                  |           |
|    approx_kl            | 2.8163536 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.208    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.03      |
|    n_updates            | 26220     |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.364     |
|    value_loss           | 12.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1313      |
|    time_elapsed         | 1753      |
|    total_timesteps      | 26260     |
| train/                  |           |
|    approx_kl            | 1.9790115 |
|    clip_fraction        | 0.608     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.173     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.15      |
|    n_updates            | 26240     |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.364     |
|    value_loss           | 12.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.6     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1314     |
|    time_elapsed         | 1754     |
|    total_timesteps      | 26280    |
| train/                  |          |
|    approx_kl            | 1.613018 |
|    clip_fraction        | 0.737    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.703    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.31     |
|    n_updates            | 26260    |
|    policy_gradient_loss | -0.27    |
|    std                  | 0.364    |
|    value_loss           | 5.16     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.6       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1315       |
|    time_elapsed         | 1755       |
|    total_timesteps      | 26300      |
| train/                  |            |
|    approx_kl            | 0.52757543 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.4        |
|    entropy_loss         | -134       |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.59       |
|    n_updates            | 26280      |
|    policy_gradient_loss | -0.244     |
|    std                  | 0.364      |
|    value_loss           | 19.3       |
----------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1316      |
|    time_elapsed         | 1757      |
|    total_timesteps      | 26320     |
| train/                  |           |
|    approx_kl            | 1.4386573 |
|    clip_fraction        | 0.78      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.657     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 26300     |
|    policy_gradient_loss | -0.25     |
|    std                  | 0.364     |
|    value_loss           | 7.12      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1317      |
|    time_elapsed         | 1758      |
|    total_timesteps      | 26340     |
| train/                  |           |
|    approx_kl            | 1.8173398 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.858     |
|    n_updates            | 26320     |
|    policy_gradient_loss | -0.258    |
|    std                  | 0.364     |
|    value_loss           | 4.1       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1318      |
|    time_elapsed         | 1759      |
|    total_timesteps      | 26360     |
| train/                  |           |
|    approx_kl            | 1.3156649 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -0.0237   |
|    learning_rate        | 0.0003    |
|    loss                 | 7.42      |
|    n_updates            | 26340     |
|    policy_gradient_loss | -0.263    |
|    std                  | 0.364     |
|    value_loss           | 20.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1319      |
|    time_elapsed         | 1761      |
|    total_timesteps      | 26380     |
| train/                  |           |
|    approx_kl            | 1.0054682 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.31      |
|    n_updates            | 26360     |
|    policy_gradient_loss | -0.23     |
|    std                  | 0.364     |
|    value_loss           | 7.03      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1320      |
|    time_elapsed         | 1762      |
|    total_timesteps      | 26400     |
| train/                  |           |
|    approx_kl            | 2.2351434 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.392     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.47      |
|    n_updates            | 26380     |
|    policy_gradient_loss | -0.255    |
|    std                  | 0.364     |
|    value_loss           | 9.85      |
---------------------------------------
----------------------------------------
| reward                  | 0.93       |
| reward_contact          | 0          |
| reward_motion           | 0.93       |
| reward_torque           | 0          |
| reward_velocity         | 0          |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 18.8       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 1321       |
|    time_elapsed         | 1763       |
|    total_timesteps      | 26420      |
| train/                  |            |
|    approx_kl            | 0.76800376 |
|    clip_fraction        | 0.778      |
|    clip_range           | 0.4        |
|    entropy_loss         | -134       |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.04       |
|    n_updates            | 26400      |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.364      |
|    value_loss           | 5.78       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1322      |
|    time_elapsed         | 1765      |
|    total_timesteps      | 26440     |
| train/                  |           |
|    approx_kl            | 22.076008 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.629     |
|    n_updates            | 26420     |
|    policy_gradient_loss | -0.244    |
|    std                  | 0.364     |
|    value_loss           | 3.96      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1323      |
|    time_elapsed         | 1766      |
|    total_timesteps      | 26460     |
| train/                  |           |
|    approx_kl            | 1.3690166 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.73      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.67      |
|    n_updates            | 26440     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.364     |
|    value_loss           | 4.75      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1324     |
|    time_elapsed         | 1767     |
|    total_timesteps      | 26480    |
| train/                  |          |
|    approx_kl            | 3.32315  |
|    clip_fraction        | 0.8      |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.532    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.21     |
|    n_updates            | 26460    |
|    policy_gradient_loss | -0.222   |
|    std                  | 0.364    |
|    value_loss           | 9.25     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1325      |
|    time_elapsed         | 1769      |
|    total_timesteps      | 26500     |
| train/                  |           |
|    approx_kl            | 1.4064149 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.64      |
|    n_updates            | 26480     |
|    policy_gradient_loss | -0.231    |
|    std                  | 0.364     |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| reward                  | 0.93     |
| reward_contact          | 0        |
| reward_motion           | 0.93     |
| reward_torque           | 0        |
| reward_velocity         | 0        |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1326     |
|    time_elapsed         | 1770     |
|    total_timesteps      | 26520    |
| train/                  |          |
|    approx_kl            | 2.505146 |
|    clip_fraction        | 0.78     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.519    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.91     |
|    n_updates            | 26500    |
|    policy_gradient_loss | -0.248   |
|    std                  | 0.364    |
|    value_loss           | 10.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1327      |
|    time_elapsed         | 1771      |
|    total_timesteps      | 26540     |
| train/                  |           |
|    approx_kl            | 2.7612133 |
|    clip_fraction        | 0.69      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.3       |
|    n_updates            | 26520     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.365     |
|    value_loss           | 7.05      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1328      |
|    time_elapsed         | 1773      |
|    total_timesteps      | 26560     |
| train/                  |           |
|    approx_kl            | 1.3686594 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.11      |
|    n_updates            | 26540     |
|    policy_gradient_loss | -0.274    |
|    std                  | 0.365     |
|    value_loss           | 10.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 18.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 1329     |
|    time_elapsed         | 1774     |
|    total_timesteps      | 26580    |
| train/                  |          |
|    approx_kl            | 2.232985 |
|    clip_fraction        | 0.76     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.557    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.98     |
|    n_updates            | 26560    |
|    policy_gradient_loss | -0.249   |
|    std                  | 0.365    |
|    value_loss           | 8.11     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1330      |
|    time_elapsed         | 1775      |
|    total_timesteps      | 26600     |
| train/                  |           |
|    approx_kl            | 1.2663081 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.636     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.88      |
|    n_updates            | 26580     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.365     |
|    value_loss           | 7.06      |
---------------------------------------
---------------------------------------
| reward                  | 0.94      |
| reward_contact          | 0         |
| reward_motion           | 0.94      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1331      |
|    time_elapsed         | 1777      |
|    total_timesteps      | 26620     |
| train/                  |           |
|    approx_kl            | 0.4358905 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.9       |
|    n_updates            | 26600     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.364     |
|    value_loss           | 6.38      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1332      |
|    time_elapsed         | 1778      |
|    total_timesteps      | 26640     |
| train/                  |           |
|    approx_kl            | 1.0175323 |
|    clip_fraction        | 0.635     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.895     |
|    n_updates            | 26620     |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.365     |
|    value_loss           | 3.43      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1333      |
|    time_elapsed         | 1779      |
|    total_timesteps      | 26660     |
| train/                  |           |
|    approx_kl            | 1.1855496 |
|    clip_fraction        | 0.668     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.666     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.51      |
|    n_updates            | 26640     |
|    policy_gradient_loss | -0.236    |
|    std                  | 0.365     |
|    value_loss           | 8.39      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1334      |
|    time_elapsed         | 1781      |
|    total_timesteps      | 26680     |
| train/                  |           |
|    approx_kl            | 1.1602293 |
|    clip_fraction        | 0.68      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.186     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.18      |
|    n_updates            | 26660     |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.365     |
|    value_loss           | 13.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1335      |
|    time_elapsed         | 1782      |
|    total_timesteps      | 26700     |
| train/                  |           |
|    approx_kl            | 2.8278008 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.982     |
|    n_updates            | 26680     |
|    policy_gradient_loss | -0.237    |
|    std                  | 0.364     |
|    value_loss           | 3.74      |
---------------------------------------
---------------------------------------
| reward                  | 0.93      |
| reward_contact          | 0         |
| reward_motion           | 0.93      |
| reward_torque           | 0         |
| reward_velocity         | 0         |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1336      |
|    time_elapsed         | 1783      |
|    total_timesteps      | 26720     |
| train/                  |           |
|    approx_kl            | 2.5641985 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.562     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.62      |
|    n_updates            | 26700     |
|    policy_gradient_loss | -0.267    |
|    std                  | 0.364     |
|    value_loss           | 9.34      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 19        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1337      |
|    time_elapsed         | 1785      |
|    total_timesteps      | 26740     |
| train/                  |           |
|    approx_kl            | 2.2660017 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.228     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.34      |
|    n_updates            | 26720     |
|    policy_gradient_loss | -0.238    |
|    std                  | 0.364     |
|    value_loss           | 11.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 19        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 1338      |
|    time_elapsed         | 1786      |
|    total_timesteps      | 26760     |
| train/                  |           |
|    approx_kl            | 1.2458416 |
|    clip_fraction        | 0.62      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.413     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.85      |
|    n_updates            | 26740     |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.365     |
|    value_loss           | 9.3       |
---------------------------------------
Traceback (most recent call last):
  File "ddpg.py", line 224, in <module>
    'info_keywords' : info_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 59, in make
    cls = load(self.entry_point)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 18, in load
    mod = importlib.import_module(mod_name)
  File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 1140
    info['reward_torque'] = np.exp(-np.linalg.norm(self.sim.data.actuator_force / 100)) self.w[1]
                                                                                           ^
SyntaxError: invalid syntax
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_35
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 29.7     |
| time/              |          |
|    fps             | 15       |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 20       |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 62.1      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 2         |
|    time_elapsed         | 2         |
|    total_timesteps      | 40        |
| train/                  |           |
|    approx_kl            | 4.9265785 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.00061   |
|    learning_rate        | 0.0003    |
|    loss                 | 12.5      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.312    |
|    std                  | 0.368     |
|    value_loss           | 46        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 77.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 3         |
|    time_elapsed         | 4         |
|    total_timesteps      | 60        |
| train/                  |           |
|    approx_kl            | 1.0446776 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.000985 |
|    learning_rate        | 0.0003    |
|    loss                 | 296       |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.368     |
|    value_loss           | 656       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 88.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 4          |
|    time_elapsed         | 5          |
|    total_timesteps      | 80         |
| train/                  |            |
|    approx_kl            | 0.65626526 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.00878    |
|    learning_rate        | 0.0003     |
|    loss                 | 501        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.252     |
|    std                  | 0.368      |
|    value_loss           | 1.07e+03   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 75.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 5         |
|    time_elapsed         | 6         |
|    total_timesteps      | 100       |
| train/                  |           |
|    approx_kl            | 1.4528958 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.000974  |
|    learning_rate        | 0.0003    |
|    loss                 | 572       |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.281    |
|    std                  | 0.368     |
|    value_loss           | 1.19e+03  |
---------------------------------------
---------------------------------------
| reward                  | 2.16      |
| reward_contact          | -0.84     |
| reward_motion           | 0.4       |
| reward_torque           | 2.03      |
| reward_velocity         | 0.566     |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 84.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 6         |
|    time_elapsed         | 8         |
|    total_timesteps      | 120       |
| train/                  |           |
|    approx_kl            | 5.2625117 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.000639 |
|    learning_rate        | 0.0003    |
|    loss                 | 108       |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.326    |
|    std                  | 0.368     |
|    value_loss           | 222       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 92.7       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 7          |
|    time_elapsed         | 9          |
|    total_timesteps      | 140        |
| train/                  |            |
|    approx_kl            | 0.48889598 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | -0.00314   |
|    learning_rate        | 0.0003     |
|    loss                 | 664        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.232     |
|    std                  | 0.368      |
|    value_loss           | 1.36e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 94.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 8          |
|    time_elapsed         | 10         |
|    total_timesteps      | 160        |
| train/                  |            |
|    approx_kl            | 0.43805447 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | -0.000665  |
|    learning_rate        | 0.0003     |
|    loss                 | 759        |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.207     |
|    std                  | 0.368      |
|    value_loss           | 1.59e+03   |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 91.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 9        |
|    time_elapsed         | 12       |
|    total_timesteps      | 180      |
| train/                  |          |
|    approx_kl            | 0.872674 |
|    clip_fraction        | 0.68     |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | 0.0023   |
|    learning_rate        | 0.0003   |
|    loss                 | 380      |
|    n_updates            | 160      |
|    policy_gradient_loss | -0.269   |
|    std                  | 0.368    |
|    value_loss           | 809      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 92.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 10        |
|    time_elapsed         | 13        |
|    total_timesteps      | 200       |
| train/                  |           |
|    approx_kl            | 0.7562052 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.000153 |
|    learning_rate        | 0.0003    |
|    loss                 | 141       |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.253    |
|    std                  | 0.368     |
|    value_loss           | 307       |
---------------------------------------
---------------------------------------
| reward                  | 3.4       |
| reward_contact          | -1.01     |
| reward_motion           | 0.5       |
| reward_torque           | 2.32      |
| reward_velocity         | 1.58      |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 93.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 11        |
|    time_elapsed         | 14        |
|    total_timesteps      | 220       |
| train/                  |           |
|    approx_kl            | 0.9339054 |
|    clip_fraction        | 0.63      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.001    |
|    learning_rate        | 0.0003    |
|    loss                 | 459       |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.256    |
|    std                  | 0.368     |
|    value_loss           | 967       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 92.3      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 12        |
|    time_elapsed         | 16        |
|    total_timesteps      | 240       |
| train/                  |           |
|    approx_kl            | 1.0113218 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.000582 |
|    learning_rate        | 0.0003    |
|    loss                 | 330       |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.26     |
|    std                  | 0.368     |
|    value_loss           | 708       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 92.8      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 13        |
|    time_elapsed         | 17        |
|    total_timesteps      | 260       |
| train/                  |           |
|    approx_kl            | 1.7422501 |
|    clip_fraction        | 0.708     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.00115   |
|    learning_rate        | 0.0003    |
|    loss                 | 182       |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.368     |
|    value_loss           | 383       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 95.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 14        |
|    time_elapsed         | 18        |
|    total_timesteps      | 280       |
| train/                  |           |
|    approx_kl            | 2.1838472 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.000118  |
|    learning_rate        | 0.0003    |
|    loss                 | 277       |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.305    |
|    std                  | 0.368     |
|    value_loss           | 581       |
---------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_36
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 29.7     |
| time/              |          |
|    fps             | 15       |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 20       |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 33.6      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 2         |
|    time_elapsed         | 2         |
|    total_timesteps      | 40        |
| train/                  |           |
|    approx_kl            | 2.5684686 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.000634 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.1      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.275    |
|    std                  | 0.368     |
|    value_loss           | 56        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.5      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 3         |
|    time_elapsed         | 3         |
|    total_timesteps      | 60        |
| train/                  |           |
|    approx_kl            | 2.1065748 |
|    clip_fraction        | 0.7       |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.0228    |
|    learning_rate        | 0.0003    |
|    loss                 | 25.2      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.287    |
|    std                  | 0.368     |
|    value_loss           | 68        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 4         |
|    time_elapsed         | 5         |
|    total_timesteps      | 80        |
| train/                  |           |
|    approx_kl            | 4.2143884 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.0109   |
|    learning_rate        | 0.0003    |
|    loss                 | 15        |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.282    |
|    std                  | 0.368     |
|    value_loss           | 37.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 5         |
|    time_elapsed         | 6         |
|    total_timesteps      | 100       |
| train/                  |           |
|    approx_kl            | 5.3246074 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.00535  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.307    |
|    std                  | 0.368     |
|    value_loss           | 37.1      |
---------------------------------------
--------------------------------------
| reward                  | 1.75     |
| reward_contact          | -0.342   |
| reward_motion           | 0.8      |
| reward_torque           | 0.707    |
| reward_velocity         | 0.583    |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 31.3     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 6        |
|    time_elapsed         | 8        |
|    total_timesteps      | 120      |
| train/                  |          |
|    approx_kl            | 3.91613  |
|    clip_fraction        | 0.81     |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | 0.00413  |
|    learning_rate        | 0.0003   |
|    loss                 | 27.9     |
|    n_updates            | 100      |
|    policy_gradient_loss | -0.299   |
|    std                  | 0.368    |
|    value_loss           | 62.9     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 7         |
|    time_elapsed         | 9         |
|    total_timesteps      | 140       |
| train/                  |           |
|    approx_kl            | 18.777054 |
|    clip_fraction        | 0.888     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | -0.00919  |
|    learning_rate        | 0.0003    |
|    loss                 | 27        |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.319    |
|    std                  | 0.368     |
|    value_loss           | 55.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 8         |
|    time_elapsed         | 10        |
|    total_timesteps      | 160       |
| train/                  |           |
|    approx_kl            | 1.5713154 |
|    clip_fraction        | 0.76      |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.00641   |
|    learning_rate        | 0.0003    |
|    loss                 | 23.3      |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.273    |
|    std                  | 0.368     |
|    value_loss           | 50.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 9         |
|    time_elapsed         | 12        |
|    total_timesteps      | 180       |
| train/                  |           |
|    approx_kl            | 12.427821 |
|    clip_fraction        | 0.88      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.00376  |
|    learning_rate        | 0.0003    |
|    loss                 | 43.9      |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.316    |
|    std                  | 0.368     |
|    value_loss           | 89.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 10        |
|    time_elapsed         | 13        |
|    total_timesteps      | 200       |
| train/                  |           |
|    approx_kl            | 1.6950871 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.00874   |
|    learning_rate        | 0.0003    |
|    loss                 | 33.2      |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.27     |
|    std                  | 0.368     |
|    value_loss           | 69.7      |
---------------------------------------
----------------------------------------
| reward                  | 1.36       |
| reward_contact          | -0.369     |
| reward_motion           | 0.4        |
| reward_torque           | 0.741      |
| reward_velocity         | 0.585      |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 32.1       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 11         |
|    time_elapsed         | 14         |
|    total_timesteps      | 220        |
| train/                  |            |
|    approx_kl            | 0.95493454 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.4        |
|    entropy_loss         | -114       |
|    explained_variance   | 0.00285    |
|    learning_rate        | 0.0003     |
|    loss                 | 29.5       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.261     |
|    std                  | 0.368      |
|    value_loss           | 64         |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32        |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 12        |
|    time_elapsed         | 16        |
|    total_timesteps      | 240       |
| train/                  |           |
|    approx_kl            | 3.8792832 |
|    clip_fraction        | 0.87      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.00691  |
|    learning_rate        | 0.0003    |
|    loss                 | 32.9      |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.309    |
|    std                  | 0.368     |
|    value_loss           | 67.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 31.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 13       |
|    time_elapsed         | 17       |
|    total_timesteps      | 260      |
| train/                  |          |
|    approx_kl            | 4.043077 |
|    clip_fraction        | 0.73     |
|    clip_range           | 0.4      |
|    entropy_loss         | -114     |
|    explained_variance   | 0.00317  |
|    learning_rate        | 0.0003   |
|    loss                 | 36.3     |
|    n_updates            | 240      |
|    policy_gradient_loss | -0.288   |
|    std                  | 0.368    |
|    value_loss           | 74.1     |
--------------------------------------
  File "ddpg.py", line 274
    tensorboard_log = log_dir,
                  ^
SyntaxError: invalid syntax
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_37
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 38.7     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 20       |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 37.7      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 2         |
|    time_elapsed         | 2         |
|    total_timesteps      | 40        |
| train/                  |           |
|    approx_kl            | 0.9422886 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.00133   |
|    learning_rate        | 0.0003    |
|    loss                 | 29.6      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.31     |
|    std                  | 0.368     |
|    value_loss           | 95.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.2      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 3         |
|    time_elapsed         | 3         |
|    total_timesteps      | 60        |
| train/                  |           |
|    approx_kl            | 1.2975533 |
|    clip_fraction        | 0.75      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.0003    |
|    loss                 | 26.6      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.284    |
|    std                  | 0.368     |
|    value_loss           | 72.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.1      |
| time/                   |           |
|    fps                  | 15        |
|    iterations           | 4         |
|    time_elapsed         | 5         |
|    total_timesteps      | 80        |
| train/                  |           |
|    approx_kl            | 2.0503507 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.0096    |
|    learning_rate        | 0.0003    |
|    loss                 | 14        |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.296    |
|    std                  | 0.368     |
|    value_loss           | 34.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 5         |
|    time_elapsed         | 6         |
|    total_timesteps      | 100       |
| train/                  |           |
|    approx_kl            | 2.7332587 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -0.0102   |
|    learning_rate        | 0.0003    |
|    loss                 | 14.1      |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.283    |
|    std                  | 0.368     |
|    value_loss           | 32.2      |
---------------------------------------
---------------------------------------
| reward                  | 1.38      |
| reward_contact          | -0.281    |
| reward_motion           | 0.2       |
| reward_torque           | 0.821     |
| reward_velocity         | 0.635     |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 31.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 6         |
|    time_elapsed         | 8         |
|    total_timesteps      | 120       |
| train/                  |           |
|    approx_kl            | 3.3540883 |
|    clip_fraction        | 0.733     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.00513   |
|    learning_rate        | 0.0003    |
|    loss                 | 24        |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.299    |
|    std                  | 0.368     |
|    value_loss           | 50.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.1      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 7         |
|    time_elapsed         | 9         |
|    total_timesteps      | 140       |
| train/                  |           |
|    approx_kl            | 2.8056002 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.0003    |
|    loss                 | 22.7      |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.296    |
|    std                  | 0.368     |
|    value_loss           | 50.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 32.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 8         |
|    time_elapsed         | 10        |
|    total_timesteps      | 160       |
| train/                  |           |
|    approx_kl            | 2.8089736 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.0003    |
|    loss                 | 26.3      |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.295    |
|    std                  | 0.368     |
|    value_loss           | 57.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 33.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 9        |
|    time_elapsed         | 12       |
|    total_timesteps      | 180      |
| train/                  |          |
|    approx_kl            | 4.90538  |
|    clip_fraction        | 0.808    |
|    clip_range           | 0.4      |
|    entropy_loss         | -113     |
|    explained_variance   | -0.00876 |
|    learning_rate        | 0.0003   |
|    loss                 | 30.9     |
|    n_updates            | 160      |
|    policy_gradient_loss | -0.279   |
|    std                  | 0.368    |
|    value_loss           | 65.6     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 33.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 10       |
|    time_elapsed         | 13       |
|    total_timesteps      | 200      |
| train/                  |          |
|    approx_kl            | 3.588205 |
|    clip_fraction        | 0.85     |
|    clip_range           | 0.4      |
|    entropy_loss         | -114     |
|    explained_variance   | 0.0909   |
|    learning_rate        | 0.0003   |
|    loss                 | 34.9     |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.295   |
|    std                  | 0.368    |
|    value_loss           | 77.1     |
--------------------------------------
--------------------------------------
| reward                  | 1.63     |
| reward_contact          | -0.28    |
| reward_motion           | 0.5      |
| reward_torque           | 0.803    |
| reward_velocity         | 0.613    |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 33.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 11       |
|    time_elapsed         | 14       |
|    total_timesteps      | 220      |
| train/                  |          |
|    approx_kl            | 6.230551 |
|    clip_fraction        | 0.76     |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.237    |
|    learning_rate        | 0.0003   |
|    loss                 | 16.1     |
|    n_updates            | 200      |
|    policy_gradient_loss | -0.277   |
|    std                  | 0.368    |
|    value_loss           | 42.7     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 33.7     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 12       |
|    time_elapsed         | 16       |
|    total_timesteps      | 240      |
| train/                  |          |
|    approx_kl            | 2.593928 |
|    clip_fraction        | 0.658    |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | -0.0723  |
|    learning_rate        | 0.0003   |
|    loss                 | 52.4     |
|    n_updates            | 220      |
|    policy_gradient_loss | -0.285   |
|    std                  | 0.368    |
|    value_loss           | 108      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 33.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 13        |
|    time_elapsed         | 17        |
|    total_timesteps      | 260       |
| train/                  |           |
|    approx_kl            | 2.1176398 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -0.145    |
|    learning_rate        | 0.0003    |
|    loss                 | 40.7      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.304    |
|    std                  | 0.368     |
|    value_loss           | 86.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 34.2     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 14       |
|    time_elapsed         | 19       |
|    total_timesteps      | 280      |
| train/                  |          |
|    approx_kl            | 5.627847 |
|    clip_fraction        | 0.89     |
|    clip_range           | 0.4      |
|    entropy_loss         | -117     |
|    explained_variance   | -0.0106  |
|    learning_rate        | 0.0003   |
|    loss                 | 47.2     |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.309   |
|    std                  | 0.368    |
|    value_loss           | 95.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.2      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 15        |
|    time_elapsed         | 20        |
|    total_timesteps      | 300       |
| train/                  |           |
|    approx_kl            | 1.7306929 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | -0.000201 |
|    learning_rate        | 0.0003    |
|    loss                 | 45.4      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.289    |
|    std                  | 0.368     |
|    value_loss           | 92.5      |
---------------------------------------
--------------------------------------
| reward                  | 1.62     |
| reward_contact          | -0.289   |
| reward_motion           | 0.533    |
| reward_torque           | 0.782    |
| reward_velocity         | 0.593    |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 34.4     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 16       |
|    time_elapsed         | 21       |
|    total_timesteps      | 320      |
| train/                  |          |
|    approx_kl            | 4.634357 |
|    clip_fraction        | 0.83     |
|    clip_range           | 0.4      |
|    entropy_loss         | -119     |
|    explained_variance   | -0.00241 |
|    learning_rate        | 0.0003   |
|    loss                 | 43.8     |
|    n_updates            | 300      |
|    policy_gradient_loss | -0.301   |
|    std                  | 0.368    |
|    value_loss           | 89       |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 34.9     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 17       |
|    time_elapsed         | 23       |
|    total_timesteps      | 340      |
| train/                  |          |
|    approx_kl            | 3.879546 |
|    clip_fraction        | 0.83     |
|    clip_range           | 0.4      |
|    entropy_loss         | -120     |
|    explained_variance   | 0.0733   |
|    learning_rate        | 0.0003   |
|    loss                 | 39.6     |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.297   |
|    std                  | 0.368    |
|    value_loss           | 87.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 18        |
|    time_elapsed         | 24        |
|    total_timesteps      | 360       |
| train/                  |           |
|    approx_kl            | 2.3585708 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -0.00368  |
|    learning_rate        | 0.0003    |
|    loss                 | 49.4      |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.276    |
|    std                  | 0.368     |
|    value_loss           | 100       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 34.5       |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 19         |
|    time_elapsed         | 25         |
|    total_timesteps      | 380        |
| train/                  |            |
|    approx_kl            | 0.72327566 |
|    clip_fraction        | 0.7        |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.116      |
|    learning_rate        | 0.0003     |
|    loss                 | 31         |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.245     |
|    std                  | 0.368      |
|    value_loss           | 74.3       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.6      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 20        |
|    time_elapsed         | 27        |
|    total_timesteps      | 400       |
| train/                  |           |
|    approx_kl            | 1.122465  |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -0.000542 |
|    learning_rate        | 0.0003    |
|    loss                 | 43        |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.268    |
|    std                  | 0.368     |
|    value_loss           | 90.3      |
---------------------------------------
--------------------------------------
| reward                  | 1.65     |
| reward_contact          | -0.281   |
| reward_motion           | 0.55     |
| reward_torque           | 0.802    |
| reward_velocity         | 0.58     |
| rollout/                |          |
|    ep_len_mean          | 20       |
|    ep_rew_mean          | 34.8     |
| time/                   |          |
|    fps                  | 14       |
|    iterations           | 21       |
|    time_elapsed         | 28       |
|    total_timesteps      | 420      |
| train/                  |          |
|    approx_kl            | 2.123312 |
|    clip_fraction        | 0.83     |
|    clip_range           | 0.4      |
|    entropy_loss         | -120     |
|    explained_variance   | -0.086   |
|    learning_rate        | 0.0003   |
|    loss                 | 40.2     |
|    n_updates            | 400      |
|    policy_gradient_loss | -0.293   |
|    std                  | 0.368    |
|    value_loss           | 83.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 22        |
|    time_elapsed         | 29        |
|    total_timesteps      | 440       |
| train/                  |           |
|    approx_kl            | 2.9749937 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.159     |
|    learning_rate        | 0.0003    |
|    loss                 | 23.2      |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.3      |
|    std                  | 0.368     |
|    value_loss           | 69.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 23        |
|    time_elapsed         | 31        |
|    total_timesteps      | 460       |
| train/                  |           |
|    approx_kl            | 10.479847 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.0204    |
|    learning_rate        | 0.0003    |
|    loss                 | 28.5      |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.3      |
|    std                  | 0.368     |
|    value_loss           | 78.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.9      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 24        |
|    time_elapsed         | 32        |
|    total_timesteps      | 480       |
| train/                  |           |
|    approx_kl            | 1.0195122 |
|    clip_fraction        | 0.72      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.6      |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.272    |
|    std                  | 0.368     |
|    value_loss           | 40.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.5      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 25        |
|    time_elapsed         | 33        |
|    total_timesteps      | 500       |
| train/                  |           |
|    approx_kl            | 1.8622681 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | -0.24     |
|    learning_rate        | 0.0003    |
|    loss                 | 50.7      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.3      |
|    std                  | 0.368     |
|    value_loss           | 111       |
---------------------------------------
---------------------------------------
| reward                  | 1.64      |
| reward_contact          | -0.306    |
| reward_motion           | 0.56      |
| reward_torque           | 0.805     |
| reward_velocity         | 0.576     |
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 34.7      |
| time/                   |           |
|    fps                  | 14        |
|    iterations           | 26        |
|    time_elapsed         | 35        |
|    total_timesteps      | 520       |
| train/                  |           |
|    approx_kl            | 0.8173423 |
|    clip_fraction        | 0.615     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -0.0567   |
|    learning_rate        | 0.0003    |
|    loss                 | 33.9      |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.257    |
|    std                  | 0.368     |
|    value_loss           | 78.4      |
---------------------------------------
